{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db21facc-a71f-43bc-badf-89a19cd026c1",
   "metadata": {},
   "source": [
    "# how to extract all document text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68213e7d-6cc4-4d56-8562-803fb4d0049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf, sys, pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2318c286-84ee-4277-9e2f-ebea5ba79c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = sys.argv[1] # get document filename\n",
    "with pymupdf.open(filename) as file:  # open document\n",
    "    text = chr(12).join([page.get_text() for page in file])\n",
    "# write as binary file to support non-ASCII characters\n",
    "pathlib.Path(filename + \".txt\").write_bytes(text.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b3c6118-d8a1-4fc5-97c7-2c89256fef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Document(filename):\n",
    "    with pymupdf.open(filename) as file:  # open document\n",
    "        text = chr(12).join([page.get_text() for page in file])\n",
    "    # write as binary file to support non-ASCII characters\n",
    "    new_file = pathlib.Path(filename + \".txt\").write_bytes(text.encode())\n",
    "\n",
    "    return new_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8190955e-49be-410a-a3f1-56bbda1dd9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "748008"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_name = '/home/ngoni97/Documents/Python Programming/Natural Language Processing/Representation Learning for Natural Language Processing.pdf'\n",
    "Get_Document(path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62112bbf-a8ac-4fc9-9aa4-95b847d6d4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('/home/ngoni97/Documents/Python Programming/Natural Language Processing/Representation Learning for Natural Language Processing.pdf')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = pymupdf.open(path_name)\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cbf49eb-bf8e-46f7-9c71-d9206095e46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 23 of /home/ngoni97/Documents/Python Programming/Natural Language Processing/Representation Learning for Natural Language Processing.pdf\n"
     ]
    }
   ],
   "source": [
    "page = file.load_page(23)\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25268f62-5b36-418c-a3be-d4e50f9b2869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document(filename):\n",
    "    import os, pathlib, pymupdf\n",
    "    Text = \"\"\n",
    "    text_file_path = pathlib.Path(filename.strip('.pdf') + '.txt')\n",
    "    with pymupdf.open(filename) as file: #open a document\n",
    "        for page in file: # iterate the document pages\n",
    "            text = page.get_text()\n",
    "            Text += text\n",
    "    with open(text_file_path, 'wb') as text_file: # create a text output\n",
    "        text_file.write(Text.encode('utf8')) # get plain text (is in UTF-8), write text of page\n",
    "        text_file.write(bytes((12,))) # write page delimiter (from feed 0x0C)\n",
    "    with open(text_file_path, 'rb') as file:\n",
    "        File = file.read()\n",
    "        return File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73cc05f2-a482-425f-bbf4-d5268f8ad1a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Representation \\nLearning for \\nNatural Language \\nProcessing \\nZhiyuan Liu \\xc2\\xb7 Yankai Lin \\xc2\\xb7 Maosong Sun\\nRepresentation Learning for Natural Language\\nProcessing\\nZhiyuan Liu\\n\\xe2\\x80\\xa2 Yankai Lin\\n\\xe2\\x80\\xa2 Maosong Sun\\nRepresentation Learning\\nfor Natural Language\\nProcessing\\n123\\nZhiyuan Liu\\nTsinghua University\\nBeijing, China\\nMaosong Sun\\nDepartment of Computer\\nScience and Technology\\nTsinghua University\\nBeijing, China\\nYankai Lin\\nPattern Recognition Center\\nTencent Wechat\\nBeijing, China\\nISBN 978-981-15-5572-5\\nISBN 978-981-15-5573-2\\n(eBook)\\nhttps://doi.org/10.1007/978-981-15-5573-2\\n\\xc2\\xa9 The Editor(s) (if applicable) and The Author(s) 2020, corrected publication 2023. This book is an open\\naccess publication.\\nOpen Access This book is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adap-\\ntation, distribution and reproduction in any medium or format, as long as you give appropriate credit to\\nthe original author(s) and the source, provide a link to the Creative Commons license and indicate if\\nchanges were made.\\nThe images or other third party material in this book are included in the book\\xe2\\x80\\x99s Creative Commons\\nlicense, unless indicated otherwise in a credit line to the material. If material is not included in the book\\xe2\\x80\\x99s\\nCreative Commons license and your intended use is not permitted by statutory regulation or exceeds the\\npermitted use, you will need to obtain permission directly from the copyright holder.\\nThe use of general descriptive names, registered names, trademarks, service marks, etc. in this publi-\\ncation does not imply, even in the absence of a speci\\xef\\xac\\x81c statement, that such names are exempt from the\\nrelevant protective laws and regulations and therefore free for general use.\\nThe publisher, the authors and the editors are safe to assume that the advice and information in this\\nbook are believed to be true and accurate at the date of publication. Neither the publisher nor the\\nauthors or the editors give a warranty, express or implied, with respect to the material contained herein or\\nfor any errors or omissions that may have been made. The publisher remains neutral with regard to\\njurisdictional claims in published maps and institutional af\\xef\\xac\\x81liations.\\nThis Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd.\\nThe registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721,\\nSingapore\\nPreface\\nIn traditional Natural Language Processing (NLP) systems, language entries such as\\nwords and phrases are taken as distinct symbols. Various classic ideas and methods,\\nsuch as n-gram and bag-of-words models, were proposed and have been widely\\nused until now in many industrial applications. All these methods take words as the\\nminimum units for semantic representation, which are either used to further esti-\\nmate the conditional probabilities of next words given previous words (e.g., n-\\ngram) or used to represent semantic meanings of text (e.g., bag-of-words models).\\nEven when people \\xef\\xac\\x81nd it is necessary to model word meanings, they either man-\\nually build some linguistic knowledge bases such as WordNet or use context words\\nto represent the meaning of a target word (i.e., distributional representation). All\\nthese semantic representation methods are still based on symbols!\\nWith the development of NLP techniques for years, it is realized that many\\nissues in NLP are caused by the symbol-based semantic representation. First, the\\nsymbol-based representation always suffers from the data sparsity problem. Take\\nstatistical NLP methods such as n-gram with large-scale corpora, for example, due\\nto the intrinsic power-law distribution of words, the performance will decay dra-\\nmatically for those few-shot words, even many smoothing methods have been\\ndeveloped to calibrate the estimated probabilities about them. Moreover, there are\\nmultiple-grained entries in natural languages from words, phrases, sentences to\\ndocuments, it is dif\\xef\\xac\\x81cult to \\xef\\xac\\x81nd a uni\\xef\\xac\\x81ed symbol set to represent the semantic\\nmeanings for all of them simultaneously. Meanwhile, in many NLP tasks, it is\\nrequired to measure semantic relatedness between these language entries at different\\nlevels.\\nFor\\nexample,\\nwe\\nhave\\nto\\nmeasure\\nsemantic\\nrelatedness\\nbetween\\nwords/phrases and documents in Information Retrieval. Due to the absence of a\\nuni\\xef\\xac\\x81ed scheme for semantic representation, there used to be distinct approaches\\nproposed and explored for different tasks in NLP, and it sometimes makes NLP\\ndoes not look like a compatible community.\\nAs an alternative approach to symbol-based representation, distributed repre-\\nsentation was originally proposed by Geoffrey E. Hinton in a technique report in\\n1984. The report was then included in the well-known two-volume book Parallel\\nDistributed Processing (PDP) that introduced neural networks to model human\\nv\\ncognition and intelligence. According to this report, distributed representation is\\ninspired by the neural computation scheme of humans and other animals, and the\\nessential idea is as follows:\\nEach entity is represented by a pattern of activity distributed over many computing ele-\\nments, and each computing element is involved in representing many different entities.\\nIt means that each entity is represented by multiple neurons, and each neuron\\ninvolves in the representation of many concepts. This also indicates the meaning of\\ndistributed in distributed representation. As opposed to distributed representation,\\npeople used to assume one neuron only represents a speci\\xef\\xac\\x81c concept or object, e.g.,\\nthere exists a single neuron that will only be activated when recognizing a person or\\nobject, such as his/her grandmother, well known as the grandmother-cell hypothesis\\nor local representation. We can see the straightforward connection between the\\ngrandmother-cell hypothesis and symbol-based representation.\\nIt was about 20 years after distributed representation was proposed, neural\\nprobabilistic language model was proposed to model natural languages by Yoshua\\nBengio in 2003, in which words are represented as low-dimensional and real-valued\\nvectors based on the idea of distributed representation. However, it was until 2013\\nthat a simpler and more ef\\xef\\xac\\x81cient framework word2vec was proposed to learn word\\ndistributed representations from large-scale corpora, we come to the popularity of\\ndistributed representation and neural network techniques in NLP. The performance\\nof almost all NLP tasks has been signi\\xef\\xac\\x81cantly improved with the support of the\\ndistributed representation scheme and the deep learning methods.\\nThis book aims to review and present the recent advances of distributed repre-\\nsentation learning for NLP, including why representation learning can improve\\nNLP, how representation learning takes part in various important topics of NLP,\\nand what challenges are still not well addressed by distributed representation.\\nBook Organization\\nThis book is organized into 11 chapters with 3 parts. The \\xef\\xac\\x81rst part of the book depicts\\nkey components in NLP and how representation learning works for them. In this\\npart, Chap. 1 \\xef\\xac\\x81rst introduces the basics of representation learning and why it is\\nimportant for NLP. Then we give a comprehensive review of representation learning\\ntechniques on multiple-grained entries in NLP, including word representation\\n(Chap. 2), phrase representation as known as compositional semantics (Chap. 3),\\nsentence representation (Chap. 4), and document representation (Chap. 5).\\nThe second part presents representation learning for those components closely\\nrelated to NLP. These components include sememe knowledge that describes the\\ncommonsense knowledge of words as human concepts, world knowledge (also\\nknown as knowledge graphs) that organizes relational facts between entities in the\\nreal world, various network data such as social networks, document networks, and\\nvi\\nPreface\\ncross-modal data that connects natural languages to other modalities such as visual\\ndata. A deep understanding of natural languages requires these complex compo-\\nnents as a rich context. Therefore, we provide an extensive introduction to these\\ncomponents, i.e., sememe knowledge representation (Chap. 6), world knowledge\\nrepresentation (Chap. 7), network representation (Chap. 8), and cross-modal rep-\\nresentation (Chap. 9).\\nIn the third part, we will further provide some widely used open resource\\ntools on representation learning techniques (Chap. 10) and \\xef\\xac\\x81nally outlook the\\nremaining challenges and future research directions of representation learning for\\nNLP (Chap. 11).\\nAlthough the book is about representation learning for NLP, those theories and\\nalgorithms can be also applied in other related domains, such as machine learning,\\nsocial network analysis, semantic web, information retrieval, data mining, and\\ncomputational biology.\\nNote that, some parts of this book are based on our previous published or\\npre-printed papers, including [1, 11] in Chap. 2, [32] in Chap. 3, [10, 5, 29] in\\nChap. 4, [12, 7] in Chap. 5, [17, 14, 24, 30, 6, 16, 2, 15] in Chap. 6, [9, 8, 13, 21,\\n22, 23, 3, 4, 31] in Chap. 7, and [25, 19, 18, 20, 26, 27, 33, 28, 34] in Chap. 8.\\nBook Cover\\nThe book cover shows an oracle bone divided into three parts, corresponding to\\nthree revolutionized stages of cognition and representation in human history.\\nThe left part shows oracle scripts, the earliest known form of Chinese writing\\ncharacters used on oracle bones in the late 1200 BC. It is used to represent the\\nemergence of human languages, especially writing systems. We consider this as the\\n\\xef\\xac\\x81rst representation revolution for human beings about the world.\\nThe upper right part shows the digitalized representation of information and\\nsignals. After the invention of electronic computers in the 1940s, big data can be\\nef\\xef\\xac\\x81ciently represented and processed in computer programs. This can be regarded\\nas the second representation revolution for human beings about the world.\\nThe bottom right part shows the distributed representation in arti\\xef\\xac\\x81cial neural\\nnetworks originally proposed in the 1980s. As the representation basis of deep\\nlearning, it has extensively revolutionized many \\xef\\xac\\x81elds in arti\\xef\\xac\\x81cial intelligence,\\nincluding natural language processing, computer vision, and speech recognition\\never since the 2010s. We consider this as the third representation revolution about\\nthe world. This book focuses on the theory, methods, and applications of distributed\\nrepresentation learning in natural language processing.\\nPreface\\nvii\\nPrerequisites\\nThis book is designed for advanced undergraduate and graduate students, post-\\ndoctoral fellows, researchers, lecturers, and industrial engineers, as well as anyone\\ninterested in representation learning and NLP. We expect the readers to have some\\nprior knowledge in Probability, Linear Algebra, and Machine Learning. We rec-\\nommend the readers who are speci\\xef\\xac\\x81cally interested in NLP to read the \\xef\\xac\\x81rst part\\n(Chaps. 1\\xe2\\x80\\x935) which should be read sequentially. The second and third parts can be\\nread in selected order according to readers\\xe2\\x80\\x99 interests.\\nContact Information\\nWe welcome any feedback, corrections, and suggestions on the book, which may\\nbe sent to liuzy@tsinghua.edu.cn. The readers can also \\xef\\xac\\x81nd updates about the book\\nfrom the personal homepage http://nlp.csai.tsinghua.edu.cn/*lzy/.\\nBeijing, China\\nZhiyuan Liu\\nYankai Lin\\nMarch 2020\\nMaosong Sun\\nReferences\\n1. Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huanbo Luan. Joint learning of\\ncharacter and word embeddings. In Proceedings of IJCAI, 2015.\\n2. Yihong Gu, Jun Yan, Hao Zhu, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Fen Lin, and Leyu\\nLin. Language modeling with sparse product of sememe experts. In Proceedings of EMNLP,\\npages 4642\\xe2\\x80\\x934651, 2018.\\n3. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge\\nfor knowledge graph completion. arXiv preprint arXiv:1611.04125, 2016.\\n4. Xu Han, Zhiyuan Liu, and Maosong Sun. Neural knowledge acquisition via mutual attention\\nbetween knowledge graph and text. In Proceedings of AAAI, pages 4832\\xe2\\x80\\x934839, 2018.\\n5. Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun.\\nFewRel: A large-scale supervised few-shot relation classi\\xef\\xac\\x81cation dataset with state-of-the-art\\nevaluation. In Proceedings of EMNLP, 2018.\\n6. Huiming Jin, Hao Zhu, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Fen Lin, and Leyu Lin.\\nIncorporating chinese characters of words for lexical sememe prediction. In Proceedings of\\nACL, 2018.\\n7. Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. Denoising distantly supervised\\nopen-domain question answering. In Proceedings of ACL, 2018.\\n8. Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, and Song Liu. Modeling\\nrelation paths for representation learning of knowledge bases. In Proceedings of EMNLP,\\n2015.\\nviii\\nPreface\\n9. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and\\nrelation embeddings for knowledge graph completion. In Proceedings of AAAI, 2015.\\n10. Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Neural relation\\nextraction with selective attention over instances. In Proceedings of ACL, 2016.\\n11. Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Topical word embeddings. In\\nProceedings of AAAI, 2015.\\n12. Zhenghao Liu, Chenyan Xiong, Maosong Sun, and Zhiyuan Liu. Entity-duet neural ranking:\\nUnderstanding the role of knowledge graph semantics in neural information retrieval. In\\nProceedings of ACL, 2018.\\n13. Zhiyuan Liu, Maosong Sun, Yankai Lin, and Ruobing Xie. Knowledge representation\\nlearning: a review. JCRD, 53(2):247\\xe2\\x80\\x93261, 2016.\\n14. Yilin Niu, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Improved word representation\\nlearning with sememes. In Proceedings of ACL, 2017.\\n15. Fanchao Qi, Junjie Huang, Chenghao Yang, Zhiyuan Liu, Xiao Chen, Qun Liu, and Maosong\\nSun. Modeling semantic compositionality with sememe knowledge. In Proceedings of ACL,\\n2019.\\n16. Fanchao Qi, Yankai Lin, Maosong Sun, Hao Zhu, Ruobing Xie, and Zhiyuan Liu.\\nCross-lingual lexical sememe prediction. In Proceedings of EMNLP, 2018.\\n17. Maosong Sun and Xinxiong Chen. Embedding for words and word senses based on human\\nannotated knowledge base: A case study on hownet. Journal of Chinese Information\\nProcessing, 30:1\\xe2\\x80\\x936, 2016.\\n18. Cunchao Tu, Hao Wang, Xiangkai Zeng, Zhiyuan Liu, and Maosong Sun. Community-\\nenhanced\\nnetwork\\nrepresentation\\nlearning\\nfor\\nnetwork\\nanalysis.\\narXiv\\npreprint\\narXiv:1611.06645, 2016.\\n19. Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, and Maosong Sun. Max-margin deepwalk:\\nDiscriminative learning of network representation. In Proceedings of IJCAI, 2016.\\n20. Cunchao Tu, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Transnet: translation-based\\nnetwork representation learning for social relation extraction. In Proceedings of IJCAI, 2017.\\n21. Ruobing\\nXie,\\nZhiyuan\\nLiu,\\nTat-seng\\nChua,\\nHuanbo\\nLuan,\\nand\\nMaosong\\nSun.\\nImage-embodied knowledge representation learning. In Proceedings of IJCAI, 2016.\\n22. Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. Representation learning\\nof knowledge graphs with entity descriptions. In Proceedings of AAAI, 2016.\\n23. Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Representation learning of knowledge graphs\\nwith hierarchical types. In Proceedings of IJCAI, 2016.\\n24. Ruobing Xie, Xingchi Yuan, Zhiyuan Liu, and Maosong Sun. Lexical sememe prediction via\\nword embeddings and matrix factorization. In Proceedings of IJCAI, 2017.\\n25. Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network rep-\\nresentation learning with rich text information. In Proceedings of IJCAI, 2015.\\n26. Cheng Yang, Maosong Sun, Zhiyuan Liu, and Cunchao Tu. Fast network embedding\\nenhancement via high order proximity approximation. In Proceedings of IJCAI, 2017.\\n27. Cheng Yang, Maosong Sun, Wayne Xin Zhao, Zhiyuan Liu, and Edward Y Chang. A neural\\nnetwork approach to jointly modeling social networks and mobile trajectories. ACM\\nTransactions on Information Systems (TOIS), 35(4):36, 2017.\\n28. Cheng Yang, Jian Tang, Maosong Sun, Ganqu Cui, and Liu Zhiyuan. Multi-scale information\\ndiffusion prediction with reinforced recurrent networks. In Proceedings of IJCAI, 2019.\\n29. Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin\\nHuang, Jie Zhou, and Maosong Sun. DocRED: A large-scale document-level relation\\nextraction dataset. In Proceedings of ACL, 2019.\\n30. Xiangkai Zeng, Cheng Yang, Cunchao Tu, Zhiyuan Liu, and Maosong Sun. Chinese liwc\\nlexicon expansion via hierarchical classi\\xef\\xac\\x81cation of word embeddings with sememe attention.\\nIn Proceedings of AAAI, 2018.\\n31. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie:\\nEnhanced language representation with informative entities. In Proceedings of ACL, 2019.\\nPreface\\nix\\n32. Yu Zhao, Zhiyuan Liu, and Maosong Sun. Phrase type sensitive tensor indexing model for\\nsemantic composition. In Proceedings of AAAI, 2015.\\n33. Jie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong\\nSun. GEAR: Graph-based evidence aggregating and reasoning for fact veri\\xef\\xac\\x81cation. In\\nProceedings of ACL 2019, 2019.\\nThe original version of the Book Frontmatter has been revised with the addition of\\nreference citations in the Preface. The corrected version of Frontmatter can be found\\nat https://doi.org/10.1007/978-981-15-5573-2_12.\\nx\\nPreface\\nAcknowledgements\\nThe authors are very grateful to the contributions of our students and research\\ncollaborators, who have prepared initial drafts of some chapters or have given us\\ncomments, suggestions, and corrections. We list main contributors for preparing\\ninitial drafts of each chapter as follows,\\n\\xe2\\x80\\xa2 Chapter 1: Tianyu Gao, Zhiyuan Liu.\\n\\xe2\\x80\\xa2 Chapter 2: Lei Xu, Yankai Lin.\\n\\xe2\\x80\\xa2 Chapter 3: Yankai Lin, Yang Liu.\\n\\xe2\\x80\\xa2 Chapter 4: Yankai Lin, Zhengyan Zhang, Cunchao Tu, Hongyin Luo.\\n\\xe2\\x80\\xa2 Chapter 5: Jiawei Wu, Yankai Lin, Zhenghao Liu, Haozhe Ji.\\n\\xe2\\x80\\xa2 Chapter 6: Fanchao Qi, Chenghao Yang.\\n\\xe2\\x80\\xa2 Chapter 7: Ruobing Xie, Xu Han.\\n\\xe2\\x80\\xa2 Chapter 8: Cheng Yang, Jie Zhou, Zhengyan Zhang.\\n\\xe2\\x80\\xa2 Chapter 9: Ji Xin, Yuan Yao, Deming Ye, Hao Zhu.\\n\\xe2\\x80\\xa2 Chapter 10: Xu Han, Zhengyan Zhang, Cheng Yang.\\n\\xe2\\x80\\xa2 Chapter 11: Cheng Yang, Zhiyuan Liu.\\nFor the whole book, we thank Chaojun Xiao and Zhengyan Zhang for drawing\\nmodel \\xef\\xac\\x81gures, thank Chaojun Xiao for unifying the styles of \\xef\\xac\\x81gures and tables in\\nthe book, thank Shengding Hu for making the notation table and unifying the\\nnotations across chapters, thank Jingcheng Yuzhi and Chaojun Xiao for organizing\\nthe format of reference, thank Jingcheng Yuzhi, Jiaju Du, Haozhe Ji, Sicong\\nOuyang, and Ayana for the \\xef\\xac\\x81rst-round proofreading, and thank Weize Chen, Ganqu\\nCui, Bowen Dong, Tianyu Gao, Xu Han, Zhenghao Liu, Fanchao Qi, Guangxuan\\nXiao, Cheng Yang, Yuan Yao, Shi Yu, Yuan Zang, Zhengyan Zhang, Haoxi Zhong\\nand Jie Zhou for the second-round proofreading. We also thank Cuncun Zhao for\\ndesigning the book cover.\\nIn this book, there is a speci\\xef\\xac\\x81c chapter talking about sememe knowledge rep-\\nresentation. Many works in this chapter are carried out by our research group. These\\nworks have received great encouragement from the inventor of HowNet,\\nMr. Zhendong Dong, who died at 82 on February 28, 2019. HowNet is the great\\nxi\\nlinguistic and commonsense knowledge base composed by Mr. Dong for about 30\\nyears. At the end of his life, he and his son Mr. Qiang Dong decided to collaborate\\nwith us and released the open-source version of HowNet, OpenHowNet. As a\\npioneer of machine translation in China, Mr. Zhendong Dong devoted his whole\\nlife to natural language processing. He will be missed by all of us forever.\\nWe thank our colleagues and friends, Yang Liu and Juanzi Li at Tsinghua\\nUniversity, and Peng Li at Tencent Wechat, who offered close and frequent dis-\\ncussions which substantially improved this book. We also want to express our\\nspecial thanks to Prof. Bo Zhang. His insights to deep learning and representation\\nlearning, and sincere encouragements to our research of representation learning on\\nNLP, have greatly stimulated us to move forward with more con\\xef\\xac\\x81dence and\\npassion.\\nWe proposed the plan of this book in 2015 after discussing it with the Springer\\nSenior Editor, Dr. Celine Lanlan Chang. As the \\xef\\xac\\x81rst of the time of preparing a\\ntechnical book, we were not expecting it took so long to \\xef\\xac\\x81nish this book. We thank\\nCeline for providing insightful comments and incredible patience to the preparation\\nof this book. We are also grateful to Springer\\xe2\\x80\\x99s Assistant Editor, Jane Li, for\\noffering invaluable help during manuscript preparation.\\nFinally, we give our appreciations to our organizations, Department of Computer\\nScience and Technology at Tsinghua University, Institute for Arti\\xef\\xac\\x81cial Intelligence\\nat Tsinghua University, Beijing Academy of Arti\\xef\\xac\\x81cial Intelligence (BAAI), Chinese\\nInformation Processing Society of China, and Tencent Wechat, who have provided\\noutstanding environment, supports, and facilities for preparing this book.\\nThis book is supported by the Natural Science Foundation of China (NSFC) and\\nthe German Research Foundation (DFG) in Project Crossmodal Learning, NSFC\\n61621136008/DFG TRR-169.\\nxii\\nAcknowledgements\\nContents\\n1\\nRepresentation Learning and NLP . . . . . . . . . . . . . . . . . . . . . . . . .\\n1\\n1.1\\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n1\\n1.2\\nWhy Representation Learning Is Important for NLP . . . . . . . . .\\n3\\n1.3\\nBasic Ideas of Representation Learning . . . . . . . . . . . . . . . . . .\\n3\\n1.4\\nDevelopment of Representation Learning for NLP . . . . . . . . . .\\n4\\n1.5\\nLearning Approaches to Representation Learning for NLP . . . .\\n7\\n1.6\\nApplications of Representation Learning for NLP . . . . . . . . . . .\\n8\\n1.7\\nThe Organization of This Book . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n2\\nWord Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n2.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n2.2\\nOne-Hot Word Representation . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n2.3\\nDistributed Word Representation . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n2.3.1\\nBrown Cluster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n15\\n2.3.2\\nLatent Semantic Analysis . . . . . . . . . . . . . . . . . . . . . .\\n17\\n2.3.3\\nWord2vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n2.3.4\\nGloVe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n2.4\\nContextualized Word Representation . . . . . . . . . . . . . . . . . . . .\\n22\\n2.5\\nExtensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n2.5.1\\nWord Representation Theories. . . . . . . . . . . . . . . . . . .\\n24\\n2.5.2\\nMulti-prototype Word Representation . . . . . . . . . . . . .\\n25\\n2.5.3\\nMultisource Word Representation . . . . . . . . . . . . . . . .\\n26\\n2.5.4\\nMultilingual Word Representation . . . . . . . . . . . . . . . .\\n30\\n2.5.5\\nTask-Speci\\xef\\xac\\x81c Word Representation . . . . . . . . . . . . . . .\\n32\\n2.5.6\\nTime-Speci\\xef\\xac\\x81c Word Representation . . . . . . . . . . . . . . .\\n33\\n2.6\\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n33\\n2.6.1\\nWord Similarity/Relatedness . . . . . . . . . . . . . . . . . . . .\\n34\\n2.6.2\\nWord Analogy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n36\\nxiii\\n2.7\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n36\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n3\\nCompositional Semantics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n3.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n3.2\\nSemantic Space. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n3.2.1\\nVector Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n3.2.2\\nMatrix-Vector Space . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n3.3\\nBinary Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n3.3.1\\nAdditive Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n3.3.2\\nMultiplicative Model . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n3.4\\nN-Ary Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n51\\n3.4.1\\nRecurrent Neural Network . . . . . . . . . . . . . . . . . . . . .\\n52\\n3.4.2\\nRecursive Neural Network . . . . . . . . . . . . . . . . . . . . .\\n53\\n3.4.3\\nConvolutional Neural Network . . . . . . . . . . . . . . . . . .\\n55\\n3.5\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n4\\nSentence Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n59\\n4.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n59\\n4.2\\nOne-Hot Sentence Representation . . . . . . . . . . . . . . . . . . . . . .\\n60\\n4.3\\nProbabilistic Language Model . . . . . . . . . . . . . . . . . . . . . . . . .\\n61\\n4.4\\nNeural Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n4.4.1\\nFeedforward Neural Network Language Model . . . . . .\\n62\\n4.4.2\\nConvolutional Neural Network Language Model . . . . .\\n63\\n4.4.3\\nRecurrent Neural Network Language Model . . . . . . . .\\n63\\n4.4.4\\nTransformer Language Model . . . . . . . . . . . . . . . . . . .\\n64\\n4.4.5\\nExtensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n4.5\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n72\\n4.5.1\\nText Classi\\xef\\xac\\x81cation . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n73\\n4.5.2\\nRelation Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n74\\n4.6\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n84\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n85\\n5\\nRETRACTED CHAPTER: Document Representation . . . . . . . . . .\\n91\\n5.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n5.2\\nOne-Hot Document Representation . . . . . . . . . . . . . . . . . . . . .\\n92\\n5.3\\nTopic Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n93\\n5.3.1\\nLatent Dirichlet Allocation . . . . . . . . . . . . . . . . . . . . .\\n94\\n5.3.2\\nExtensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97\\n5.4\\nDistributed Document Representation . . . . . . . . . . . . . . . . . . . .\\n100\\n5.4.1\\nParagraph Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n100\\n5.4.2\\nNeural Document Representation. . . . . . . . . . . . . . . . .\\n102\\nxiv\\nContents\\n5.5\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n107\\n5.5.1\\nNeural Information Retrieval . . . . . . . . . . . . . . . . . . . .\\n107\\n5.5.2\\nQuestion Answering . . . . . . . . . . . . . . . . . . . . . . . . . .\\n110\\n5.6\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120\\n6\\nSememe Knowledge Representation . . . . . . . . . . . . . . . . . . . . . . . .\\n125\\n6.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n125\\n6.1.1\\nLinguistic Knowledge Graphs . . . . . . . . . . . . . . . . . . .\\n126\\n6.2\\nSememe Knowledge Representation . . . . . . . . . . . . . . . . . . . . .\\n128\\n6.2.1\\nSimple Sememe Aggregation Model . . . . . . . . . . . . . .\\n129\\n6.2.2\\nSememe Attention over Context Model . . . . . . . . . . . .\\n129\\n6.2.3\\nSememe Attention over Target Model . . . . . . . . . . . . .\\n131\\n6.3\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n132\\n6.3.1\\nSememe-Guided Word Representation . . . . . . . . . . . . .\\n132\\n6.3.2\\nSememe-Guided Semantic Compositionality\\nModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n134\\n6.3.3\\nSememe-Guided Language Modeling . . . . . . . . . . . . .\\n139\\n6.3.4\\nSememe Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n142\\n6.3.5\\nOther Sememe-Guided Applications . . . . . . . . . . . . . .\\n153\\n6.4\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n157\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n158\\n7\\nWorld Knowledge Representation . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163\\n7.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163\\n7.1.1\\nWorld Knowledge Graphs. . . . . . . . . . . . . . . . . . . . . .\\n164\\n7.2\\nKnowledge Graph Representation . . . . . . . . . . . . . . . . . . . . . .\\n166\\n7.2.1\\nNotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n7.2.2\\nTransE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n7.2.3\\nExtensions of TransE . . . . . . . . . . . . . . . . . . . . . . . . .\\n172\\n7.2.4\\nOther Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n7.3\\nMultisource Knowledge Graph Representation . . . . . . . . . . . . .\\n189\\n7.3.1\\nKnowledge Graph Representation with Texts . . . . . . . .\\n190\\n7.3.2\\nKnowledge Graph Representation with Types . . . . . . .\\n192\\n7.3.3\\nKnowledge Graph Representation with Images. . . . . . .\\n194\\n7.3.4\\nKnowledge Graph Representation with Logic Rules . . .\\n195\\n7.4\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n196\\n7.4.1\\nKnowledge Graph Completion . . . . . . . . . . . . . . . . . .\\n197\\n7.4.2\\nKnowledge-Guided Entity Typing . . . . . . . . . . . . . . . .\\n199\\n7.4.3\\nKnowledge-Guided Information Retrieval . . . . . . . . . .\\n201\\n7.4.4\\nKnowledge-Guided Language Models . . . . . . . . . . . . .\\n205\\n7.4.5\\nOther Knowledge-Guided Applications . . . . . . . . . . . .\\n208\\n7.5\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n210\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n211\\nContents\\nxv\\n8\\nNetwork Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n217\\n8.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n217\\n8.2\\nNetwork Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n219\\n8.2.1\\nSpectral Clustering Based Methods . . . . . . . . . . . . . . .\\n219\\n8.2.2\\nDeepWalk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n223\\n8.2.3\\nMatrix Factorization Based Methods . . . . . . . . . . . . . .\\n230\\n8.2.4\\nStructural Deep Network Methods . . . . . . . . . . . . . . . .\\n232\\n8.2.5\\nExtensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n234\\n8.2.6\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n247\\n8.3\\nGraph Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n252\\n8.3.1\\nMotivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n253\\n8.3.2\\nGraph Convolutional Networks . . . . . . . . . . . . . . . . . .\\n254\\n8.3.3\\nGraph Attention Networks . . . . . . . . . . . . . . . . . . . . .\\n259\\n8.3.4\\nGraph Recurrent Networks . . . . . . . . . . . . . . . . . . . . .\\n260\\n8.3.5\\nExtensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n262\\n8.3.6\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n266\\n8.4\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n275\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n277\\n9\\nCross-Modal Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n285\\n9.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n285\\n9.2\\nCross-Modal Representation . . . . . . . . . . . . . . . . . . . . . . . . . .\\n286\\n9.2.1\\nVisual Word2vec . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n286\\n9.2.2\\nCross-Modal Representation for Zero-Shot\\nRecognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n288\\n9.2.3\\nCross-Modal Representation for Cross-Media\\nRetrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n292\\n9.3\\nImage Captioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n294\\n9.3.1\\nRetrieval Models for Image Captioning . . . . . . . . . . . .\\n294\\n9.3.2\\nGeneration Models for Image Captioning. . . . . . . . . . .\\n295\\n9.3.3\\nNeural Models for Image Captioning . . . . . . . . . . . . . .\\n296\\n9.4\\nVisual Relationship Detection . . . . . . . . . . . . . . . . . . . . . . . . .\\n301\\n9.4.1\\nVisual Relationship Detection with Language Priors . . .\\n301\\n9.4.2\\nVisual Translation Embedding Network . . . . . . . . . . . .\\n303\\n9.4.3\\nScene Graph Generation . . . . . . . . . . . . . . . . . . . . . . .\\n303\\n9.5\\nVisual Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n307\\n9.5.1\\nVQA and VQA Datasets. . . . . . . . . . . . . . . . . . . . . . .\\n307\\n9.5.2\\nVQA Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n308\\n9.6\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n311\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n314\\nxvi\\nContents\\n10\\nResources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n319\\n10.1\\nOpen-Source Frameworks for Deep Learning . . . . . . . . . . . . . .\\n319\\n10.1.1\\nCaffe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n319\\n10.1.2\\nTheano . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n320\\n10.1.3\\nTensorFlow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n321\\n10.1.4\\nTorch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n321\\n10.1.5\\nPyTorch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n322\\n10.1.6\\nKeras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n323\\n10.1.7\\nMXNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n323\\n10.2\\nOpen Resources for Word Representation . . . . . . . . . . . . . . . .\\n324\\n10.2.1\\nWord2Vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n324\\n10.2.2\\nGloVe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n324\\n10.3\\nOpen Resources for Knowledge Graph Representation . . . . . . .\\n325\\n10.3.1\\nOpenKE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n325\\n10.3.2\\nScikit-Kge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n326\\n10.4\\nOpen Resources for Network Representation . . . . . . . . . . . . . .\\n326\\n10.4.1\\nOpenNE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n326\\n10.4.2\\nGEM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n326\\n10.4.3\\nGraphVite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n327\\n10.4.4\\nCogDL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n327\\n10.5\\nOpen Resources for Relation Extraction . . . . . . . . . . . . . . . . . .\\n327\\n10.5.1\\nOpenNRE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n327\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n328\\n11\\nOutlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n329\\n11.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n329\\n11.2\\nUsing More Unsupervised Data . . . . . . . . . . . . . . . . . . . . . . . .\\n330\\n11.3\\nUtilizing Fewer Labeled Data . . . . . . . . . . . . . . . . . . . . . . . . .\\n330\\n11.4\\nEmploying Deeper Neural Architectures . . . . . . . . . . . . . . . . . .\\n331\\n11.5\\nImproving Model Interpretability . . . . . . . . . . . . . . . . . . . . . . .\\n332\\n11.6\\nFusing the Advances from Other Areas . . . . . . . . . . . . . . . . . .\\n333\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n333\\nCorrection to: Representation Learning for Natural Language\\nProcessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\nC1\\nContents\\nxvii\\nAcronyms\\nACNN\\nAnisotropic Convolutional Neural Network\\nAI\\nArti\\xef\\xac\\x81cial Intelligence\\nAUC\\nArea Under the Receiver Operating Characteristic Curve\\nBERT\\nBidirectional Encoder Representations from Transformers\\nBFS\\nBreadth-First Search\\nBiDAF\\nBi-Directional Attention Flow\\nBRNN\\nBidirectional Recurrent Neural Network\\nCBOW\\nContinuous Bag-of-Words\\nccDCLM\\nContext-to-Context Document-Context Language Model\\nCIDEr\\nConsensus-based Image Description Evaluation\\nCLN\\nColumn Network\\nCLSP\\nCross-Lingual Lexical Sememe Prediction\\nCNN\\nConvolutional Neural Network\\nCNRL\\nCommunity-enhanced Network Representation Learning\\nCOCO-QA\\nCommon Objects in COntext Question Answering\\nConSE\\nConvex Combination of Semantic Embeddings\\nConv-KNRM\\nConvolutional Kernel-based Neural Ranking Model\\nCSP\\nCharacter-enhanced Sememe Prediction\\nCWE\\nCharacter-based Word Embeddings\\nDCNN\\nDiffusion-Convolutional Neural Network\\nDeViSE\\nDeep Visual-Semantic Embedding Model\\nDFS\\nDepth-First Search\\nDGCN\\nDual Graph Convolutional Network\\nDGE\\nDirected Graph Embedding\\nDKRL\\nDescription-embodied Knowledge Graph Representation Learning\\nDRMM\\nDeep Relevance Matching Model\\nDSSM\\nDeep Structured Semantic Model\\nECC\\nEdge-Conditioned Convolution\\nERNIE\\nEnhanced Language Representation Model with Informative\\nEntities\\nxix\\nFM-IQA\\nFreestyle Multilingual Image Question Answering\\nGAAN\\nGated Attention Network\\nGAT\\nGraph Attention Networks\\nGCN\\nGraph Convolutional Network\\nGCNN\\nGeodesic Convolutional Neural Network\\nGEAR\\nGraph-based Evidence Aggregating and Reasoning\\nGENQA\\nGenerative Question Answering Model\\nGGNN\\nGated Graph Neural Network\\nGloVe\\nGlobal Vectors for Word Representation\\nGNN\\nGraph Neural Networks\\nGRN\\nGraph Recurrent Network\\nGRU\\nGated Recurrent Unit\\nHAN\\nHeterogeneous Graph Attention Network\\nHMM\\nHidden Markov Model\\nHOPE\\nHigh-Order Proximity preserved Embeddings\\nIDF\\nInverse Document Frequency\\nIE\\nInformation Extraction\\nIKRL\\nImage-embodied Knowledge Graph Representation Learning\\nIR\\nInformation Retrieval\\nKALM\\nKnowledge-Augmented Language Model\\nKB\\nKnowledge Base\\nKBC\\nKnowledge Base Completion\\nKG\\nKnowledge Graph\\nKL\\nKullback-Leibler\\nKNET\\nKnowledge-guided Attention Neural Entity Typing\\nK-NRM\\nKernel-based Neural Ranking Model\\nKR\\nKnowledge Representation\\nLBSN\\nLocation-Based Social Network\\nLDA\\nLatent Dirichlet Allocation\\nLIWC\\nLinguistic Inquiry and Word Count\\nLLE\\nLocally Linear Embedding\\nLM\\nLanguage Model\\nLSA\\nLatent Semantic Analysis\\nLSHM\\nLatent Space Heterogeneous Model\\nLSTM\\nLong Short-Term Memory\\nMAP\\nMean Average Precision\\nMETEOR\\nMetric for Evaluation of Translation with Explicit ORdering\\nMMD\\nMaximum Mean Discrepancy\\nMMDW\\nMax-Margin DeepWalk\\nM-NMF\\nModularized Nonnegative Matrix Factorization\\nmovMF\\nmixture of von Mises-Fisher distributions\\nMRF\\nMarkov Random Field\\nMSLE\\nMean-Square Log-Transformed Error\\nMST\\nMinimum Spanning Tree\\nMV-RNN\\nMatrix-Vector Recursive Neural Network\\nxx\\nAcronyms\\nNEU\\nNetwork Embedding Update\\nNKLM\\nNeural Knowledge Language Model\\nNLI\\nNatural Language Inference\\nNLP\\nNatural Language Processing\\nNRE\\nNeural Relation Extraction\\nOOKB\\nOut-of-Knowledge-Base\\nPCNN\\nPiece-wise Convolution Neural Network\\npLSI\\nProbabilistic Latent Semantic Indexing\\nPMI\\nPoint-wise Mutual Information\\nPOS\\nPart-of-Speech\\nPPMI\\nPositive Point-wise Mutual Information\\nPTE\\nPredictive Text Embedding\\nPV-DBOW\\nParagraph Vector with Distributed Bag-of-Words\\nPV-DM\\nParagraph Vector with Distributed Memory\\nQA\\nQuestion Answering\\nRBF\\nRestricted Boltzmann Machine\\nRC\\nRelation Classi\\xef\\xac\\x81cation\\nR-CNN\\nRegion-based Convolutional Neural Network\\nRDF\\nResource Description Framework\\nRE\\nRelation Extraction\\nRMSE\\nRoot Mean Squared Error\\nRNN\\nRecurrent Neural Network\\nRNTN\\nRecursive Neural Tensor Network\\nRPN\\nRegion Proposal Network\\nSAC\\nSememe Attention over Context Model\\nSAT\\nSememe Attention over Target Model\\nSC\\nSemantic Compositionality\\nSCAS\\nSemantic Compositionality with Aggregated Sememe\\nSCMSA\\nSemantic Compositionality with Mutual Sememe Attention\\nSDLM\\nSememe-Driven Language Model\\nSDNE\\nStructural Deep Network Embeddings\\nSE-WRL\\nSememe-Encoded Word Representation Learning\\nSGD\\nStochastic Gradient Descent\\nSGNS\\nSkip-gram with Negative Sampling Model\\nS-LSTM\\nSentence Long Short-Term Memory\\nSPASE\\nSememe Prediction with Aggregated Sememe Embeddings\\nSPCSE\\nSememe Prediction with Character and Sememe Embeddings\\nSPICE\\nSemantic Propositional Image Caption Evaluation\\nSPSE\\nSememe Prediction with Sememe Embeddings\\nSPWCF\\nSememe Prediction with Word-to-Character Filtering\\nSPWE\\nSememe Prediction with Word Embeddings\\nSSA\\nSimple Sememe Aggregation Model\\nSSWE\\nSentiment-Speci\\xef\\xac\\x81c Word Embeddings\\nSVD\\nSingular Value Decomposition\\nSVM\\nSupport Vector Machine\\nAcronyms\\nxxi\\nTADW\\nText-associated DeepWalk\\nTF\\nTerm Frequency\\nTF-IDF\\nTerm Frequency\\xe2\\x80\\x93Inverse Document Frequency\\nTKRL\\nType-embodied Knowledge Graph Representation Learning\\nTSP\\nTraveling Salesman Problem\\nTWE\\nTopical Word Embeddings\\nVQA\\nVisual Question Answering\\nVSM\\nVector Space Model\\nWRL\\nWord Representation Learning\\nWSD\\nWord Sense Disambiguation\\nYAGO\\nYet Another Great Ontology\\nxxii\\nAcronyms\\nSymbols and Notations\\nTokyo\\nWord example\\nConvolution operator\\n,\\nDe\\xef\\xac\\x81ned as\\n\\x01\\nElement-wise multiplication (Hadamard product)\\n)\\nInduces\\n/\\nProportional to\\nP\\nSummation operator\\nmin\\nMinimize\\nmax\\nMaximize/max pooling\\narg mink\\nThe parameter that minimizes a function\\narg maxk\\nThe parameter that maximizes a function\\nsim\\nSimilarity\\nexp\\nExponential function\\nAtt\\nAttention function\\nAvg\\nAverage function\\nF1-Score\\nF1 score\\nPMI\\nPair-wise mutual information\\nReLU\\nReLU activation function\\nSigmoid\\nSigmoid function\\nSoftmax\\nSoftmax function\\nV\\nVocabulary set\\nw;w\\nWord; word embedding vector\\nE\\nWord embedding matrix\\nR\\nRelation set\\nr;r\\nRelation; relation embedding vector\\na\\nAnswer to question\\nq\\nQuery\\nW;M;U\\nWeight matrix\\nb;d\\nbias vector\\nMi;:;M:;j\\nMatrix\\xe2\\x80\\x99s ith row; matrix\\xe2\\x80\\x99s jth column\\nxxiii\\naT;MT\\nTranspose of a vector or a matrix\\ntr\\xc3\\xb0\\x03\\xc3\\x9e\\nTrace of a matrix\\nM\\n!\\nTensor\\na; b; k\\nHyperparameters\\ng; f ; /; U; d\\nFunctions\\nNa\\nNumber of occurrences of a\\nd\\xc3\\xb0\\x03; \\x03\\xc3\\x9e\\nDistance function\\ns\\xc3\\xb0\\x03; \\x03\\xc3\\x9e\\nSimilarity function\\nP \\x03\\xc3\\xb0 \\xc3\\x9e; p \\x03\\xc3\\xb0 \\xc3\\x9e\\nProbability\\nI\\xc3\\xb0\\x03; \\x03\\xc3\\x9e\\nMutual information\\nH\\xc3\\xb0\\x03\\xc3\\x9e\\nEntropy\\nO \\x03\\xc3\\xb0 \\xc3\\x9e\\nTime complexity\\nDKL \\x03jj\\x03\\n\\xc3\\xb0\\n\\xc3\\x9e\\nKL divergence\\nL\\nLoss function\\nO\\nObjective function\\nE\\nEnergy function\\na\\nAttention score\\n/\\x04\\nOptimal value of a variable/function /\\nj \\x03 j\\nVector length; set size\\n\\x03k ka\\na -norm\\n1\\xc3\\xb0\\x03\\xc3\\x9e\\nIndicator function\\nh\\nParameter in a neural network\\nE\\nExpectation of a random variable\\n\\x03 \\xc3\\xbe ;\\x03\\x05\\nPositive sample; negative sample\\nc\\nMargin\\nl\\nMean of normal distribution\\nl\\nMean vector of Gaussian distribution\\nr\\nStandard error of normal distribution\\nR\\nCovariance matrix of Gaussian distribution\\nIn\\nn-dimensional identity matrix\\nN\\nNormal distribution\\nxxiv\\nSymbols and Notations\\nChapter 1\\nRepresentation Learning and NLP\\nAbstract Natural languages are typical unstructured information. Conventional\\nNatural Language Processing (NLP) heavily relies on feature engineering, which\\nrequires careful design and considerable expertise. Representation learning aims to\\nlearnrepresentationsofrawdataasusefulinformationforfurtherclassi\\xef\\xac\\x81cationorpre-\\ndiction. This chapter presents a brief introduction to representation learning, includ-\\ning its motivation and basic idea, and also reviews its history and recent advances in\\nboth machine learning and NLP.\\n1.1\\nMotivation\\nMachine learning addresses the problem of automatically learning computer pro-\\ngrams from data. A typical machine learning system consists of three components [5]:\\nMachine Learning = Representation + Objective + Optimization.\\n(1.1)\\nThat is, to build an effective machine learning system, we \\xef\\xac\\x81rst transform useful\\ninformation on raw data into internal representations such as feature vectors. Then by\\ndesigning appropriate objective functions, we can employ optimization algorithms\\nto \\xef\\xac\\x81nd the optimal parameter settings for the system.\\nData representation determines how much useful information can be extracted\\nfrom raw data for further classi\\xef\\xac\\x81cation or prediction. If there is more useful infor-\\nmation transformed from raw data to feature representations, the performance of\\nclassi\\xef\\xac\\x81cation or prediction will tend to be better. Hence, data representation is a\\ncrucial component to support effective machine learning.\\nConventional machine learning systems adopt careful feature engineering as\\npreprocessing to build feature representations from raw data. Feature engineering\\nneeds careful design and considerable expertise, and a speci\\xef\\xac\\x81c task usually requires\\ncustomized feature engineering algorithms, which makes feature engineering labor\\nintensive, time consuming, and in\\xef\\xac\\x82exible.\\nRepresentation learning aims to learn informative representations of objects from\\nraw data automatically. The learned representations can be further fed as input to\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_1\\n1\\n2\\n1\\nRepresentation Learning and NLP\\nmachine learning systems for prediction or classi\\xef\\xac\\x81cation. In this way, machine learn-\\ning algorithms will be more \\xef\\xac\\x82exible and desirable while handling large-scale and\\nnoisy unstructured data, such as speech, images, videos, time series, and texts.\\nDeep learning [9] is a typical approach for representation learning, which has\\nrecently achieved great success in speech recognition, computer vision, and natural\\nlanguage processing. Deep learning has two distinguishing features:\\n\\xe2\\x80\\xa2 Distributed Representation. Deep learning algorithms typically represent each\\nobject with a low-dimensional real-valued dense vector, which is named as dis-\\ntributed representation. As compared to one-hot representation in conventional\\nrepresentation schemes (such as bag-of-words models), distributed representation\\nis able to represent data in a more compact and smoothing way, as shown in Fig. 1.1,\\nand hence is more robust to address the sparsity issue in large-scale data.\\n\\xe2\\x80\\xa2 Deep Architecture. Deep learning algorithms usually learn a hierarchical deep\\narchitecture to represent objects, known as multilayer neural networks. The deep\\narchitecture is able to extract abstractive features of objects from raw data, which\\nis regarded as an important reason for the great success of deep learning for speech\\nrecognition and computer vision.\\nCurrently, the improvements caused by deep learning for NLP may still not be\\nso signi\\xef\\xac\\x81cant as compared to speech and vision. However, deep learning for NLP\\nhas been able to signi\\xef\\xac\\x81cantly reduce the work of feature engineering in NLP in the\\nmeantime of performance improvement. Hence, many researchers are devoting to\\ndeveloping ef\\xef\\xac\\x81cient algorithms on representation learning (especially deep learning)\\nfor NLP.\\nIn this chapter, we will \\xef\\xac\\x81rst discuss why representation learning is important for\\nNLP and introduce the basic ideas of representation learning. Afterward, we will\\nApple Inc.\\nSteve Jobs\\nTim Cook\\niPhone\\nCEO\\nfounder\\nproduct\\nSteve Jobs\\nTim Cook\\niPhone\\nApple Inc.\\nEntities\\nEmbeddings\\nFig. 1.1 Distributed representation of words and entities in human languages\\n1.1 Motivation\\n3\\nbrie\\xef\\xac\\x82y review the development history of representation learning for NLP, introduce\\ntypical approaches of contemporary representation learning, and summarize existing\\nand potential applications of representation learning. Finally, we will introduce the\\ngeneral organization of this book.\\n1.2\\nWhy Representation Learning Is Important for NLP\\nNLPaimstobuildlinguistic-speci\\xef\\xac\\x81cprogramsformachinestounderstandlanguages.\\nNatural language texts are typical unstructured data, with multiple granularities, mul-\\ntiple tasks, and multiple domains, which make NLP challenging to achieve satisfac-\\ntory performance.\\nMultiple Granularities. NLP concerns about multiple levels of language entries,\\nincluding but not limited to characters, words, phrases, sentences, paragraphs, and\\ndocuments. Representation learning can help to represent the semantics of these\\nlanguage entries in a uni\\xef\\xac\\x81ed semantic space, and build complex semantic relations\\namong these language entries.\\nMultiple Tasks. There are various NLP tasks based on the same input. For exam-\\nple, given a sentence, we can perform multiple tasks such as word segmentation,\\npart-of-speech tagging, named entity recognition, relation extraction, and machine\\ntranslation. In this case, it will be more ef\\xef\\xac\\x81cient and robust to build a uni\\xef\\xac\\x81ed repre-\\nsentation space of inputs for multiple tasks.\\nMultiple Domains. Natural language texts may be generated from multiple\\ndomains, including but not limited to news articles, scienti\\xef\\xac\\x81c articles, literary works,\\nand online user-generated content such as product reviews. Moreover, we can also\\nregard texts in different languages as multiple domains. Conventional NLP systems\\nhave to design speci\\xef\\xac\\x81c feature extraction algorithms for each domain according to its\\ncharacteristics. In contrast, representation learning enables us to build representations\\nautomatically from large-scale domain data.\\nIn summary, as shown in Fig. 1.2, representation learning can facilitate knowledge\\ntransfer across multiple language entries, multiple NLP tasks, and multiple appli-\\ncation domains, and signi\\xef\\xac\\x81cantly improve the effectiveness and robustness of NLP\\nperformance.\\n1.3\\nBasic Ideas of Representation Learning\\nIn this book, we focus on the distributed representation scheme (i.e., embedding),\\nand talk about recent advances of representation learning methods for multiple lan-\\nguage entries, including words, phrases, sentences, and documents, and their closely\\nrelated objects including sememe-based linguistic knowledge, entity-based world\\nknowledge, networks, and cross-modal entries.\\n4\\n1\\nRepresentation Learning and NLP\\nNLP Tasks\\nLanguage Entries\\nKnowledge\\nNetwork\\nDocument\\nSentence\\nPhrase\\nWord\\nNLP Applications\\nSemantic Analysis\\nSyntactic Analysis\\nLexical Analysis\\nFig. 1.2 Distributed representation can provide uni\\xef\\xac\\x81ed semantic space for multi-grained language\\nentries and for multiple NLP tasks\\nBy distributed representation learning, all objects that we are interested in are\\nprojected into a uni\\xef\\xac\\x81ed low-dimensional semantic space. As demonstrated in Fig. 1.1,\\nthe geometric distance between two objects in the semantic space indicates their\\nsemantic relatedness; the semantic meaning of an object is related to which objects\\nare close to it. In other words, it is the relative closeness with other objects that reveals\\nan object\\xe2\\x80\\x99s meaning rather than the absolute position.\\n1.4\\nDevelopment of Representation Learning for NLP\\nIn this section, we introduce the development of representation learning for NLP,\\nalso shown in Fig. 1.3. To study representation schemes in NLP, words would be a\\ngood start, since they are the minimum units in natural languages. The easiest way\\nto represent a word in a computer-readable way (e.g., using a vector) is one-hot\\nvector, which has the dimension of the vocabulary size and assigns 1 to the word\\xe2\\x80\\x99s\\ncorresponding position and 0 to others. It is apparent that one-hot vectors hardly\\ncontain any semantic information about words except simply distinguishing them\\nfrom each other.\\nOne of the earliest ideas of word representation learning can date back to n-gram\\nmodels [15]. It is easy to understand: when we want to predict the next word in a\\nsequence, we usually look at some previous words (and in the case of n-gram, they\\nare the previous n \\xe2\\x88\\x921 words). And if going through a large-scale corpus, we can\\ncount and get a good probability estimation of each word under the condition of all\\ncombinations of n \\xe2\\x88\\x921 previous words. These probabilities are useful for predicting\\nwords in sequences, and also form vector representations for words since they re\\xef\\xac\\x82ect\\nthe meanings of words.\\nThe idea of n-gram models is coherent with the distributional hypothesis: lin-\\nguistic items with similar distributions have similar meanings [7]. In another phrase,\\n\\xe2\\x80\\x9ca word is characterized by the company it keeps\\xe2\\x80\\x9d [6]. It became the fundamental\\nidea of many NLP models, from word2vec to BERT.\\n1.4 Development of Representation Learning for NLP\\n5\\n1948\\nN-gram Model\\n1954\\nDistributional Hypothesis\\nBag-of-words\\nDistributed Representation\\n1986\\nNeural Probabilistic\\nLanguage Model\\n2003\\n2013\\nWord2vec\\n2018\\nPre-trained Language Model\\nPredicts the next item in \\na sequence based on \\nits previous n-1 items.\\nA word is characterized by the \\ncompany it keeps.\\nRepresents a sentence or a \\ndocument as the bag of its words.\\nRepresents items by a pattern of \\nactivation distributed over elements.\\nLearns a distributed representation \\nof words for language modeling.\\nA simple and e cient distributed \\nword representation used in many \\nNLP models.\\nContextual word representation, \\npipeline, larger corpora and \\ndeeper neural architectures\\nFig. 1.3 The timeline for the development of representation learning in NLP. With the growing\\ncomputing power and large-scale text data, distributed representation trained with neural networks\\nand large corpora has become the mainstream\\nAnother example of the distributional hypothesis is Bag-Of-Words (BOW) mod-\\nels [7]. BOW models regard a document as a bag of its words, disregarding the orders\\nof these words in the document. In this way, the document can be represented as a\\nvocabulary-size vector, in which each word that has appeared in the document cor-\\nresponds to a unique and nonzero dimension. Then a score can be further computed\\nfor each word (e.g., the numbers of occurrences) to indicate the weights of these\\nwords in the document. Though very simple, BOW models work great in applica-\\ntions like spam \\xef\\xac\\x81ltering, text classi\\xef\\xac\\x81cation, and information retrieval, proving that\\nthe distributions of words can serve as a good representation for text.\\nIn the above cases, each value in the representation clearly matches one entry\\n(e.g., word scores in BOW models). This one-to-one correspondence between con-\\ncepts and representation elements is called local representation or symbol-based\\nrepresentation, which is natural and simple.\\nIn distributed representation, on the other hand, each entity (or attribute) is\\nrepresented by a pattern of activation distributed over multiple elements, and each\\ncomputing element is involved in representing multiple entities [11]. Distributed rep-\\nresentation has been proved to be more ef\\xef\\xac\\x81cient because it usually has low dimen-\\nsions that can prevent the sparsity issue. Useful hidden properties can be learned from\\nlarge-scale data and emerged in distributed representation. The idea of distributed\\nrepresentation was originally inspired by the neural computation scheme of humans\\nand other animals. It comes from neural networks (activations of neurons), and with\\nthe great success of deep learning, distributed representation has become the most\\ncommonly used approach for representation learning.\\nOne of the pioneer practices of distributed representation in NLP is Neural Prob-\\nabilistic Language Model (NPLM) [1]. A language model is to predict the joint\\nprobability of sequences of words (n-gram models are simple language models).\\nNPLM \\xef\\xac\\x81rst assigns a distributed vector for each word, then uses a neural network\\nto predict the next word. By going through the training corpora, NPLM successfully\\nlearns how to model the joint probability of sentences, while brings word embed-\\ndings (i.e., low-dimensional word vectors) as learned parameters in NPLM. Though\\n6\\n1\\nRepresentation Learning and NLP\\nWord Embedding Model\\nPre-training Objective\\nWord Embedding\\nTarget Task Model\\nTarget Task Objective\\nWord Embedding\\nPre-trained Language Model\\nPre-training Objective\\nWord Embedding\\nTarget Task Objective\\nTarget Task Model\\nWord Embedding\\nWord Embedding\\nPre-trained Language Model\\nFig. 1.4 This \\xef\\xac\\x81gure shows how word embeddings and pre-trained language models work in NLP\\npipelines. They both learn distributed representations for language entries (e.g., words) through\\npretraining objectives and transfer them to target tasks. Furthermore, pre-trained language models\\ncan also transfer model parameters\\nit is hard to tell what each element of a word embedding actually means, the vectors\\nindeed encode semantic meanings about the words, veri\\xef\\xac\\x81ed by the performance of\\nNPLM.\\nInspired by NPLM, there came many methods that embed words into distributed\\nrepresentations and use the language modeling objective to optimize them as model\\nparameters. Famous examples include word2vec [12], GloVe [13], and fastText [3].\\nThough differing in detail, these methods are all very ef\\xef\\xac\\x81cient to train, utilize large-\\nscale corpora, and have been widely adopted as word embeddings in many NLP\\nmodels. Word embeddings in the NLP pipeline map discrete words into informative\\nlow-dimensional vectors, and help to shine a light on neural networks in comput-\\ning and understanding languages. It makes representation learning a critical part of\\nnatural language processing.\\nThe research on representation learning in NLP took a big leap when ELMo\\n[14] and BERT [4] came out. Besides using larger corpora, more parameters, and\\nmore computing resources as compared to word2vec, they also take complicated\\ncontext in text into consideration. It means that instead of assigning each word\\nwith a \\xef\\xac\\x81xed vector, ELMo and BERT use multilayer neural networks to calculate\\ndynamic representations for the words based on their context, which is especially\\nuseful for the words with multiple meanings. Moreover, BERT starts a new fashion\\n(though not originated from it) of the pretrained \\xef\\xac\\x81ne-tuning pipeline. Previously,\\nword embeddings are simply adopted as input representation. But after BERT, it\\nbecomes a common practice to keep using the same neural network structure such as\\nBERT in both pretraining and \\xef\\xac\\x81ne-tuning, which is taking the parameters of BERT\\nfor initialization and \\xef\\xac\\x81ne-tuning the model on downstream tasks (Fig. 1.4).\\nThough not a big theoretical breakthrough, BERT-like models (also known as\\nPre-trained Language Models (PLM), for they are pretrained through language\\nmodeling objective on large corpora) have attracted wide attention in the NLP and\\nmachine learning community, for they have been so successful and achieved state-\\nof-the-art on almost every NLP benchmarks. These models show what large-scale\\ndata and computing power can lead to, and new research works on the topic of Pre-\\nTrained language Models (PLMs) emerge rapidly. Probing experiments demonstrate\\nthat PLMs implicitly encode a variety of linguistic knowledge and patterns inside\\n1.4 Development of Representation Learning for NLP\\n7\\ntheir multilayer network parameters [8, 10]. All these signi\\xef\\xac\\x81cant performances and\\ninteresting analyses suggest that there are still a lot of open problems to explore in\\nPLMs, as the future of representation learning for NLP.\\nBased on the distributional hypothesis, representation learning for NLP has\\nevolved from symbol-based representation to distributed representation. Starting\\nfrom word2vec, word embeddings trained from large corpora have shown signi\\xef\\xac\\x81cant\\npower in most NLP tasks. Recently, emerged PLMs (like BERT) take complicated\\ncontext into word representation and start a new trend of the pretraining \\xef\\xac\\x81ne-tuning\\npipeline, bringing NLP to a new level. What will be the next big change in repre-\\nsentation learning for NLP? We hope the contents of this book can give you some\\ninspiration.\\n1.5\\nLearning Approaches to Representation Learning for\\nNLP\\nPeople have developed various effective and ef\\xef\\xac\\x81cient approaches to learn semantic\\nrepresentations for NLP. Here we list some typical approaches.\\nStatistical Features: As introduced before, semantic representations for NLP in\\nthe early stage often come from statistics, instead of emerging from the optimization\\nprocess. For example, in n-gram or bag-of-words models, elements in the representa-\\ntion are usually frequencies or numbers of occurrences of the corresponding entries\\ncounted in large-scale corpora.\\nHand-craft Features: In certain NLP tasks, syntactic and semantic features are\\nuseful for solving the problem. For example, types of words and entities, semantic\\nroles and parse trees, etc. These linguistic features may be provided with the tasks\\nor can be extracted by speci\\xef\\xac\\x81c NLP systems. In a long period before the wide use\\nof distributed representation, researchers used to devote lots of effort into designing\\nuseful features and combining them as the inputs for NLP models.\\nSupervised Learning: Distributed representations emerge from the optimization\\nprocess of neural networks under supervised learning. In the hidden layers of neu-\\nral networks, the different activation patterns of neurons represent different entities\\nor attributes. With a training objective (usually a loss function for the target task)\\nand supervised signals (usually the gold-standard labels for training instances of the\\ntarget tasks), the networks can learn better parameters via optimization (e.g., gra-\\ndient descent). With proper training, the hidden states will become informative and\\ngeneralized as good semantic representations of natural languages.\\nFor example, to train a neural network for a sentiment classi\\xef\\xac\\x81cation task, the loss\\nfunction is usually set as the cross-entropy of the model predictions with respect to\\nthe gold-standard sentiment labels as supervision. While optimizing the objective,\\nthe loss gets smaller, and the model performance gets better. In the meantime, the\\nhidden states of the model gradually form good sentence representations by encoding\\nthe necessary information for sentiment classi\\xef\\xac\\x81cation inside the continuous hidden\\nspace.\\n8\\n1\\nRepresentation Learning and NLP\\nSelf-supervised Learning: In some cases, we simply want to get good represen-\\ntations for certain elements, so that these representations can be transferred to other\\ntasks. For example, in most neural NLP models, words in sentences are \\xef\\xac\\x81rst mapped\\nto their corresponding word embeddings (maybe from word2vec or GloVe) before\\nsent to the networks. However, there are no human-annotated \\xe2\\x80\\x9clabels\\xe2\\x80\\x9d for learning\\nword embeddings. To acquire the training objective necessary for neural networks,\\nwe need to generate \\xe2\\x80\\x9clabels\\xe2\\x80\\x9d intrinsically from existing data. This is called self-\\nsupervised learning (one way for unsupervised learning).\\nFor example, language modeling is a typical \\xe2\\x80\\x9cself-supervised\\xe2\\x80\\x9d objective, for it\\ndoes not require any human annotations. Based on the distributional hypothesis,\\nusing the language modeling objective can lead to hidden representations that encode\\nthe semantics of words. You may have heard of a famous equation: w(king) \\xe2\\x88\\x92\\nw(man) + w(woman) = w(queen), which demonstrates the analogical properties\\nthat the word embeddings have possessed through self-supervised learning.\\nWe can see another angle of self-supervised learning in autoencoders. It is also a\\nway to learn representations for a set of data. Typical autoencoders have a reduction\\n(encoding) phase and a reconstruction (decoding) phase. In the reduction phase, an\\nitem from the data is encoded into a low-dimensional representation, and in the\\nreconstruction phase, the model tries to reconstruct the item from the intermediate\\nrepresentation. Here, the training objective is the reconstruction loss, derived from\\nthe data itself. During the training process, meaningful information is encoded and\\nkept in the latent representation, while noise signals are discarded.\\nSelf-supervised learning has made a great success in NLP, for the plain text itself\\ncontains abundant knowledge and patterns about languages, and self-supervised\\nlearning can fully utilize the existing large-scale corpora. Nowadays, it is still the\\nmost exciting research area of representation learning for natural languages, and\\nresearchers continue to put their efforts into this direction.\\nBesides, many other machine learning approaches have also been explored in\\nrepresentation learning for NLP, such as adversarial training, contrastive learning,\\nfew-shot learning, meta-learning, continual learning, reinforcement learning, et al.\\nHow to develop more effective and ef\\xef\\xac\\x81cient approaches of representation learning\\nfor NLP and to better take advantage of large-scale and complicated corpora and\\ncomputing power, is still an important research topic.\\n1.6\\nApplications of Representation Learning for NLP\\nIn general, there are two kinds of applications of representation learning for NLP. In\\none case, the semantic representation is trained in a pretraining task (or designed by\\nhuman experts) and is transferred to the model for the target task. Word embedding\\nis an example of the application. It is trained by using language modeling objective\\nand is taken as inputs for other down-stream NLP models. In this book, we will\\n1.6 Applications of Representation Learning for NLP\\n9\\nalso introduce sememe knowledge representation and world knowledge representa-\\ntion, which can also be integrated into some NLP systems as additional knowledge\\naugmentation to enhance their performance in certain aspects.\\nIn other cases, the semantic representation lies within the hidden states of the neu-\\nral model and directly aims for better performance of target tasks as an end-to-end\\nfashion. For example, many NLP tasks want to semantically compose sentence or\\ndocument representation: tasks like sentiment classi\\xef\\xac\\x81cation, natural language infer-\\nence, and relation extraction require sentence representation and the tasks like ques-\\ntion answering need document representation. As shown in the latter part of the\\nbook, many representation learning methods have been developed for sentences and\\ndocuments and bene\\xef\\xac\\x81t these NLP tasks.\\n1.7\\nThe Organization of This Book\\nWe start the book from word representation. By giving a thorough introduction to\\nword representation, we hope the readers can grasp the basic ideas for representa-\\ntion learning for NLP. Based on that, we further talk about how to compositionally\\nacquire the representation for higher level language components, from sentences to\\ndocuments.\\nAs shown in Fig. 1.5, representation learning will be able to incorporate various\\ntypes of structural knowledge to support a deep understanding of natural languages,\\nnamed as knowledge-guided NLP. Hence, we next introduce two forms of knowledge\\nrepresentation that are closely related to NLP. On the one hand, sememe represen-\\ntation tries to encode linguistic and commonsense knowledge in natural languages.\\nSememe is de\\xef\\xac\\x81ned as the minimum indivisible unit of semantic meaning [2]. With\\nthe help of sememe representation learning, we can get more interpretable and more\\nrobust NLP models. On the other hand, world knowledge representation studies how\\nto encode world facts into continuous semantic space. It can not only help with\\nknowledge graph tasks but also bene\\xef\\xac\\x81t knowledge-guided NLP applications.\\nBesides, the network is also a natural way to represent objects and their relation-\\nships. In the network representation section, we study how to embed vertices and\\nedges in a network and how these elements interact with each other. Through the\\napplications, we further show how network representations can help NLP tasks.\\nAnother interesting topic related to NLP is the cross-modal representation, which\\nstudies how to model uni\\xef\\xac\\x81ed semantic representations across different modalities\\n(e.g., text, audios, images, videos, etc.). Through this section, we review several\\ncross-modal problems along with representative models.\\nAt the end of the book, we introduce some useful resources to the readers, includ-\\ning deep learning frameworks and open-source codes. We also share some views\\nabout the next big topics in representation learning for NLP. We hope that the\\nresources and the outlook can help our readers have a better understanding of the\\ncontent of the book, and inspire our readers about how representation learning in\\nNLP would further develop.\\n10\\n1\\nRepresentation Learning and NLP\\nDeep Network\\nOpen Data\\nSymbol\\nEmbedding\\nKnowledge\\nGuidance\\nKnowledge\\nExtraction\\nLearning\\nUnder-\\nstanding\\nGNN\\nKRL\\nDeep Learning\\nKnowledge Graph\\nFig. 1.5 The architecture of knowledge-guided NLP\\nReferences\\n1. Yoshua Bengio, R\\xc3\\xa9jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\\nlanguage model. Journal of Machine Learning Research, 3(Feb):1137\\xe2\\x80\\x931155, 2003.\\n2. Leonard Bloom\\xef\\xac\\x81eld. A set of postulates for the science of language. Language, 2(3):153\\xe2\\x80\\x93164,\\n1926.\\n3. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors\\nwith subword information. Transactions of the Association for Computational Linguistics,\\n5:135\\xe2\\x80\\x93146, 2017.\\n4. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019.\\n5. Pedro Domingos. A few useful things to know about machine learning. Communications of\\nthe ACM, 55(10):78\\xe2\\x80\\x9387, 2012.\\n6. John R Firth. A synopsis of linguistic theory, 1930\\xe2\\x80\\x931955. 1957.\\n7. Zellig S Harris. Distributional structure. Word, 10(2\\xe2\\x80\\x933):146\\xe2\\x80\\x93162, 1954.\\n8. John Hewitt and Christopher D. Manning. A structural probe for \\xef\\xac\\x81nding syntax in word repre-\\nsentations. In Proceedings of NAACL-HLT, 2019.\\n9. Goodfellow Ian, Yoshua Bengio, and Aaron Courville. Deep learning. Book in preparation for\\nMIT Press, 2016.\\n10. Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Lin-\\nguistic knowledge and transferability of contextual representations. In Proceedings of NAACL-\\nHLT, 2019.\\nReferences\\n11\\n11. James L McClelland, David E Rumelhart, PDP Research Group, et al. Parallel distributed\\nprocessing. Explorations in the Microstructure of Cognition, 2:216\\xe2\\x80\\x93271, 1986.\\n12. T Mikolov and J Dean. Distributed representations of words and phrases and their composi-\\ntionality. Proceedings of NeurIPS, 2013.\\n13. Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\\nrepresentation. In Proceedings of EMNLP, 2014.\\n14. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\\nand Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of NAACL-\\nHLT, pages 2227\\xe2\\x80\\x932237, 2018.\\n15. Claude E Shannon. A mathematical theory of communication. Bell system technical journal,\\n27(3):379\\xe2\\x80\\x93423, 1948.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 2\\nWord Representation\\nAbstract Word representation, aiming to represent a word with a vector, plays\\nan essential role in NLP. In this chapter, we \\xef\\xac\\x81rst introduce several typical word\\nrepresentation learning methods, including one-hot representation and distributed\\nrepresentation. After that, we present two widely used evaluation tasks for measuring\\nthe quality of word embeddings. Finally, we introduce the recent extensions for word\\nrepresentation learning models.\\n2.1\\nIntroduction\\nWords are usually considered as the smallest meaningful units of speech or writing in\\nhuman languages. High-level structures in a language, such as phrases and sentences,\\nare further composed of words. For human beings, to understand a language, it is\\ncrucial to understand the meanings of words. Therefore, it is essential to accurately\\nrepresent words, which could help models better understand, categorize, or generate\\ntext in NLP tasks.\\nA word can be naturally represented as a sequence of several characters. However,\\nit is very inef\\xef\\xac\\x81cient and ineffective only to use raw character sequences to represent\\nwords. First, the variable lengths of words make it hard to be processed and used in\\nmachine learning methods. Moreover, it is very sparse, because only a tiny proportion\\nof arrangements are meaningful. For example, English words are usually character\\nsequences which are composed of 1\\xe2\\x80\\x9320 characters in the English alphabet, but most\\nof these character sequences such as \\xe2\\x80\\x9caaaaa\\xe2\\x80\\x9d are meaningless.\\nOne-hot representation is another natural approach to represent words, which\\nassignsauniqueindextoeachword.Itisalsonotgoodenoughtorepresentwordswith\\none-hot representation. First, one-hot representation could not capture the seman-\\ntic relatedness among words. Second, one-hot representation is a high-dimensional\\nsparse representation, which is very inef\\xef\\xac\\x81cient. Third, it is very in\\xef\\xac\\x82exible for one-hot\\nrepresentation to deal with new words, which requires assigning new indexes for new\\nwords and would change the dimensions of the representation. The change may lead\\nto some problems for existing NLP systems.\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_2\\n13\\n14\\n2\\nWord Representation\\nRecently, distributed word representation approaches are proposed to address the\\nproblem of one-hot word representation. The distributional hypothesis [23, 30] that\\nlinguistic objects with similar distributions have similar meanings is the basis for dis-\\ntributed word representation learning. Based on the distributional hypothesis, various\\nword representation models, such as CBOW and Skip-gram, have been proposed and\\napplied in different areas.\\nIn the remaining part of this chapter, we start with one-hot word representation.\\nFurther, we introduce distributed word representation models, including Brown Clus-\\nter, Latent Semantic Analysis, Word2vec, and GloVe in detail. Then we introduce\\ntwo typical evaluation tasks for word representation. Finally, we discuss various\\nextensions of word representation models.\\n2.2\\nOne-Hot Word Representation\\nIn this section, we will introduce one-hot word representation in details. Given a\\n\\xef\\xac\\x81xed set of vocabulary V = {w1, w2, . . . , w|V |}, one very intuitive way to represent\\na word w is to encode it with a |V |-dimensional vector w, where each dimension of\\nw is either 0 or 1. Only one dimension in w can be 1 while all the other dimensions\\nare 0. Formally, each dimension of w can be represented as\\nwi =\\n\\x02\\n1 if w = wi\\n0 otherwise.\\n(2.1)\\nOne-hot word representation, in essence, maps each word to an index of the\\nvocabulary, which can be very ef\\xef\\xac\\x81cient for storage and computation. However, it\\ndoesnotcontainrichsemanticandsyntacticinformationofwords.Therefore,one-hot\\nrepresentation cannot capture the relatedness among words. The difference between\\ncat and dog is as much as the difference between cat and bed in one-hot word\\nrepresentation. Besides, one-hot word representation embeds each word into a |V |-\\ndimensional vector, which can only work for a \\xef\\xac\\x81xed vocabulary. Therefore, it is\\nin\\xef\\xac\\x82exible to deal with new words in a real-world scenario.\\n2.3\\nDistributed Word Representation\\nRecently, distributed word representation approaches are proposed to address the\\nproblem of one-hot word representation. The distributional hypothesis [23, 30] that\\nlinguistic objects with similar distributions have similar meanings is the basis for\\nsemantic word representation learning.\\nBased on the distributional hypothesis, Brown Cluster [9] groups words into hier-\\narchical clusters where words in the same cluster have similar meanings. The cluster\\n2.3 Distributed Word Representation\\n15\\nlabel can roughly represent the similarity between words, but it cannot precisely\\ncompare words in the same group. To address this issue, distributed word represen-\\ntation1 aims to embed each word into a continuous real-valued vector. It is a dense\\nrepresentation, and \\xe2\\x80\\x9cdense\\xe2\\x80\\x9d means that one concept is represented by more than one\\ndimension of the vector, and each dimension of the vector is involved in representing\\nmultiple concepts. Due to its continuous characteristic, distributed word represen-\\ntation can be easily applied in deep neural models for NLP tasks. Distributed word\\nrepresentation approaches such as Word2vec and GloVe usually learn word vectors\\nfrom a large corpus based on the distributional hypothesis. In this section, we will\\nintroduce several distributed word representation approaches in detail.\\n2.3.1\\nBrown Cluster\\nBrown Cluster classi\\xef\\xac\\x81es words into several clusters that have similar semantic mean-\\nings. Detailedly, Brown Cluster learns a binary tree from a large-scale corpus, in\\nwhich the leaves of the tree indicate the words and the internal nodes of the tree\\nindicate word hierarchical clusters. This is a hard clustering method since each word\\nbelongs to exactly one group.\\nThe idea of Brown Cluster to cluster the words comes from the n-gram language\\nmodel. A language model evaluates the probability of a sentence. For example,\\nthe sentence have a nice day should have a higher probability than a random\\nsequence of words. Using a k-gram language model, the probability of a sentence\\ns = {w1, w2, w3, . . . , wn} can be represented as\\nP(s) =\\nn\\x03\\ni=1\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92k).\\n(2.2)\\nIt is easy to estimate P(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92k) from a large corpus, but the model has |V |k \\xe2\\x88\\x921\\nindependent parameters which is a huge number for computers in the 1990s. Even if k\\nis 2, the number of parameters is considerable. Moreover, the estimation is inaccurate\\nfor rare words. To address these problems, [9] proposes to group words into clusters\\nand train a cluster-level n-gram language model rather than a word-level model. By\\nassigning a cluster to each word, the probability can be written as\\nP(s) =\\nn\\x03\\ni=1\\nP(ci|ci\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92k)P(wi|ci),\\n(2.3)\\n1We emphasize that distributed representation and distributional representation are two completely\\ndifferent aspects of representations. A word representation method may belong to both categories.\\nDistributed representation indicates that the representation is a real-valued vector, while distri-\\nbutional representation indicates that the meaning of a word is learned under the distributional\\nhypothesis.\\n16\\n2\\nWord Representation\\nwhere ci is the corresponding cluster of wi. In cluster-level language model, there\\nare only |Ck| \\xe2\\x88\\x921 + |V | \\xe2\\x88\\x92|C| independent parameters, where C is the cluster set\\nwhich is usually much smaller than the vocabulary |V |.\\nThe quality of the cluster affects the performance of the language model. Given a\\ntraining text s, for a 2-gram language model, the quality of a mapping \\xcf\\x80 from words\\nto clusters is de\\xef\\xac\\x81ned as\\nQ(\\xcf\\x80) = 1\\nn log P(s)\\n(2.4)\\n= 1\\nn\\nn\\n\\x04\\ni=1\\n\\x05\\nlog P(ci|ci\\xe2\\x88\\x921) + log P(wi|ci)\\n\\x06\\n.\\n(2.5)\\nLet Nw be the number of times word w appears in corpus s, Nw1w2 be the number\\nof times bigram w1w2 appears, and N\\xcf\\x80(w) be the number of times a cluster appears.\\nThen the quality function Q(\\xcf\\x80) can be rewritten in a statistical way as follows:\\nQ(\\xcf\\x80) = 1\\nn\\nn\\n\\x04\\ni=1\\n\\x05\\nlog P(ci|ci\\xe2\\x88\\x921) + log P(wi|ci)\\n\\x06\\n(2.6)\\n=\\n\\x04\\nw1,w2\\nNw1w2\\nn\\nlog P(\\xcf\\x80(w2)|\\xcf\\x80(w1))P(w2|\\xcf\\x80(w2))\\n(2.7)\\n=\\n\\x04\\nw1,w2\\nNw1w2\\nn\\nlog N\\xcf\\x80(w1)\\xcf\\x80(w2)\\nN\\xcf\\x80(w1)\\nNw2\\nN\\xcf\\x80(w2)\\n(2.8)\\n=\\n\\x04\\nw1,w2\\nNw1w2\\nn\\nlog N\\xcf\\x80(w1)\\xcf\\x80(w2)n\\nN\\xcf\\x80(w1)N\\xcf\\x80(w2)\\n+\\n\\x04\\nw1,w2\\nNw1w2\\nn\\nlog Nw2\\nn\\n(2.9)\\n=\\n\\x04\\nc1,c2\\nNc1c2\\nn\\nlog Nc1c2n\\nNc1 Nc2\\n+\\n\\x04\\nw2\\nNw2\\nn\\nlog Nw2\\nn .\\n(2.10)\\nSince P(w) = Nw\\nn , P(c) = Nc\\nn , and P(c1c2) =\\nNc1c2\\nn , the quality function can be\\nrewritten as\\nQ(\\xcf\\x80) =\\n\\x04\\nc1,c2\\nP(c1c2) log\\nP(c1c2)\\nP(c1)P(c2) +\\n\\x04\\nw\\nP(w) log P(w)\\n(2.11)\\n= I (C) \\xe2\\x88\\x92H(V ),\\n(2.12)\\nwhere I (C) is the mutual information between clusters and H(V ) is the entropy of\\nthe word distribution, which is a constant value. Therefore, to optimize Q(\\xcf\\x80) equals\\nto optimize the mutual information.\\nThere is no practical method to obtain optimum partitions. Nevertheless, Brown\\nCluster uses a greedy strategy to obtain a suboptimal result. Initially, it assigns a\\ndistinct class for each word. Then it merges two classes with the least average mutual\\ninformation. After |V | \\xe2\\x88\\x92|C| mergences, the partition is generated. Keeping the |C|\\n2.3 Distributed Word Representation\\n17\\nTable 2.1 Some clusters of Brown Cluster\\nCluster #1\\nFriday\\nMonday\\nThursday\\nWednesday\\nTuesday\\nSaturday\\nCluster #2\\nJune\\nMarch\\nJuly\\nApril\\nJanuary\\nDecember\\nCluster #3\\nWater\\nGas\\nCoal\\nLiquid\\nAcid\\nSand\\nCluster #4\\nGreat\\nBig\\nVast\\nSudden\\nMere\\nSheer\\nCluster #5\\nMan\\nWoman\\nBoy\\nGirl\\nLawyer\\nDoctor\\nCluster #6\\nAmerican\\nIndian\\nEuropean\\nJapanese\\nGerman\\nAfrican\\nclusters, we can continuously perform |C| \\xe2\\x88\\x921 mergences to get a binary tree. With\\ncertain care in implementation, the complexity of this algorithm is O(|V |3).\\nWe show some clusters in Table 2.1. From the table, we can \\xef\\xac\\x81nd that each cluster\\nrelates to a sense in the natural language. The words in the same cluster tend to\\nexpress similar meanings or could be used exchangeably.\\n2.3.2\\nLatent Semantic Analysis\\nLatent Semantic Analysis (LSA) is a family of strategies derived from vector space\\nmodels, which could capture word semantics much better. LSA aims to explore latent\\nfactors for words and documents by matrix factorization to improve the estimation\\nof word similarities. Reference [14] applies Singular Value Decomposition (SVD)\\non the word-document matrix and exploits uncorrelated factors for both words and\\ndocuments. The SVD of word-document matrix M yields three matrices E, \\xce\\xa3 and\\nD such that\\nM = E\\xce\\xa3D\\xe2\\x8a\\xa4,\\n(2.13)\\nwhere \\xce\\xa3 is the diagonal matrix of singular values of M, each row vector wi in\\nmatrix E corresponds to word wi, and each row vector di in matrix D corresponds\\nto document di. Then the similarity between two words could be\\nsim(wi, w j) = Mi,:M\\xe2\\x8a\\xa4\\nj,: = wi\\xce\\xa32w j.\\n(2.14)\\nHere, the number of singular values k included in \\xce\\xa3 is a hyperparameter that\\nneeds to be tuned. With a reasonable amount of the largest singular values used, LSA\\ncould capture much useful information in the word-document matrix and provide a\\nsmoothing effect that prevents large variance.\\nWith a relatively small k, once the matrices E, \\xce\\xa3 and D are computed, measuring\\nword similarity could be very ef\\xef\\xac\\x81cient because there are often fewer nonzero dimen-\\nsions in word vectors. However, the computation of E and D can be costly because\\nfull SVD on a n \\xc3\\x97 m matrix takes O(min{m2n, mn2}) time, while the parallelization\\nof SVD is not trivial.\\n18\\n2\\nWord Representation\\nAnother algorithm for LSA is Random Indexing [34, 55]. It overcomes the dif\\xef\\xac\\x81-\\nculty of SVD-based LSA, by avoiding costly preprocessing of a huge word-document\\nmatrix. In random indexing, each document is assigned with a randomly generated\\nhigh-dimensional sparse ternary vector (called index vector). Then for each word\\nin the document, the index vector is added to the word\\xe2\\x80\\x99s vector. The index vectors\\nare supposed to be orthogonal or nearly orthogonal. This algorithm is simple and\\nscalable, which is easy to parallelize and implemented incrementally. Moreover, its\\nperformance is comparable with the SVD-based LSA, according to [55].\\n2.3.3\\nWord2vec\\nGoogle\\xe2\\x80\\x99s word2vec2 toolkit was released in 2013. It can ef\\xef\\xac\\x81ciently learn word vectors\\nfrom a large corpus. The toolkit has two models, including Continuous Bag-Of-\\nWords (CBOW) and Skip-gram. Based on the assumption that the meaning of a\\nword can be learned from its context, CBOW optimizes the embeddings so that they\\ncan predict a target word given its context words. Skip-gram, on the contrary, learns\\nthe embeddings that can predict the context words given a target word. In this section,\\nwe will introduce these two models in detail.\\n2.3.3.1\\nContinuous Bag-of-Words\\nCBOW predicts the center word given a window of context. Figure2.1 shows the\\nidea of CBOW with a window of 5 words.\\nFormally, CBOW predicts wi according to its contexts as\\nP(wi|w j(| j\\xe2\\x88\\x92i|\\xe2\\x89\\xa4l, j\\xcc\\xb8=i)) = Softmax\\n\\xe2\\x8e\\x9b\\n\\xe2\\x8e\\x9dM\\n\\xe2\\x8e\\x9b\\n\\xe2\\x8e\\x9d\\n\\x04\\n| j\\xe2\\x88\\x92i|\\xe2\\x89\\xa4l, j\\xcc\\xb8=i\\nw j\\n\\xe2\\x8e\\x9e\\n\\xe2\\x8e\\xa0\\n\\xe2\\x8e\\x9e\\n\\xe2\\x8e\\xa0,\\n(2.15)\\nwhere P(wi|w j(| j\\xe2\\x88\\x92i|\\xe2\\x89\\xa4l, j\\xcc\\xb8=i))istheprobabilityofwordwi givenitscontexts,l isthesize\\nof training contexts, M is the weight matrix in R|V |\\xc3\\x97m, V indicates the vocabulary,\\nand m is the dimension of the word vector.\\nThe CBOW model is optimized by minimizing the sum of negative log probabil-\\nities:\\nL = \\xe2\\x88\\x92\\n\\x04\\ni\\nlog P(wi|w j(| j\\xe2\\x88\\x92i|\\xe2\\x89\\xa4l, j\\xcc\\xb8=i)).\\n(2.16)\\nHere, the window size l is a hyperparameter to be tuned. A larger window size\\nmay lead to a higher accuracy as well as the more expense of the training time.\\n2https://code.google.com/archive/p/word2vec/.\\n2.3 Distributed Word Representation\\n19\\nClassifier\\nWord Matrix\\nAverage/Concatenate\\nwi\\nwi-1\\nwi+1\\nwi+2\\nW\\nW\\nW\\nwi-2\\nW\\nFig. 2.1 The architecture of CBOW model\\n2.3.3.2\\nSkip-Gram\\nOn the contrary to CBOW, Skip-gram predicts the context given the center word.\\nFigure2.2 shows the model.\\nFormally, given a word wi, Skip-gram predicts its context as\\nP(w j|wi) = softmax(Mwi)\\n\\x05\\n| j \\xe2\\x88\\x92i| \\xe2\\x89\\xa4l, j \\xcc\\xb8= i\\n\\x06\\n,\\n(2.17)\\nwhere P(w j|wi) is the probability of context word w j given wi, and M is the weight\\nmatrix. The loss function is de\\xef\\xac\\x81ned similar to CBOW as\\nL = \\xe2\\x88\\x92\\n\\x04\\ni\\n\\x04\\nj(| j\\xe2\\x88\\x92i|\\xe2\\x89\\xa4l, j\\xcc\\xb8=i)\\nP(w j|wi).\\n(2.18)\\nWord Matrix\\nClassifier\\nwi-2\\nwi-1\\nwi+1\\nwi+2\\nwi\\nW\\nFig. 2.2 The architecture of skip-gram model\\n20\\n2\\nWord Representation\\n2.3.3.3\\nHierarchical Softmax and Negative Sampling\\nTo train CBOW or Skip-gram directly is very time consuming. The most time-\\nconsuming part is the softmax layer. The conventional softmax layer needs to obtain\\nthe scores of all words even though only one word is used in computing the loss\\nfunction. An intuitive idea to improve ef\\xef\\xac\\x81ciency is to get a reasonable but much\\nfaster approximation of that word. Here, we will introduce two typical approxima-\\ntion methods which are included in the toolkit, including hierarchical softmax and\\nnegative sampling. We explain these two methods using CBOW as an example.\\nThe idea of hierarchical softmax is to build hierarchical classes for all words\\nand to estimate the probability of a word by estimating the conditional probability\\nof its corresponding hierarchical class. Figure2.3 gives an example. Each internal\\nnode of the tree indicates a hierarchical class and has a feature vector, while each\\nleaf node of the tree indicates a word. In this example, the probability of word\\nthe is p0 \\xc3\\x97 p01 while the probability of cat is p0 \\xc3\\x97 p00 \\xc3\\x97 p001. The conditional\\nprobability is computed by the feature vector of each node and the context vector.\\nFor example,\\np0 =\\nexp(w0 \\xc2\\xb7 wc)\\nexp(w0 \\xc2\\xb7 wc) + exp(w1 \\xc2\\xb7 wc),\\n(2.19)\\np1 = 1 \\xe2\\x88\\x92p0,\\n(2.20)\\nwhere wc is the context vector, w0 and w1 are the feature vectors.\\nHierarchical softmax generates the hierarchical classes according to the word\\nfrequency, i.e., a Huffman tree. By the approximation, it can compute the probability\\nof each word much faster, and the complexity of calculating the probability of each\\nword is O(log |V |).\\nNegative sampling is more straightforward. To calculate the probability of a word,\\nnegative sampling directly samples k words as negative samples according to the\\nword frequency. Then, it computes a softmax over the k + 1 words to approximate\\nthe probability of the target word.\\nFig. 2.3 An illustration of\\nhierarchical softmax\\n1\\ndog\\ncat\\nthe\\nis\\n1\\n1\\n2.3 Distributed Word Representation\\n21\\nTable 2.2 Co-occurrence probabilities and the ratio of probabilities for target words ice and\\nsteam with context word solid, gas, water, and fashion\\nProbability and ratio\\nk = solid\\nk = gas\\nk = water\\nk = f ashion\\nP(k|ice)\\n1.9e \\xe2\\x88\\x924\\n6.6e \\xe2\\x88\\x925\\n3e \\xe2\\x88\\x923\\n1.7e \\xe2\\x88\\x925\\nP(k|steam)\\n2.2e \\xe2\\x88\\x925\\n7.8e \\xe2\\x88\\x924\\n2.2e \\xe2\\x88\\x923\\n1.8e \\xe2\\x88\\x925\\nP(k|ice)/P(k|steam)\\n8.9\\n8.5e \\xe2\\x88\\x922\\n1.36\\n0.96\\n2.3.4\\nGloVe\\nMethods like Skip-gram and CBOW are shallow window-based methods. These\\nmethods scan a context window across the entire corpus, which fails to take advantage\\nof some global information. Global Vectors for Word Representation (GloVe), on the\\ncontrary, can capture corpus statistics directly.\\nAs shown in Table 2.2, the meaning of a word can be learned from the co-\\noccurrence matrix. The ratio of co-occurrence probabilities can be especially useful.\\nIn the example, the meaning of ice and water can be examined by studying the\\nratio of their co-occurrence probabilities with various probe words. For words related\\nto ice but not steam, for example, solid, the ratio P(solid|ice)/P(solid|\\nsteam) will be large. Similarly, gas is related to steam but not ice, so\\nP(gas|ice)/P(gas|steam) will be small. For words that are relevant or irrel-\\nevant to both words, the ratio is close to 1.\\nBased on this idea, GloVe models\\nF(wi, w j, \\xcb\\x9cwk) = Pik\\nPjk\\n,\\n(2.21)\\nwhere \\xcb\\x9cw \\xe2\\x88\\x88Rd are separate context word vectors, and Pi j is the probability of word\\nj to be in the context of word i, formally\\nPi j = Ni j\\nNi\\n,\\n(2.22)\\nwhere Ni j is the number of occurrences of word j in the context of word i, and\\nNi = \\x0b\\nk Nik is the number of times any word appears in the context of word j.\\nF(\\xc2\\xb7) is supposed to encode the information presented in the ratio Pik/Pjk in the\\nword vector space. To keep the inherently linear structure, F should only depend on\\nthe difference of two target words\\nF(wi \\xe2\\x88\\x92w j, \\xcb\\x9cwk) = Pik\\nPjk\\n.\\n(2.23)\\nThe arguments of F are vectors while the right side of the equation is a scalar, to\\navoid F obfuscating the linear structure, a dot product is used:\\n22\\n2\\nWord Representation\\nF\\n\\x05\\n(wi \\xe2\\x88\\x92w j)\\xe2\\x8a\\xa4\\xcb\\x9cwk\\n\\x06\\n= Pik\\nPjk\\n.\\n(2.24)\\nThe model keeps the invariance under relabeling the target word and context word.\\nIt requires F to be a homomorphism between the groups (R, +) and (R>0, \\xc3\\x97). The\\nsolution is F = exp. Then\\nw\\xe2\\x8a\\xa4\\ni \\xcb\\x9cwk = log Nik \\xe2\\x88\\x92log Ni.\\n(2.25)\\nTo keep exchange symmetry, log Ni is eliminated by adding biases bi and \\xcb\\x9cbk. The\\nmodel becomes\\nw\\xe2\\x8a\\xa4\\ni \\xcb\\x9cwk + bi + \\xcb\\x9cbk = log Nik,\\n(2.26)\\nwhich is signi\\xef\\xac\\x81cantly simpler than Eq.(2.21).\\nThe loss function is de\\xef\\xac\\x81ned as\\nL =\\n|V |\\n\\x04\\ni, j=1\\nf (Ni j)(w\\xe2\\x8a\\xa4\\ni \\xcb\\x9cw j + bi + \\xcb\\x9cb j \\xe2\\x88\\x92log Ni j),\\n(2.27)\\nwhere f (\\xc2\\xb7) is a weighting function:\\nf (x) =\\n\\x02\\n(x/xmax)\\xce\\xb1\\nif x < xmax,\\n1\\notherwise.\\n(2.28)\\n2.4\\nContextualized Word Representation\\nIn natural language, the meaning of an individual word usually relates to its context\\nin a sentence. For example,\\n\\xe2\\x80\\xa2 The central bank has slashed its forecast for economic\\ngrowth this year from 4.1 to 2.6%.\\n\\xe2\\x80\\xa2 More recently, on a blazing summer day, he took me back\\nto one of the den sites, in a slumping bank above the\\nSouth Saskatchewan River.\\nInthesetwosentences,althoughtheword bankisalwaysthesame,theirmeanings\\nare different. However, most of the traditional word embeddings (CBOW, Skip-gram,\\nGloVe, etc.) cannot well understand the different nuances of the meanings of words\\nwith the different surrounding texts. The reason is that these models only learn a\\nunique representation for each word, and therefore it is impossible for these models\\nto capture how the meanings of words change based on their surrounding contexts.\\nTo address this issue, [48] proposes ELMo, which uses a deep, bidirectional LSTM\\nmodel to build word representations. ELMo could represent each word depending\\n2.4 Contextualized Word Representation\\n23\\non the entire context in which it is used. More speci\\xef\\xac\\x81cally, rather than having a look-\\nup table of word embedding matrix, ELMo converts words into low-dimensional\\nvectors on-the-\\xef\\xac\\x82y by feeding the word and its surrounding text into a deep neural\\nnetwork. ELMo utilizes a bidirectional language model to conduct word representa-\\ntion. Formally, given a sequence of N words, (w1, w2, . . . , wN), a forward language\\nmodel (LM, the details of language model are in Sect. 4) models the probability of\\nthe sequence by predicting the probability of each word tk according to the historical\\ncontext:\\nP(w1, w2, . . . , wN) =\\nN\\n\\x03\\nk=1\\nP(wk | w1, w2, . . . , wk\\xe2\\x88\\x921).\\n(2.29)\\nThe forward LM in ELMo is a multilayer LSTM, and the jth layer of the LSTM-\\nbased forward LM will generate the context-dependent word representation \\xe2\\x88\\x92\\xe2\\x86\\x92\\nh LM\\nk, j\\nfor the word wk. The backward LM is similar to the forward LM. The only difference\\nis that it reverses the input word sequence to (wN, wN\\xe2\\x88\\x921, . . . , w1) and predicts each\\nword according to the future context:\\nP(w1, w2, . . . , wN) =\\nN\\n\\x03\\nk=1\\nP(wk | wk+1, wk+2, . . . , wN).\\n(2.30)\\nAs the same as the forward LM, the jth backward LM layer generates the repre-\\nsentations \\xe2\\x86\\x90\\xe2\\x88\\x92\\nh LM\\nk, j for the word wk.\\nELMogeneratesatask-speci\\xef\\xac\\x81cwordrepresentation,whichcombinesalllayerrep-\\nresentations of the bidirectional LM. Formally, it computes a task-speci\\xef\\xac\\x81c weighting\\nof all bidirectional LM layers:\\nwk = \\xce\\xb1task\\nL\\n\\x04\\nj=0\\nstask\\nj\\nhLM\\nk, j ,\\n(2.31)\\nwhere stask are softmax-normalized weights and \\xce\\xb1task is the weight of the entire word\\nvector for the task.\\n2.5\\nExtensions\\nBesides those very popular toolkits, such as word2vec and GloVe, various works\\nare focusing on different aspects of word representation, contributing to numerous\\nextensions. These extensions usually focus on the following directions.\\n24\\n2\\nWord Representation\\n2.5.1\\nWord Representation Theories\\nWith the success of word representation, researchers begin to explore the theories of\\nword representation. Some works attempt to give more theoretical analysis to prove\\nthe reasonability of existing tricks on word representation learning [39, 45], while\\nsome works try to discuss the new learning methods [26, 61].\\nReasonability. Word2vec and other similar tools are empirical methods of word\\nrepresentationlearning.Manytricksareproposedin[43]tolearntherepresentationof\\nwords from a large corpus ef\\xef\\xac\\x81ciently, for example, negative sampling. Considering\\nthe effectiveness of these methods, a more theoretical analysis should be done to\\nprove the reasonability of these tricks. Reference [39] gives some theoretical analysis\\nof these tricks. They formalize the Skip-gram model with negative sampling as\\nan implicit matrix factorization process. The Skip-gram model generates a word\\nembedding matrix E and a context matrix C. The size of the word embedding matrix\\nE is |V | \\xc3\\x97 m. Each row of context matrix C is a context word\\xe2\\x80\\x99s m-dimensional vector.\\nThe training process of Skip-gram is an implicit factorization of M = EC\\xe2\\x8a\\xa4. C is not\\nexplicitly considered in word2vec. This work further analyzes that the matrix M is\\nMi j = wi \\xc2\\xb7 c j = PMI(wi, c j) \\xe2\\x88\\x92log k,\\n(2.32)\\nwhere k is the number of negative samples, PMI(w, c) is the point-wise mutual\\ninformation\\nPMI(w, c) = log\\nP(w, c)\\nP(w)P(c).\\n(2.33)\\nThe shifted PMI matrix can directly be used to compare the similarity of words.\\nAnother intuitive idea is to factorize the shifted PMI matrix directly. Reference [39]\\nevaluates the performance of using the SVD matrix factorization method on the\\nimplicit matrix M. Matrix factorization achieves signi\\xef\\xac\\x81cantly better objective value\\nwhen the embedding size is smaller than 500 dimensions and the number of negative\\nsamples is 1. With more negative samples and higher embedding dimensions, Skip-\\ngram with negative sampling gets better objective value. This is because when the\\nnumber of zeros increases in M, and SVD prefers to factorize a matrix with mini-\\nmum values. With 1,000 dimensional embeddings and different numbers of negative\\nsamples in {1, 5, 15}, SVD achieves slightly better performance on word analogy and\\nword similarity. In contrast, Skip-gram with negative sampling achieves 2% better\\nperformance on syntactical analogy.\\nInterpretability. Most existing distributional word representation methods could\\ngenerate a dense real-valued vector for each word. However, the word embeddings\\nobtained by these models are hard to be interpreted. Reference [26] introduces non-\\nnegative and sparsity embeddings, where the models are interpretable and each\\ndimension indicates a unique concept. This method factorizes the corpus statistics\\nmatrix X \\xe2\\x88\\x88R|V |\\xc3\\x97|D| into a word embedding matrix E \\xe2\\x88\\x88R|V |\\xc3\\x97m and a document\\nstatistics matrix D \\xe2\\x88\\x88Rm\\xc3\\x97|D|. Its training objective is\\n2.5 Extensions\\n25\\narg min\\nE,D\\n1\\n2\\n|V |\\n\\x04\\ni=1\\n\\xe2\\x88\\xa5Xi,: \\xe2\\x88\\x92Ei,:D\\xe2\\x88\\xa52 + \\xce\\xbb\\xe2\\x88\\xa5Ei,:\\xe2\\x88\\xa51,\\ns.t. Di,:D\\xe2\\x8a\\xa4\\ni,: \\xe2\\x89\\xa41, \\xe2\\x88\\x801 \\xe2\\x89\\xa4i \\xe2\\x89\\xa4m,\\nEi, j \\xe2\\x89\\xa50, 1 \\xe2\\x89\\xa4i \\xe2\\x89\\xa4|V |, 1 \\xe2\\x89\\xa4j \\xe2\\x89\\xa4m.\\n(2.34)\\nBy iteratively optimizing E and D via gradient descent, this model can learn non-\\nnegative and sparse embeddings for words. Since the embeddings are sparse and\\nnonnegative, words with the highest scores in each dimension show high similarity,\\nwhich can be viewed as a concept of this dimension. To further improve the embed-\\ndings, this work also proposes phrasal-level constraints into the loss function. With\\nnew constraints, it could achieve both interpretability and compositionality.\\n2.5.2\\nMulti-prototype Word Representation\\nUsing only one single vector to represent a word is problematic due to the ambiguity\\nof words. A single vector cannot represent multiple meanings of a word well because\\nit may lead to semantic confusion among the different senses of this word.\\nThe multi-prototype vector space model [51] is proposed to better represent dif-\\nferent meanings of a word. In multi-prototype vector space model, a mixture of\\nvon Mises-Fisher distributions (movMF) clustering method with \\xef\\xac\\x81rst-order unigram\\ncontexts [5] is used to cluster different meanings of a word. Formally, it assigns a\\ndifferent word representation wi(x) to the same word x in each different cluster i.\\nWhen the multi-prototype embedding is used, the similarity between two words x, y\\nis computed straightforwardly. If contexts of words are not available, the similarity\\nbetween two words is de\\xef\\xac\\x81ned as\\nAvgSim(x, y) = 1\\nK 2\\nK\\n\\x04\\ni=1\\nK\\n\\x04\\nj=1\\ns(wi(x), w j(y)),\\n(2.35)\\nMaxSim(x, y) =\\nmax\\n1\\xe2\\x89\\xa4i, j\\xe2\\x89\\xa4K s(wi(x), w j(y)),\\n(2.36)\\nwhere K is a hyperparameter indicating the number of the clusters and s(\\xc2\\xb7) is a simi-\\nlarity function of two vectors such as cosine similarity. When contexts are available,\\nthe similarity can be computed more precisely as:\\nAvgSimC(x, y) = 1\\nK 2\\nK\\n\\x04\\ni=1\\nK\\n\\x04\\nj=1\\nsc,x,isc,y, js(wi(x), w j(y)),\\n(2.37)\\nMaxSimC(x, y) = s( \\xcb\\x86w(x), \\xcb\\x86w(y)),\\n(2.38)\\n26\\n2\\nWord Representation\\nwheresc,x,i = s(wi(c), wi(x))isthelikelihoodofcontextc belongingtoclusteri,and\\n\\xcb\\x86w(x) = warg max1\\xe2\\x89\\xa4i\\xe2\\x89\\xa4K sc,x,i(x) is the maximum likelihood cluster for x in context c. With\\nmulti-prototype embeddings, the accuracy on the word similarity task is signi\\xef\\xac\\x81cantly\\nimproved, but the performance is still sensitive to the number of clusters.\\nAlthough the multi-prototype embedding method can effectively cluster different\\nmeaningsofawordviaitscontexts,theclusteringisof\\xef\\xac\\x82ine,andthenumberofclusters\\nis \\xef\\xac\\x81xed and needs to be prede\\xef\\xac\\x81ned. It is dif\\xef\\xac\\x81cult for a model to select an appropriate\\namount of meanings for different words, to adapt to new senses, new words, or new\\ndata,andtoalignthesenseswithprototypes.Toaddresstheseproblems,[12]proposes\\na uni\\xef\\xac\\x81ed model for word sense representation and word sense disambiguation. This\\nmodel uses available knowledge bases such as WordNet [46] to determine the senses\\nof a word. Each word and each sense had a single vector and are trained jointly. This\\nmodel can learn representations of both words and senses, and two simple methods\\nare proposed to do disambiguation using the word and sense vectors.\\n2.5.3\\nMultisource Word Representation\\nThereis muchinformationabout words that canbeleveragedtoimprovethequalityof\\nword representations. We will introduce other kinds of word representation learning\\nmethods utilizing multisource information.\\n2.5.3.1\\nWord Representation with Internal Information\\nThere is much information locating inside words, which can be utilized to improve\\nthe quality of word representations further.\\nUsing Character Information. Many languages such as Chinese and Japanese\\nhave thousands of characters, and the words in these languages are composed of\\nseveral characters. Characters in these languages have richer semantic information\\ncomparing with other languages containing only dozens of characters. Hence, the\\nmeaning of a word can not only be learned from its contexts but also the composition\\nof characters. Driven by this intuitive idea, [13] proposes a joint learning model for\\nCharacter and Word Embeddings (CWE). In CWE, a word is a composition of a\\nword embedding and its character embeddings. Formally,\\nx = w + 1\\n|w|\\n\\x04\\ni\\nci,\\n(2.39)\\nwhere x is the representation of a word, which is the composition of a word vector w\\nand several character vectors ci, and |w| is the number of characters in the word. Note\\nthat this model can be integrated with various models such as Skip-gram, CBOW,\\nand GloVe.\\n2.5 Extensions\\n27\\nFurther, position-based and cluster-based methods are proposed to address this\\nissue that characters are highly ambiguous. In position-based approach, each char-\\nacter is assigned three vectors which appear in begin, middle and end of a word\\nrespectively. Since the meaning of a character varies when it appears in the different\\npositions of a word, this method can signi\\xef\\xac\\x81cantly resolve the ambiguity problem.\\nHowever, characters that appear in the same position may also have different mean-\\nings. In the cluster-based method, a character is assigned K different vectors for its\\ndifferent meanings, in which a word\\xe2\\x80\\x99s context is used to determine which vector to\\nbe used.\\nBy introducing character embeddings, the representation of low-frequency words\\ncan be signi\\xef\\xac\\x81cantly improved. Besides, this method can deal with new words while\\nother methods fail. Experiments show that the joint learning method can achieve bet-\\nter performance on both word similarity and word analogy tasks. By disambiguating\\ncharacters using the position-based and cluster-based method, it can further improve\\nthe performance.\\nUsing Morphology Information. Many languages such as English have rich\\nmorphology information and plenty of rare words. Most word representation models\\nassign a distinct vector to each word ignoring the rich morphology information. This\\nis a limitation because the af\\xef\\xac\\x81xes of a word can help infer the meaning of a word\\nand the morphology information of word is essential especially when facing rare\\ncontexts.\\nTo address this issue, [8] proposes to represent a word as a bag of morphology n-\\ngrams.ThismodelsubstituteswordvectorsinSkip-gramwiththesumofmorphology\\nn-gram vectors. When creating the dictionary of n-grams, they select all n-grams with\\na length greater or equal than 3 and smaller or equal than 6. To distinguish pre\\xef\\xac\\x81xes and\\nsuf\\xef\\xac\\x81xes with other af\\xef\\xac\\x81xes, they also add special characters to indicate the beginning\\nand the end of a word. This model is ef\\xef\\xac\\x81cient and straightforward, which achieves\\ngood performance on word similarity and word analogy tasks especially when the\\ntraining set is small.\\nReference [41] further uses a bidirectional LSTM to generate word representation\\nby composing morphologies. This model does not use a look-up table to assign a\\ndistinct vector to each word like what those independent word embedding methods\\nare doing. Hence, this model not only signi\\xef\\xac\\x81cantly reduces the number of parameters\\nbut also addresses some disadvantages of independent word embeddings. Moreover,\\nthe embeddings of words in this model could affect each other.\\n2.5.3.2\\nWord Representation with External Knowledge\\nBesides internal information of words, there is much external knowledge that could\\nhelp us learn the word representations.\\nUsing Knowledge Base. Some languages have rich internal information, whereas\\npeople have also annotated lots of knowledge bases which can be used in word\\nrepresentation learning to constrain embeddings. Reference [62] introduces relation\\nconstraints into the CBOW model. With these constraints, the embeddings can not\\n28\\n2\\nWord Representation\\nonly predict its contexts, but also predict words with relations. The objective is to\\nmaximize the sum of log probability of all relations as\\nO = 1\\nN\\nN\\n\\x04\\ni=1\\n\\x04\\nw\\xe2\\x88\\x88Rwi\\nlog P(w|wi),\\n(2.40)\\nwhere Rwi indicates a set of words which have relation with wi. Then the joint\\nobjective is de\\xef\\xac\\x81ned as\\nO = 1\\nN\\nN\\n\\x04\\ni=1\\nlog P(wi|w j(| j\\xe2\\x88\\x92i|<l, j\\xcc\\xb8=i)) + \\xce\\xb2\\nN\\nN\\n\\x04\\ni=1\\n\\x04\\nw\\xe2\\x88\\x88Rwi\\nlog p(w|wi),\\n(2.41)\\nwhere \\xce\\xb2 is a hyperparameter. The external information helps to train a better word\\nrepresentation, which shows signi\\xef\\xac\\x81cant improvements on word similarity bench-\\nmarks.\\nMoreover, Retro\\xef\\xac\\x81tting [19] introduces a post-processing step which can introduce\\nknowledge bases into word representation learning. It is more modular than other\\napproacheswhichconsiderknowledgebaseduringtraining.Letthewordembeddings\\nlearned by existing word representation approaches be E. Retro\\xef\\xac\\x81tting attempts to \\xef\\xac\\x81nd\\nanother embedding space \\xcb\\x86E, which is close to E but considers the relations in the\\nknowledge base. Formally,\\nL =\\n\\x04\\ni\\n\\x05\\n\\xce\\xb1i\\xe2\\x88\\xa5wi \\xe2\\x88\\x92\\xcb\\x86wi\\xe2\\x88\\xa52 +\\n\\x04\\n(i, j)\\xe2\\x88\\x88R\\n\\xce\\xb2i j\\xe2\\x88\\xa5wi \\xe2\\x88\\x92w j\\xe2\\x88\\xa52\\n\\x06\\n,\\n(2.42)\\nwhere \\xce\\xb1 and \\xce\\xb2 are hyperparameters indicating the strength of the associations, and\\nR is a set of relations in the knowledge base. The adapted embeddings \\xcb\\x86E can be\\noptimized by several iterations of the following online updates:\\n\\xcb\\x86wi =\\n\\x0b\\n{ j|(i, j)\\xe2\\x88\\x88R} \\xce\\xb2i j \\xcb\\x86w j + \\xce\\xb1iwi\\n\\x0b\\n{ j|(i, j)\\xe2\\x88\\x88R} \\xce\\xb2i j + \\xce\\xb1i\\n,\\n(2.43)\\nwhere \\xce\\xb1 is usually set to 1 and \\xce\\xb2i j is deg(i)\\xe2\\x88\\x921 (deg(\\xc2\\xb7) is a node\\xe2\\x80\\x99s degree in a knowledge\\ngraph). With knowledge bases such as the paraphrase database [27], WordNet [46]\\nand FrameNet [3], this model can achieve consistent improvement on word similarity\\ntasks. But it also may signi\\xef\\xac\\x81cantly reduce the performance on the analogy of syntactic\\nrelations. Since this module is a post-processing of word embeddings, it is compatible\\nwith various distributed representation models.\\nIn addition to the aforementioned synonym-based knowledge bases, there are also\\nsememe-based knowledge bases, in which the sememe is de\\xef\\xac\\x81ned as the minimum\\nsemantic unit of word meanings. HowNet [16] is one of such knowledge bases,\\nwhich annotates each Chinese word with one or more relevant sememes. General\\n2.5 Extensions\\n29\\nknowledge injecting methods could not apply to HowNet. As a result, [47] proposes\\na speci\\xef\\xac\\x81c model to introduce HowNet into word representation learning.\\nBases on Skip-gram model, [47] introduces sense and sememe embeddings to\\nrepresent target word wi. More speci\\xef\\xac\\x81cally, this model leverages context words,\\nwhich are represented with original word embeddings, as attention over multiple\\nsenses of target word wi to obtain its new embeddings.\\nwi =\\n|S(wi )|\\n\\x04\\nk=1\\nAtt(s(wi)\\nk\\n)s(wi)\\nk\\n,\\n(2.44)\\nwhere s(wi)\\nk\\ndenotes the kth sense embedding of wi and S(wi) is the sense set of wi.\\nThe attention term is as follows:\\nAtt(s(wi)\\nk\\n) =\\nexp(w\\xe2\\x80\\xb2\\nc \\xc2\\xb7 \\xcb\\x86s(wi)\\nk\\n)\\n\\x0b|S(wi )|\\nn=1 exp(w\\xe2\\x80\\xb2c \\xc2\\xb7 \\xcb\\x86s(wi)\\nn\\n)\\n,\\n(2.45)\\nwhere \\xcb\\x86s(wi)\\nk\\nstands for the average of sememe embeddings x, \\xcb\\x86s(wi)\\nk\\n= Avg(x(sk)) and\\nw\\xe2\\x80\\xb2\\nc is the average of context word embeddings, w\\xe2\\x80\\xb2\\nc = Avg(w j)(| j \\xe2\\x88\\x92i| \\xe2\\x89\\xa4l, j \\xcc\\xb8= i).\\nThis model shows a substantial advance in both word similarity and analogy\\ntasks. Moreover, the introduction of sense embeddings can also be used in word\\nsense disambiguation.\\nConsidering Document Information. Word embedding methods like Skip-gram\\nsimply consider the context information within a window to learn word represen-\\ntation. However, the information in the whole document could help our word rep-\\nresentation learning. Topical Word Embeddings (TWE) [42] introduces topic infor-\\nmation generated by Latent Dirichlet Allocation (LDA) to help distinguish different\\nmeanings of a word. The model is de\\xef\\xac\\x81ned to maximize the following average log\\nprobability:\\nO = 1\\nN\\nN\\n\\x04\\ni=1\\n\\x04\\n\\xe2\\x88\\x92k\\xe2\\x89\\xa4c\\xe2\\x89\\xa4k,c\\xcc\\xb8=0\\n(log P(wi+c|wi) + log P(wi+c|zi)) ,\\n(2.46)\\nwhere wi is the word embedding and zi is the topic embedding of wi. Each word wi\\nis assigned a unique topic, and each topic has a topic embedding. The topical word\\nembedding model shows advantages of contextual word similarity and document\\nclassi\\xef\\xac\\x81cation tasks.\\nHowever, TWE simply combines the LDA with word embeddings and lacks statis-\\ntical foundations. The LDA topic model needs numerous documents to learn seman-\\ntically coherent topics. Reference [40] further proposes the TopicVec model, which\\nencodes words and topics in the same semantic space. TopicVec outperforms TWE\\nand other word embedding methods on text classi\\xef\\xac\\x81cation datasets. It can learn coher-\\nent topics on only one document which is not possible for other topic models.\\n30\\n2\\nWord Representation\\n2.5.3.3\\nWord Representation with Hierarchical Structure\\nHuman knowledge is in a hierarchical structure. Recently, many works also introduce\\na hierarchical structure of texts into word representation learning.\\nDependency-based Word Representation. Continuous word embeddings are\\ncombinations of semantic and syntactic information. However, existing word repre-\\nsentation models depend solely on linear contexts and show more semantic infor-\\nmation than syntactic information. To make the embeddings show more syntactic\\ninformation, the dependency-based word embedding [38] uses the dependency-based\\ncontext. The dependency-based embeddings are less topical and exhibit more func-\\ntional similarity than the original Skip-gram embeddings. It takes the information of\\ndependency parsing tree into consideration when learning word representations. The\\ncontexts of a target word w are the modi\\xef\\xac\\x81ers of this word, i.e., (m1,r1), . . . , (mk,rk),\\nwhere ri is the type of the dependency relation between the head node and the mod-\\ni\\xef\\xac\\x81er. When training, the model optimizes the probability of dependency-based con-\\ntexts rather than neighboring contexts. This model gains some improvements on\\nword similarity benchmarks compared with Skip-gram. Experiments also show that\\nwords with syntactic similarity are more similar in the vector space.\\nSemantic Hierarchies. Because of the linear substructure of the vector space,\\nit is proven that word embeddings can make simple analogies. For example, the\\ndifference between Japan and Tokyo is similar to the difference between China\\nand Beijing. But it has trouble identifying hypernym-hyponym relations since\\nthese relationships are complicated and do not necessarily have linear substructure.\\nTo address this issue, [25] tries to identify hypernym-hyponym relationships using\\nword embeddings. The basic idea is to learn a linear projection rather than simply\\nuse the embedding offset to represent the relationship. The model optimizes the\\nprojection as\\nM\\xe2\\x88\\x97= arg min\\nM\\n1\\nN\\n\\x04\\n(i, j)\\n\\xe2\\x88\\xa5Mxi \\xe2\\x88\\x92y j\\xe2\\x88\\xa52,\\n(2.47)\\nwhere xi and y j are hypernym and hyponym embeddings.\\nTo further increase the capability of the model, they propose to \\xef\\xac\\x81rst cluster word\\npairs into several groups and learn a linear projection for each group. The linear\\nprojection can help identify various hypernym-hyponym relations.\\n2.5.4\\nMultilingual Word Representation\\nThere are thousands of languages in the world. In word level, how to represent words\\nfrom different languages in a uni\\xef\\xac\\x81ed vector space is an interesting problem. The\\nbilingual word embedding model [64] uses machine translation word alignments as\\nconstraining translational evidence and embeds words of two languages into a single\\nvector space. The basic idea is (1) to initialize each word according to its aligned\\n2.5 Extensions\\n31\\nwords in another language and (2) to constrain the distance between two languages\\nduring the training using translation pairs.\\nWhen learning bilingual word embeddings, it \\xef\\xac\\x81rstly trains source word embed-\\ndings. Then they use aligned sentence pairs to count the co-occurrence of source and\\ntarget words. The target word embeddings can be initialized as\\nEt\\xe2\\x88\\x92init =\\nS\\n\\x04\\ns=1\\nNts + 1\\nNt + S Es,\\n(2.48)\\nwhere Es and Et\\xe2\\x88\\x92init are the trained embeddings of the source word and the initial\\nembedding of the target word, respectively. Nts is the number of target words being\\naligned with source word. S is all the possible alignments of word t. So Nt + S\\nnormalizes the weights as a distribution. During the training, they jointly optimize\\nthe word embedding objective as well as the bilingual constraint. The constraint is\\nde\\xef\\xac\\x81ned as\\nLcn\\xe2\\x86\\x92en = \\xe2\\x88\\xa5Een \\xe2\\x88\\x92Nen\\xe2\\x86\\x92cnEcn\\xe2\\x88\\xa52,\\n(2.49)\\nwhere Nen\\xe2\\x86\\x92cn is the normalized align counts.\\nWhen given a lexicon of bilingual word pairs, [44] proposes a simple model that\\ncan learn bilingual word embeddings in a uni\\xef\\xac\\x81ed space. Based on the distributional\\ngeometric similarities of word vectors of two languages, this model learns a linear\\ntransformation matrix T that transforms the vector space of source language to that\\nof the target language. The training loss is\\nL = \\xe2\\x88\\xa5TEs \\xe2\\x88\\x92Et\\xe2\\x88\\xa52,\\n(2.50)\\nwhere Et is the word vector matrix of aligned words in target language.\\nHowever, this model performs badly when the seed lexicon is small. To tackle\\nthis limitation, some works introduce the idea of bootstrapping into bilingual word\\nrepresentation learning. Let\\xe2\\x80\\x99s take [63] for example. In this work, in addition to\\nmonolingual word embedding learning and bilingual word embedding alignment\\nbased on seed lexicon, a new matching mechanism is introduced. The main idea of\\nmatching is to \\xef\\xac\\x81nd the most probably matched source (target) word for each target\\n(source) word and make their embeddings closer. Next, we explain the target-to-\\nsource matching process formally, and the source-to-target side is similar.\\nThe target-to-source matching loss function is de\\xef\\xac\\x81ned as\\nLT 2S = \\xe2\\x88\\x92log P\\n\\x05\\nC(T )|E(S)\\x06\\n= \\xe2\\x88\\x92log\\n\\x04\\nm\\nP\\n\\x05\\nC(T ), m|E(S)\\x06\\n,\\n(2.51)\\nwhere C(T ) denotes the target corpus and m is a latent variable specifying the matched\\nsource word for each target word. On independency assumption, it has\\n32\\n2\\nWord Representation\\nP\\n\\x05\\nC(T ), m|E(S)\\x06\\n=\\n\\x03\\nw(T )\\ni\\n\\xe2\\x88\\x88C(T )\\nP\\n\\x0c\\nw(T )\\ni\\n, m|E(S)\\r\\n=\\n|V (T )|\\n\\x03\\ni=1\\nP\\n\\x0c\\nw(T)\\ni\\n|w(S)\\nmi\\n\\rNw(T )\\ni\\n, (2.52)\\nwhere Nw(T )\\ni\\nis the number of w(T )\\ni\\noccurrences in the target corpus. By training using\\nViterbi EM algorithm, this method can improve bilingual word embeddings on its\\nown and address the limitation of a small seed lexicon.\\n2.5.5\\nTask-Speci\\xef\\xac\\x81c Word Representation\\nIn recent years, word representation learning has achieved great success and played\\na crucial role in NLP tasks. People \\xef\\xac\\x81nd that word representation learning of the\\ngeneral \\xef\\xac\\x81eld is still a limitation in a speci\\xef\\xac\\x81c task and begin to explore the learning\\nof task-speci\\xef\\xac\\x81c word representation. In this section, we will take sentiment analysis\\nas an example.\\nWord Representation for Sentiment Analysis. Most word representation meth-\\nods capture syntactic and semantic information while ignoring sentiment of text. This\\nis problematic because words with similar syntactic polarity but opposite sentiment\\npolarity obtain closed word vectors. Reference [58] proposes to learn Sentiment-\\nSpeci\\xef\\xac\\x81c Word Embeddings (SSWE) by integrating the sentiment information. An\\nintuitive idea is to jointly optimize the sentiment classi\\xef\\xac\\x81cation model using word\\nembeddings as its feature and SSWE minimizes the cross-entropy loss to achieve\\nthis goal. To better combine the unsupervised word embedding method and the super-\\nviseddiscriminativemodel,theyfurtherusethewordsinawindowratherthanawhole\\nsentence to classify sentiment polarity. They propose the following ranking-based\\nloss:\\nLr(t) = max(0, 1 \\xe2\\x88\\x921s(t) f r\\n0 (t) + 1s(t) f r\\n1 (t)),\\n(2.53)\\nwhere f r\\n0 , f r\\n1 are the predicted positive and negative scores. 1s(t) is an indicator\\nfunction:\\n1s(t) =\\n\\x02\\n1\\nif t is positive,\\n\\xe2\\x88\\x921 if t is negative.\\n(2.54)\\nThis loss function only punishes the model when the model gives an incorrect\\nresult.\\nTo get massive training data, they use distant-supervision technology to gener-\\nate sentiment labels for a document. The increase of labeled data can improve the\\nsentiment information in word embeddings. On sentiment classi\\xef\\xac\\x81cation tasks, senti-\\nment embeddings outperform other strong baselines including SVM and other word\\nembedding methods. SSWE also shows strong polarity consistency, where the clos-\\nest words of a word are more likely to have the same sentiment polarity compared\\n2.5 Extensions\\n33\\nwith existing word representation models. This sentiment speci\\xef\\xac\\x81c word embedding\\nmethod provides us a general way to learn task-speci\\xef\\xac\\x81c word embeddings, which is\\nto design a joint loss function and to generate massive labeled data automatically.\\n2.5.6\\nTime-Speci\\xef\\xac\\x81c Word Representation\\nThe meaning of a word changes during the time. Analyzing the changing meaning\\nof a word is an exciting topic in both linguistic and NLP research. With the rise\\nof word embedding methods, some works [29, 35] use embeddings to analyze the\\nchange of words\\xe2\\x80\\x99 meanings. They separate corpus into bins with respect to years\\nto train time-speci\\xef\\xac\\x81c word embeddings and compare embeddings of different time\\nseries to analyze the change of word semantics. This method is intuitive but has some\\nproblems. Dividing corpus into bins causes the data sparsity issue. The objective of\\nword embedding methods is nonconvex so that different random initialization leads\\nto different results, which makes comparing word embeddings dif\\xef\\xac\\x81cult. Embeddings\\nof a word in different years are in different semantic spaces and cannot be compared\\ndirectly. Most work indirectly compares the meanings of a word in a different time\\nby the changes of a word\\xe2\\x80\\x99s closest words in the semantic space.\\nTo address these issues, [4] proposes a dynamic Skip-gram model which connects\\nseveral Bayesian Skip-gram models [6] using Kalman \\xef\\xac\\x81lters [33]. In this model, the\\nembeddings of words in different periods could affect each other. For example, a word\\nthat appears in the 1990s\\xe2\\x80\\x99 document can affect the embeddings of that word in the\\n1980s and 2000s. Moreover, it also trains the embedding in different periods by the\\nwholecorpustoreducethesparsityissue.Thismodelalsoputsalltheembeddingsinto\\nthe same semantic space, which is a signi\\xef\\xac\\x81cant improvement against other methods\\nand makes word embeddings in different periods comparable. Therefore, the change\\nof word embeddings in this model is continuous and smooth. Experimental results\\nshow that the cosine distance between two words changes much more smoothly in\\nthis model than those models which simply divide the corpus into bins.\\n2.6\\nEvaluation\\nIn recent years, various methods to embed words into a vector space have been\\nproposed. Hence, it is essential to evaluate different methods. There are two gen-\\neral evaluations of word embeddings, including word similarity and word analogy.\\nThey both aim to check if the word distribution is reasonable. These two evaluations\\nsometimes give different results. For example, CBOW achieves better performance\\non word similarity, whereas Skip-gram outperforms CBOW on word analogy. There-\\nfore, which method to choose depends on the high-level application. Task-speci\\xef\\xac\\x81c\\nword embedding methods are usually designed for speci\\xef\\xac\\x81c high-level tasks and\\n34\\n2\\nWord Representation\\nachieve signi\\xef\\xac\\x81cant improvement on these tasks compared with baselines such as\\nCBOW and Skip-gram. However, they only marginally outperform baselines on two\\ngeneral evaluations.\\n2.6.1\\nWord Similarity/Relatedness\\nThe dynamics of words are very complex and subtle. There is no static, \\xef\\xac\\x81nite set of\\nrelations that can describe all interactions between two words. It is also not trivial for\\ndownstream tasks to leverage different kinds of word relations. A more practical way\\nis to assign a score to a pair of words to represent to what extent they are related. This\\nmeasurement is called word similarity. When talking about the term word similarity,\\nthe precise meaning may vary a lot in different situations. There are several kinds of\\nsimilarity that may be referred to in various literature.\\nMorphological similarity. Many languages including English de\\xef\\xac\\x81ne morphol-\\nogy. The same morpheme can have multiple surface forms according to the syntac-\\ntical function. For example, the word active is an adjective and activeness\\nis its noun version. The word activate is a verb and activation is its noun\\nversion. The morphology is an important dimension when considering the meaning\\nand usage of words. It de\\xef\\xac\\x81nes some relations between words from a syntactical view.\\nSome relations are used in the Syntactic Word Relationship test set [43], including\\nadjectives to adverbs, past tense, and so on. However, in many higher level applica-\\ntions and tasks, the words are often morphologically normalized by the base form\\n(this process is also known as lemmatization). One widely used technique is the\\nPorter stemming algorithm [49]. This algorithm converts active, activeness,\\nactivate, and activation to the same root format activ. By removing mor-\\nphological features, the semantic meaning of words is more emphasized.\\nSemantic Similarity. Two words are semantically similar if they can express\\nthe same concept, or sense, like article and document. One word may have\\ndifferent senses, and each of its synonyms is associated with one or more of its senses.\\nWordNet [46] is a lexical database that organizes the words as groups according to the\\nsenses. Each group of words is called a synset, which contains all synonymous words\\nsharing the same speci\\xef\\xac\\x81c sense. The words within the same synset are considered\\nsemantically similar. Words from two synsets that are linked by some certain relation\\n(such as hyponym) are also considered semantically similar to some degree, like\\nbank(river) and bank\\n\\x05\\nbank(river)is the hyponym of bank\\n\\x06\\n.\\nSemantic relatedness. Most modern literature that considers word similarity\\nrefers to the semantic relatedness of words. Semantic relatedness is more general\\nthan semantic similarity. Words that are not semantically similar could still be related\\nin many ways such as meronymy (car and wheel) or antonymy (hot and cold).\\nSemantic relatedness often yields co-occurrence, but they are not equivalent. The\\nsyntactic structure could also yield co-occurrence. Reference [10] argues that distri-\\nbutional similarity is not an adequate proxy for semantic relatedness.\\n2.6 Evaluation\\n35\\nTable 2.3 Datasets for evaluating word similarity/relatedness\\nDataset\\nSimilarity Type\\nRG-65 [52]\\nWord Similarity\\nWordSim-353 [22]\\nMixed\\nWordSim-353 REL [1]\\nWord Relatedness\\nWordSim-353 SIM [1]\\nWord Similarity\\nMTurk-287 [50]\\nWord Relatedness\\nSimLex-999 [31]\\nWord Similarity\\nToevaluatethewordrepresentationsystemintrinsically,themostpopularapproach\\nis to collect a set of word pairs and compute the correlation between human judg-\\nment and system output. So far, many datasets are collected and made public. Some\\ndatasets focus on the word similarity, such as RG-65 [52] and SimLex-999 [31].\\nOther datasets concern word relatedness, such as MTurk [50]. WordSim-353 [22] is\\na very popular dataset for word representation evaluation, but its annotation guideline\\ndoes not differentiate similarity and relatedness very clearly. Reference [1] conducts\\nanother round of annotation based on WordSim-353 and generates two subsets, one\\nfor similarity and the other for relatedness. Some information about these datasets is\\nsummarized in Table 2.3.\\nTo evaluate the similarity of two distributed word vectors, researchers usually\\nselect cosine similarity as an evaluation metric. The cosine similarity of word w and\\nword v is de\\xef\\xac\\x81ned as\\nsim(w, v) =\\nw \\xc2\\xb7 v\\n\\xe2\\x88\\xa5w\\xe2\\x88\\xa5\\xe2\\x88\\xa5v\\xe2\\x88\\xa5.\\n(2.55)\\nWhen evaluating a word representation approach, the similarity of each word pair\\nis computed in advance using cosine similarity. After that, Spearman\\xe2\\x80\\x99s correlation\\ncoef\\xef\\xac\\x81cient \\xcf\\x81 is then used to evaluate the similarity between human annotator and\\nword representation model as\\n\\xcf\\x81 = 1 \\xe2\\x88\\x926 \\x0b d2\\ni\\nn3 \\xe2\\x88\\x92n ,\\n(2.56)\\nwhere a higher Spearman\\xe2\\x80\\x99s correlation coef\\xef\\xac\\x81cient indicates they are more similar.\\nReference [10] describes a series of methods based on WordNet to evaluate the\\nsimilarity of a pair of words. After the comparison between the traditional WordNet-\\nbased methods and distributed word representations, [1] addresses that relatedness\\nandsimilarityaretwodifferentconcerns.TheypointoutthatWordNet-basedmethods\\nperform better on similarity than on relatedness, while distributed word representa-\\ntion shows similar performance on both. A series of distributed word representations\\nare compared on a wide variety of datasets in [56]. The state-of-the-art on both\\nsimilarity and relatedness is achieved by distributed representation, without a doubt.\\n36\\n2\\nWord Representation\\nThis evaluation method is simple and straightforward. However, as stated in [20],\\nthere are several problems with this evaluation. Since the datasets are small (less than\\n1,000 word pairs in each dataset), one system may yield many different scores on\\ndifferent partitions. Testing on the whole dataset makes it easier to over\\xef\\xac\\x81t and hard\\nto compute the statistical signi\\xef\\xac\\x81cance. Moreover, the performance of a system on\\nthese datasets may not be very correlated to its performance on downstream tasks.\\nThe word similarity measurement can come in an alternative format, the TOEFL\\nsynonyms test. In this test, a cue word is given, and the test is required to choose one\\nfrom four words that are the synonym of the cue word. The exciting part of this task is\\nthat the performance of a system could be compared with human beings. Reference\\n[37] evaluates the system with the TOEFL synonyms test to address the knowledge\\ninquiring and representing of LSA. The reported score is 64.4%, which is very close\\nto the average rating of the human test-takers. On this test set with 80 queries, [54]\\nreported a score of 72.0%. Reference [24] extends the original dataset with the help\\nof WordNet and generates a new dataset3 (named WordNet-based synonymy test)\\ncontaining thousands of queries.\\n2.6.2\\nWord Analogy\\nBesides word similarity, the word analogy task is an alternative way to measure\\nhow well representations capture semantic meanings of words. This task gives three\\nwords w1, w2, and w3, then it requires the system to predict a word w4 such that\\nthe relation between w1 and w2 is the same as that between w3 and w4. This task is\\nused since [43, 45] to exploit the structural relationships among words. Here, the\\nword relations could be divided into two categories, including semantic relations\\nand syntactic relations. This is a relatively novel method for word representation\\nevaluation but quickly becomes a standard evaluation metric since the dataset is\\nreleased. Unlike the TOEFL synonyms test, most words in this dataset are frequent\\nacrossallkindsofthecorpus,butthefourthwordischosenfromthewholevocabulary\\ninstead of four options. This test favors distributed word representations because it\\nemphasizes the structure of word space.\\nThe comparison between different models on the word analogy task measured by\\naccuracy could be found in [7, 56, 57, 61].\\n2.7\\nSummary\\nIn this chapter, we \\xef\\xac\\x81rst introduce word representation methods, including one-hot\\nrepresentation and various distributed representation methods. These classical meth-\\nods are the important foundation of various NLP models, and meanwhile present the\\n3http://www.cs.cmu.edu/~dayne/wbst-nanews.tar.gz.\\n2.7 Summary\\n37\\nmajor concepts and mechanisms of word representation learning for the reader. Next,\\nconsidering classical word representation methods often suffer from the word poly-\\nsemy, we further introduce the effective contextualized word representation methods\\nELMo, to show the approach to capture complex word features across different\\nlinguistic contexts. As word representation methods are widely utilized in various\\ndownstream tasks, we then overview numerous extensions toward some representa-\\ntive directions and discuss how to adapt word representations for speci\\xef\\xac\\x81c scenarios.\\nFinally, we introduce several evaluation tasks of word representation, including word\\nsimilarity and word analogy, which are the basic experimental settings for research-\\ning word representation methods.\\nIn the past decade, learning methods and applications of word representation\\nhave been studied in depth. Here we recommend some surveys and books on word\\nrepresentative learning for reading:\\n\\xe2\\x80\\xa2 Erk. Vector Space Models of Word Meaning and Phrase Meaning: A Survey [18].\\n\\xe2\\x80\\xa2 Lai et al. How to Generate a Good Word Embedding [36].\\n\\xe2\\x80\\xa2 Camacho et al. From Word to Sense Embeddings: A Survey on Vector Represen-\\ntations of Meaning [11].\\n\\xe2\\x80\\xa2 Ruder et al. A Survey of Cross-lingual Word Embedding Models [53].\\n\\xe2\\x80\\xa2 Bakarov. A Survey of Word Embeddings Evaluation Methods [2].\\nIn the future, toward more effective word representation learning, some directions\\nare requiring further efforts:\\n(1) Utilizing More Knowledge. Current word representation learning models focus\\non representing words based on plain textual corpora. In fact, besides rich\\nsemantic information in text, there are also various kinds of word-related infor-\\nmation hidden in heterogeneous knowledge in the real world, such as visual\\nknowledge, factual knowledge, and commonsense knowledge. Some prelimi-\\nnary explorations have attempted [59, 60] to utilize heterogeneous knowledge\\nfor learning better word representations, and these explorations indicate that\\nutilizing more knowledge is a promising direction toward enhancing word rep-\\nresentations. There remain open problems for further explorations.\\n(2) Considering More Contexts. As shown in this chapter, those word representa-\\ntion learning methods considering contexts can achieve more expressive word\\nembeddings, which can grasp richer semantic information and further bene-\\n\\xef\\xac\\x81t downstream NLP tasks than classical distributed methods. Context-aware\\nword representations have been systematically veri\\xef\\xac\\x81ed for their effectiveness\\nin existing works [32, 48], and adopting those context-aware word representa-\\ntions has also become a necessary and mainstream operation for various NLP\\ntasks. After BERT [15] has been proposed, language models pretrained on large-\\nscale corpora have entered the public vision and their \\xef\\xac\\x81ne-tuning models have\\nalso achieved the state-of-the-art performance on speci\\xef\\xac\\x81c NLP tasks. These new\\nexplorations based on large-scale textual corpora and pretrained \\xef\\xac\\x81ne-tuning lan-\\nguage representation architectures indicate a promising direction to consider\\nmore contexts with more powerful representation architectures, and we will dis-\\ncuss them more in the next chapter.\\n38\\n2\\nWord Representation\\n(3) Orienting Finer Granularity. Polysemy is a widespread phenomenon for\\nwords. Hence, it is essential and meaningful to consider the \\xef\\xac\\x81ner granulated\\nsemantic information than the words themselves. As some linguistic knowledge\\nbases have been developed, such as synonym-based knowledge bases Word-\\nNet [21] and sememe-based knowledge bases HowNet [17], we thus have ways\\nto study the atomic semantics of words. The current work on word representa-\\ntions learning is coarse-grained, and mainly focuses on shallow semantics of the\\nwords themselves in text, and ignores the rich semantic information inside the\\nwords, which is also an important resource for achieving better word embed-\\ndings. Reference [28] explores to inject \\xef\\xac\\x81ner granulated atomic semantics of\\nwords into word representations and performs much better language understand-\\ning. Although these explorations are still preliminary, orienting \\xef\\xac\\x81ner granularity\\nof word representations is important. In the next chapter, we will also introduce\\nmore details in this part.\\nIn the past decade, learning methods and applications of distributed representation\\nhave been studied in depth. Because of its ef\\xef\\xac\\x81ciency and effectiveness, lots of task-\\nspeci\\xef\\xac\\x81c models have been proposed for various tasks. Word representation learning\\nhas become a popular and important topic in NLP. However, word representation\\nlearning is still challenging due to its ambiguity, data sparsity, and interpretability. In\\nrecent years, word representation learning has been no longer studied in isolation, but\\nexplored together with sentence or document representation learning using pretrained\\nlanguage models. Readers are recommended to refer to the following chapters to\\nfurther learn the integration of word representations in other scenarios.\\nReferences\\n1. Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pa\\xc2\\xb8sca, and Aitor Soroa.\\nA study on similarity and relatedness using distributional and wordnet-based approaches. In\\nProceedings of HLT-NAACL, 2009.\\n2. Amir\\nBakarov.\\nA\\nsurvey\\nof\\nword\\nembeddings\\nevaluation\\nmethods.\\narXiv\\npreprint arXiv:1801.09536, 2018.\\n3. Collin F Baker, Charles J Fillmore, and John B Lowe. The berkeley framenet project. In\\nProceedings of ACL, 1998.\\n4. Robert Bamler and Stephan Mandt. Dynamic word embeddings via skip-gram \\xef\\xac\\x81ltering. arXiv\\npreprint arXiv:1702.08359, 2017.\\n5. Arindam Banerjee, Inderjit S Dhillon, Joydeep Ghosh, and Suvrit Sra. Clustering on the unit\\nhypersphere using von mises-\\xef\\xac\\x81sher distributions. Journal of Machine Learning Research, 2005.\\n6. Oren Barkan. Bayesian neural word embedding. In Proceedings of AAAI, 2017.\\n7. Marco Baroni, Georgiana Dinu, and Germ\\xc3\\xa1n Kruszewski. Dont count, predict a systematic\\ncomparison of context-counting vs. context-predicting semantic vectors. In Proceedings of\\nACL, 2014.\\n8. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors\\nwith subword information. Transactions of the Association for Computational Linguistics,\\n5:135\\xe2\\x80\\x93146, 2017.\\n9. Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai.\\nClass-based n-gram models of natural language. Computational linguistics, 18(4):467\\xe2\\x80\\x93479,\\n1992.\\nReferences\\n39\\n10. AlexanderBudanitskyandGraeme Hirst.Evaluatingwordnet-basedmeasuresoflexical seman-\\ntic relatedness. Computational Linguistics, 32(1):13\\xe2\\x80\\x9347, 2006.\\n11. Jose Camacho-Collados and Mohammad Taher Pilehvar. From word to sense embeddings:\\nA survey on vector representations of meaning. Journal of Arti\\xef\\xac\\x81cial Intelligence Research,\\n63:743\\xe2\\x80\\x93788, 2018.\\n12. Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. A uni\\xef\\xac\\x81ed model for word sense representation\\nand disambiguation. In Proceedings of EMNLP, 2014.\\n13. Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huanbo Luan. Joint learning of\\ncharacter and word embeddings. In Proceedings of IJCAI, 2015.\\n14. Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A.\\nHarshman. Indexing by latent semantic analysis. Japan Analytical & Scienti\\xef\\xac\\x81c Instruments\\nShow, 41(6):391\\xe2\\x80\\x93407, 1990.\\n15. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019.\\n16. Zhendong Dong and Qiang Dong. Hownet-a hybrid language and knowledge resource. In\\nProceedings of NLP-KE, 2003.\\n17. Zhendong Dong and Qiang Dong. HowNet and the Computation of Meaning (With CD-Rom).\\nWorld Scienti\\xef\\xac\\x81c, 2006.\\n18. Katrin Erk. Vector space models of word meaning and phrase meaning: A survey. Language\\nand Linguistics Compass, 6(10):635\\xe2\\x80\\x93653, 2012.\\n19. Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A\\nSmith. Retro\\xef\\xac\\x81tting word vectors to semantic lexicons. In Proceedings of NAACL-HLT, 2015.\\n20. ManaalFaruqui,YuliaTsvetkov,PushpendreRastogi,andChrisDyer.Problemswithevaluation\\nof word embeddings using word similarity tasks. In Proceedings of RepEval, 2016.\\n21. Christiane Fellbaum. Wordnet. The encyclopedia of applied linguistics, 2012.\\n22. Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman,\\nand Eytan Ruppin. Placing search in context: The concept revisited. In Proceedings of WWW,\\n2001.\\n23. John R Firth. A synopsis of linguistic theory, 1930\\xe2\\x80\\x931955. 1957.\\n24. Dayne Freitag, Matthias Blume, John Byrnes, Edmond Chow, Sadik Kapadia, Richard Rohwer,\\nand Zhiqiang Wang. New experiments in distributional representations of synonymy. In Pro-\\nceedings of CoNLL, 2005.\\n25. Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu. Learning semantic\\nhierarchies via word embeddings. In Proceedings of ACL, 2014.\\n26. Alona Fyshe, Leila Wehbe, Partha Pratim Talukdar, Brian Murphy, and Tom M Mitchell. A\\ncompositional and interpretable semantic space. In Proceedings of HLT-NAACL, 2015.\\n27. Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. Ppdb: The paraphrase\\ndatabase. In Proceedings of HLT-NAACL, 2013.\\n28. Yihong Gu, Jun Yan, Hao Zhu, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Fen Lin, and Leyu\\nLin. Language modeling with sparse product of sememe experts. In Proceedings of EMNLP,\\npages 4642\\xe2\\x80\\x934651, 2018.\\n29. William L Hamilton, Jure Leskovec, and Dan Jurafsky. Diachronic word embeddings reveal\\nstatistical laws of semantic change. In Proceedings of ACL, 2016.\\n30. Zellig S Harris. Distributional structure. Word, 10(2\\xe2\\x80\\x933):146\\xe2\\x80\\x93162, 1954.\\n31. Felix Hill, Roi Reichart, and Anna Korhonen. Simlex-999: Evaluating semantic models with\\n(genuine) similarity estimation. Computational Linguistics, 2015.\\n32. Jeremy Howard and Sebastian Ruder. Universal language model \\xef\\xac\\x81ne-tuning for text classi\\xef\\xac\\x81-\\ncation. In Proceedings of ACL, pages 328\\xe2\\x80\\x93339, 2018.\\n33. Rudolph Emil Kalman et al. A new approach to linear \\xef\\xac\\x81ltering and prediction problems. Journal\\nof Basic Engineering, 82(1):35\\xe2\\x80\\x9345, 1960.\\n34. Pentti Kanerva, Jan Kristofersson, and Anders Holst. Random indexing of text samples for\\nlatent semantic analysis. In Proceedings of CogSci, 2000.\\n35. Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, and Slav Petrov. Temporal analysis of\\nlanguage through neural language models. In Proceedings of the ACL Workshop, 2014.\\n40\\n2\\nWord Representation\\n36. Siwei Lai, Kang Liu, Shizhu He, and Jun Zhao. How to generate a good word embedding.\\nIEEE Intelligent Systems, 31(6):5\\xe2\\x80\\x9314, 2016.\\n37. Thomas K Landauer and Susan T Dumais. A solution to plato\\xe2\\x80\\x99s problem: The latent seman-\\ntic analysis theory of acquisition, induction, and representation of knowledge. Psychological\\nreview, 104(2):211, 1997.\\n38. Omer Levy and Yoav Goldberg. Dependency-based word embeddings. In Proceedings of ACL,\\n2014.\\n39. Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In\\nProceedings of NeurIPS, 2014.\\n40. Shaohua Li, Tat-Seng Chua, Jun Zhu, and Chunyan Miao. Generative topic embedding: a\\ncontinuous representation of documents. In Proceedings of ACL, 2016.\\n41. Wang Ling, Chris Dyer, Alan W Black, Isabel Trancoso, Ram\\xc3\\xb3n Fermandez, Silvio Amir, Luis\\nMarujo, and Tiago Lu\\xc3\\xads. Finding function in form: Compositional character models for open\\nvocabulary word representation. In Proceedings of EMNLP, 2015.\\n42. Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Topical word embeddings. In\\nProceedings of AAAI, 2015.\\n43. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\\xef\\xac\\x81cient estimation of word\\nrepresentations in vector space. In Proceedings of ICLR, 2013.\\n44. Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploiting similarities among languages for\\nmachine translation. arXiv preprint arXiv:1309.4168, 2013.\\n45. Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space\\nword representations. In Proceedings of HLT-NAACL, 2013.\\n46. George A Miller. Wordnet: a lexical database for english. Communications of the ACM,\\n38(11):39\\xe2\\x80\\x9341, 1995.\\n47. Yilin Niu, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Improved word representation learn-\\ning with sememes. In Proceedings of ACL, 2017.\\n48. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\\nand Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of NAACL-\\nHLT, pages 2227\\xe2\\x80\\x932237, 2018.\\n49. Martin F Porter. An algorithm for suf\\xef\\xac\\x81x stripping. Program, 14(3):130\\xe2\\x80\\x93137, 1980.\\n50. Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. A word at a\\ntime: computing word relatedness using temporal semantic analysis. In Proceedings of WWW,\\n2011.\\n51. Joseph Reisinger and Raymond J Mooney. Multi-prototype vector-space models of word mean-\\ning. In Proceedings of HLT-NAACL, 2010.\\n52. Herbert Rubenstein and John B Goodenough. Contextual correlates of synonymy. Communi-\\ncations of the ACM, 8(10):627\\xe2\\x80\\x93633, 1965.\\n53. Sebastian Ruder, Ivan Vuli\\xc2\\xb4c, and Anders S\\xc3\\xb8gaard. A survey of cross-lingual word embedding\\nmodels. Journal of Arti\\xef\\xac\\x81cial Intelligence Research, 65:569\\xe2\\x80\\x93631, 2019.\\n54. Magnus Sahlgren. Vector-based semantic analysis: Representing word meanings based on\\nrandom labels. In Proceedings of Workshop on SKAC, 2001.\\n55. Magnus Sahlgren. An introduction to random indexing. In Proceedings of TKE, 2005.\\n56. Tobias Schnabel, Igor Labutov, David Mimno, and Thorsten Joachims. Evaluation methods for\\nunsupervised word embeddings. In Proceedings of EMNLP, 2015.\\n57. Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu, and Xueqi Cheng. Learning word representations\\nby jointly modeling syntagmatic and paradigmatic relations. In Proceedings of ACL, 2015.\\n58. Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. Learning sentiment-\\nspeci\\xef\\xac\\x81c word embedding for twitter sentiment classi\\xef\\xac\\x81cation. In Proceedings of ACL, 2014.\\n59. KristinaToutanova,DanqiChen,PatrickPantel,HoifungPoon,PallaviChoudhury,andMichael\\nGamon. Representing text for joint embedding of text and knowledge bases. In Proceedings of\\nthe EMNLP, pages 1499\\xe2\\x80\\x931509, 2015.\\n60. Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph and text jointly\\nembedding. In Proceedings of EMNLP, pages 1591\\xe2\\x80\\x931601, 2014.\\nReferences\\n41\\n61. DaniYogatama,ManaalFaruqui,ChrisDyer,andNoahASmith.Learningwordrepresentations\\nwith hierarchical sparse coding. In Proceedings of ICML, 2015.\\n62. Mo Yu and Mark Dredze. Improving lexical embeddings with semantic knowledge. In Pro-\\nceedings of ACL, 2014.\\n63. Meng Zhang, Haoruo Peng, Yang Liu, Huan-Bo Luan, and Maosong Sun. Bilingual lexicon\\ninduction from non-parallel data with minimal supervision. In Proceedings of AAAI, 2017.\\n64. Will Y Zou, Richard Socher, Daniel M Cer, and Christopher D Manning. Bilingual word\\nembeddings for phrase-based machine translation. In Proceedings of EMNLP, 2013.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 3\\nCompositional Semantics\\nAbstract Many important applications in NLP \\xef\\xac\\x81elds rely on understanding more\\ncomplex language units such as phrases, sentences, and documents beyond words.\\nTherefore, compositional semantics has remained a core task in NLP. In this chapter,\\nwe \\xef\\xac\\x81rst introduce various models for binary semantic composition, including additive\\nmodels and multiplicative models. After that, we present various typical models for\\nN-ary semantic composition including recurrent neural network, recursive neural\\nnetwork, and convolutional neural network.\\n3.1\\nIntroduction\\nFrom the previous chapter, following the distributed hypothesis, one could project\\nthe semantic meaning of a word into a low-dimensional real-valued vector according\\nto its context information, which is named as word vectors. Here comes a further\\nproblem: how to compress a higher semantic unit into a vector or other kinds of\\nmathematical representations like a matrix or a tensor. In other words, using repre-\\nsentation learning to model a semantic composition function remains an unsolved\\nbut surging research topic recently.\\nCompositionality enables natural languages to construct complex semantic mean-\\nings from the combinations of simpler semantic elements. This property is often\\ncaptured with the following principle: the semantic meaning of a whole is a function\\nof the semantic meanings of its several parts. Therefore, the semantic meanings of\\ncomplex structures will depend on how their semantic elements combine.\\nHere we express the composition of two semantic units, which are denoted as\\nu and v, respectively, and the most intuitive way to de\\xef\\xac\\x81ne the joint representation\\ncould be formulated as follows:\\np = f (u, v),\\n(3.1)\\nwhere p corresponds to the representation of the joint semantic unit (u, v). It should\\nbe noted that here u and v could denote words, phrases, sentences, paragraphs, or\\neven higher level semantic units.\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_3\\n43\\n44\\n3\\nCompositional Semantics\\nHowever, given the representations of two semantic constituents, it is not enough\\nto derive their joint embeddings with the lack of syntactic information. For instance,\\nalthough the phrase machine learning and learning machine have the\\nsame vocabulary, they contain different meanings: machine learning refers to\\na research \\xef\\xac\\x81eld in arti\\xef\\xac\\x81cial intelligence while learning machine means some\\nspeci\\xef\\xac\\x81c learning algorithms. This phenomenon stresses the importance of syntactic\\nand order information in a compositional sentence. Reference [12] takes the role of\\nsyntactic and order information into consideration and suggests a further re\\xef\\xac\\x81nement\\nof the above principle: the meaning of a whole is a function of the meaning of its\\nseveral parts and the way they are syntactically combined. Therefore, the composition\\nfunction in Eq. (3.1) is rede\\xef\\xac\\x81ned to combine the syntactic relationship rule R between\\nthe semantic units u and v:\\np = f (u, v, R),\\n(3.2)\\nwhere R denotes the syntactic relationship rule between two constituent semantic\\nunits.\\nUnfortunately, even this formulation may not be fully adequate. Therefore, [7]\\nclaims that the meaning of a whole is greater than the meanings of its several\\nparts. It implies that people may suffer from the problem of constructing com-\\nplex meanings rather than simply understanding the meanings of several parts and\\ntheir syntactic relations. In real language composition, in different contexts, the\\nsame sentence could have different meanings, which means that some sentences\\nare hard to understand without any background information. For example, the\\nsentence Tom and Jerry is one of the most popular comedies\\nin that style. needs two main backgrounds: Firstly, Tom and Jerry is\\na special noun phrase or knowledge entity which indicates a cartoon comedy, rather\\nthantwoordinarypeople.Theotherpriorknowledgeshouldbethat style,which\\nneeds further explanation in the previous sentences. Hence, a full understanding of\\nthe compositional semantics needs to take existing knowledge into account. Here,\\nthe argument K is added into the composition function, incorporating knowledge\\ninformation as a prior in the compositional process:\\np = f (u, v, R, K ),\\n(3.3)\\nwhere K represents the background knowledge.\\nReference [4] claims that we should ask for the meaning of a word in isolation but\\nonly in the context of a statement. That is, the meaning of a whole is constructed from\\nits parts, and the meanings of the parts are meanwhile derived from the whole. More-\\nover, compositionality is a matter of degree rather than a binary notion. Linguistic\\nstructures range from fully compositional (e.g., black hair), to partly compositional\\nsyntactically \\xef\\xac\\x81xed expressions, (e.g., take advantage), in which the constituents can\\nstill be assigned separate meanings, and non-compositional idioms (e.g., kick the\\nbucket) or multi-word expressions (e.g., by and large), whose meaning cannot be\\ndistributed across their constituents [11].\\n3.1 Introduction\\n45\\nFrom the above three equations formulating composition function, it could be\\nconcluded that composition could be viewed as a speci\\xef\\xac\\x81c binary operation but\\nbeyond this. The syntactic message could help to indicate a particular approach while\\nbackground knowledge helps to explain some obscure words or speci\\xef\\xac\\x81c context-\\ndependent entities such as pronouns. Beyond binary compositional operations, one\\ncould build the sentence-level composition by applying binary composition oper-\\nations recursively. In this chapter, we will \\xef\\xac\\x81rst explain some sorts of basic binary\\ncomposition functions in both the semantic vector space and matrix-vector space.\\nAfter, we will climb up to more complex composition scenarios and introduce several\\napproaches to model sentence-level composition.\\n3.2\\nSemantic Space\\n3.2.1\\nVector Space\\nIn general, the central task in semantic representation is projecting words from an\\nabstract semantic space to a mathematical low-dimensional space. As introduced in\\nthe previous chapters, to make the transformation reasonable, the purpose is to main-\\ntain the word similarity in this new projected space. In other words, the more similar\\nthe words are, the closer their vectors should be. For instance, we hope the word\\nvectors w(book) and w(magazine) are close while the word vectors w(apple) and\\nw(computer) are far away. In this chapter, we will introduce several widely used\\ntypical semantic vector space including one-hot representation, distributed represen-\\ntation, and distributional representation.\\n3.2.2\\nMatrix-Vector Space\\nDespite the wide use of semantic vector spaces, an alternative semantic space is\\nproposed to be a more powerful and general compositional semantic framework.\\nDifferent from conventional vector spaces, matrix-vector semantic space utilizes a\\nmatrix to represent the word meaning rather than a skinny vector. The motivation\\nbehind this is when modeling the semantic meaning under a speci\\xef\\xac\\x81c context, one is\\nwondering not only what is the meaning of each word, but also the holistic meaning\\nof the whole sentence. Thus, we concern about the semantic transformation between\\nadjacent words inside each sentence. However, the semantic vector space could not\\ncharacterize the semantic transformation of one word on the others explicitly.\\nDriven by the idea of modeling semantic transformation, some researchers have\\nproposed to use a matrix to represent the transformation operation of one word on the\\nothers. Different from those vector space models, it could incorporate some structural\\ninformation like the word order and syntax composition.\\n46\\n3\\nCompositional Semantics\\n3.3\\nBinary Composition\\nThe goal is to construct vector representations for phrases, sentences, paragraphs,\\nand documents. Without loss of generality, we assume that each constituent of a\\nphrase (sentence, paragraph, or document) is embedded into a vector which will\\nbe subsequently combined in some way to generate a representation vector for the\\nphrase (sentence, paragraph, or document).1\\nIn this section, we focus on binary composition. We will take phrases consisting\\nof a head and a modi\\xef\\xac\\x81er or complement as an example. If we cannot model the binary\\ncomposition (or phrase representation), there is little hope that we can construct more\\ncomplex compositional representations for sentences or even documents. Therefore,\\ngiven a phrase such as \\xe2\\x80\\x9cmachine learning\\xe2\\x80\\x9d and the vectors u and v representing the\\nconstituents \\xe2\\x80\\x9cmachine\\xe2\\x80\\x9d and \\xe2\\x80\\x9clearning\\xe2\\x80\\x9d, respectively, we aim to produce a represen-\\ntation vector p of the whole phrase. Let the hypothetical vectors for machine and\\nlearning be [0, 3, 1, 5, 2] and [1, 4, 2, 2, 0], respectively. This simpli\\xef\\xac\\x81ed seman-\\ntic space will serve to illustrate examples of the composition functions which we\\nconsider in this section.\\nThe fundamental problem of semantic composition modeling in representing a\\ntwo-word phrase is designing a primitive composition function as a binary operator.\\nBased on this function, one could apply it on a word sequence recursively and derive\\nsentence-level composition. Here a word sequence could be any level of the seman-\\ntic units, such as a phrase, a sentence, a paragraph, a knowledge entity, or even a\\ndocument.\\nFrom the previous section, one of the basic formulae is to formulate semantic\\ncomposition f in the following equation:\\np = f (u, v, R, K ),\\n(3.4)\\nwhere u, v denote the representations of the constituent parts in this semantic unit,\\np denotes the joint representation, R indicates the relationship while K indicates\\nthe necessary background knowledge. The expression de\\xef\\xac\\x81nes a wide class of com-\\nposition functions. For easier discussion, we give some appropriate constraints to\\nnarrow the space of our considering function. First, we will ignore the background\\nknowledge K to explore what can be achieved without any utilization of background\\nor world knowledge. Second, for the consideration of the syntactic relation R, we\\ncan proceed by investigating only one relation at a time. And then we can remove\\nany explicit dependence on R which allows us to explore any possible distinct com-\\nposition function for various syntactic relations. That is, we simplify the formula\\np = f (u, v) by simply ignoring the background knowledge and relationship.\\n1Note that, the problem of combining semantic vectors of small units to make a representation for a\\nmulti-word sequence is different from the problem of incorporating information about multi-word\\ncontexts into a distributional representation for a single target word.\\n3.3 Binary Composition\\n47\\nIn recent years, modeling the binary composition function is a well-studied but\\nstill challenging problem. There are mainly two perspectives toward this question,\\nincluding the additive model and the multiplicative model.\\n3.3.1\\nAdditive Model\\nThe additive model has a constraint in which it assumes that p, u, and v lie in the\\nsame semantic space. This essentially means that all syntactic types have the same\\ndimension. One of the simplest ways is to directly use the sum to represent the joint\\nrepresentation:\\np = u + v.\\n(3.5)\\nAccording to Eq. (3.5), the sum of the two vectors representing machine and\\nlearning would be w(machine) + w(learning) = [1, 7, 3, 7, 2]. It assumes that\\nthe composition of different constituents is a symmetric function of them; in other\\nwords, it does not consider the order of constituents. Although having lots of draw-\\nbacks such as lack of the ability to model word orders and absence from background\\nsyntactic or knowledge information, this approach still provides a relatively strong\\nbaseline [9].\\nTo overcome the word order issue, one easy variant is applying a weighted sum\\ninstead of uniform weights. This is to say, the composition has the following form:\\np = \\xce\\xb1u + \\xce\\xb2v,\\n(3.6)\\nwhere \\xce\\xb1 and \\xce\\xb2 correspond to different weights for two vectors. Under this setting, two\\nsequences (u, v) and (v, u) have different representations, which is consistent with\\nreal language phenomena. For example, \\xe2\\x80\\x9cmachine learning\\xe2\\x80\\x9d and \\xe2\\x80\\x9clearning machine\\xe2\\x80\\x9d\\nhave different meanings which requires different representations. In this setting, we\\ncould give greater emphasis to heads than other constituents. As an example, if we\\nset \\xce\\xb1 to 0.3 and \\xce\\xb2 to 0.7, the 0.3 \\xc3\\x97 w(machine) = [0, 0.9, 0.3, 1.5, 0.6] and 0.7 \\xc3\\x97\\nw(learning) = [0.7, 2.8, 1.4, 1.4, 0],and\\xe2\\x80\\x9cmachinelearning\\xe2\\x80\\x9disrepresentedbytheir\\naddition 0.3 \\xc3\\x97 w(machine) + 0.7 \\xc3\\x97 w(learning) = [0.7, 3.6, 1.7, 2.9, 0.6].\\nHowever, this model could not consider prior knowledge and syntax information.\\nTo incorporate prior information into the additive model, one method combines\\nnearest neighborhood semantics into composition, deriving\\np = u + v +\\nK\\n\\x02\\ni=1\\nni,\\n(3.7)\\nwhere n1, n2, . . . , nK denote all semantic neighbors of v. Therefore, this method\\ncould ensemble all synonyms of the component as a smoothing factor into com-\\nposition function, which reduces the variance of language. For example, if in\\n48\\n3\\nCompositional Semantics\\nthe composition of \\xe2\\x80\\x9cmachine\\xe2\\x80\\x9d and \\xe2\\x80\\x9clearning\\xe2\\x80\\x9d, the chosen neighbor is \\xe2\\x80\\x9coptimiz-\\ning\\xe2\\x80\\x9d, with w(optimizing) = [1, 5, 3, 2, 1], then this leads to the situation that\\nthe representation of \\xe2\\x80\\x9cmachine learning\\xe2\\x80\\x9d becomes w(machine) + w(learning) +\\nw(optimizing) = [2, 12, 6, 9, 3].\\nSince the joint representations of one additive model still lie in the same semantic\\nspace with their original component vectors, it is natural to conduct cosine similarity\\nto measure their semantic relationships. Thus, under a naive additive model, we have\\nthe following similarity equation:\\ns(p, w) =\\np \\xc2\\xb7 w\\n\\xe2\\x88\\xa5p\\xe2\\x88\\xa5\\xc2\\xb7 \\xe2\\x88\\xa5w\\xe2\\x88\\xa5=\\n(u + v)w\\n\\xe2\\x88\\xa5u + v\\xe2\\x88\\xa5\\xe2\\x88\\xa5w\\xe2\\x88\\xa5\\n(3.8)\\n=\\n\\xe2\\x88\\xa5u\\xe2\\x88\\xa5\\n\\xe2\\x88\\xa5u + v\\xe2\\x88\\xa5s(u, w) +\\n\\xe2\\x88\\xa5v\\xe2\\x88\\xa5\\n\\xe2\\x88\\xa5u + v\\xe2\\x88\\xa5s(v, w),\\n(3.9)\\nwhere w denotes any other word in the vocabulary and s indicates the similarity\\nfunction. From derivation ahead, it could be concluded that this composition function\\ncomposes both magnitude and directions of two component vectors. In other words, if\\nonevectordominatesthemagnitude,itwillalsodominatethesimilarity.Furthermore,\\nwe have\\n\\xe2\\x88\\xa5p\\xe2\\x88\\xa5= \\xe2\\x88\\xa5u + v\\xe2\\x88\\xa5\\xe2\\x89\\xa4\\xe2\\x88\\xa5u\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5v\\xe2\\x88\\xa5.\\n(3.10)\\nThis lemma suggests that the semantic unit with a deeper-rooted parsing tree could\\ndetermine the joint representation when combining with a shallow unit. Because the\\ndeeper the semantic unit is, the larger the magnitude it has.\\nMoreover, incorporating geometry insight, we can observe that the additive model\\nbuilds a more solid understanding of semantic composition. Supposing that our com-\\nponent vectors are u and v, the additive model aims to project them to x and y, where\\nx follows the direction of u while y is orthogonal to u. The following \\xef\\xac\\x81gure could\\nclearly illustrate this issue (Fig.3.1).\\nFig. 3.1 An illustration of\\nthe additive model\\nA\\nB\\nC\\nD\\ny\\nu\\nx\\nv\\n3.3 Binary Composition\\n49\\nFrom the \\xef\\xac\\x81gure, the vector x and the vector y could be represented as\\nx = u \\xc2\\xb7 v\\nu \\xc2\\xb7 u \\xc2\\xb7 u,\\ny = v \\xe2\\x88\\x92x = v \\xe2\\x88\\x92u \\xc2\\xb7 v\\nu \\xc2\\xb7 u \\xc2\\xb7 u.\\n(3.11)\\nThen, using the linear combination of these two new vectors x, y yields a new\\nadditive model:\\np = \\xce\\xb1x + \\xce\\xb2y\\n(3.12)\\n= \\xce\\xb1 u \\xc2\\xb7 v\\nu \\xc2\\xb7 u \\xc2\\xb7 u + \\xce\\xb2\\n\\x03\\nv \\xe2\\x88\\x92u \\xc2\\xb7 v\\nu \\xc2\\xb7 u \\xc2\\xb7 u\\n\\x04\\n(3.13)\\n= (\\xce\\xb1 \\xe2\\x88\\x92\\xce\\xb2) \\xc2\\xb7 u \\xc2\\xb7 v\\nu \\xc2\\xb7 u \\xc2\\xb7 u + \\xce\\xb2v.\\n(3.14)\\nFurthermore, using cosine similarity measurement, the relationship could be writ-\\nten as follows:\\ns(p, w) = |\\xce\\xb1 \\xe2\\x88\\x92\\xce\\xb2|\\n|\\xce\\xb1|\\ns(u, w) + |\\xce\\xb2|\\n|\\xce\\xb1|s(v, w).\\n(3.15)\\nFrom similarity measurement derivation, it is indicated that with this projection\\nmethod, the composition similarity could be viewed as a linear combination of the\\nsimilarities of two components, which means that combining semantic units with\\ndifferent semantic depths, the deeper one will not dominate the representation.\\n3.3.2\\nMultiplicative Model\\nThough the additive model achieves great success in semantic composition, the sim-\\npli\\xef\\xac\\x81cation it adopted may be too restrictive because it assumes all words, phrases,\\nsentences, and documents are substantially similar enough to be represented in a uni-\\n\\xef\\xac\\x81ed semantic space. Different from the additive model which regards composition as\\na simple linear transformation, the multiplicative model aims to make higher order\\ninteraction. Among all models from this perspective, the most intuitive approach\\ntried to apply the pair-wise product as a composition function approximation. In this\\nmethod, the composition function is shown as the following:\\np = u \\xe2\\x8a\\x99v,\\n(3.16)\\nwhere, pi = ui \\xc2\\xb7 vi, which implies each dimension of the output only depends on\\nthe corresponding dimension of two input vectors. However, similar to the simplest\\nadditive model, this model is also suffering from the lack of the ability to model word\\norder, and the absence from background syntactic or knowledge information.\\n50\\n3\\nCompositional Semantics\\nIn the additive model, we have p = \\xce\\xb1u + \\xce\\xb2v to alleviate the word order issue.\\nNote that here \\xce\\xb1 and \\xce\\xb2 are two scalars, which could be easily changed to two matrices.\\nTherefore, the composition function could be represented as\\np = W\\xce\\xb1 \\xc2\\xb7 u + W\\xce\\xb2 \\xc2\\xb7 v,\\n(3.17)\\nwhere W\\xce\\xb1 and W\\xce\\xb2 are matrices which determine the importance of u and v to p.\\nWith this expression, the composition could be more expressive and \\xef\\xac\\x82exible although\\nmuch harder to train.\\nGeneralizing multiplicative model ahead, another approach is to utilize tensors as\\nmultiplicative descriptors and the composition function could be viewed as\\np = \\xe2\\x88\\x92\\xe2\\x86\\x92\\nW \\xc2\\xb7 uv,\\n(3.18)\\nwhere \\xe2\\x88\\x92\\xe2\\x86\\x92\\nW denotes a 3-order tensor, i.e., the formula above could be written as\\npk = \\x05\\ni, j Wi jk \\xc2\\xb7 ui \\xc2\\xb7 v j. Hence, this model makes that each element of p could be\\nin\\xef\\xac\\x82uenced by all elements of both u and v, with a relationship of linear combination\\nby assigning each (i, j) a unique weight.\\nStarting from this simple but general baseline, some researchers proposed to\\nmake the function not symmetric to consider word order in the sequence. Paying\\nmore attention to the \\xef\\xac\\x81rst element, the composition function could be\\np = \\xe2\\x88\\x92\\xe2\\x86\\x92\\nW \\xc2\\xb7 uuv,\\n(3.19)\\nwhere \\xe2\\x88\\x92\\xe2\\x86\\x92\\nW denotes a 4-order tensor. This method could be understood as replacing\\nlinear transformation of u and v to a quadratic in u asymmetrically. So this is a variant\\nof the tensor multiplicative compositional model.\\nDifferent from expanding a simple multiplicative model to complex ones, other\\nkinds of approaches are proposed to reduce the parameter space. With the reduction\\nof parameter size, people could make compositions much more ef\\xef\\xac\\x81cient rather than\\nhave an O(n3) time complexity in the tensor-based model. Thus, some compression\\ntechniques could be applied in the original tensor model. One representative instance\\nis the circular convolution model, which could be shown as\\np = u \\xe2\\x8a\\x9bv,\\n(3.20)\\nwhere \\xe2\\x8a\\x9brepresents the circular convolution operation with the following de\\xef\\xac\\x81nition:\\npi =\\n\\x02\\nj\\nu j \\xc2\\xb7 vi\\xe2\\x88\\x92j.\\n(3.21)\\nIf we assign each pair with unique weights, the composition function will be\\npi =\\n\\x02\\nj\\nWi j \\xc2\\xb7 u j \\xc2\\xb7 vi\\xe2\\x88\\x92j.\\n(3.22)\\n3.3 Binary Composition\\n51\\nNote that the circular convolution model could be viewed as a special instance of\\na tensor-based composition model. If we write the circular convolution in the tensor\\nform, we have Wi jk = 0, where k \\xcc\\xb8= i + j. Thus, the parameter number could be\\nreduced from n3 to n2, while maintaining the interactions between each pair of\\ndimensions in the input vectors.\\nBoth in the additive and multiplicative models, the basic condition is all compo-\\nnents lie in the same semantic space as the output. Nevertheless, different modeling\\ntypes of words in different semantic spaces could bring us a different perspective.\\nFor instance, given (u, v), the multiplicative model could be reformulated as\\np = W \\xc2\\xb7 (u \\xc2\\xb7 v) = U \\xc2\\xb7 v.\\n(3.23)\\nThis implies that each left unit could be treated as an operation on the repre-\\nsentation of the right one. In other words, each remaining unit could be formulated\\nas a transformation matrix, while the right one should be represented as a seman-\\ntic vector. This argument could be meaningful, especially for some kinds of phrase\\ncompositions. Reference [2] argues that for ADJ-NOUN phrases, the joint semantic\\ninformation could be viewed as the conjunction of the semantic meanings of two\\ncomponents. Given a phrase red car, its semantic meaning is the conjunction of\\nall red things and all different kinds of cars. Thus, red could be formulated as an\\noperator on the vector of car, deriving the new semantic vector, which expressed\\nthe meaning of red car. These observations lead to another genre of semantic\\ncompositional modeling: semantic matrix-composition space.\\n3.4\\nN-Ary Composition\\nIn real-world NLP tasks, the input is usually a sequence of multiple words rather than\\njust a pair of words. Therefore, besides designing a suitable binary compositional\\noperator, the order to apply binary operations is also important. In this section, we\\nwill introduce three mainstream strategies in N-ary composition by taking language\\nmodeling as an example.\\nTo illustrate the language modeling task more clearly, the composition problem\\nto model a sentence or even a document could be formulated as\\nGiven a sentence/document consisting of a word sequence {w0, w1, w2, . . . , wn},\\nwe aim to design following functions to obtain the joint semantic representation of\\nthe whole sentence/document:\\n1. A semantic representation method like semantic vector space or compositional\\nmatrix space.\\n2. A binary compositional operation function f (u, v) like we introduced in the pre-\\nvious sections. Here the input u and v denote the representations of two constitute\\nsemantic units, while the output is also the representation in the same space.\\n52\\n3\\nCompositional Semantics\\n3. A sequential order to apply the binary function in step 2. To describe in detail,\\nwe could use a bracket to identify the order to apply the composition function.\\nFor instance, we could use ((w1, w2), w3) to represent the sequential order from\\nbeginning to end.\\nIn this section, we will introduce several systematic strategies to model sentence\\nsemantics by describing the solutions for the three problems above. We will classify\\nthe methods by word-level order: sequential order, recursive order (following parsing\\ntrees), and convolution order.\\n3.4.1\\nRecurrent Neural Network\\nTo design orders to apply binary compositional functions, the most intuitive method\\nis utilizing sequentiality. Namely, the sequence order should be sn = (sn\\xe2\\x88\\x921, wn),\\nwhere sn\\xe2\\x88\\x921 is the order of the \\xef\\xac\\x81rst n \\xe2\\x88\\x921 words. Motivated by this thought, the neural\\nnetwork model used is the Recurrent Neural Network (RNN).\\nAn RNN applies the composition function sequentially and derives the represen-\\ntations of hidden semantic units. Based on these hidden semantic units, we could\\nuse them on some speci\\xef\\xac\\x81c NLP tasks like sentiment analysis or text classi\\xef\\xac\\x81cation.\\nAlso, note that the basic RNN only utilizes the sequential information from head to\\ntail of a sentence/document. To improve its representation ability, the RNN could\\nbe enhanced as bi-directional RNN by considering sequential and reverse-sequential\\ninformation.\\nAfter deciding sequential order to model sentence-level semantics, the next ques-\\ntion is determining the binary composition functions. In detail, supposing that ht\\ndenotes the representation of the \\xef\\xac\\x81rst t words and wt represents the tth word, the\\ngeneral composition could be formulated as\\nht = f (ht\\xe2\\x88\\x921, xt),\\n(3.24)\\nwhere f is a well-designed binary composition function.\\nFrom the de\\xef\\xac\\x81nition of the RNN, the composition function could be formulated as\\nfollows:\\nht = tanh(W1ht\\xe2\\x88\\x921 + W2wt),\\n(3.25)\\nwhere W1 and W2 are two weighted matrices.\\nWe could see that here we use a matrix-weighted summation to represent binary\\nsemantic composition:\\np = W\\xce\\xb1u + W\\xce\\xb2v.\\n(3.26)\\nLSTM. Since the raw RNN only utilizes the simple tangent function, it is hard\\nto obtain the long-term dependency of a long sentence/document. Reference [5]\\nreinvents Long Short-Term Memory (LSTM) networks to strengthen the ability to\\n3.4 N-Ary Composition\\n53\\nmodel long-term semantic dependency in RNN. In detail, the composition function of\\nthe LSTM allows information from previous layers to \\xef\\xac\\x82ow directly to their following\\nlayers. The composition function could be de\\xef\\xac\\x81ned as\\nft = Sigmoid(Wh\\nf ht\\xe2\\x88\\x921 + Wx\\nf xt + b f ),\\n(3.27)\\nit = Sigmoid(Wh\\ni ht\\xe2\\x88\\x921 + Wx\\ni xt + bi),\\n(3.28)\\not = Sigmoid(Wh\\noht\\xe2\\x88\\x921 + Wx\\noxt + bo),\\n(3.29)\\n\\xcb\\x86ct = tanh(Wh\\ncht\\xe2\\x88\\x921 + Wx\\ncxt + bc),\\n(3.30)\\nct = ft \\xe2\\x8a\\x99ct\\xe2\\x88\\x921 + it \\xe2\\x8a\\x99\\xcb\\x86ct,\\n(3.31)\\nht = ot \\xe2\\x8a\\x99ct.\\n(3.32)\\nVariants of LSTM. To simplify LSTM and obtain more ef\\xef\\xac\\x81cient algorithms,\\n[3] proposes to utilize a simple but comparable RNN architecture, named Gated\\nRecurrent Unit (GRU). Compared with LSTM, GRU has fewer parameters, which\\nbring higher ef\\xef\\xac\\x81ciency. The composition function is showed as\\nzt = Sigmoid(Wh\\nz ht\\xe2\\x88\\x921 + Wx\\nz xt + bz),\\n(3.33)\\nrt = Sigmoid(Wh\\nr ht\\xe2\\x88\\x921 + Wx\\nr xt + br),\\n(3.34)\\n\\xcb\\x86ht = tanh(Wh(rt \\xe2\\x8a\\x99ht\\xe2\\x88\\x921) + Wx\\nhxt + bh),\\n(3.35)\\nht = (1 \\xe2\\x88\\x92zt) \\xe2\\x8a\\x99ht\\xe2\\x88\\x921 + zt \\xe2\\x8a\\x99\\xcb\\x86ht.\\n(3.36)\\n3.4.2\\nRecursive Neural Network\\nBesides the recurrent neural network, another strategy to apply binary compositional\\nfunction follows a parsing tree instead of sequential word order. Based on this philos-\\nophy, [15] proposes a recursive neural network to model different levels of semantic\\nunits. In this subsection, we will introduce some algorithms following the recursive\\nparsing tree with different binary compositional functions.\\nSince all the recursive neural networks are binary trees, the basic problem we need\\nto consider is how to derive the representation of the father component on the tree\\ngiven its two children semantic components. Reference [15] proposes a recursive\\nmatrix-vector model (MV-RNN) which captures constituent parsing tree structure\\ninformation by assigning a matrix-vector representation for each constituent. The\\nvector captures the meaning of the constituent itself, and the matrix represents how\\nit modi\\xef\\xac\\x81es the meaning of the word it combines with. Suppose we have two children\\ncomponents a, b and their father component p, the composition can be formulated\\nas follows:\\n54\\n3\\nCompositional Semantics\\np = fvec(a, b) = g\\n\\x06\\nW1\\n\\x07Ba\\nAb\\n\\x08\\t\\n,\\n(3.37)\\nP = fmatrix(a, b) = W2\\n\\x07A\\nB\\n\\x08\\n,\\n(3.38)\\nwhere a, b, p are the embedding vectors for each component and A, B, P are the\\nmatrices, W1 is a matrix that maps the transformed words into another semantic\\nspace, the element-wise function g is an activation function, and W2 is a matrix that\\nmaps the two matrices into one combined matrix P with the same dimension. The\\nwhole process is illustrated in Fig. 3.2. And then MV-RNN selects the highest node\\nof the path in the parse tree between the two target entities to represent the input\\nsentence.\\nIn fact, the composition operation used in the above recursive network is similar\\nto an RNN unit introduced in the previous subsection. And the RNN unit here can\\nbe replaced by LSTM units or GRU units. Reference [16] proposes two types of\\ntree-structured LSTMs including the Child-Sum Tree-LSTM and the N-ary Tree-\\nLSTM to capture constituent or dependency parsing tree structure information. For\\nthe Child-Sum Tree-LSTM, given a tree, let C(t) denote the children set of the node\\nt. Its transition equations are de\\xef\\xac\\x81ned as follows:\\nvery\\nf(Ba,Ab)=\\nBa=\\nAb=\\nbeautiful\\ngirl\\n(a,A)\\n(b,B)\\n(c,C)\\n...\\n...\\n...\\nFig. 3.2 The architecture of the matrix-vector recursive encoder\\n3.4 N-Ary Composition\\n55\\n\\xcb\\x86ht =\\n\\x02\\nk\\xe2\\x88\\x88C(t)\\nhk,\\n(3.39)\\nit = Sigmoid(W(i)wt + Ui \\xcb\\x86ht + b(i)),\\n(3.40)\\nftk = Sigmoid(W( f )wt + U f \\xcb\\x86hk + b( f )) (k \\xe2\\x88\\x88C(t)),\\n(3.41)\\not = Sigmoid(W(o)wt + Uo \\xcb\\x86ht + b(o)),\\n(3.42)\\nut = tanh(W(u)wt + Uu \\xcb\\x86ht + b(u)),\\n(3.43)\\nct = it \\xe2\\x8a\\x99ut +\\n\\x02\\nk\\xe2\\x88\\x88C(t)\\nftk \\xe2\\x8a\\x99ct\\xe2\\x88\\x921,\\n(3.44)\\nht = ot \\xe2\\x8a\\x99tanh(ct).\\n(3.45)\\nThe N-ary Tree-LSTM has similar transition equations as the Child-Sum Tree-\\nLSTM. The only difference is that it limits the tree structures to have at most N\\nbranches.\\n3.4.3\\nConvolutional Neural Network\\nReference [6] proposes to embed an input sentence using a Convolutional Neural\\nNetwork (CNN) which extracts local features by a convolution layer and combines\\nall local features via a max-pooling operation to obtain a \\xef\\xac\\x81xed-sized vector for the\\ninput sentence.\\nFormally, the convolution operation is de\\xef\\xac\\x81ned as a matrix multiplication between\\na sequence of vectors, a convolution matrix W, and a bias vector b with a sliding\\nwindow. Let us de\\xef\\xac\\x81ne the vector qi as the concatenation of the subsequence of input\\nrepresentations in the ith window, we have\\nh j = max\\ni\\n[ f (Wqi + b)] j,\\n(3.46)\\nwhere f indicates a nonlinear function such as sigmoid or tangent function, and h\\nindicates the \\xef\\xac\\x81nal representation of the sentence.\\n3.5\\nSummary\\nIn this chapter, we \\xef\\xac\\x81rst introduce the semantic space for compositional semantics.\\nAfterwards, we take phrase representation as an example to introduce various models\\nfor binary semantic composition, including additive models and multiplicative mod-\\nels. Finally, we introduce typical models for N-ary semantic composition including\\nrecurrent neural network, recursive neural network, and convolutional neural net-\\nwork. Compositional semantics allows languages to construct complex meanings\\nfrom the combinations of simpler elements, and its binary semantic composition\\n56\\n3\\nCompositional Semantics\\nand N-ary semantic composition is the foundation of multiple NLP tasks including\\nsentence representation, document representation, relational path representation, etc.\\nWe will give a detailed introduction to these scenarios in the following chapters.\\nFor further understanding of compositional semantics, there are also some rec-\\nommended surveys and books:\\n\\xe2\\x80\\xa2 Pelletier et al., The principle of semantic compositionality [13].\\n\\xe2\\x80\\xa2 Jeff et al., Composition in distributional models of semantics [10].\\nFor better modeling compositional semantics, some directions require further\\nefforts in the future:\\n(1) Neurobiology-inspired Compositional Semantics. What is the neurobiology\\nfor dealing with compositional semantics in human language? Recently, [14]\\n\\xef\\xac\\x81nds that the human combinatory system is related to rapidly peaking activity\\nin the left anterior temporal lobe and later engagement of the medial prefrontal\\ncortex. The analysis of how language builds meaning and lays out directions\\nin neurobiological research may bring some instructive reference for modeling\\ncompositional semantics in representation learning. It is valuable to design novel\\ncompositional forms inspired by recent neurobiological advances.\\n(2) Combination of Symbolic and Distributed Representation. Human language\\nis inherently a discrete symbolic representation of knowledge. However, we\\nrepresent the semantics of discrete symbols with distributed/distributional rep-\\nresentations when dealing with natural language in deep learning. Recently, there\\nare some approaches such as neural module networks [1] and neural symbolic\\nmachine [8] attempting to consider discrete symbols in neural networks. How\\nto take advantage of these symbolic neural models to represent the composition\\nof semantics is an open problem to be explored.\\nReferences\\n1. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In\\nProceedings of CVPR, pages 39\\xe2\\x80\\x9348, 2016.\\n2. MarcoBaroniandRobertoZamparelli.Nounsarevectors,adjectivesarematrices:Representing\\nadjective-noun constructions in semantic space. In Proceedings of EMNLP, 2010.\\n3. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Gated feedback\\nrecurrent neural networks. In Proceedings of ICML, 2015.\\n4. Gottlob Frege. Die grundlagen derarithmetik. Eine logisch mathematische Untersuchung u\\xe2\\x80\\x99ber\\nden Begrijfder Zahl. Breslau: Koebner, 1884.\\n5. Sepp Hochreiter and J\\xc3\\xbcrgen Schmidhuber. Long short-term memory. Neural Computation,\\n9(8):1735\\xe2\\x80\\x931780, 1997.\\n6. Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network\\nfor modelling sentences. In Proceedings of ACL, 2014.\\n7. George Lakoff. Linguistic gestalts. In Proceedings of ILGISA, 1977.\\n8. Chen Liang, Jonathan Berant, Quoc Le, Kenneth Forbus, and Ni Lao. Neural symbolic\\nmachines: Learning semantic parsers on freebase with weak supervision. In Proceedings of\\nACL, pages 23\\xe2\\x80\\x9333, 2017.\\nReferences\\n57\\n9. JeffMitchell andMirella Lapata.Vector-basedmodelsofsemantic composition.In Proceedings\\nof ACL, 2008.\\n10. Jeff Mitchell and Mirella Lapata. Composition in distributional models of semantics. Cognitive\\nscience, 34(8):1388\\xe2\\x80\\x931429, 2010.\\n11. Geoffrey Nunberg, Ivan A Sag, and Thomas Wasow. Idioms. Language, pages 491\\xe2\\x80\\x93538, 1994.\\n12. Barbara Partee. Lexical semantics and compositionality. An Invitation to Cognitive Science:\\nLanguage, 1:311\\xe2\\x80\\x93360, 1995.\\n13. Francis Jeffry Pelletier. The principle of semantic compositionality. Topoi, 13(1):11\\xe2\\x80\\x9324, 1994.\\n14. Liina Pylkk\\xc3\\xa4nen. The neural basis of combinatory syntax and semantics. Science,\\n366(6461):62\\xe2\\x80\\x9366, 2019.\\n15. Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. Semantic compo-\\nsitionality through recursive matrix-vector spaces. In Proceedings of EMNLP, 2012.\\n16. Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representa-\\ntions from tree-structured long short-term memory networks. In Proceedings of ACL, 2015.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 4\\nSentence Representation\\nAbstract Sentence is an important linguistic unit of natural language. Sentence Rep-\\nresentation has remained as a core task in natural language processing, because many\\nimportant applications in related \\xef\\xac\\x81elds lie on understanding sentences, for example,\\nsummarization, machine translation, sentiment analysis, and dialogue system. Sen-\\ntence representation aims to encode the semantic information into a real-valued rep-\\nresentation vector, which will be utilized in further sentence classi\\xef\\xac\\x81cation or match-\\ning tasks. With large-scale text data available on the Internet and recent advances\\non deep neural networks, researchers tend to employ neural networks (e.g., con-\\nvolutional neural networks and recurrent neural networks) to learn low-dimensional\\nsentence representations and achieve great progress on relevant tasks. In this chapter,\\nwe \\xef\\xac\\x81rst introduce the one-hot representation for sentences and the n-gram sentence\\nrepresentation (i.e., probabilistic language model). Then we extensively introduce\\nneural-based models for sentence modeling, including feedforward neural network,\\nconvolutional neural network, recurrent neural network, and the latest Transformer,\\nand pre-trained language models. Finally, we introduce several typical applications\\nof sentence representations.\\n4.1\\nIntroduction\\nNatural language sentences consist of words or phrases, follow grammatical rules,\\nand convey complete semantic information. Compared with words and phrases, sen-\\ntences have more complex structures, including both sequential and hierarchical\\nstructures, which are essential for understanding sentences. In NLP, how to rep-\\nresent sentences is critical for related applications, such as sentence classi\\xef\\xac\\x81cation,\\nsentiment analysis, sentence matching, and so on.\\nBefore deep learning took off, sentences were usually represented as one-hot vec-\\ntors or TF-IDF vectors, following the assumption of bag-of-words. In this case, a\\nsentence is represented as a vocabulary-sized vector, in which each element repre-\\nsents the importance of a speci\\xef\\xac\\x81c word (either term frequency or TF-IDF) to the\\nsentence. However, this method confronts two issues. Firstly, the dimension of such\\nrepresentation vectors is usually up to thousands or millions. Thus, they usually face\\nsparsity problem and bring in computational ef\\xef\\xac\\x81ciency problem. Secondly, such a\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_4\\n59\\n60\\n4\\nSentence Representation\\nrepresentation method follows the bag-of-words assumption and ignores the sequen-\\ntial and structural information, which can be crucial for understanding the semantic\\nmeanings of sentences.\\nInspired by recent advances of deep learning models in computer vision and\\nspeech, researchers proposed to model sentences with deep neural networks, such as\\nconvolutional neural network, recurrent neural network, and so on. Compared with\\nconventional word frequency-based sentence representations, deep neural networks\\ncan capture the internal structures of sentences, e.g., sequential and dependency\\ninformation, through convolutional or recurrent operations. Thus, neural network-\\nbased sentence representations have achieved great success in sentence modeling\\nand NLP tasks.\\n4.2\\nOne-Hot Sentence Representation\\nOne-hot representation is the most simple and straightforward method for word rep-\\nresentation tasks. This method represents each word with a \\xef\\xac\\x81xed length binary vector.\\nSpeci\\xef\\xac\\x81cally, for a vocabulary V = {w1, w2, . . . , w|V |}, the one-hot representation of\\nword w is w = [0, . . . , 0, 1, 0, . . . , 0]. Based on the one-hot word representation and\\nthe vocabulary, it can be extended to represent a sentence s = {w1, w2, . . . , wl} as\\ns =\\nl\\x02\\nk=1\\nwi,\\n(4.1)\\nwhere l indicates the length of the sentence s. The sentence representation s is the\\nsum of the one-hot representations of n words within the sentence, i.e., each element\\nin s represents the Term Frequency (TF) of the corresponding word.\\nMoreover, researchers usually take the importance of different words into consid-\\neration, rather than treat all the words equally. For example, the function words such\\nas \\xe2\\x80\\x9ca\\xe2\\x80\\x9d, \\xe2\\x80\\x9can\\xe2\\x80\\x9d, and \\xe2\\x80\\x9cthe\\xe2\\x80\\x9d usually appear in different sentences, and reserve little mean-\\nings. Therefore, the Inverse Document Frequency (IDF) is employed to measure the\\nimportance of wi in V as follows:\\nidfwi = log |D|\\ndfwi\\n,\\n(4.2)\\nwhere |D| is the number of all documents in the corpus D and dfwi represents the\\nDocument Frequency (DF) of wi.\\nWith the importance of each word, the sentences are represented more precisely\\nas follows:\\n\\xcb\\x86s = s \\xe2\\x8a\\x97idf,\\n(4.3)\\nwhere \\xe2\\x8a\\x97is the element-wise product.\\nHere, \\xcb\\x86s is the TF-IDF representation of the sentence s.\\n4.3 Probabilistic Language Model\\n61\\n4.3\\nProbabilistic Language Model\\nOne-hot sentence representation usually neglects the structure information in a sen-\\ntence. To address this issue, researchers propose probabilistic language model, which\\ntreats n-grams rather than words as the basic components. An n-gram means a subse-\\nquence of words in a context window of length n, and probabilistic language model\\nde\\xef\\xac\\x81nes the probability of a sentence s = [w1, w2, . . . , wl] as\\nP(s) =\\nl\\x03\\ni=1\\nP(wi|wi\\xe2\\x88\\x921\\n1\\n).\\n(4.4)\\nActually, model indicated in Eq.(4.4) is not practicable due to its enormous param-\\neter space. In practice, we simplify the model and set an n-sized context window,\\nassuming that the probability of word wi only depends on [wi\\xe2\\x88\\x92n+1 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 wi\\xe2\\x88\\x921]. More\\nspeci\\xef\\xac\\x81cally, an n-gram language model predicts word wi in the sentence s based\\non its previous n \\xe2\\x88\\x921 words. Therefore, the simpli\\xef\\xac\\x81ed probability of a sentence is\\nformalized as\\nP(s) =\\nl\\x03\\ni=1\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n+1),\\n(4.5)\\nwhere the probability of selecting the word wi can be calculated from n-gram model\\nfrequency counts:\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n+1) = P(wi\\ni\\xe2\\x88\\x92n+1)\\nP(wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n+1)\\n.\\n(4.6)\\nTypically, the conditional probabilities in n-gram language models are not cal-\\nculated directly from the frequency counts, since it suffers severe problems when\\nconfronted with any n-grams that have not explicitly been seen before. Therefore,\\nresearchers proposed several types of smoothing approaches, which assign some of\\nthe total probability mass to unseen words or n-grams, such as \\xe2\\x80\\x9cadd-one\\xe2\\x80\\x9d smoothing,\\nGood-Turing discounting, or back-off models.\\nn-gram model is a typical probabilistic language model for predicting the next\\nword in an n-gram sequence, which follows the Markov assumption that the proba-\\nbility of the target word only relies on the previous n \\xe2\\x88\\x921 words. The idea is employed\\nby most of current sentence modeling methods. n-gram language model is used as\\nan approximation of the true underlying language model. This assumption is crucial\\nbecause it massively simpli\\xef\\xac\\x81es the problem of learning the parameters of language\\nmodels from data. Recent works on word representation learning [3, 40, 43] are\\nmainly based on the n-gram language model.\\n62\\n4\\nSentence Representation\\n4.4\\nNeural Language Model\\nAlthough smoothing approaches could alleviate the sparse problem in the probabilis-\\ntic language model, it still performs poorly for those unseen or uncommon words and\\nn-grams. Moreover, since probabilistic language models are constructed on larger\\nand larger texts, the number of unique words (the vocabulary) increases and the\\nnumber of possible sequences of words increases exponentially with the size of the\\nvocabulary, causing a data sparsity problem. Thus statistics are needed to estimate\\nprobabilities accurately.\\nTo address this issue, researchers propose neural language models which use\\ncontinuousrepresentationsorembeddingsofwordsandneuralnetworkstomaketheir\\npredictions, in which embeddings in the continuous space help to alleviate the curse\\nof dimensionality in language modeling, and neural networks avoid this problem by\\nrepresenting words in a distributed way, as nonlinear combinations of weights in a\\nneural net [2]. An alternate description is that a neural network approximates the\\nlanguage function. The neural net architecture might be feedforward or recurrent,\\nand while the former is simpler, the latter is more common.\\nSimilar to probabilistic language models, neural language models are constructed\\nand trained as probabilistic classi\\xef\\xac\\x81ers that learn to predict a probability distribution:\\nP(s) =\\nl\\x03\\ni=1\\nP(wi|wi\\xe2\\x88\\x921\\n1\\n),\\n(4.7)\\nwhere the conditional probability of the selecting word wi can be calculated by\\nvarious kinds of neural networks such as feedforward neural networks, recurrent\\nneural networks, and so on. In the following sections, we will introduce these neural\\nlanguage models in detail.\\n4.4.1\\nFeedforward Neural Network Language Model\\nThe goal of neural network language model is to estimate the conditional probabil-\\nity P(wi|w1, . . . , wi\\xe2\\x88\\x921). However, the feedforward neural network (FNN) lacks an\\neffective way to represent the long-term historical context. Therefore, it adopts the\\nidea of n-gram language models to approximate the conditional probability, which\\nassumes that each word in a word sequence more statistically depends on those\\nwords closer to it, and only n \\xe2\\x88\\x921 context words are used to calculate the conditional\\nprobability, i.e., P(wi|wi\\xe2\\x88\\x921\\n1\\n) \\xe2\\x89\\x88P(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n+1).\\nTheoverallarchitectureoftheFNNlanguagemodelisproposedby[3].Toevaluate\\nthe conditional probability of the word wi, it \\xef\\xac\\x81rst projects its n \\xe2\\x88\\x921 context-related\\nwords to their word vector representations x = [wi\\xe2\\x88\\x92n+1, . . . , wi\\xe2\\x88\\x921], and then feeds\\nthem into an FNN, which can be generally represented as\\n4.4 Neural Language Model\\n63\\ny = M f (Wx + b) + d,\\n(4.8)\\nwhere W is a weighted matrix to transform word vectors to hidden representations,\\nM is a weighted matrix for the connections between the hidden layer and the output\\nlayer, and b, d are bias vectors. And then the conditional probability of the word wi\\ncan be calculated as\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) =\\nexp(ywi)\\n\\x04\\nj exp(y j).\\n(4.9)\\n4.4.2\\nConvolutional Neural Network Language Model\\nThe Convolutional Neural Network (CNN) is the family of neural network models\\nthat features a type of layer known as the convolutional layer. This layer can extract\\nfeatures by a learnable \\xef\\xac\\x81lter (or kernel) at the different positions of an input. Pham\\net al. [47] propose the CNN language model to enhance the FNN language model.\\nThe proposed CNN network is produced by injecting a convolutional layer after the\\nword input representation x = [wi\\xe2\\x88\\x92n, . . . , wi\\xe2\\x88\\x921]. Formally, the convolutional layer\\ninvolves a sliding window of the input vectors centered on each word vector using a\\nparameter matrix Wc, which can be generally represented as\\ny = M\\n\\x05\\nmax(Wcx)\\n\\x06\\n,\\n(4.10)\\nwhere max(\\xc2\\xb7) indicates a max-pooling layer. The architecture of CNN is shown in\\nFig.4.1.\\nMoreover, [12] also introduces a convolutional neural network for language mod-\\neling with a novel gating mechanism.\\n4.4.3\\nRecurrent Neural Network Language Model\\nTo address the lack of ability for modeling long-term dependency in the FNN lan-\\nguage model, [41] proposes a Recurrent Neural Network (RNN) language model\\nwhich applies RNN in language modeling. RNNs are fundamentally different from\\nFNNs in the sense that they operate on not only an input space but also an internal\\nstate space, and the internal state space enables the representation of sequentially\\nextended dependencies. Therefore, the RNN language model can deal with those\\nsentences of arbitrary length. At every time step, its input is the vector of its previous\\nword instead of the concatenation of vectors of its n previous words, and the infor-\\nmation of all other previous words can be taken into account by its internal state.\\nFormally, the RNN language model can be de\\xef\\xac\\x81ned as\\n64\\n4\\nSentence Representation\\nFig. 4.1 The architecture of\\nCNN\\nWc *\\nNon-linear \\nLayer\\nMax\\nPooling\\nConvolution\\nLayer\\nInput\\nRepresentation\\ntanh\\nhi = f (W1hi\\xe2\\x88\\x921 + W2wi + b),\\n(4.11)\\ny = Mhi\\xe2\\x88\\x921 + d,\\n(4.12)\\nwhere W1, W2, M are weighted matrices and b, d are bias vectors. Here, the RNN\\nunit can also be implemented by LSTM or GRU. The architecture of RNN is shown\\nin Fig.4.2.\\nRecently, researchers make some comparisons among neural network language\\nmodels with different architectures on both small and large corpora. The experimental\\nresults show that, generally, the RNN language model outperforms the CNN language\\nmodel.\\n4.4.4\\nTransformer Language Model\\nIn 2018, Google proposed a pre-trained language model (PLM), called BERT, which\\nachieved state-of-the-art results on a variety of NLP tasks. At that time, it was very\\nbig news. Since then, all the NLP researchers began to consider how PLMs can\\nbene\\xef\\xac\\x81t their research tasks.\\n4.4 Neural Language Model\\n65\\ntanh\\nRNN\\nUnit\\nRNN\\nUnit\\nRNN\\nUnit\\nRNN\\nUnit\\n\\xcf\\x83\\n\\xcf\\x83\\n\\xcf\\x83\\ntanh\\nGRU Cell\\n\\xcf\\x83\\n\\xcf\\x83\\ntanh\\n1-\\nxt\\nht\\nh0\\nh1\\nhn\\nx0\\nx1\\nxn\\nht-1\\nct-1\\nxt\\nht\\nct\\nxt\\nht\\nht-1\\nLSTM Cell\\nFig. 4.2 The architecture of RNN\\nIn this section, we will \\xef\\xac\\x81rst introduce the Transformer architecture and then talk\\nabout BERT and other PLMs in detail.\\n4.4.4.1\\nTransformer\\nTransformer [65] is a nonrecurrent encoder-decoder architecture with a series of\\nattention-based blocks. For the encoder, there are 6 layers and each layer is composed\\nof a multi-head attention sublayer and a position-wise feedforward sublayer. And\\nthere is a residual connection between sublayers. The architecture of the Transformer\\nis as shown in Fig.4.3.\\nThere are several attention heads in the multi-head attention sublayer. A head\\nrepresents a scaled dot-product attention structure, which takes the query matrix Q,\\nthe key matrix K, and the value matrix V as the inputs, and the output is computed\\nby\\nAttention(Q, K, V) = Softmax\\n\\x07QKT\\n\\xe2\\x88\\x9adk\\n\\x08\\nV,\\n(4.13)\\nwhere dk is the dimension of query matrix.\\nThe multi-head attention sublayer linearly projects the input hidden states H\\nseveral times into the query matrix, the key matrix, and the value matrix for h heads.\\nThe dimensions of the query, key, and value vectors are dk, dk, and dv, respectively.\\n66\\n4\\nSentence Representation\\nInputs\\nOutputs\\n(shifted right)\\nN \\xc3\\x97\\nFeed\\nForward\\nAdd & Norm\\nMulti-Head\\nAttention\\nAdd & Norm\\nMasked\\nMulti-Head\\nAttention\\nAdd & Norm\\nFeed\\nForward\\nAdd & Norm\\nMulti-Head\\nAttention\\nAdd & Norm\\nN \\xc3\\x97\\nPositional\\nEncoding\\nPositional\\nEncoding\\nLinear\\nSoftmax\\nOutput\\nProbabilities\\nFig. 4.3 The architecture of Transformer\\nThe multi-head attention sublayer could be formulated as\\nMultihead(H) = [head1, head2, . . . , headh]WO,\\n(4.14)\\nwhere headi = Attention(HWQ\\ni , HWK\\ni , HWV\\ni ), and WQ\\ni , WK\\ni and WV\\ni are linear\\nprojections. WO is also a linear projection for the output. Here, the fully connected\\nposition-wise feedforward sublayer contains two linear transformations with ReLU\\nactivation:\\n4.4 Neural Language Model\\n67\\nFFN(x) = W2 max(0, W1x + b1) + b2.\\n(4.15)\\nTransformer is better than RNNs for modeling the long-term dependency, where\\nall tokens will be equally considered during the attention operation. The Transformer\\nwas proposed to solve the problem of machine translation. Since Transformer has a\\nvery powerful ability to model sequential data, it becomes the most popular backbone\\nof NLP applications.\\n4.4.4.2\\nTransformer-Based PLM\\nNeural models can learn large amounts of language knowledge from language mod-\\neling. Since the language knowledge covers the demands of many downstream\\nNLP tasks and provides powerful representations of words and sentences, some\\nresearchers found that knowledge can be transferred to other NLP tasks easily. The\\ntransferred models are called Pre-trained Language Models (PLMs).\\nLanguage modeling is the most basic and most important NLP task. It contains a\\nvariety of knowledge for language understanding, such as linguistic knowledge and\\nfactual knowledge. For example, the model needs to decide whether it should add\\nan article before a noun. This requires linguistic knowledge about articles. Another\\nexample is the question of what is the following word after \\xe2\\x80\\x9cTrump is the president\\nof\\xe2\\x80\\x9d. The answer is \\xe2\\x80\\x9cAmerica\\xe2\\x80\\x9d, which requires factual knowledge. Since language\\nmodeling is very complex, the models can learn a lot from this task.\\nOn the other hand, language modeling only requires plain text without any human\\nannotation. With this feature, the models can learn complex NLP abilities from a very\\nlarge-scale corpus. Since deep learning needs large amounts of data and language\\nmodeling can make full use of all texts in the world, PLMs signi\\xef\\xac\\x81cantly bene\\xef\\xac\\x81t the\\ndevelopment of NLP research.\\nInspired by the success of the Transformer, GPT [50] and BERT [14] begin to\\nadopt the Transformer as the backbone of the pre-trained language models. GPT and\\nBERT are the most representative Transformer-based pre-trained language models\\n(PLMs). Since they achieved state-of-the-art performance on various NLP tasks,\\nnearly all PLMs after them are based on the Transformer. In this subsection, we will\\ntalk about GPT and BERT in more detail.\\nGPT is the \\xef\\xac\\x81rst work to pretrain a PLM based on the Transformer. The train-\\ning procedure of GPT [50] contains two classic stages: generative pretraining and\\ndiscriminative \\xef\\xac\\x81ne-tuning.\\nIn the pretraining stage, the input of the model is a large-scale unlabeled corpus\\ndenoted as U = {u1, u2, . . . , un}. The pretraining stage aims to optimize a language\\nmodel. The learning objective over the corpus is to maximize a conditional likelihood\\nin a \\xef\\xac\\x81xed-size window:\\nL1(U ) =\\n\\x02\\ni\\nlog P(ui|ui\\xe2\\x88\\x92k, . . . , ui\\xe2\\x88\\x921; \\xce\\x98),\\n(4.16)\\n68\\n4\\nSentence Representation\\nwhere k represents the size of the window, the conditional likelihood P is modeled\\nby a neural network with parameters \\xce\\x98.\\nFor a supervised dataset \\xcf\\x87, the input is a sequence of words s = (w1, w2, .., wl)\\nand the output is a label y. The pretraining stage provides an advantageous start\\npoint of parameters that can be used to initialize subsequent supervised tasks. At\\nthis occasion, the objective is a discriminative task that maximizes the conditional\\npossibility distribution:\\nL2(\\xcf\\x87) =\\n\\x02\\n(s,y)\\nlog P(y|w1, . . . , wl),\\n(4.17)\\nwhere P(y|w1, . . . , wl) is modeled by a K-layer Transformer. After the input tokens\\npass through the pretrained GPT, a hidden vector of the \\xef\\xac\\x81nal layer hK\\nl will be pro-\\nduced. To obtain the output distribution, a linear transformation layer is added, which\\nhas the same size as the number of labels:\\nP(y|w1, . . . , wm) = Softmax(WyhK\\nl ).\\n(4.18)\\nThe \\xef\\xac\\x81nal training objective is combined with a language modeling L1 for better\\ngeneralization:\\nL (\\xcf\\x87) = L2(\\xcf\\x87) + \\xce\\xbb \\xe2\\x88\\x97L1(\\xcf\\x87),\\n(4.19)\\nwhere \\xce\\xbb is a weight hyperparameter.\\nBERT [14] is a milestone work in the \\xef\\xac\\x81eld of PLM. BERT achieved signi\\xef\\xac\\x81cant\\nempirical results on 17 different NLP tasks, including SQuAD (outperform human\\nbeing), GLUE (7.7% point absolute improvement), MultiNLI (4.6% point absolute\\nimprovement), etc. Compared to GPT, BERT uses a bidirectional deep Transformer\\nas the model backbone. As illustrated in Fig.4.4, BERT contains pretraining and\\n\\xef\\xac\\x81ne-tuning stages.\\nIn the pretraining stage, two objectives are designed: Masked Language Model\\n(MLM) and Next Sentence Prediction (NSP). (1) For MLM, tokens are randomly\\nPre-training\\nUnlabeled Sentence A and B Pair\\nMasked Sentence A\\nMasked Sentence B\\n[CLS]\\nTok1\\nTokN\\n[SEP]\\n\\xe2\\x80\\xa6\\nTok1\\nTokM\\n\\xe2\\x80\\xa6\\nE[CLS]\\nE1\\nEN\\nE[SEP]\\nE\\xe2\\x80\\x991\\nE\\xe2\\x80\\x99M\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nE[CLS]\\nE1\\nEN\\nE[SEP]\\nE\\xe2\\x80\\x991\\nE\\xe2\\x80\\x99M\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nNSP\\nMask LM\\nMask LM\\nBERT\\nFine-Tuning\\nMNLI\\nNER\\nQuestion Answer Pair\\nQuestion\\nParagraph\\n[CLS]\\nTok1\\nTokN\\n[SEP]\\n\\xe2\\x80\\xa6\\nTok1\\nTokM\\n\\xe2\\x80\\xa6\\nE[CLS]\\nE1\\nEN\\nE[SEP]\\nE\\xe2\\x80\\x991\\nE\\xe2\\x80\\x99M\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nE[CLS]\\nE1\\nEN\\nE[SEP]\\nE\\xe2\\x80\\x991\\nE\\xe2\\x80\\x99M\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nBERT\\nSQuAD\\nStart/End Span\\nFig. 4.4 The pretraining and \\xef\\xac\\x81ne-tuning stages for BERT\\n4.4 Neural Language Model\\n69\\nmasked with a special token [MASK]. The training objective is to predict the masked\\ntokens based on the contexts. Compared with the standard unidirectional conditional\\nlanguage model, which can only be trained in one direction, MLM aims to train a deep\\nbidirectional representation model. This task is inspired by Cloze [64]. (2) The objec-\\ntive of NSP is to capture relationships between sentences for some sentence-based\\ndownstream tasks such as natural language inference (NLI) and question answering\\n(QA). In this task, a binary classi\\xef\\xac\\x81er is trained to predict whether the sentence is\\nthe next sentence for the current. This task effectively captures the deep relationship\\nbetween sentences, exploring semantic information from a different level.\\nAfter pretraining, BERT can capture various language knowledge for downstream\\nsupervised tasks. By modifying inputs and outputs, BERT can be \\xef\\xac\\x81ne-tuned for any\\nNLP tasks, which contain the applications with the input of single text or text pairs.\\nThe input consists of sentence A and sentence B, which can represent (1) sentence\\npairs in paraphrase, (2) hypothesis-premise pairs in entailment, (3) question-passage\\npairs in QA, and (4) text-\\xe2\\x88\\x85for text classi\\xef\\xac\\x81cation task or sequence tagging. For the\\noutput, BERT can produce the token-level representation for each token, which is\\nused to sequence tagging task or question answering. Besides, the special token\\n[CLS] in BERT is fed into the classi\\xef\\xac\\x81cation layer for sequence classi\\xef\\xac\\x81cation.\\n4.4.4.3\\nPLM Family\\nPre-trained language models have rapid progress after BERT. We summarize sev-\\neral important directions of PLMs and show some representative models and their\\nrelationship in Fig.4.5.\\nHere is a brief introduction of the PLMs after BERT. Firstly, there are some\\nvariants of BERT for better general language representation, such as RoBERTa [38]\\nand XLNet [70]. These models mainly focus on the improvement of pretraining tasks.\\nSecondly, some people work on pretrained generation models, such as MASS [57]\\nand UniLM [15]. These models achieve promising results on the generation tasks\\ninstead of the Natural Language Understanding (NLU) tasks used by BERT. Thirdly,\\nthesentencepairformatofBERTinspiredworksonthecross-lingualandcross-modal\\n\\xef\\xac\\x81elds. XLM [8], ViLBERT [39], and VideoBERT [59] are the important works in this\\ndirection. Lastly, there are some works [46, 81] that explore to incorporate external\\nknowledge into PLMs since some low-frequency knowledge cannot be ef\\xef\\xac\\x81ciently\\nlearned by PLMs.\\n4.4.5\\nExtensions\\n4.4.5.1\\nImportance Sampling\\nInspired by the contrastive divergence model, [4] proposes to adopt importance sam-\\npling to accelerate the training of neural language models. They \\xef\\xac\\x81rst normalize the\\n70\\n4\\nSentence Representation\\nFig. 4.5 The Pre-trained language model family\\noutputs of neural network language model and view neural network language models\\nas a special case of energy-based probability models as following:\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) =\\nexp(\\xe2\\x88\\x92ywi)\\n\\x04\\nj exp(\\xe2\\x88\\x92y j).\\n(4.20)\\nThe key idea of importance sampling is to approximate the mean of log-likelihood\\ngradient of the loss function of neural network language model by sampling several\\nimportant words instead of calculating the explicit gradient. Here, the log-likelihood\\ngradient of the loss function of neural network language model can be generally\\nrepresented as\\n\\xe2\\x88\\x82P(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n)\\n\\xe2\\x88\\x82\\xce\\xb8\\n= \\xe2\\x88\\x92\\xe2\\x88\\x82ywi\\n\\xe2\\x88\\x82\\xce\\xb8 +\\n|V |\\n\\x02\\nj=1\\nP(w j|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n)\\xe2\\x88\\x82y j\\n\\xe2\\x88\\x82\\xce\\xb8\\n= \\xe2\\x88\\x92\\xe2\\x88\\x82yi\\n\\xe2\\x88\\x82\\xce\\xb8 + Ewk\\xe2\\x88\\xbcP\\n\\t\\xe2\\x88\\x82yk\\n\\xe2\\x88\\x82\\xce\\xb8\\n\\n,\\n(4.21)\\nwhere \\xce\\xb8 indicates all parameters of the neural network language model. Here, the\\nlog-likelihood gradient of the loss function consists of two parts including positive\\ngradient for target word wi and negative gradient for all words w j, i.e., Ewi\\xe2\\x88\\xbcP[ \\xe2\\x88\\x82y j\\n\\xe2\\x88\\x82\\xce\\xb8 ].\\nHere, the second part can be approximated by sampling important words following\\nthe probability distribution P:\\n4.4 Neural Language Model\\n71\\nEwk\\xe2\\x88\\xbcP\\n\\t\\xe2\\x88\\x82yk\\n\\xe2\\x88\\x82\\xce\\xb8\\n\\n\\xe2\\x89\\x88\\n\\x02\\nwk\\xe2\\x88\\x88V \\xe2\\x80\\xb2\\n1\\n|V \\xe2\\x80\\xb2|\\n\\xe2\\x88\\x82yk\\n\\xe2\\x88\\x82\\xce\\xb8 ,\\n(4.22)\\nwhere V \\xe2\\x80\\xb2 is the word set sampled under P.\\nHowever, since we cannot obtain probability distribution P in advance, it is impos-\\nsible to sample important words following the probability distribution P. Therefore,\\nimportance sampling adopts a Monte Carlo scheme which uses an existing proposal\\ndistribution Q to approximate P, and then we have\\nEwk\\xe2\\x88\\xbcP\\n\\t\\xe2\\x88\\x82yk\\n\\xe2\\x88\\x82\\xce\\xb8\\n\\n\\xe2\\x89\\x88\\n1\\n|V \\xe2\\x80\\xb2\\xe2\\x80\\xb2|\\n\\x02\\nwl\\xe2\\x88\\x88V \\xe2\\x80\\xb2\\xe2\\x80\\xb2\\n\\xe2\\x88\\x82yl\\n\\xe2\\x88\\x82\\xce\\xb8 P(wl|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n)/Q(wl),\\n(4.23)\\nwhere V \\xe2\\x80\\xb2\\xe2\\x80\\xb2 is the word set sampled under Q. Moreover, the sample size of impor-\\ntance sampling approach should be increased as training processes in order to avoid\\ndivergence, which aims to ensure its effective sample size S:\\nS =\\n(\\x04\\nwl\\xe2\\x88\\x88V \\xe2\\x80\\xb2\\xe2\\x80\\xb2 rl)2\\n\\x04\\nwl\\xe2\\x88\\x88V \\xe2\\x80\\xb2\\xe2\\x80\\xb2 r2\\nl\\n,\\n(4.24)\\nwhere rl is further de\\xef\\xac\\x81ned as\\nrl =\\nP(wl|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n)/Q(wl)\\n\\x04\\nw j\\xe2\\x88\\x88V \\xe2\\x80\\xb2\\xe2\\x80\\xb2 P(w j|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n)/Q(w j)\\n.\\n(4.25)\\n4.4.5.2\\nWord Classi\\xef\\xac\\x81cation\\nBesides important sampling, researchers [7, 22] also propose class-based language\\nmodel, which adopts word classi\\xef\\xac\\x81cation to improve the performance and speed of a\\nlanguage model. In class-based language model, all words are assigned to a unique\\nclass, and the conditional probability of a word given its context can be decomposed\\ninto the probability of the word\\xe2\\x80\\x99s class given its previous words and the probability\\nof the word given its class and history, which is formally de\\xef\\xac\\x81ned as\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) =\\n\\x02\\nc(wi)\\xe2\\x88\\x88C\\nP(wi|c(wi))P(c(wi)|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n),\\n(4.26)\\nwhere C indicates the set of all classes and c(wi) indicates the class of word wi.\\nMoreover, [44] proposes a hierarchical neural network language model, which\\nextends word classi\\xef\\xac\\x81cation to a hierarchical binary clustering of words in a language\\nmodel. Instead of simply assigning each word with a unique class, it \\xef\\xac\\x81rst builds\\na hierarchical binary tree of words according to the word similarity obtained from\\nWordNet.Next,itassignsauniquebitvectorc(wi) = [c1(wi), c2(wi), . . . , cl(wi)]for\\n72\\n4\\nSentence Representation\\neach word, which indicates the hierarchical classes of them. And then the conditional\\nprobability of each word can be de\\xef\\xac\\x81ned as\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) =\\nl\\x03\\nj=0\\nP(c j(wi)|c1(wi), c2(wi), . . . , c j\\xe2\\x88\\x921(wi), wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n).\\n(4.27)\\nThe hierarchical neural network language model can achieve O(k/ log k) speed\\nup as compared to a standard language model. However, the experimental results of\\n[44] show that although the hierarchical neural network language model achieves\\nan impressive speed up for modeling sentences, it has worse performance than the\\nstandard language model. The reason is perhaps that the introduction of hierarchical\\narchitecture or word classes imposes a negative in\\xef\\xac\\x82uence on the word classi\\xef\\xac\\x81cation\\nby neural network language models.\\n4.4.5.3\\nCaching\\nCaching is also one of the important extensions in language model. A type of cache-\\nbased language model assumes that each word in recent context is more likely to\\nappear again [58]. Hence, the conditional probability of a word can be calculated by\\nthe information from history and caching:\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) = \\xce\\xbbPs(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) + (1 \\xe2\\x88\\x92\\xce\\xbb)Pc(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n),\\n(4.28)\\nwhere Ps(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) indicates the conditional probability generated by standard lan-\\nguage and Pc(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) indicates the conditional probability generated by caching,\\nand \\xce\\xbb is a constant.\\nAnother cache-based language model is also used to speed up the RNN language\\nmodeling [27]. The main idea of this approach is to store the outputs and states of\\nlanguage models for future predictions given the same contextual history.\\n4.5\\nApplications\\nIn this section, we will introduce two typical sentence-level NLP applications includ-\\ning text classi\\xef\\xac\\x81cation and relation extraction, as well as how to utilize sentence rep-\\nresentation for these applications.\\n4.5 Applications\\n73\\n4.5.1\\nText Classi\\xef\\xac\\x81cation\\nText classi\\xef\\xac\\x81cation is a typical NLP application and has lots of important real-world\\ntasks such as parsing and semantic analysis. Therefore, it has attracted the interest of\\nmany researchers. The conventional text classi\\xef\\xac\\x81cation models (e.g., the LDA [6] and\\ntree kernel [48] models) focus on capturing more contextual information and correct\\nword order by extracting more useful and distinct features, but still expose a few\\nissues (e.g., data sparseness) which has the signi\\xef\\xac\\x81cant impact on the classi\\xef\\xac\\x81cation\\naccuracy. Recently, with the development of deep learning in the various \\xef\\xac\\x81elds of\\narti\\xef\\xac\\x81cial intelligence, neural models have been introduced into the text classi\\xef\\xac\\x81cation\\n\\xef\\xac\\x81eld due to their abilities of text representation learning. In this section, we will\\nintroduce the two typical tasks of text classi\\xef\\xac\\x81cation, including sentence classi\\xef\\xac\\x81cation\\nand sentiment classi\\xef\\xac\\x81cation.\\n4.5.1.1\\nSentence Classi\\xef\\xac\\x81cation\\nSentence classi\\xef\\xac\\x81cation aims to assign a sentence an appropriate category, which is a\\nbasic task of the text classi\\xef\\xac\\x81cation application.\\nConsidering the effectiveness of the CNN models in capturing sentence semantic\\nmeanings, [31] \\xef\\xac\\x81rst proposes to utilize the CNN models trained on the top of pre-\\ntrained word embeddings to classify sentences, which achieved promising results on\\nseveral sentence classi\\xef\\xac\\x81cation datasets. Then, [30] introduces a dynamic CNN model\\nto model the semantic meanings of sentences. This model handles sentences of vary-\\ning lengths and uses dynamic max-pooling over linear sequences, which could help\\nthe model capture both short-range and long-range semantic relations in sentences.\\nFurthermore, [9] proposes a novel CNN-based model named as Very Deep CNN,\\nwhich operates directly at the character level. It shows that those deeper models have\\nbetter results on sentence classi\\xef\\xac\\x81cation and can capture the hierarchical information\\nfrom scattered characters to whole sentences. Yin and Sch\\xc3\\xbctze [74] also propose\\nMV-CNN, which utilizes multiple types of pretrained word embeddings and extracts\\nfeatures from multi-granular phrases with variable-sized convolutional layers. To\\naddress the drawbacks of MV-CNN such as model complexity and the requirement\\nfor the same dimension of embeddings, [80] proposes a novel model called MG-CNN\\nto capture multiple features from multiple sets of embeddings that are concatenated\\nat the penultimate layer. Zhang et al. [79] present RA-CNN to jointly exploit labels\\non documents and their constituent sentences, which can estimate the probability\\nthat a given sentence is informative and then scales the contribution of each sentence\\nto aggregate a document representation in proportion to the estimates.\\nThe RNN model which aims to capture the sequential information of sentences\\nis also widely used in sentence classi\\xef\\xac\\x81cation. Lai et al. [32] propose a neural net-\\nwork for text classi\\xef\\xac\\x81cation, which applies a recurrent structure to capture contextual\\ninformation. Moreover, [37] introduces a multitask learning framework based on the\\nRNN to jointly learn across multiple sentence classi\\xef\\xac\\x81cation tasks, which employs\\n74\\n4\\nSentence Representation\\nthree different mechanisms of sharing information to model sentences with both task-\\nspeci\\xef\\xac\\x81c and shared layers. Yang et al. [71] introduce word-level and sentence-level\\nattention mechanisms into an RNN-based model as well as a hierarchical structure\\nto capture the hierarchical information of documents for sentence classi\\xef\\xac\\x81cation.\\n4.5.1.2\\nSentiment Classi\\xef\\xac\\x81cation\\nSentiment classi\\xef\\xac\\x81cation is a special task of the sentence classi\\xef\\xac\\x81cation application,\\nwhose objective is to classify the sentimental polarities of opinions a piece of text\\ncontains, e.g., favorable or unfavorable, positive or negative. This task appeals the\\nNLP community since it has lots of potential downstream applications such as movie\\nreview suggestions.\\nSimilar to text classi\\xef\\xac\\x81cation, the sentence representation based on neural models\\nhas also been widely explored for sentiment classi\\xef\\xac\\x81cation. Glorot et al. [20] use a\\nstacked denoising autoencoder in sentiment classi\\xef\\xac\\x81cation for the \\xef\\xac\\x81rst time. Then,\\na series of recursive neural network models based on the recursive tree structure\\nof sentences are conducted to learn sentence representations for sentiment classi-\\n\\xef\\xac\\x81cation, including the recursive autoencoder (RAE) [55], matrix-vector recursive\\nneural network (MV-RNN) [54], and recursive neural tensor network (RNTN) [56].\\nBesides, [29] adopts a CNN to learn sentence representations and achieves promising\\nperformance in sentiment classi\\xef\\xac\\x81cation.\\nThe RNN models also bene\\xef\\xac\\x81t sentiment classi\\xef\\xac\\x81cation as they are able to capture\\nthe sequential information. Li et al. [35] and Tai et al. [62] investigate a tree-structured\\nLSTMmodelontextclassi\\xef\\xac\\x81cation.Therearealsosomehierarchicalmodelsproposed\\nto deal with document-level sentiment classi\\xef\\xac\\x81cation [5, 63], which generate seman-\\ntic representations at different levels (e.g., phrase, sentence, or document) within\\na document. Moreover, the attention mechanism is also introduced into sentiment\\nclassi\\xef\\xac\\x81cation, which aims to select important words from a sentence or important\\nsentences from a document [71].\\n4.5.2\\nRelation Extraction\\nTo enrich existing KGs, researchers have devoted many efforts to automatically \\xef\\xac\\x81nd-\\ning novel relational facts in text. Therefore, relation extraction (RE), which aims\\nat extracting relational facts according to semantic information in plain text, has\\nbecome a crucial NLP application. As RE is also an important downstream applica-\\ntion of sentence representation, we will, respectively, introduce the techniques and\\nextensions to show how to utilize sentence representation for different RE scenarios.\\nConsidering neural networks have become the backbone of the recent NLP research,\\nwe mainly focus on Neural RE (NRE) models in this section.\\n4.5 Applications\\n75\\nFig. 4.6 An example of sentence-level relation extraction\\n4.5.2.1\\nSentence-Level NRE\\nSentence-level NRE aims at predicting the semantic relations between the given\\nentity (or nominal) pair in a sentence. As shown in Fig.4.6, given the input sentence\\ns which consists of n words s = {w1, w2, . . . , wn} and its corresponding entity pair\\ne1 and e2 as input, sentence-level NRE wants to obtain the conditional probability\\nP(r|s, e1, e2) of relation r (r \\xe2\\x88\\x88R) via a neural network, which can be formalized\\nas\\nP(r|s, e1, e2) = P(r|s, e1, e2, \\xce\\xb8),\\n(4.29)\\nwhere \\xce\\xb8 is all parameters of the neural network and r is a relation in the relation set\\nR.\\nA basic form of sentence-level NRE consists of three components: (a) an input\\nencoder to give a representation for each input word, (b) a sentence encoder which\\ncomputes either a single vector or a sequence of vectors to represent the original\\nsentence, and (c) a relation classi\\xef\\xac\\x81er which calculates the conditional probability\\ndistribution of all relations.\\nInput Encoder. First, a sentence-level NRE system projects the discrete words\\nof the source sentence into a continuous vector space, and obtains the input repre-\\nsentation w = {w1, w2, . . . , wm} of the source sentence.\\n(1) Word Embeddings. Word embeddings aim to transform words into distributed\\nrepresentations to capture the syntactic and semantic meanings of the words.\\nIn the sentence s, every word wi is represented by a real-valued vector. Word\\nrepresentations are encoded by column vectors in an embedding matrix E \\xe2\\x88\\x88\\nRda\\xc3\\x97|V | where V is a \\xef\\xac\\x81xed-sized vocabulary. Although word embeddings are\\nthe most common way to represent input words, there are also efforts made to\\nutilize more complicated information of input sentences for RE.\\n(2) Position Embeddings. In RE, the words close to the target entities are usually\\ninformative to determine the relation between the entities. Therefore, position\\nembeddings are used to help models keep track of how close each word is to\\nthe head or tail entities. It is de\\xef\\xac\\x81ned as the combination of the relative distances\\nfrom the current word to the head or tail entities. For example, in the sentence\\nBill_Gates is the founder of Microsoft., the relative distance\\n76\\n4\\nSentence Representation\\nfrom the word founder to the head entity Bill_Gates is \\xe2\\x88\\x923 and the tail\\nentity Microsoft is 2. Besides word position embeddings, more linguistic\\nfeatures are also considered in addition to the word embeddings to enrich the\\nlinguistic features of the input sentence.\\n(3) Part-of-speech (POS) Tag Embeddings. POS tag embeddings are to represent the\\nlexical information of the target word in the sentence. Because word embeddings\\nare obtained from a large-scale general corpus, the general information they\\ncontain may not be in accordance with the meaning in a speci\\xef\\xac\\x81c sentence. Hence,\\nit is necessary to align each word with its linguistic information considering its\\nspeci\\xef\\xac\\x81c context, e.g., noun and verb. Formally, each word wi is encoded by the\\ncorresponding column vector in an embedding matrix Ep \\xe2\\x88\\x88Rd p\\xc3\\x97|V p|, where d p\\nis the dimension of embedding vector and V p indicates a \\xef\\xac\\x81xed-sized POS tag\\nvocabulary.\\n(4) WordNet Hypernym Embeddings. WordNet hypernym embeddings aim to take\\nadvantages of the prior knowledge of hypernym to help RE models. When\\ngiven the hypernym information of each word in WordNet (e.g., noun.food and\\nverb.motion), it is easier to build the connections between different but concep-\\ntually similar words. Formally, each word wi is encoded by the corresponding\\ncolumn vector in an embedding matrix Eh \\xe2\\x88\\x88Rdh\\xc3\\x97|V h|, where dh is the dimension\\nof embedding vector and V h indicates a \\xef\\xac\\x81xed-sized hypernym vocabulary.\\nFor each word, the NRE models often concatenate some of the above four feature\\nembeddings as their input embeddings. Therefore, the feature embeddings of all\\nwords are concatenated and denoted as a \\xef\\xac\\x81nal input sequence w = {w1, w2, . . . , wm},\\nwhere wi \\xe2\\x88\\x88Rd, d is the total dimension of all feature embeddings concatenated for\\neach word.\\nSentence Encoder. The sentence encoder is the core for sentence representation,\\nwhich encodes input representations into either a single vector or a sequence of\\nvectors x to represent sentences. We will introduce the different sentence encoders\\nin the following.\\n(1) Convolutional Neural Network Encoder. Zeng et al. [76] propose to encode\\ninput sentences using a CNN model, which extracts local features by a convolutional\\nlayer and combines all local features via a max-pooling operation to obtain a \\xef\\xac\\x81xed-\\nsized vector for the input sentence. Formally, a convolutional layer is de\\xef\\xac\\x81ned as an\\noperation on a vector sequence w:\\np = CNN(w),\\n(4.30)\\nwhere CNN indicates the convolution operation inside the convolutional layer.\\nAnd the ith element of the sentence vector x can be calculated as follows:\\n[x]i = f (max(pi)),\\n(4.31)\\nwhere f is a nonlinear function applied at the output, such as the hyperbolic tangent\\nfunction.\\n4.5 Applications\\n77\\nFurther, PCNN [75], which is a variation of CNN, adopts a piece-wise max-\\npooling operation. All hidden vectors {p1, p2, . . .} are divided into three segments\\nby the head and tail entities. The max-pooling operation is performed over the three\\nsegments separately, and the x is the concatenation of the pooling results over the\\nthree segments.\\n(2) Recurrent Neural Network Encoder. Zhang and Wang [78] propose to embed\\ninput sentences using an RNN model which can learn the temporal features. Formally,\\neach input word representation is put into recurrent layers step by step. For each step\\ni, the network takes the ith word representation vector wi and the output of the\\nprevious i \\xe2\\x88\\x921 steps hi\\xe2\\x88\\x921 as input:\\nhi = RNN(wi, hi\\xe2\\x88\\x921),\\n(4.32)\\nwhere RNN indicates the transform function inside the RNN cell, which can be the\\nLSTM units or the GRU units mentioned before.\\nThe conventional RNN models typically deal with text sequences from start to\\nend, and build the hidden state of each word only considering its preceding words.\\nIt has been veri\\xef\\xac\\x81ed that the hidden state considering its following words is more\\neffective. Hence, the bi-directional RNN (BRNN) [52] is adopted to learn hidden\\nstates using both preceding and following words.\\nSimilar to the previous CNN models in RE, the RNN model combines the output\\nvectors of the recurrent layer as local features, and then uses a max-pooling operation\\nto extract the global feature, which forms the representation of the whole input\\nsentence. The max-pooling layer could be formulated as\\n[x] j = max\\ni\\n[hi] j.\\n(4.33)\\nBesides max-pooling, word attention can also combine all local feature vectors\\ntogether. The attention mechanism [1] learns attention weights on each step. Sup-\\nposing H = [h1, h2, . . . , hm] is the matrix consisting of all output vectors produced\\nby the recurrent layer, the feature vector of the whole sentence x is formed by a\\nweighted sum of these output vectors:\\n\\xce\\xb1 = Softmax(s\\xe2\\x8a\\xa4tanh(H)),\\n(4.34)\\nx = H\\xce\\xb1\\xe2\\x8a\\xa4,\\n(4.35)\\nwhere s is a trainable query vector.\\nBesides,[42]proposesamodelthatcapturesinformationfrombothwordsequence\\nand tree-structured dependency by stacking bidirectional path-based LSTM-RNNs\\n(i.e., bottom-up and top-down). More speci\\xef\\xac\\x81cally, it focuses on the shortest path\\nbetween the two target entities in the dependency tree, and utilizes the stacked layers\\nto encode the shortest path for the whole sentence representation. In fact, some\\npreliminary work [69] has shown that these paths are useful in RE, and various\\n78\\n4\\nSentence Representation\\nrecursive neural models are also proposed for this. Next, we will introduce these\\nrecursive models in detail.\\n(3) Recursive Neural Network Encoder. The recursive encoder aims to extract\\nfeatures from the information of syntactic parsing trees, considering the syntactic\\ninformation is bene\\xef\\xac\\x81cial for extracting relations from sentences. Generally, these\\nencoders treat the tree structure inside syntactic parsing trees as a strategy of com-\\nposition as well as a direction to combine each word feature.\\nSocher et al. [54] propose a recursive matrix-vector model (MV-RNN) which\\ncaptures the structure information by assigning a matrix-vector representation for\\neach constituent of the constituents in parsing trees. The vector captures the meaning\\nof the constituent itself and the matrix represents how it modi\\xef\\xac\\x81es the meaning of the\\nword it combines with. Tai et al. [62] further propose two types of tree-structured\\nLSTMs including the Child-Sum Tree-LSTM and the N-ary Tree-LSTM to capture\\ntree structure information. For the Child-Sum Tree-LSTM, given a tree, let C(t)\\ndenote the set of children of node t. Its transition equations are de\\xef\\xac\\x81ned as follows:\\n\\xcb\\x86ht =\\n\\x02\\nk\\xe2\\x88\\x88C(t)\\nTLSTM(hk),\\n(4.36)\\nwhere TLSTM(\\xc2\\xb7) indicates a Tree-LSTM cell, which is simply modi\\xef\\xac\\x81ed from LSTM\\ncell. The N-ary Tree-LSTM has similar transition equations as the Child-Sum Tree-\\nLSTM. The only difference is that it limits the tree structures to have at most N\\nbranches.\\nRelation Classi\\xef\\xac\\x81er. When obtaining the representation x of the input sentence,\\nrelation classi\\xef\\xac\\x81er calculates the conditional probability P(r|x, e1, e2) via a softmax\\nlayer as follows:\\nP(r|x, e1, e2) = Softmax(Mx + b),\\n(4.37)\\nwhere M indicates the relation matrix and b is a bias vector.\\n4.5.2.2\\nBag-Level NRE\\nAlthough existing neural models have achieved great success for extracting novel\\nrelational facts, it always suffers the lack of training data. To address this issue,\\nresearchers proposed a distant supervision assumption to generate training data\\nvia aligning KGs and plain text automatically. The intuition of distant supervision\\nassumption is that all sentences that contain two entities will express their relations\\nin KGs. For example, (New York, city_of, United States) is a relational\\nfact in a KG, distant supervision assumption will regard all sentences that contain\\nthese two entities as positive instances for the relation city_of. It offers a natural\\nway of utilizing information from multiple sentences (bag-level) rather than a single\\nsentence (sentence-level) to decide if a relation holds between two entities.\\nTherefore, bag-level NRE aims to predict the semantic relations between an entity\\npair using all involved sentences. As shown in Fig.4.7, given the input sentence set\\n4.5 Applications\\n79\\nFig. 4.7 An example of bag-level relation extraction\\nS which consists of n sentences S = {s1, s2, . . . , sn} and its corresponding entity\\npair e1 and e2 as inputs, bag-level NRE wants to obtain the conditional probability\\nP(r|S, e1, e2) of relation r (r \\xe2\\x88\\x88R) via a neural network, which can be formalized\\nas\\nP(r|S, e1, e2) = P(r|S, e1, e2, \\xce\\xb8).\\n(4.38)\\nA basic form of bag-level NRE consists of four components: (a) an input encoder\\nsimilar to sentence-level NRE, (b) a sentence encoder similar to sentence-level NRE,\\n(c) a bag encoder which computes a vector representing all related sentences in a bag,\\nand (d) a relation classi\\xef\\xac\\x81er similar to sentence-level NRE which takes bag vectors\\nas input instead of sentence vectors. As the input encoder, sentence encoder, and\\nrelation classi\\xef\\xac\\x81er of bag-level NRE are similar to the ones of sentence-level NRE,\\nwe will thus mainly focus on introducing the bag encoder in detail.\\nBag Encoder. The bag encoder encodes all sentence vectors into a single vector\\nS. We will introduce the different bag encoders in the following:\\n(1) Random Encoder. It simply assumes that each sentence can express the relation\\nbetween two target entities and randomly select one sentence to represent the bag.\\nFormally, the bag representation is de\\xef\\xac\\x81ned as\\nS = si (i \\xe2\\x88\\x88{1, 2, . . . , n}),\\n(4.39)\\nwhere si indicates the sentence representation of si \\xe2\\x88\\x88S and i is a random index.\\n(2) Max Encoder. As introduced above, not all sentences containing two target\\nentities can express their relations. For example, the sentence New York City\\nis the premier gateway for legal immigration to the\\nUnited States does not express the relation city of. Hence, in [75], they\\nfollow the at-least-one assumption which assumes that at least one sentence that\\ncontains these two target entities can express their relations, and select the sentence\\n80\\n4\\nSentence Representation\\nwith the highest probability for the relation to represent the bag. Formally, bag rep-\\nresentation is de\\xef\\xac\\x81ned as\\nS = si (i = arg max\\ni\\nP(r|si, e1, e2)).\\n(4.40)\\n(3) Average Encoder. Both random encoder or max encoder use only one sentence\\nto represent the bag, which ignores the rich information of different sentences. To\\nexploit the information of all sentences, [36] believes that the representation S of\\nthe bag depends on all sentences\\xe2\\x80\\x99 representations. Each sentence representation si\\ncan give the relation information about two entities to a certain extent. The average\\nencoder assumes that all sentences contribute equally to the representation of the\\nbag. It means the embedding S of the bag is the average of all the sentence vectors:\\nS =\\n\\x02\\ni\\n1\\nn si.\\n(4.41)\\n(4) Attentive Encoder. Due to the wrong label issue brought by distant supervision\\nassumption inevitably, the performance of average encoder will be in\\xef\\xac\\x82uenced by\\nthose sentences that contain no relation information. To address this issue, [36]\\nfurther proposes to employ a selective attention to reduce those noisy sentences.\\nFormally, the bag representation is de\\xef\\xac\\x81ned as a weighted sum of sentence vectors:\\nS =\\n\\x02\\ni\\n\\xce\\xb1isi,\\n(4.42)\\nwhere \\xce\\xb1i is de\\xef\\xac\\x81ned as\\n\\xce\\xb1i =\\nexp(s\\xe2\\x8a\\xa4\\ni Ar)\\n\\x04\\nj exp(x\\xe2\\x8a\\xa4\\nj Ar),\\n(4.43)\\nwhere A is a diagonal matrix and r is the representation vector of relation r.\\nRelation Classi\\xef\\xac\\x81er. Similar to sentence-level NRE, when obtaining the bag rep-\\nresentation S, relation classi\\xef\\xac\\x81er also calculates the conditional probability P(r|S, e1,\\ne2) via a softmax layer as follows:\\nP(r|S, e1, e2) = Softmax(MS + b),\\n(4.44)\\nwhere M indicates the relation matrix and b is a bias vector.\\n4.5.2.3\\nExtensions\\nRecently, NRE systems have achieved signi\\xef\\xac\\x81cant improvements in both, the super-\\nvised and distantly supervised scenarios. However, there are still many challenges in\\nthe task of RE, and many researchers have been focusing on other aspects to improve\\n4.5 Applications\\n81\\nthe performance of NRE as well. In this section, we will introduce these extensions\\nin detail.\\nUtilization of External Information. Most existing NRE systems stated above\\nonly concentrate on the sentences which are extracted, regardless of the rich external\\ninformation such as KGs. This heterogeneous information could provide additional\\nknowledge from KG and is essential when extracting new relational facts.\\nHan et al. [24] propose a novel joint representation learning framework for knowl-\\nedge acquisition. The key idea is that the joint model learns knowledge and text\\nrepresentations within a uni\\xef\\xac\\x81ed semantic space via KG-text alignments. For the text\\npart, the sentence with two entities Mark Twain and Florida is regarded as\\nthe input for a CNN encoder, and the output of CNN is considered to be the latent\\nrelation PlaceOfBirth of this sentence. For the KG part, entity and relation rep-\\nresentations are learned via translation-based methods. The learned representations\\nof KG and text parts are aligned during training. Besides this preliminary attempt,\\nmany efforts have been devoted to this direction [25, 28, 51, 67, 68].\\nIncorporating Relational Paths. Although existing NRE systems have achieved\\npromising results, they still suffer a major problem: the models can only directly\\nlearn from those sentences which contain both two-target entities. However, those\\nsentences containing only one of the entities could also provide useful information\\nand help build inference chains. For example, if we know that \\xe2\\x80\\x9cA is the son of B\\xe2\\x80\\x9d\\nand \\xe2\\x80\\x9cB is the son of C\\xe2\\x80\\x9d, we can infer that A is the grandson of C.\\nTo utilize the information of both direct and indirect sentences, [77] introduces\\na path-based NRE model that incorporates textual relational paths. The model \\xef\\xac\\x81rst\\nemploys a CNN encoder to embed the semantic meanings of sentences. Then, the\\nmodel builds a relation path encoder, which measures the probability of relations\\ngiven an inference chain in the text. Finally, the model combines information from\\nboth direct sentences and relational paths, and then predicts the con\\xef\\xac\\x81dence of each\\nrelationship. This work is the \\xef\\xac\\x81rst effort to consider the knowledge of relation path\\nin text for NRE, and there are also several methods later to consider the reasoning\\npath of sentence semantic meanings for RE [11, 19].\\nDocument-level Relation Extraction. In fact, not all relational facts can be\\nextracted by sentence-level RE, i.e., a large number of relational facts are expressed\\nin multiple sentences. Taking Fig.4.9 as an example, multiple entities are mentioned\\nin the document and exhibit complex interactions. In order to identify the relational\\nfact (Riddarhuset, country, Sweden), one has to \\xef\\xac\\x81rst identify the fact that\\nRiddarhuset is located in Stockholm from Sentence 4, then identify the facts\\nStockholm is the capital of Sweden and Sweden is a country from Sentence 1.\\nWith the above facts, we can \\xef\\xac\\x81nally infer that the sovereign state of Riddarhuset\\nis Sweden. This process requires reading and reasoning over multiple sentences in\\na document, which is intuitively beyond the reach of sentence-level RE methods.\\nAccording to the statistics on a human-annotated corpus sampled from Wikipedia\\ndocuments [72], at least 40.7% relational facts can only be extracted from multi-\\nple sentences, which is not negligible. Swampillai and Stevenson [61] and Verga\\net al. [66] also report similar observations. Therefore, it is necessary to move RE\\n82\\n4\\nSentence Representation\\nFig. 4.8 An example of document-level relation extraction\\nforward from the sentence level to the document level. Figure4.8 is an example for\\ndocument-level RE.\\nFig. 4.9 An example from DocRED [72]\\n4.5 Applications\\n83\\nHowever, existing datasets for document-level RE either only have a small num-\\nber of manually annotated relations and entities [34], or exhibit noisy annotations\\nfrom distant supervision [45, 49], or serve speci\\xef\\xac\\x81c domains or approaches [33]. To\\naddress this issue, [72] constructs a large-scale, manually annotated, and general-\\npurpose document-level RE dataset, named as DocRED. DocRED is constructed\\nfrom Wikipedia and Wikidata, and has two key features. First, DocRED contains\\n132, 375 entities and 56, 354 relational facts annotated on 5, 053 Wikipedia docu-\\nments, which is the largest human-annotated document-level RE dataset now. Sec-\\nond, over 40% of the relational facts in DocRED can only be extracted from multiple\\nsentences. This makes DocRED require reading multiple sentences in a document\\nto recognize entities and inferring their relations by synthesizing all information of\\nthe document.\\nThe experimental results on DocRED show that the performance of existing\\nsentence-level RE methods declines signi\\xef\\xac\\x81cantly on DocRED, indicating the task\\ndocument-level RE is more challenging than sentence-level RE and remains an open\\nproblem. It also relates to the document representation which will be introduced in\\nthe next chapter.\\nFew-shot Relation Extraction.\\nAs we mentioned before, the performance of the conventional RE models [23,\\n76] heavily depend on time-consuming and labor-intensive annotated data, which\\nmake themselves hard to generalize well. Although adopting distant supervision\\nis a primary approach to alleviate this problem, the distantly supervised data also\\nexhibits a long-tail distribution, where most relations have very limited instances.\\nFurthermore, distant supervision suffers the wrong labeling problem, which makes\\nit harder to classify long-tail relations. Hence, it is necessary to study training RE\\nmodels with insuf\\xef\\xac\\x81cient training instances. Figure4.10 is an example for few-shot\\nRE.\\nFig. 4.10 An example of few-shot relation extraction\\n84\\n4\\nSentence Representation\\nTable 4.1 An example for a 3 way 2 shot scenario. Different colors indicate different entities,\\nunderline for head entity, and emphasize for tail entity\\nSupporting set\\n(A) capital_of\\n(1) London is the capital of the U.K\\n(2) Washington is the capital of the U.S.A\\n(B) member_of\\n(1) Newton served as the president of the Royal\\nSociety\\n(2) Leibniz was a member of the Prussian\\nAcademy of Sciences\\n(C) birth_name\\n(1) Samuel Langhorne Clemens, better known\\nby his pen name Mark Twain, was an American\\nwriter\\n(2) Alexei Maximovich Peshkov, primarily\\nknown as Maxim Gorky, was a Russian and\\nSoviet writer\\nTest instance\\n(A) or (B) or (C)\\nEuler was elected a foreign member of the\\nRoyal Swedish Academy of Sciences\\nFewRel [26] is a new large-scale supervised few-shot RE dataset, which requires\\nmodels capable of handling classi\\xef\\xac\\x81cation task with a handful of training instances,\\nas shown in Table4.1. Bene\\xef\\xac\\x81ting from the FewRel dataset, there are some efforts to\\nexploring few-shot RE [17, 53, 73] and achieve promising results. Yet, few-shot RE\\nstill remains a challenging problem for further research [18].\\n4.6\\nSummary\\nIn this chapter, we introduce sentence representation learning. Sentence representa-\\ntion encodes the semantic information of a sentence into a real-valued representation\\nvector, and can be utilized in further sentence classi\\xef\\xac\\x81cation or matching tasks. First,\\nwe introduce the one-hot representation for sentences and probabilistic language\\nmodels. Secondly, we extensively introduce several neural language models, includ-\\ning adopting the feedforward neural networks, the convolutional neural networks, the\\nrecurrent neural networks, and the Transformer for language models. These neural\\nmodels can learn rich linguistic and semantic knowledge from language modeling.\\nBene\\xef\\xac\\x81ting from this, the pre-trained language models trained with large-scale cor-\\npora have achieved state-of-the-art performance on various downstream NLP tasks\\nby transferring the learned semantic knowledge from general corpora to the target\\ntasks. Finally, we introduce several typical applications of sentence representation\\nincluding text classi\\xef\\xac\\x81cation and relation extraction.\\n4.6 Summary\\n85\\nFor further understanding of sentence representation learning and its applications,\\nthere are also some recommended surveys and books including\\n\\xe2\\x80\\xa2 Yoav, Neural network methods for natural language processing [21].\\n\\xe2\\x80\\xa2 Deng & Liu, Deep learning in natural language processing [13].\\nIn the future, for better sentence representation, some directions are requiring\\nfurther efforts:\\n(1) Exploring Advanced Architectures. The improvement of model architectures\\nis the key factor in the success of sentence representation. From the feedforward\\nneural networks to the Transformer, people are designing more suitable neural\\nmodels for sequential inputs. Based on the Transformer, some researchers are\\nworking on new NLP architectures. For instance, Transformer-XL [10] is pro-\\nposed to solve the problem of \\xef\\xac\\x81xed-length context in the Transformer. Since\\nthe Transformer is the state-of-the-art NLP architecture, current works mainly\\nadopt attention mechanisms. Beyond these works, is it possible to introduce\\nmore human cognitive mechanisms to neural models?\\n(2) Modeling Long Documents. The representation of long documents is an impor-\\ntant extension of sentence representation. There are some new challenges during\\nmodeling long documents, such as discourse analysis and co-reference resolu-\\ntion. Although some existing works already provide document-level NLP tasks\\n(e.g., DocRED [72]), the model performance on these tasks is still much lower\\nthan the human performance. We will also introduce the advances in document\\nrepresentation learning in the following chapter.\\n(3) Performing Ef\\xef\\xac\\x81cient Representation. Although the combination of Trans-\\nformer and large-scale data leads to very powerful sentence representation, these\\nrepresentation models require expensive computational cost, which limits the\\napplications in downstream tasks. Some existing works explore to use model\\ncompression techniques for more ef\\xef\\xac\\x81cient models. These techniques include\\nknowledge distillation [60], parameter pruning [16], etc. Beyond these works,\\nthere remain lots of unsolved problems for developing better representation mod-\\nels, which can ef\\xef\\xac\\x81ciently learn from large-scale data and provide effective vectors\\nin downstream tasks.\\nReferences\\n1. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. In Proceedings of ICLR, 2015.\\n2. Yoshua Bengio. Neural net language models. Scholarpedia, 3(1):3881, 2008.\\n3. Yoshua Bengio, R\\xc3\\xa9jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\\nlanguage model. Journal of Machine Learning Research, 3(Feb):1137\\xe2\\x80\\x931155, 2003.\\n4. Yoshua Bengio, Jean-S\\xc3\\xa9bastien Sen\\xc3\\xa9cal, et al. Quick training of probabilistic neural nets by\\nimportance sampling. In Proceedings of AISTATS, 2003.\\n86\\n4\\nSentence Representation\\n5. Parminder Bhatia, Yangfeng Ji, and Jacob Eisenstein. Better document-level sentiment analysis\\nfrom rst discourse parsing. In Proceedings of EMNLP, 2015.\\n6. David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of\\nMachine Learning Research, 3:993\\xe2\\x80\\x931022, 2003.\\n7. Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai.\\nClass-based n-gram models of natural language. Computational linguistics, 18(4):467\\xe2\\x80\\x93479,\\n1992.\\n8. Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In Pro-\\nceedings of NeurIPS, 2019.\\n9. Alexis Conneau, Holger Schwenk, Lo\\xc3\\xafc Barrault, and Yann Lecun. Very deep convolutional\\nnetworks for text classi\\xef\\xac\\x81cation. In Proceedings of EACL, volume 1, 2017.\\n10. ZihangDai,ZhilinYang,YimingYang,JaimeGCarbonell,QuocLe,andRuslanSalakhutdinov.\\nTransformer-xl: Attentive language models beyond a \\xef\\xac\\x81xed-length context. In Proceedings of\\nACL, page 2978\\xe2\\x80\\x932988, 2019.\\n11. Rajarshi Das, Arvind Neelakantan, David Belanger, and Andrew McCallum. Chains of reason-\\ning over entities, relations, and text using recurrent neural networks. In Proceedings of EACL,\\npages 132\\xe2\\x80\\x93141, 2017.\\n12. Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with\\ngated convolutional networks. In Proceedings of ICML, 2017.\\n13. Li Deng and Yang Liu. Deep learning in natural language processing. Springer, 2018.\\n14. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019.\\n15. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming\\nZhou, and Hsiao-Wuen Hon. Uni\\xef\\xac\\x81ed language model pre-training for natural language under-\\nstanding and generation. In Proceedings of NeurIPS, 2019.\\n16. Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with\\nstructured dropout. In Proceedings of ICLR, 2020.\\n17. Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun. Hybrid attention-based prototypical\\nnetworks for noisy few-shot relation classi\\xef\\xac\\x81cation. In Proceedings of AAAI, pages 6407\\xe2\\x80\\x936414,\\n2019.\\n18. Tianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. FewRel 2.0:\\nTowards more challenging few-shot relation classi\\xef\\xac\\x81cation. In Proceedings of EMNLP-IJCNLP,\\npages 6251\\xe2\\x80\\x936256, 2019.\\n19. Michael Glass, Al\\xef\\xac\\x81o Gliozzo, Oktie Hassanzadeh, Nandana Mihindukulasooriya, and Gae-\\ntano Rossiello. Inducing implicit relations from text using distantly supervised deep nets. In\\nInternational Semantic Web Conference, pages 38\\xe2\\x80\\x9355. Springer, 2018.\\n20. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale senti-\\nment classi\\xef\\xac\\x81cation: A deep learning approach. In Proceedings of ICML, 2011.\\n21. Yoav Goldberg. Neural network methods for natural language processing. Synthesis Lectures\\non Human Language Technologies, 10(1):1\\xe2\\x80\\x93309, 2017.\\n22. Joshua Goodman. Classes for fast maximum entropy training. In Proceedings of ASSP, 2001.\\n23. Matthew R Gormley, Mo Yu, and Mark Dredze. Improved relation extraction with feature-rich\\ncompositional embedding models. In Proceedings of EMNLP, 2015.\\n24. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge\\nfor knowledge graph completion. arXiv preprint arXiv:1611.04125, 2016.\\n25. Xu Han, Zhiyuan Liu, and Maosong Sun. Neural knowledge acquisition via mutual attention\\nbetween knowledge graph and text. In Proceedings of AAAI, pages 4832\\xe2\\x80\\x934839, 2018.\\n26. Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun.\\nFewRel: A large-scale supervised few-shot relation classi\\xef\\xac\\x81cation dataset with state-of-the-art\\nevaluation. In Proceedings of EMNLP, 2018.\\n27. Zhiheng Huang, Geoffrey Zweig, and Benoit Dumoulin. Cache based recurrent neural network\\nlanguage model inference for \\xef\\xac\\x81rst pass speech recognition. In Proceedings of ICASSP, 2014.\\n28. Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao. Distant supervision for relation extraction\\nwith sentence-level attention and entity descriptions. In Proceedings of AAAI, pages 3060\\xe2\\x80\\x93\\n3066, 2017.\\nReferences\\n87\\n29. Rie Johnson and Tong Zhang. Effective use of word order for text categorization with convo-\\nlutional neural networks. In Proceedings of ACL-HLT, 2015.\\n30. Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network\\nfor modelling sentences. In Proceedings of ACL, 2014.\\n31. Yoon Kim. Convolutional neural networks for sentence classi\\xef\\xac\\x81cation. In Proceedings of\\nEMNLP, 2014.\\n32. Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. Recurrent convolutional neural networks for\\ntext classi\\xef\\xac\\x81cation. In Proceedings of AAAI, 2015.\\n33. Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction\\nvia reading comprehension. In Proceedings of CoNLL, 2017.\\n34. Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman,\\nAllan Peter Davis, Carolyn J. Mattingly, Thomas C. Wiegers, and Zhiyong Lu. BioCreative V\\nCDR task corpus: a resource for chemical disease relation extraction. Database, pages 1\\xe2\\x80\\x9310,\\n2016.\\n35. Jiwei Li, Minh-Thang Luong, Dan Jurafsky, and Eduard Hovy. When are tree structures nec-\\nessary for deep learning of representations? In Proceedings of EMNLP, 2015.\\n36. Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Neural relation extrac-\\ntion with selective attention over instances. In Proceedings of ACL, 2016.\\n37. Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Recurrent neural network for text classi\\xef\\xac\\x81cation\\nwith multi-task learning. In Proceedings of IJCAI, 2016.\\n38. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\\napproach. arXiv preprint arXiv:1907.11692, 2019.\\n39. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visi-\\nolinguistic representations for vision-and-language tasks. In Proceedings of NeurIPS, 2019.\\n40. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\\xef\\xac\\x81cient estimation of word\\nrepresentations in vector space. In Proceedings of ICLR, 2013.\\n41. Tomas Mikolov, Martin Kara\\xef\\xac\\x81\\xc3\\xa1t, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recur-\\nrent neural network based language model. In Proceedings of InterSpeech, 2010.\\n42. Makoto Miwa and Mohit Bansal. End-to-end relation extraction using lstms on sequences and\\ntree structures. In Proceedings of ACL, 2016.\\n43. Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic\\nlanguage models. In Proceedings of ICML, 2012.\\n44. Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model.\\nIn Proceedings of Aistats, 2005.\\n45. Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. Cross-\\nsentence n-ary relation extraction with graph LSTMs. Transactions of the Association for\\nComputational Linguistics, 5:101\\xe2\\x80\\x93115, 2017.\\n46. Matthew E Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh,\\nand Noah A Smith. Knowledge enhanced contextual word representations. In Proceedings of\\nEMNLP-IJCNLP, 2019.\\n47. Ngoc-Quan Pham, German Kruszewski, and Gemma Boleda. Convolutional neural network\\nlanguage models. In Proceedings of EMNLP, 2016.\\n48. Matt Post and Shane Bergsma. Explicit and implicit syntactic features for text classi\\xef\\xac\\x81cation.\\nIn Proceedings of ACL, 2013.\\n49. Chris Quirk and Hoifung Poon. Distant supervision for relation extraction beyond the sentence\\nboundary. In Proceedings of EACL, 2017.\\n50. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training. URL https://s3-us-west-2.amazonaws.com/openai-\\nassets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf, 2018.\\n51. Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M Marlin. Relation extraction\\nwith matrix factorization and universal schemas. In Proceedings of NAACL-HLT, pages 74\\xe2\\x80\\x9384,\\n2013.\\n88\\n4\\nSentence Representation\\n52. Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE Transac-\\ntions on Signal Processing, 45(11):2673\\xe2\\x80\\x932681, 1997.\\n53. Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. Matching the\\nBlanks: Distributional similarity for relation learning. In Proceedings of ACL, pages 2895\\xe2\\x80\\x93\\n2905, 2019.\\n54. Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. Semantic compo-\\nsitionality through recursive matrix-vector spaces. In Proceedings of EMNLP, 2012.\\n55. Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning.\\nSemi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings\\nof EMNLP, 2011.\\n56. Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y\\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over a senti-\\nment treebank. In Proceedings of EMNLP, 2013.\\n57. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to\\nsequence pre-training for language generation. In Proceedings of ICML, 2019.\\n58. Daniel Soutner, Zden\\xcb\\x87ek Loose, Lud\\xcb\\x87ek M\\xc3\\xbcller, and Ale\\xc5\\xa1 Pra\\xc5\\xbe\\xc3\\xa1k. Neural network language\\nmodel with cache. In Proceedings of ICTSD, 2012.\\n59. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A\\njoint model for video and language representation learning. In Proceedings of ICCV, 2019.\\n60. Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model\\ncompression. In Proceedings of EMNLP-IJCNLP, page 4314\\xe2\\x80\\x934323, 2019.\\n61. Kumutha Swampillai and Mark Stevenson. Inter-sentential relations in information extraction\\ncorpora. In Proceedings of LREC, 2010.\\n62. Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representa-\\ntions from tree-structured long short-term memory networks. In Proceedings of ACL, 2015.\\n63. Duyu Tang, Bing Qin, and Ting Liu. Document modeling with gated recurrent neural network\\nfor sentiment classi\\xef\\xac\\x81cation. In Proceedings of EMNLP, 2015.\\n64. Wilson L Taylor. \\xe2\\x80\\x9ccloze procedure\": A new tool for measuring readability. Journalism Bulletin,\\n30(4):415\\xe2\\x80\\x93433, 1953.\\n65. Ashish Vaswani, Noam Shazeer, Niki Parmar, Llion Jones, Jakob Uszkoreit, Aidan N Gomez,\\nand Lukasz Kaiser. Attention is all you need. In Proceedings of NeurIPS, 2017.\\n66. Patrick Verga, Emma Strubell, and Andrew McCallum. Simultaneously self-attending to all\\nmentions for full-abstract biological relation extraction. In Proceedings of NAACL-HLT, 2018.\\n67. Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph and text jointly\\nembedding. In Proceedings of EMNLP, pages 1591\\xe2\\x80\\x931601, 2014.\\n68. Zhigang Wang and Juan-Zi Li. Text-enhanced representation learning for knowledge graph. In\\nProceedings of IJCAI, pages 1293\\xe2\\x80\\x931299, 2016.\\n69. Kun Xu, Yansong Feng, Songfang Huang, and Dongyan Zhao. Semantic relation classi\\xef\\xac\\x81cation\\nvia convolutional neural networks with simple negative sampling. In Proceedings of EMNLP,\\n2015.\\n70. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V\\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. In Proceedings\\nof NeurIPS, 2019.\\n71. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. Hierarchical\\nattention networks for document classi\\xef\\xac\\x81cation. In Proceedings of NAACL, 2016.\\n72. Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang,\\nJie Zhou, and Maosong Sun. DocRED: A large-scale document-level relation extraction dataset.\\nIn Proceedings of ACL, 2019.\\n73. Zhi-Xiu Ye and Zhen-Hua Ling. Multi-level matching and aggregation network for few-shot\\nrelation classi\\xef\\xac\\x81cation. In Proceedings of ACL, pages 2872\\xe2\\x80\\x932881, 2019.\\n74. Wenpeng Yin and Hinrich Sch\\xc3\\xbctze. Multichannel variable-size convolution for sentence clas-\\nsi\\xef\\xac\\x81cation. In Proceedings of CoNLL, 2015.\\n75. Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. Distant supervision for relation extraction\\nvia piecewise convolutional neural networks. In Proceedings of EMNLP, 2015.\\nReferences\\n89\\n76. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation classi\\xef\\xac\\x81cation via\\nconvolutional deep neural network. In Proceedings of COLING, 2014.\\n77. Wenyuan Zeng, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Incorporating relation paths in\\nneural relation extraction. In Proceedings of EMNLP, 2017.\\n78. Dongxu Zhang and Dong Wang. Relation classi\\xef\\xac\\x81cation via recurrent neural network. arXiv\\npreprint arXiv:1508.01006, 2015.\\n79. Ye Zhang, Iain Marshall, and Byron C Wallace. Rationale-augmented convolutional neural\\nnetworks for text classi\\xef\\xac\\x81cation. In Proceedings of EMNLP, 2016.\\n80. Ye Zhang, Stephen Roller, and Byron C Wallace. Mgnc-cnn: A simple approach to exploiting\\nmultiple word embeddings for sentence classi\\xef\\xac\\x81cation. In Proceedings of NAACL, 2016.\\n81. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie:\\nEnhanced language representation with informative entities. In Proceedings of ACL, 2019.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 5\\nRETRACTED CHAPTER: Document\\nRepresentation\\nThe authors have retracted this Chapter because signi\\xef\\xac\\x81cant portions of the text are\\nduplicated from [1] and [2]. The authors apologise to readers for this error. All authors\\nagree with this retraction.\\n1. Blei DM. Probabilistic Topic Models. Communications of the ACM, April 2012,\\nVol. 55 No. 4, Pages 77\\xe2\\x80\\x9384, 10.1145/2133806.2133826\\n2. Le Quoc and Tomas Mikolov. Distributed Representations of Sentences and Doc-\\numents. Proceedings of the 31st International Conference on Machine Learning,\\nPMLR 32(2):1188\\xe2\\x80\\x931196, 2014\\n\\xc2\\xa9 The Author(s) 2020, corrected publication 2023\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_5\\n91\\nChapter 6\\nSememe Knowledge Representation\\nAbstract Linguistic Knowledge Graphs (e.g., WordNet and HowNet) describe lin-\\nguistic knowledge in formal and structural language, which can be easily incorpo-\\nrated in modern natural language processing systems. In this chapter, we focus on\\nthe research about HowNet. We \\xef\\xac\\x81rst brie\\xef\\xac\\x82y introduce the background and basic\\nconcepts of HowNet and sememe. Next, we introduce the motivations of sememe\\nrepresentation learning and existing approaches. At the end of this chapter, we review\\nimportant applications of sememe representation.\\n6.1\\nIntroduction\\nIn the \\xef\\xac\\x81eld of Natural Language Processing (NLP), words are generally the smallest\\nobjects of study because they are considered as the smallest meaningful units that\\ncan stand by themselves of human languages. However, the meanings of words\\ncan be further divided into smaller parts. For example, the meaning of man can be\\nconsidered as the combination of the meanings of human, male and adult, and\\nthe meaning of boy is composed of the meanings of human, male, and child.\\nIn linguistics, the minimum indivisible units of meaning, i.e., semantic units, are\\nde\\xef\\xac\\x81ned as sememes [8]. And some linguists believe that meanings of all the words\\ncan be composed of a limited closed set of sememes.\\nHowever, sememes are implicit and as a result, it is hard to intuitively de\\xef\\xac\\x81ne the set\\nof sememes and determine which sememes a word can have at a glance. Therefore,\\nsome researchers spend tens of years sifting sememes from all kinds of dictionaries\\nand linguistic Knowledge Bases (KBs), and annotating words with these selected\\nsememes to construct sememe-based linguistic KB. WordNet and HowNet [17] are\\nthe two most famous ones of such KBs. In this section, we focus on the representation\\nof linguistic knowledge in HowNet.\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_6\\n125\\n126\\n6\\nSememe Knowledge Representation\\n6.1.1\\nLinguistic Knowledge Graphs\\n6.1.1.1\\nWordNet\\nWordNet is a large lexical database for the English language and could also be viewed\\nas a KG containing multi-relational data. It was \\xef\\xac\\x81rst started in 1985, and created under\\nthe direction of George Armitage Miller, a psychology professor in the Cognitive\\nScience Laboratory of Princeton University. Nowadays, WordNet is becoming the\\nmost popular lexicon dictionary in the world that could be available through the Web\\nfor free and is widely used in NLP applications such as text analysis, information\\nretrieval, and relation extraction. There is also a Global WordNet Association aiming\\nto provide a public and noncommercial platform for WordNets of all languages in\\nthe world.\\nBased on meanings, WordNet groups English nouns, verbs, adjectives, and\\nadverbs into synsets (i.e., sets of cognitive synonyms), which represent a distinct\\nconcept. Each synset possesses a brief description, and in most cases, there are even\\nsome short sentences functioning as examples illustrating the use of words in this\\nsynset. The conceptual-semantic and lexical relations link the synsets and words. The\\nmain relation among words is synonymy, which indicates that the words share similar\\nmeanings and could be replaced by others in some contexts, while the main relation\\namong synsets is hyperonymy/hyponymy (i.e., the ISA relation), which indicates\\nthe relationship between a more general synset and a more speci\\xef\\xac\\x81c synset. There are\\nalso hierarchical structures for verb synsets, and the antonymy is describing the rela-\\ntion between adjectives with opposite meanings. To sum up, all WordNets\\xe2\\x80\\x99 117, 000\\nsynsets are linked to each other by a small number of conceptual relations.\\n6.1.1.2\\nHowNet\\nHowNet was initially designed and constructed by Zhendong Dong and his son Qiang\\nDong in the 1990s. And it has been kept frequently updated since it was published\\nin 1999.\\nThe sememe set of HowNet is determined by extracting, analyzing, merging,\\nand \\xef\\xac\\x81ltering semantics of thousands of Chinese characters. And the sememe set can\\nalso be adjusted or expanded in the subsequent process of annotating words. Each\\nsememe in HowNet is represented by a word or phrase in Chinese and English such\\nas (human | {\\n}) and (ProperName | {\\n}).\\nHowNet also builds a taxonomy for the sememes. All the sememes of HowNet\\ncan be classi\\xef\\xac\\x81ed as one of the following types: Thing, Part, Attribute, Time, Space,\\nAttribute Value, and Event. In addition, to depict the semantics of words more pre-\\ncisely, HowNet incorporates relations between sememes, which are called \\xe2\\x80\\x9cdynamic\\nroles\\xe2\\x80\\x9d, into the sememe annotations of words.\\nConsidering the polysemy, HowNet differentiates diverse senses of each word\\nin the sememe annotations. And each sense is also expressed in both Chinese and\\n6.1 Introduction\\n127\\nword\\nsense\\nsememe\\napple\\ncomputer\\nbring\\nbring\\nSpecificBrand\\nSpecificBrand\\ncommunicate\\nPatternValue\\nPatternValue\\ntool\\nfruit\\nfruit\\nable\\nable\\ntree\\nreproduce\\napple(computer)\\napple(phone)\\napple(fruit)\\napple(tree)\\npatient\\npatient\\nagent\\nmodifier\\nCoEvent\\nCoEvent\\ninstrument\\nPatientProdect\\nscope\\nscope\\nmodifier\\nFig. 6.1 An example of word annotated with sememes in HowNet\\nTable 6.1 Statistics of HowNet\\nType\\nCount\\nSense\\n229,767\\nDistinct Chinese word\\n127,266\\nDistinct English word\\n104,025\\nSememe\\n2,187\\nEnglish. An example of sememe annotation for a word is illustrated in Fig.6.1.\\nWe can see from the \\xef\\xac\\x81gure that the word apple has four senses including\\napple(computer), apple(phone), apple(fruit), and apple(tree),\\nand each sense is the root node of a \\xe2\\x80\\x9csememe tree\\xe2\\x80\\x9d where each pair of father and son\\nsememe nodes is multi-relational. Additionally, HowNet annotates the POS tag for\\neach sense, and adds sentiment category as well as some usage examples for certain\\nsenses.\\nThe latest version of HowNet was published in January 2019 and the statistics are\\nshown in Table6.1.\\nSince HowNet was published, it has attracted wide attention. People use HowNet\\nand sememe in various NLP tasks including word similarity computation [40], word\\nsense disambiguation [70], question classi\\xef\\xac\\x81cation [62], and sentiment analysis [16,\\n20]. Among these researches, [40] is one of the most in\\xef\\xac\\x82uential works, in which the\\nsimilarity of given two words is computed by measuring the degree of resemblance\\nof their sememe trees.\\nRecent years also witnessed some works incorporating sememes into neural net-\\nwork models. Reference [49] proposes a novel word representation learning model\\nnamed SST that reforms Skip-gram [43] by adding contextual attention to senses of\\n128\\n6\\nSememe Knowledge Representation\\nthe target word, which are represented with combinations of corresponding sememes\\xe2\\x80\\x99\\nembeddings. Experimental results show that SST can not only improve the quality\\nof word embeddings but also learn satisfactory sense embeddings to do word sense\\ndisambiguation.\\nReference [23] incorporates sememes into the decoding phase of language mod-\\neling where sememes are predicted \\xef\\xac\\x81rst, and then senses and words are predicted in\\nsuccession. The proposed model shows enhancement in the perplexity of language\\nmodeling and the performance of the downstream task headline generation.\\nBesides, HowNet is also utilized in lexicon expansion [68], semantic rationality\\nevaluation [41], etc.\\nConsidering that human annotation is time-consuming and labor-intensive, some\\nworks attempt to employ machine learning methods to predict sememes for new\\nwords automatically. Reference [66] proposes the task \\xef\\xac\\x81rstly and presents two sim-\\nple but effective models: SPWE, which is based on collaborative \\xef\\xac\\x81ltering, and SPSE,\\nwhich is based on matrix factorization. Reference [30] further takes the internal infor-\\nmation of words into account when predicting sememes and achieves a considerable\\nboost of performance. And [38] takes advantage of de\\xef\\xac\\x81nitions of words to predict\\nsememes. As for [56], they propose the task of cross-lingual lexical sememe pre-\\ndiction and present a bilingual word representation learning and alignment-based\\nmodel, which demonstrates effectiveness in predicting sememes for cross-lingual\\nwords.\\n6.2\\nSememe Knowledge Representation\\nWordRepresentationLearning(WRL)isafundamentalandcriticalstepinmanyNLP\\ntasks such as language modeling [4] and neural machine translation [64]. There have\\nbeen a lot of researches for learning word representations, among which Word2vec\\n[43] achieves a nice balance between effectiveness and ef\\xef\\xac\\x81ciency. In Word2vec, each\\nword corresponds to one single embedding, ignoring the polysemy of most words.\\nTo address this issue, [29] introduces a multi-prototype model for WRL, conducting\\nunsupervised word sense induction and embeddings according to context clusters.\\nReference [13] further utilizes the synset information in WordNet to instruct word\\nsense representation learning.\\nThese previous studies demonstrate that word sense disambiguation is critical\\nfor WRL, and the sememe annotation of word senses in HowNet can provide nec-\\nessary semantic regularization for these tasks [63]. To explore its feasibility, we\\nintroduce the Sememe-Encoded Word Representation Learning (SE-WRL) model,\\nwhich detects word senses and learns representations simultaneously. More specif-\\nically, this framework regards each word sense as a combination of its sememes,\\nand iteratively performs word sense disambiguation according to their contexts and\\nlearns representations of sememes, senses, and words by extending Skip-gram in\\nWord2vec [43]. In this framework, an attention-based method is proposed to select\\nappropriate word senses according to contexts automatically. To take full advantage\\n6.2 Sememe Knowledge Representation\\n129\\nof sememes, we introduce three different learning and attention strategies SSA, SAC,\\nand SAT for SE-WRL, which will be described in the following paragraphs.\\n6.2.1\\nSimple Sememe Aggregation Model\\nThe Simple Sememe Aggregation model (SSA) is a straightforward idea based on\\nSkip-gram model. For each word, SSA considers all sememes in all senses of the\\nword together, and represents the target word using the average of all its sememe\\nembeddings. Formally, we have\\nw = 1\\nm\\n\\x02\\ns(w)\\ni\\n\\xe2\\x88\\x88S(w)\\n\\x02\\nx\\n(si )\\nj\\n\\xe2\\x88\\x88X(w)\\ni\\nx(si)\\nj ,\\n(6.1)\\nwhich means the word embedding of w is composed by the average of all its sememe\\nembeddings. Here, S(w) is the sense set of w and X(w)\\ni\\nis the sememe set of the ith\\nsense of w. m stands for the overall number of sememes belonging to w.\\nThis model follows the assumption that the semantic meaning of a word is com-\\nposed of the semantic units, i.e., sememes. As compared to the conventional Skip-\\ngram model, since sememes are shared by multiple words, this model can utilize\\nsememe information to encode latent semantic correlations between words. In this\\ncase, similar words that share the same sememes may \\xef\\xac\\x81nally obtain similar repre-\\nsentations.\\n6.2.2\\nSememe Attention over Context Model\\nThe SSA Model replaces the target word embedding with the aggregated sememe\\nembeddings to encode sememe information into word representation learning. How-\\never, each word in the SSA model still has only one single representation in different\\ncontexts, which cannot deal with the polysemy of most words. It is intuitive that we\\nshould construct distinct embeddings for a target word according to speci\\xef\\xac\\x81c contexts,\\nwith the favor of word sense annotation in HowNet.\\nTo address this issue, the Sememe Attention over Context model (SAC) is pro-\\nposed. SAC utilizes the attention scheme to automatically select appropriate senses\\nfor context words according to the target word. That is, SAC conducts word sense\\ndisambiguation for context words to learn better representations of target words. The\\nstructure of the SAC model is shown in Fig.6.2.\\nMore speci\\xef\\xac\\x81cally, SAC utilizes the original word embedding for target word w,\\nand uses sememe embeddings to represent context word wc instead of the original\\ncontext word embeddings. Suppose a word typically demonstrates some speci\\xef\\xac\\x81c\\nsenses in one sentence. Here, the target word embedding is employed as attention\\n130\\n6\\nSememe Knowledge Representation\\nsememe\\nsense\\ncontext\\nword\\ns1\\ns2\\ns3\\nAtt\\nAtt\\nAtt\\nwt-2\\nwt-1\\nwt+1\\nwt+2\\nwt\\nFig. 6.2 The architecture of SAC model\\nto select the most appropriate senses to make up the context word embeddings. The\\ncontext word embedding wc can be formalized as follows:\\nwc =\\n|S(wc)|\\n\\x02\\nj=1\\nAtt(s(wc)\\nj\\n)s(wc)\\nj\\n,\\n(6.2)\\nwhere s(wc)\\nj\\nstands for the jth sense embedding of wc, and Att(s(wc)\\nj\\n) represents the\\nattention score of the jth sense with respect to the target word w, de\\xef\\xac\\x81ned as follows:\\nAtt(s(wc)\\nj\\n) =\\nexp(w \\xc2\\xb7 \\xcb\\x86s(wc)\\nj\\n)\\n\\x03|S(wc)|\\nk=1\\nexp(w \\xc2\\xb7 \\xcb\\x86s(wc)\\nk\\n)\\n.\\n(6.3)\\nNote that, when calculating attention, the average of sememe embeddings is used\\nto represent each sense s(wc)\\nj\\n:\\n\\xcb\\x86s(wc)\\nj\\n=\\n1\\n|X(wc)\\nj\\n|\\n|X(wc)\\nj\\n|\\n\\x02\\nk=1\\nx\\n(s j)\\nk\\n.\\n(6.4)\\nThe attention strategy assumes that the more relevant a context word sense embed-\\nding is to the target word w, the more this sense should be considered when building\\ncontext word embeddings. With the favor of attention scheme, each context word\\n6.2 Sememe Knowledge Representation\\n131\\nsememe\\nsense\\ncontext\\nword\\ns1\\ns2\\ns3\\nAtt\\nAtt\\nAtt\\nwt-2\\nwt-1\\nwt+1\\nwt+2\\nwt\\ncontextual\\nembedding\\nFig. 6.3 The architecture of SAT model\\ncan be represented as a particular distribution over its sense. This can be regarded as\\nsoft WSD and it helps learn better word representations.\\n6.2.3\\nSememe Attention over Target Model\\nThe Sememe Attention over Context Model can \\xef\\xac\\x82exibly select appropriate senses\\nand sememes for context words according to the target word. The process can also\\nbe applied to select appropriate senses for the target word by taking context words\\nas attention. Hence, the Sememe Attention over Target model (SAT) is proposed,\\nwhich is shown in Fig.6.3.\\nDifferent from the SAC model, SAT learns the original word embeddings for\\ncontext words and sememe embeddings for target words. Then SAT applies context\\nwords to perform attention over multiple senses of the target word w to build the\\nembedding of w, formalized as follows:\\nw =\\n|S(w)|\\n\\x02\\nj=1\\nAtt(s(w)\\nj\\n)s(w)\\nj ,\\n(6.5)\\nand the context-based attention is de\\xef\\xac\\x81ned as follows:\\n132\\n6\\nSememe Knowledge Representation\\nAtt(s(w)\\nj\\n) =\\nexp(w\\xe2\\x80\\xb2\\nc \\xc2\\xb7 \\xcb\\x86s(w)\\nj )\\n\\x03|S(w)|\\nk=1 exp(w\\xe2\\x80\\xb2c \\xc2\\xb7 \\xcb\\x86s(w)\\nk )\\n,\\n(6.6)\\nwhere the average of sememe embeddings \\xcb\\x86s(w)\\nj\\nis also used to represent each sense\\ns(w)\\nj\\n. Here, w\\xe2\\x80\\xb2\\nc is the context embedding, consisting of a constrained window of word\\nembeddings in C(wi). We have\\nw\\xe2\\x80\\xb2\\nc =\\n1\\n2K \\xe2\\x80\\xb2\\nk=i+K \\xe2\\x80\\xb2\\n\\x02\\nk=i\\xe2\\x88\\x92K \\xe2\\x80\\xb2\\nwk,\\nk \\xcc\\xb8= i.\\n(6.7)\\nNote that, since in experiments, the sense selection of the target word is found to\\nbe only dependent on more limited context words for calculating attention, hence a\\nsmaller K \\xe2\\x80\\xb2 is selected as compared to K.\\nRecall that SAC only uses one target word as attention to select senses of context\\nwords whereas SAT uses several context words together as attention to select appro-\\npriate senses of target words. Hence SAT is expected to conduct more reliable WSD\\nand result in more accurate word representations, which is explored in experiments.\\n6.3\\nApplications\\nIn the previous section, we introduce HowNet and sememe representation. In fact,\\nlinguistic knowledge graphs such as HowNet contain rich information which could\\neffectivelyhelpdownstreamapplications.Therefore,inthissection,wewillintroduce\\nthe major applications of sememe representation, including sememe-based word\\nrepresentation, linguistic knowledge graph construction, and language modeling.\\n6.3.1\\nSememe-Guided Word Representation\\nSememe-Guided word representation is intended for improving word embeddings\\nfor sememe prediction by introducing the information of sememe-based linguistic\\nKBs of the source language. Qi et al. [56] present two methods of the sememe-guided\\nword representation.\\n6.3.1.1\\nRelation-Based Word Representation\\nA simple and intuitive method is to let words with similar sememe annotations tend\\nto have similar word embeddings, which is named as word relation-based approach.\\nTo begin with, a synonym list is constructed from sememe-based linguistic KBs,\\n6.3 Applications\\n133\\nwhere words sharing a certain number of sememes are regarded as synonyms. Next,\\nsynonyms are forced to have closer word embeddings.\\nFormally, let wi be the original word embedding of wi and \\xcb\\x86wi be its adjusted\\nword embedding. And let Syn(wi) denote the synonym set of word wi. Then the loss\\nfunction is de\\xef\\xac\\x81ned as\\nLsememe =\\n\\x02\\nwi\\xe2\\x88\\x88V\\n\\x04\\n\\xce\\xb1i\\xe2\\x88\\xa5wi \\xe2\\x88\\x92\\xcb\\x86wi\\xe2\\x88\\xa52 +\\n\\x02\\nw j\\xe2\\x88\\x88Syn(wi)\\n\\xce\\xb2i j\\xe2\\x88\\xa5\\xcb\\x86wi \\xe2\\x88\\x92\\xcb\\x86w j\\xe2\\x88\\xa52\\x05\\n,\\n(6.8)\\nwhere \\xce\\xb1 and \\xce\\xb2 control the relative strengths of the two terms. It should be noted\\nthat the idea of forcing similar words to have close word embeddings is similar\\nto the state-of-the-art retro\\xef\\xac\\x81tting approach [19]. However, the retro\\xef\\xac\\x81tting approach\\ncannot be applied here because sememe-based linguistic KBs such as HowNet cannot\\ndirectly provide its needed synonym list.\\n6.3.1.2\\nSememe Embedding-Based Word Representation\\nSimple and effective as the word relation-based approach is, it cannot make full\\nuse of the information of sememe-based linguistic KBs because it disregards the\\ncomplicated relations between sememes and words as well as relations between dif-\\nferent sememes. To address this limitation, the sememe embedding-based approach\\nis proposed, which learns both sememe and word embeddings jointly.\\nIn this approach, sememes are represented with distributed vectors as well and\\nplace them into the same semantic space as words. Similar to SPSE [66], which\\nlearns sememe embeddings by decomposing the word-sememe matrix and sememe-\\nsememe matrix, the method utilizes sememe embeddings as regularizers to learn\\nbetter word embeddings. Different from SPSE, the model described in [56] does not\\nuse pretrained word embeddings. Instead, it learns word embeddings and sememe\\nembeddings simultaneously. More speci\\xef\\xac\\x81cally, a word-sememe matrix M can be\\nextracted from HowNet, where Mi j = 1 indicates word wi is annotated with sememe\\nx j, otherwise Mi j = 0. Hence by factorizing M, the loss function can be de\\xef\\xac\\x81ned as\\nLsememe =\\n\\x02\\nwi\\xe2\\x88\\x88V,x j\\xe2\\x88\\x88X\\n(wi \\xc2\\xb7 x j + bs + b\\xe2\\x80\\xb2\\nj \\xe2\\x88\\x92Mi j)2,\\n(6.9)\\nwhere bi and b\\xe2\\x80\\xb2\\nj are the biases of wi and x j, and X denotes sememe set.\\nIn this approach, word and sememe embeddings are obtained in a uni\\xef\\xac\\x81ed seman-\\ntic space. The sememe embeddings bear all the information about the relationships\\nbetween words and sememes, and they inject the information into word embed-\\ndings. Therefore, the word embeddings are expected to be more suitable for sememe\\nprediction.\\n134\\n6\\nSememe Knowledge Representation\\n6.3.2\\nSememe-Guided Semantic Compositionality Modeling\\nSemantic Compositionality (SC) is de\\xef\\xac\\x81ned as the linguistic phenomenon that the\\nmeaning of a syntactically complex unit is a function of meanings of the complex\\nunit\\xe2\\x80\\x99s constituents and their combination rule [50]. Some linguists regard SC as the\\nfundamental truth of semantics [51]. In the \\xef\\xac\\x81eld of NLP, SC has proved effective in\\nmany tasks including language modeling [47], sentiment analysis [42, 61], syntactic\\nparsing [59], etc.\\nMost literature on SC pays attention to using vector-based distributional mod-\\nels of semantics to learn representations of Multiword Expressions (MWEs), i.e.,\\nembeddings of phrases or compounds. Reference [46] conducts a pioneering work\\nwhich introduces a general framework to formulate this task:\\np = f (w1, w2, R, K ),\\n(6.10)\\nwhere1 f is the compositionality function, p denotes the embedding of an MWE,\\nw1 and w2 represent the embeddings of the MWE\\xe2\\x80\\x99s two constituents, R stands for\\nthe combination rule, and K refers to the additional knowledge which is needed to\\nconstruct the semantics of the MWE.\\nMost of the proposed approaches ignore R and K , centering on reforming com-\\npositionality function f [3, 21, 60, 61]. Some try to integrate combination rule R\\ninto SC models [7, 35, 65, 71]. A few works consider external knowledge K. Ref-\\nerence [72] tries to incorporate task-speci\\xef\\xac\\x81c knowledge into an LSTM model for\\nsentence-level SC.\\nReference [55] proposes a novel sememe-based method to model semantic com-\\npositionality. They argue that sememes are bene\\xef\\xac\\x81cial to modeling SC. To verify this,\\nthey \\xef\\xac\\x81rst design a simple SC degree (SCD) measurement experiment and \\xef\\xac\\x81nd that\\nthe SCDs of MWEs computed by simple sememe-based formulae are highly corre-\\nlated with human judgment. This result shows that sememes can \\xef\\xac\\x81nely depict mean-\\nings of MWEs and their constituents, and capture the semantic relations between\\nthe two sides. Moreover, they propose two sememe-incorporated SC models for\\nlearning embeddings of MWEs, namely Semantic Compositionality with Aggre-\\ngated Sememe (SCAS) model and Semantic Compositionality with Mutual Sememe\\nAttention (SCMSA) model. When learning the embedding of an MWE, the SCAS\\nmodel concatenates the embeddings of the MWE\\xe2\\x80\\x99s constituents and their sememes,\\nwhile the SCMSA model considers the mutual attention between a constituent\\xe2\\x80\\x99s\\nsememes and the other constituent. Finally, they integrate the combination rule, i.e.,\\nR in Eq. (6.10), into the two models. Their models achieve signi\\xef\\xac\\x81cant performance\\nover the MWE similarity computation task and sememe prediction task compared\\nwith baseline methods.\\nIn this section, we focus on the work conducted by [55]. We will \\xef\\xac\\x81rst intro-\\nduce sememe-based SC Degree (SCD) computation formulae, and then expand their\\nSememe-incorporated SC models.\\n1This formula only applies to two-word MWEs but can be easily extended to longer MWEs.\\n6.3 Applications\\n135\\n6.3.2.1\\nSememe-Based SCD Computation Formulae\\nAlthough SC widely exists in MWEs, not every MWE is fully semantically com-\\npositional. In fact, different MWEs show different degrees of SC. Reference [55]\\nbelieves that sememes can be used to measure SCD conveniently.\\nTo this end, based on the assumption that all the sememes of a word accurately\\ndepict the word\\xe2\\x80\\x99s meaning, they intuitively design a set of SCD computation formu-\\nlae, which are consistent with the principle of SCD.\\nThe formulae are illustrated in Table6.2. They de\\xef\\xac\\x81ne four SCDs denoted by\\nnumbers 3, 2, 1, and 0, where larger numbers mean higher SCDs. Sp, Sw1, and Sw2\\nrepresent the sememe sets of an MWE, its \\xef\\xac\\x81rst and second constituent, respectively.\\nHere is a brief explanation for their SCD computation formulae:\\n(1) For SCD 3, the sememe set of an MWE is identical to the union of the two\\nconstituents\\xe2\\x80\\x99 sememe sets, which means the meaning of the MWE is exactly the\\nsame as the combination of the constituents\\xe2\\x80\\x99 meanings. Therefore, the MWE is fully\\nsemantically compositional and should have the highest SCD.\\n(2) For SCD 0, an MWE has totally different sememes from its constituents, which\\nmeans the MWE\\xe2\\x80\\x99s meaning cannot be derived from its constituents\\xe2\\x80\\x99 meanings. Hence\\nthe MWE is completely non-compositional, and its SCD should be the lowest.\\n(3) As for SCD 2, the sememe set of an MWE is a proper subset of the union of\\nits constituents\\xe2\\x80\\x99 sememe sets, which means the meanings of the constituents cover\\nthe MWE\\xe2\\x80\\x99s meaning but cannot precisely infer the MWE\\xe2\\x80\\x99s meaning.\\n(4) Finally, for SCD 1, an MWE shares some sememes with its constituents, but\\nboth the MWE itself and its constituents have some unique sememes.\\nThere is an example for each SCD in Table6.2, including a Chinese MWE, its\\ntwo constituents, and their sememes.2\\n6.3.2.2\\nEvaluating SCD Computation Formulae\\nTo evaluate their sememe-based SCD computation formulae, [55] constructs a\\nhuman-annotated SCD dataset. They ask several native speakers to label SCDs for\\n500 Chinese MWEs, where there are four degrees to choose. Before labeling an\\nMWE, they are shown the dictionary de\\xef\\xac\\x81nitions of both the MWE and its con-\\nstituents.\\nEach MWE is labeled by 3 annotators, and the average of the 3 SCDs given by\\nthem is the MWE\\xe2\\x80\\x99s \\xef\\xac\\x81nal SCD.\\nEventually, they obtain a dataset containing 500 Chinese MWEs together with\\ntheir human-annotated SCDs.\\nThen they evaluate the correlativity between SCDs of the MWEs in the dataset\\ncomputed by sememe-based rules and those given by humans. They \\xef\\xac\\x81nd Pearson\\xe2\\x80\\x99s\\ncorrelation coef\\xef\\xac\\x81cient is up to 0.75, and Spearman\\xe2\\x80\\x99s rank correlation coef\\xef\\xac\\x81cient is\\n2In Chinese, most MWEs are words consisting of more than two characters which are actually\\nsingle-morpheme words.\\n136\\n6\\nSememe Knowledge Representation\\nTable 6.2 Sememe-based semantic compositionality degree computation formulae and examples. Bold sememes of constituents are shared with the constituents\\xe2\\x80\\x99\\ncorresponding MWE\\n6.3 Applications\\n137\\n0.74. These results manifest remarkable capability of sememes to compute SCDs of\\nMWEs and provide a proof that sememes of a word can \\xef\\xac\\x81nely represent the word\\xe2\\x80\\x99s\\nmeaning.\\n6.3.2.3\\nSememe-Incorporated SC Models\\nIn this section, we \\xef\\xac\\x81rst introduce two basic sememe-incorporated SC models in detail,\\nnamely Semantic Compositionality with Aggregated Sememe (SCAS) and Semantic\\nCompositionality with Mutual Sememe Attention (SCMSA). SCAS model simply\\nconcatenates the embeddings of the MWE\\xe2\\x80\\x99s constituents and their sememes, while\\nthe SCMSA model takes account of the mutual attention between a constituent\\xe2\\x80\\x99s\\nsememes and the other constituent. Then we describe how to integrate combination\\nrules into the two basic models.\\nIncorporating Sememes Only. Following the notations in Eq. (6.10), for an\\nMWE p = {w1, w2}, its embedding can be represented as\\np = f (w1, w2, K ),\\n(6.11)\\nwhere p, w1, w2 \\xe2\\x88\\x88Rd, and d is the dimension of embeddings, K denotes the\\nsememe knowledge here, and we assume that we only know the sememes of w1\\nand w2, considering that MWEs are normally not in the sememe KBs. X indicates\\nthe set of all the sememes and Xw = {x1, ..., x|Xw|} \\xe2\\x8a\\x82X to signify the sememe set\\nof w. In addition, x \\xe2\\x88\\x88Rd denotes the embedding of sememe x.\\n(1) SCAS Model The \\xef\\xac\\x81rst model we introduce is the SCAS model, which is\\nillustrated in Fig.6.4. The idea of the SCAS model is straightforward, i.e., simply\\nMWE\\nConstituent\\nSememe\\nw\\xe2\\x80\\xb21\\nw1\\nw\\xe2\\x80\\xb22\\nw2\\nFig. 6.4 The architecture of SCAS model\\n138\\n6\\nSememe Knowledge Representation\\nconcatenating word embedding of a constituent and the aggregation of its sememes\\xe2\\x80\\x99\\nembeddings. Formally, we have\\nw\\xe2\\x80\\xb2\\n1 =\\n\\x02\\nxi\\xe2\\x88\\x88Xw1\\nxi,\\nw\\xe2\\x80\\xb2\\n2 =\\n\\x02\\nx j\\xe2\\x88\\x88Xw2\\nxj,\\n(6.12)\\nwhere w\\xe2\\x80\\xb2\\n1 and w\\xe2\\x80\\xb2\\n2 represent the aggregated sememe embeddings of w1 and w2, respec-\\ntively. Then p can be obtained by\\np = tanh(Wc[w1 + w2;w\\xe2\\x80\\xb2\\n1 + w\\xe2\\x80\\xb2\\n2] + bc),\\n(6.13)\\nwhere Wc \\xe2\\x88\\x88Rd\\xc3\\x972d is the composition matrix and bc \\xe2\\x88\\x88Rd is a bias vector.\\n(2) SCMSA Model\\nThe SCAS model simply uses the sum of all the sememes\\xe2\\x80\\x99 embeddings of a\\nconstituent as the external information. However, a constituent\\xe2\\x80\\x99s meaning may vary\\nwith the other constituent, and accordingly, the sememes of a constituent should have\\ndifferent weights when the constituent is combined with different constituents (there\\nis an example in the case study).\\nCorrespondingly, we introduce the SCMSA model (Fig.6.5), which adopts the\\nmutual attention mechanism to dynamically endow sememes with weights. Formally,\\nwe have\\ne1 = tanh(Waw1 + ba),\\na2,i =\\nexp (si \\xc2\\xb7 e1)\\n\\x03\\nx j\\xe2\\x88\\x88Xw2 exp (x j \\xc2\\xb7 e1),\\nw\\xe2\\x80\\xb2\\n2 =\\n\\x02\\nxi\\xe2\\x88\\x88Xw2\\na2,ixi,\\n(6.14)\\nwhere Wa \\xe2\\x88\\x88Rd\\xc3\\x97d is the weight matrix and ba \\xe2\\x88\\x88Rd is a bias vector. Similarly, w\\xe2\\x80\\xb2\\n1\\ncan be calculated. Then they still use Eq. (6.13) to obtain p.\\nIntegrating Combination Rules. Reference [55] further integrates combination\\nrules into their sememe-incorporated SC models. In other words,\\np = f (w1, w2, K, R).\\n(6.15)\\nWe can use totally different composition matrices for MWEs with different com-\\nbination rules:\\nWc = Wr\\nc,\\nr \\xe2\\x88\\x88Rs,\\n(6.16)\\nwhere Wr\\nc \\xe2\\x88\\x88Rd\\xc3\\x972d and Rs refers to combination rule set containing syntax rules of\\nMWEs, e.g., adjective-noun and noun-noun.\\nHowever, there are many different combination rules, and some rules have sparse\\ninstances which are not enough to train the corresponding composition matrices\\n6.3 Applications\\n139\\nMWE\\nConstituent\\nSememe\\nw\\xe2\\x80\\xb21\\nw1\\nw\\xe2\\x80\\xb22\\nw2\\na21\\na22\\na23\\na11\\na12\\nFig. 6.5 The architecture of SCMSA model\\nwith d \\xc3\\x97 2d parameters. In addition, we believe that the composition matrix should\\ncontain common compositionality information except the combination rule-speci\\xef\\xac\\x81c\\ncompositionality information. Hence, they let composition matrix Wc be the sum of\\na low-rank matrix containing combination rule information and a matrix containing\\ncommon compositionality information:\\nWc = Ur\\n1Ur\\n2 + Wc\\nc,\\n(6.17)\\nwhere Ur\\n1 \\xe2\\x88\\x88Rd\\xc3\\x97dr , Ur\\n2 \\xe2\\x88\\x88Rdr\\xc3\\x972d, and dr \\xe2\\x88\\x88N+ is a hyperparameter and may vary\\nwith the combination rule, and Wc\\nc \\xe2\\x88\\x88Rd\\xc3\\x972d.\\n6.3.3\\nSememe-Guided Language Modeling\\nLanguage Modeling (LM) aims to measure the probability of a word sequence,\\nre\\xef\\xac\\x82ecting its \\xef\\xac\\x82uency and likelihood as a feasible sentence in a human language.\\nLanguage Modeling is an essential component in a wide range of natural language\\n140\\n6\\nSememe Knowledge Representation\\n(a)\\n(b)\\nConventional Decoder\\nSememe-Driven Decoder\\nSememe\\nPredictor\\ncontext\\nvector\\nword\\ndistribution\\nsememe\\ndistribution\\nsense\\ndistribution\\nSense\\nPredictor\\nWord\\nPredictor\\nword\\ndistribution\\ncontext\\nvector\\nFig. 6.6 Decoders of a conventional LM, b sememe-driven LM\\nprocessing (NLP) tasks, such as machine translation [9, 10], speech recognition [34],\\ninformation retrieval [5, 24, 45, 54], and document summarization [2, 57].\\nA probabilistic language model calculates the conditional probability of the next\\nword given their contextual words, which are typically learned from large-scale text\\ncorpora. Taking the simplest language model, for example, n-gram estimates the\\nconditional probabilities according to maximum likelihood over text corpora [31].\\nRecent years have witnessed the advances of Recurrent Neural Networks (RNNs)\\nas the state-of-the-art approach for language modeling [44], in which the context is\\nrepresented as a low-dimensional hidden state to predict the next word (Fig.6.6).\\nThose conventional language models, including neural models, typically assume\\nwords as atomic symbols and model sequential patterns at the word level. However,\\nthis assumption does not necessarily hold to some extent. Consider the following\\nexample sentence for which people want to predict the next word in the blank,\\nThe U.S. trade deficit last year is initially estimated to be 40 billion\\n.\\nPeople may \\xef\\xac\\x81rst realize a unit should be \\xef\\xac\\x81lled in, then realize it should be a\\ncurrency unit. Based on the country this sentence is talking about, the U.S.,\\none may con\\xef\\xac\\x81rm it should be an American currency unit and predict the\\nword dollars. Here, the unit, currency, and American, which are basic\\nsemantic units of the word dollars, are also the sememes of the word dollars.\\nHowever,thisprocesshasnotbeenexplicitlytakenintoconsiderationbyconventional\\nlanguage models. That is in most cases, words are atomic language units, words are\\nnot necessarily atomic semantic units for language modeling. Thus, explicit modeling\\nof sememes could improve both the performance and the interpretability of language\\nmodels. However, as far as we know, a few efforts have been devoted to exploring the\\neffectiveness of sememes in language models, especially neural language models.\\nIt is nontrivial for neural language models to incorporate discrete sememe knowl-\\nedge, as it is not compatible with continuous representations in neural models. In\\nthis part, Sememe-Driven Language Model (SDLM) is proposed to leverage lexi-\\ncal sememe knowledge. In order to predict the next word, SDLM utilizes a novel\\n6.3 Applications\\n141\\nsememe-sense-word generation process: (1) First, SDLM estimates sememes\\xe2\\x80\\x99 dis-\\ntribution according to the context. (2) Regarding these sememes as experts, SDLM\\nemploys a sparse product of expert method to select the most probable senses. (3)\\nFinally, SDLM calculates the distribution of words by marginalizing out the distri-\\nbution of senses.\\nSDLM is composed of three modules in series: Sememe Predictor, Sense Pre-\\ndictor, and Word Predictor (Fig.6.6). The Sememe Predictor \\xef\\xac\\x81rst takes the context\\nvector as input and assigns a weight to each sememe. Then each sememe is regarded\\nas an expert and makes predictions about the probability distribution over a set of\\nsenses in the Sense Predictor. Finally, the probability of each word is obtained in the\\nWord Predictor.\\nSememe Predictor. The Sememe Predictor takes the context vector g \\xe2\\x88\\x88RH1\\nas input and assigns a weight to each sememe. Assume that given the context\\nw1, w2, . . . , wt\\xe2\\x88\\x921, the events that word wt contains sememe xk (k \\xe2\\x88\\x88{1, 2, . . . , K})\\nare independent, since the sememe is the minimum semantic unit and there is no\\nsemantic overlap between any two different sememes. For simplicity, the superscript\\nt is ignored. The Sememe Predictor is designed as a linear decoder with the sig-\\nmoid activation function. Therefore, pk, the probability that the next word contains\\nsememe xk, is formulated as\\npk = P(xk|g) = Sigmoid(g \\xc2\\xb7 vk + bk),\\n(6.18)\\nwhere vk \\xe2\\x88\\x88RH1, bk \\xe2\\x88\\x88R are trainable parameters, and Sigmoid(\\xc2\\xb7) denotes the sig-\\nmoid activation function.\\nSense Predictor and Word Predictor. The architecture of the Sense Predictor is\\nmotivated by Product of Experts (PoE) [25]. Each sememe is regarded as an expert\\nthat only makes predictions on the senses connected with it. Let S(xk) denote the\\nset of senses that contain sememe xk, the kth expert. Different from conventional\\nneural language models, which directly use the inner product of the context vector\\ng \\xe2\\x88\\x88RH1 and the output embedding w \\xe2\\x88\\x88RH2 for word w to generate the score for\\neach word, Sense Predictor uses \\xcf\\x86(k)(g, w) to calculate the score given by expert\\nxk. And a bilinear function parameterized with a matrix Uk \\xe2\\x88\\x88RH1\\xc3\\x97H2 is chosen as a\\nstraight implementation of \\xcf\\x86(k)(\\xc2\\xb7, \\xc2\\xb7):\\n\\xcf\\x86(k)(g, w) = g\\xe2\\x8a\\xa4Ukw.\\n(6.19)\\nThe score of sense s provided by sememe expert xk can be written as \\xcf\\x86(k)(g, s).\\nTherefore, P(xk)(s|g), the probability of sense s given by expert xk, is formulated as\\nP(xk)(s|g) =\\nexp(qkCk,s\\xcf\\x86(k)(g, s))\\n\\x03\\ns\\xe2\\x80\\xb2\\xe2\\x88\\x88S(xk ) exp(qkCk,s\\xe2\\x80\\xb2\\xcf\\x86(k)(g, s\\xe2\\x80\\xb2)),\\n(6.20)\\nwhere Ck,s is a normalization constant because sense s is not connected to all experts\\n(the connections are sparse with approximately \\xce\\xbbN edges, \\xce\\xbb < 5). Here we can\\n142\\n6\\nSememe Knowledge Representation\\nchoose either Ck,s = 1/|X(s)| (left normalization) or Ck,s = 1/\\n\\x06\\n|X(s)||S(xk)| (sym-\\nmetric normalization).\\nIn the Sense Predictor, qk can be viewed as a gate which controls the magnitude of\\nthe term Ck,s\\xcf\\x86(k)(g, s), thus controlling the \\xef\\xac\\x82atness of the sense distribution provided\\nby sememe expert xk. Consider the extreme case when pk \\xe2\\x86\\x920, the prediction will\\nconverge to the discrete uniform distribution. Intuitively, it means that the sememe\\nexpert will refuse to provide any useful information when it is not likely to be related\\nto the next word.\\nFinally, the predictions can be summarized on sense s by taking the product of\\nthe probabilities given by relevant experts and then normalize the result; that is to\\nsay, P(s|g), the probability of sense s, satis\\xef\\xac\\x81es\\nP(s|g) \\xe2\\x88\\x9d\\n\\x07\\nxk\\xe2\\x88\\x88X(s)\\nP(xk)(s|g).\\n(6.21)\\nUsing Eqs.6.19 and 6.20, P(s|g) can be formulated as\\nP(s|g) =\\nexp(\\x03\\nxk\\xe2\\x88\\x88X(s) qkCk,sg\\xe2\\x8a\\xa4Uks)\\n\\x03\\ns\\xe2\\x80\\xb2 exp(\\x03\\nxk\\xe2\\x88\\x88X(s\\xe2\\x80\\xb2) qkCk,s\\xe2\\x80\\xb2g\\xe2\\x8a\\xa4Uks\\xe2\\x80\\xb2).\\n(6.22)\\nIt should be emphasized that all the supervision information provided by HowNet\\nis embodied in the connections between the sememe experts and the senses. If the\\nmodel wants to assign a high probability to sense s, it must assign a high probability to\\nsome of its relevant sememes. If the model wants to assign a low probability to sense\\ns, it can assign a low probability to its relevant sememes. Moreover, the prediction\\nmade by sememe expert xk has its own tendency because of its own \\xcf\\x86(k)(\\xc2\\xb7, \\xc2\\xb7). Besides,\\nthe sparsity of connections between experts and senses is also determined by HowNet\\nitself.\\nAs illustrated in Fig.6.7, in the Word Predictor, P(w|g), the probability of word\\nw is calculated by summing up probabilities of corresponding s given by the Sense\\nPredictor, that is\\nP(w|g) =\\n\\x02\\ns\\xe2\\x88\\x88S(w)\\nP(s|g).\\n(6.23)\\n6.3.4\\nSememe Prediction\\nThe manual construction of HowNet is actually time-consuming and labor-intensive,\\ne.g., HowNet has been built for more than 10 years by several linguistic experts.\\nHowever, as the development of communications and techniques, new words and\\nphrases are emerging, the semantic meanings of existing words are also dynamically\\nevolving. In this case, sustained manual annotation and updates are becoming much\\nmore overwhelmed. Moreover, due to the high complexity of sememe ontology and\\n6.3 Applications\\n143\\n0.9\\n0.1\\n0.2\\nLSTM\\nLSTM\\nLSTM\\nLSTM\\n0.1\\n0.3\\n0.2\\nP\\nword\\nP\\nsense\\nsememe\\nexperts\\ncontext\\nvector\\nFig. 6.7 The architecture of SDLM model\\nword meanings, it is also challenging to maintain annotation consistency among\\nexperts when they collaboratively annotate lexical sememes.\\nTo address the issues of in\\xef\\xac\\x82exibility and inconsistency of manual annotation, the\\nautomatic lexical sememe prediction task is proposed, which is expected to assist\\nexpert annotation and reduce manual workload. Note that for simplicity, most works\\nintroduced in this part do not consider the complicated hierarchies of word sememes,\\nand simply group all annotated sememes of each word as the sememe set for learning\\nand prediction.\\nThe basic idea of sememe prediction is that those words of similar semantic mean-\\nings may share overlapped sememes. Hence, the key challenge of sememe prediction\\nis how to represent semantic meanings of words and sememes to model the semantic\\nrelatedness between them. In this part, we will focus on introducing the sememe pre-\\ndiction word accomplished by Xie et al. [66]. In their work, they propose to model\\nthe semantics of words and sememes using distributed representation learning [26].\\nDistributed representation learning aims to encode objects into a low-dimensional\\nsemantic space, which has shown its impressive capability of modeling semantics of\\nhuman languages, e.g., word embeddings [43] have been widely studied and utilized\\nin various tasks of NLP.\\nAs shown in previous work [43], it is effective to measure word similarities using\\ncosine similarity or Euclidean distance of their word embeddings learned from a\\nlarge-scale text corpus. Hence, a straightforward method for sememe prediction is\\n144\\n6\\nSememe Knowledge Representation\\nthat, given an unlabeled word, we \\xef\\xac\\x81nd its most related words in HowNet according\\nto their word embeddings, and recommend the annotated sememes of these related\\nwords to the given word. The method is intrinsically similar to collaborative \\xef\\xac\\x81ltering\\n[58] in recommendation systems, capable of capturing semantic relatedness between\\nwords and sememes based on their annotation co-occurrences.\\nWord embeddings can also be learned with techniques of matrix factorization\\n[37]. Inspired by the successful practice of matrix factorization for personalized\\nrecommendation [36], a new model which factorizes the word-sememe matrix from\\nHowNet and obtains sememe embeddings is proposed. In this way, the relatedness of\\nwords and sememes can be measured directly using dot products of their embeddings,\\naccording to which we could recommend the most related sememes to an unlabeled\\nword.\\nThe two methods are named as Sememe Prediction with Word Embeddings\\n(SPWE) and with Sememe Embeddings (SPSE/SPASE), respectively.\\n6.3.4.1\\nSememe Prediction with Word Embeddings\\nGiven an unlabeled word, it is straightforward to recommend sememes according to\\nits most related words, assuming that similar words should have similar sememes.\\nThis idea is similar to collaborative \\xef\\xac\\x81ltering in the personalized recommendation, for\\nin the scenario of sememe prediction words can be regarded as users and sememes\\nas the items/products to be recommended. Inspired by this, Sememe Prediction\\nwith Word Embeddings (SPWE) model is proposed, which uses similarities of word\\nembeddings to judge user distances.\\nFormally, the score function P(x j|w) of sememes x j given a word w is de\\xef\\xac\\x81ned\\nas\\nP(x j|w) =\\n\\x02\\nwi\\xe2\\x88\\x88V\\ncos(w, wi)Mi jcri,\\n(6.24)\\nwhere cos(w, wi) is the cosine similarity between word embeddings of w and wi\\npretrained by GloVe. Mi j indicates the annotation of sememe x j on word wi, where\\nMi j = 1 indicates the word wi which has the sememe x j in HowNet and otherwise\\nhas not. Higher the score function P(x j|w) is, more possible the word w should be\\nrecommended with x j.\\nDiffering from classical collaborative \\xef\\xac\\x81ltering in recommendation systems, only\\nthe most similar words should be concentrated when predicting sememes for new\\nwords since irrelevant words have totally different sememes which may be noises\\nfor sememe prediction. To address this problem, a declined con\\xef\\xac\\x81dence factor cri is\\nassigned for each word wi, whereri is the descend rank of word similarity cos(w, wi),\\nand c \\xe2\\x88\\x88(0, 1) is a hyperparameter. In this way, only a few top words that are similar\\nto w have strong in\\xef\\xac\\x82uences on predicting sememes.\\nSPWE only uses word embeddings for word similarities and is simple and effec-\\ntive for sememe prediction. It is because, differing from the noisy and incomplete\\nuser-item matrix in most recommender systems, HowNet is carefully annotated by\\n6.3 Applications\\n145\\nhuman experts, and thus the word-sememe matrix is with high con\\xef\\xac\\x81dence. Therefore,\\nthe word-sememe matrix can be con\\xef\\xac\\x81dently applied to collaboratively recommend\\nreliable sememes based on similar words.\\n6.3.4.2\\nSememe Prediction with Sememe Embeddings\\nSememe Prediction with Word Embeddings model follows the assumption that the\\nsememes of a word can be predicted according to its related words\\xe2\\x80\\x99 sememes. How-\\never, simply considering sememes as discrete labels may inevitably neglect the latent\\nrelations between sememes. To take the latent relations of sememes into consider-\\nation, Sememe Prediction with Sememe Embeddings (SPSE) model is proposed,\\nwhich projects both words and sememes into the same semantic vector space, learn-\\ning sememe embeddings according to the co-occurrences of words and sememes in\\nHowNet.\\nSimilar to GloVe [53] which decomposes co-occurrence matrix of words to learn\\nword embeddings, sememe embeddings can be learned by factorizing word-sememe\\nmatrix and sememe-sememe matrix simultaneously. These two matrices are both\\nconstructed from HowNet. As for word embeddings, similar to SPWE, SPSE uses\\nword embeddings pretrained from a large-scale corpus and \\xef\\xac\\x81xes them during fac-\\ntorizing of the word-sememe matrix. With matrix factorization, both sememe and\\nword embeddings can be encoded into the same low-dimensional semantic space,\\nand then computed the cosine similarity between normalized embeddings of words\\nand sememes for sememe prediction.\\nMore speci\\xef\\xac\\x81cally, similar to M, a sememe-sememe matrix C can also be extracted,\\nwhere C jk is de\\xef\\xac\\x81ned as point-wise mutual information that C jk = PMI(x j, xk) to\\nindicate the correlations between two sememes x j and xk. Note that, by factorizing\\nC, two distinct embeddings for each sememe s will be obtained, denoted as x and \\xc2\\xafx,\\nrespectively. The loss function of learning sememe embeddings is de\\xef\\xac\\x81ned as follows:\\nL =\\n\\x02\\nwi\\xe2\\x88\\x88W,x j\\xe2\\x88\\x88X\\n\\x08\\nwi \\xc2\\xb7 (x j +Nx j) + bi + b\\xe2\\x80\\xb2\\nj \\xe2\\x88\\x92Mi j\\n\\t2 + \\xce\\xbb\\n\\x02\\nx j,xk\\xe2\\x88\\x88X\\n\\x08\\nx j \\xc2\\xb7 \\xc2\\xafxk \\xe2\\x88\\x92C jk\\n\\t2,\\n(6.25)\\nwhere bi and b\\xe2\\x80\\xb2\\nj denote the bias of wi and x j. These two parts correspond to the\\nlosses of factorizing matrices M and C, adjusted by the hyperparameter \\xce\\xbb. Since\\nthe sememe embeddings are shared by both factorizations, our SPSE model enables\\njointly encoding both words and sememes into a uni\\xef\\xac\\x81ed semantic space.\\nSince each word is typically annotated with 2\\xe2\\x80\\x935 sememes in HowNet, most ele-\\nments in the word-sememe matrix are zeros. If all zero elements and nonzero ele-\\nments are treated equally during factorization, the performance will be much worse.\\nTo address this issue, different factorization strategies are assigned for zero and\\nnonzero elements. For each zero element, the model chooses to factorize them with\\na small probability like 0.5%, and otherwise, the model chooses to ignore. While for\\nnonzero elements, the model always chooses to factorize them. With the help of this\\nstrategy, the model can pay more attention to those annotated word-sememe pairs.\\n146\\n6\\nSememe Knowledge Representation\\nIn SPSE, sememe embeddings are learned accompanying with word embeddings\\nvia matrix factorization into the uni\\xef\\xac\\x81ed low-dimensional semantic space. Matrix\\nfactorization has been veri\\xef\\xac\\x81ed as an effective approach in the personalized recom-\\nmendation, because it can accurately model relatedness between users and items, and\\nis highly robust to noises in user-item matrices. Using this model, we can \\xef\\xac\\x82exibly\\ncompute semantic relatedness of words and sememes, which provides us an effec-\\ntive tool to manipulate and manage sememes, including but not limited to sememe\\nprediction.\\n6.3.4.3\\nSememe Prediction with Aggregated Sememe Embeddings\\nInspired by the characteristics of sememes, we assume that the word embeddings are\\nsemantically composed of sememe embeddings. In the word-sememe joint space, we\\ncan simply implement semantic composition as additive operations that each word\\nembedding is expected to be the sum of its all sememes\\xe2\\x80\\x99 embeddings. Following this\\nassumption, Sememe Prediction with Aggregated Sememe Embeddings (SPASE)\\nmodel is proposed. SPASE is also based on matrix factorization, and is formally\\ndenoted as\\nwi =\\n\\x02\\nx j\\xe2\\x88\\x88Xwi\\nM\\xe2\\x80\\xb2\\ni jx j,\\n(6.26)\\nwhere Xwi is the sememe set of the word wi and M\\xe2\\x80\\xb2\\ni j represents the weight of\\nsememe x j for word wi, which only has value on nonzero elements of word-sememe\\nlabeled matrix M. To learn sememe embeddings, we attempt to decompose the word\\nembedding matrix V into M\\xe2\\x80\\xb2 and sememe embedding matrix X, with pretrained word\\nembeddings \\xef\\xac\\x81xed during training, which could also be written as V = M\\xe2\\x80\\xb2X.\\nThe contribution of SPASE is that it complies with the de\\xef\\xac\\x81nition of sememes\\nin HowNet that sememes are the semantic components of words. In SPASE, each\\nsememe can be regarded as a tiny semantic unit, and all words can be represented\\nby composing several semantic units, i.e., sememes, which make up an interesting\\nsemantic regularity. However, SPASE is dif\\xef\\xac\\x81cult to train because word embeddings\\nare \\xef\\xac\\x81xed, and the number of words is much larger than the number of sememes. In the\\ncase of modeling complex semantic compositions of sememes into words, the rep-\\nresentation capability of SPASE may be strongly constrained by limited parameters\\nof sememe embeddings and excessive simpli\\xef\\xac\\x81cation of additive assumption.\\n6.3.4.4\\nLexical Sememe Prediction with Internal Information\\nIn the previous section, we introduce the automatic lexical sememe prediction pro-\\nposed by Xie et al. [66]. These methods ignore the internal information within words\\n(e.g., the characters in Chinese words), which is also signi\\xef\\xac\\x81cant for word understand-\\ning, especially for words which are of low frequency or do not appear in the corpus\\n6.3 Applications\\n147\\nWors embedding\\n(iron)\\nExternal information\\nInternal information\\nHostOf\\nRelate To\\ndomain\\n(craftsman)\\n(ironsmith)\\nironsmith\\n(human)\\n(occupation)\\n(metal)\\n(industrial)\\nWors embedding\\nword\\nsense\\nsememe\\nFig. 6.8 Sememes of the word\\n(ironsmith) in HowNet, where occupation, human,\\nand industrial can be inferred by both external (contexts) and internal (characters) information,\\nwhile metal is well-captured only by the internal information within the character\\n(iron)\\nat all. In this section, we introduce the work of Jin et al. [30], which takes Chinese\\nas an example and explores methods of taking full advantage of both external and\\ninternal information of words for sememe prediction.\\nIn Chinese, words are composed of one or multiple characters, and most characters\\nhavecorrespondingsemanticmeanings. As shownby[67], morethan90%of Chinese\\ncharacters in modern Chinese corpora are morphemes. Chinese words can be divided\\ninto single-morpheme words and compound words, where compound words account\\nfor a dominant proportion. The meanings of compound words are closely related\\nto their internal characters as shown in Fig. 6.8. Taking a compound word\\n(ironsmith), for instance, it consists of two Chinese characters:\\n(iron)\\nand\\n(craftsman), and the semantic meaning of\\ncan be inferred from the\\ncombinationof its twocharacters (iron +craftsman \\xe2\\x86\\x92ironsmith). Evenfor\\nsome single-morpheme words, their semantic meanings may also be deduced from\\ntheir characters. For example, both characters of the single-morpheme word\\n(hover) represent the meaning of hover or linger. Therefore, it is intuitive to\\ntake the internal character information into consideration for sememe prediction.\\nReference [30] proposes a novel framework for Character-enhanced Sememe\\nPrediction (CSP), which leverages both internal character information and external\\ncontext for sememe prediction. CSP predicts the sememe candidates for a target word\\nfrom its word embedding and the corresponding character embeddings. Speci\\xef\\xac\\x81cally,\\nfollowing SPWE and SPSE as introduced by [66] to model external information,\\nSememe Prediction with Word-to-Character Filtering (SPWCF) and Sememe Pre-\\ndiction with Character and Sememe Embeddings (SPCSE) are proposed to model\\ninternal character information.\\nSememe Prediction with Word-to-Character Filtering. Inspired by collabora-\\ntive \\xef\\xac\\x81ltering [58], Jin et al. [30] propose to recommend sememes for an unlabeled\\nword according to its similar words based on internal information. And words are\\nconsidered as similar if they contain the same characters at the same positions.\\n148\\n6\\nSememe Knowledge Representation\\nFig. 6.9 An example of the\\nposition of characters in a\\nword\\nBegin\\nEnd\\nMiddle\\nIn Chinese, the meaning of a character may vary according to its position within\\na word [14]. Three positions within a word are considered: Begin, Middle, and\\nEnd. For example, as shown in Fig. 6.9, the character at the Begin position of the\\nword\\n(railway station) is\\n(fire), while\\n(vehicle) and\\n(station) are at the Middle and End position, respectively. The character\\nusually means station when it is at the End position, while it usually means\\nstand at the Begin position like in\\n(stand),\\n(standing\\nguard), and\\n(stand up).\\nFormally, for a word w = c1c2...c|w|, we de\\xef\\xac\\x81ne \\xcf\\x80B(w) = {c1}, \\xcf\\x80M(w) =\\n{c2, ..., c|w\\xe2\\x88\\x921|}, \\xcf\\x80E(w) = {c|w|}, and\\nPp(x j|c) \\xe2\\x88\\xbc\\n\\x03\\nwi\\xe2\\x88\\x88W\\xe2\\x88\\xa7c\\xe2\\x88\\x88\\xcf\\x80p(wi) Mi j\\n\\x03\\nwi\\xe2\\x88\\x88W\\xe2\\x88\\xa7c\\xe2\\x88\\x88\\xcf\\x80p(wi) |Xwi|,\\n(6.27)\\nthat represents the score of a sememe x j given a character c and a position p, where\\n\\xcf\\x80p may be \\xcf\\x80B, \\xcf\\x80M, or \\xcf\\x80E. M is the same matrix used in SPWE. Finally, the score\\nfunction P(x j|w) of sememe x j given a word w is de\\xef\\xac\\x81ned as\\nP(x j|w) \\xe2\\x88\\xbc\\n\\x02\\np\\xe2\\x88\\x88{B,M,E}\\n\\x02\\nc\\xe2\\x88\\x88\\xcf\\x80p(w)\\nPp(x j|c).\\n(6.28)\\nSPWCF is a simple and ef\\xef\\xac\\x81cient method. It performs well because compositional\\nsemantics are pervasive in Chinese compound words, which makes it straightforward\\nand effective to \\xef\\xac\\x81nd similar words according to common characters.\\nSememe Prediction with Character and Sememe Embeddings (SPCSE). The\\nmethod Sememe Prediction with Word-to-Character Filtering (SPWCF) can effec-\\ntively recommend the sememes that have strong correlations with characters. How-\\never, just like SPWE, it ignores the relations between sememes. Hence, inspired\\nby SPSE, Sememe Prediction with Character and Sememe Embeddings (SPCSE) is\\nproposed to take the relations between sememes into account. In SPCSE, the model\\ninstead learns the sememe embeddings based on internal character information, then\\ncomputes the semantic distance between sememes and words for prediction.\\nInspired by GloVe [53] and SPSE, matrix factorization is adopted in SPCSE, by\\ndecomposing the word-sememe matrix and the sememe-sememe matrix simultane-\\nously. Instead of using pretrained word embeddings in SPSE, pretrained character\\nembeddings are used in SPCSE. Since the ambiguity of characters is stronger than\\nthat of words, multiple embeddings are learned for each character [14]. The most rep-\\nresentative character and its embedding are selected to represent the word meaning.\\nBecause low-frequency characters are much rare than those low-frequency words,\\n6.3 Applications\\n149\\nFig. 6.10 An example of adopting multiple-prototype character embeddings. The numbers are the\\ncosine distances. The sememe\\n(metal) is the closest to one embedding of\\n(iron)\\nand even low-frequency words are usually composed of common characters, it is\\nfeasible to use pretrained character embeddings to represent rare words. During fac-\\ntorizing of the word-sememe matrix, the character embeddings are \\xef\\xac\\x81xed.\\nNe is set as the number of embeddings for each character, and each character c\\nhas Ne embeddings c1, ..., cNe. Given a word w and a sememe x, the embedding of\\na character of w closest to the sememe embedding by cosine distance is selected as\\nthe representation of the word w, as shown in Fig. 6.10. Speci\\xef\\xac\\x81cally, given a word\\nw = c1...c|w| and a sememe x j, we de\\xef\\xac\\x81ne\\nk\\xe2\\x88\\x97,r\\xe2\\x88\\x97= arg min\\nk,r\\n\\n1 \\xe2\\x88\\x92cos(cr\\nk, x\\xe2\\x80\\xb2\\nj + \\xc2\\xafx\\xe2\\x80\\xb2\\nj)\\n\\x0b\\n,\\n(6.29)\\nwhere k\\xe2\\x88\\x97and r\\xe2\\x88\\x97indicate the indices of the character and its embedding closest to\\nthe sememe x j in the semantic space. With the same word-sememe matrix M and\\nsememe-sememe correlation matrix C in SPSE, the sememe embeddings are learned\\nwith the loss function:\\nL =\\n\\x02\\nwi\\xe2\\x88\\x88W,x j\\xe2\\x88\\x88X\\n\\x08\\ncr\\xe2\\x88\\x97\\nk\\xe2\\x88\\x97\\xc2\\xb7\\n\\x08\\nx\\xe2\\x80\\xb2\\nj + \\xc2\\xafx\\xe2\\x80\\xb2\\nj\\n\\t\\n+ bc\\nk\\xe2\\x88\\x97+ b\\xe2\\x80\\xb2\\xe2\\x80\\xb2\\nj \\xe2\\x88\\x92Mi j\\n\\t2 + \\xce\\xbb\\xe2\\x80\\xb2 \\x02\\nx j,xq\\xe2\\x88\\x88X\\n\\x08\\nx\\xe2\\x80\\xb2\\nj \\xc2\\xb7 \\xc2\\xafx\\xe2\\x80\\xb2\\nq \\xe2\\x88\\x92C jq\\n\\t2 ,\\n(6.30)\\nwhere x\\xe2\\x80\\xb2\\nj and \\xc2\\xafx\\xe2\\x80\\xb2\\nj are the sememe embeddings for sememe x j, and cr\\xe2\\x88\\x97\\nk\\xe2\\x88\\x97is the embedding\\nof the character that is the closest to sememe x j within wi. Note that, as the characters\\nand the words are not embedded into the same semantic space, new sememe embed-\\ndings are learned instead of using those learned in SPSE, hence different notations\\nare used for the sake of distinction. bc\\nk and b\\xe2\\x80\\xb2\\xe2\\x80\\xb2\\nj denote the biases of ck and x j, and\\n\\xce\\xbb\\xe2\\x80\\xb2 is the hyperparameter adjusting the two parts. Finally, the score function of word\\nw = c1...c|w| is de\\xef\\xac\\x81ned as\\nP(x j|w) \\xe2\\x88\\xbccr\\xe2\\x88\\x97\\nk\\xe2\\x88\\x97\\xc2\\xb7\\n\\x08\\nx\\xe2\\x80\\xb2\\nj + \\xc2\\xafx\\xe2\\x80\\xb2\\nj\\n\\t\\n.\\n(6.31)\\n150\\n6\\nSememe Knowledge Representation\\nSPWE\\nSPSE\\nSPWCF\\nSPCSE\\nLegend\\nhigh-frequency words\\nlow-frequency words\\nword\\nExternal\\nInternal\\nCSP\\nFig. 6.11 An illustration of model ensembling in sememe prediction\\nModel Ensembling. SPWCF/SPCSE and SPWE/SPSE take different sources\\nof information as input, which means that they have different characteristics:\\nSPWCF/SPCSE only have access to internal information, while SPWE/SPSE can\\nonly make use of external information. On the other hand, just like the difference\\nbetween SPWE and SPSE, SPWCF originates from collaborative \\xef\\xac\\x81ltering, whereas\\nSPCSE uses matrix factorization. All of those methods have in common that they tend\\nto recommend the sememes of similar words, but they diverge in their interpretation\\nof similar.\\nTherefore,toobtainbetterpredictionperformance,itisnecessarytocombinethese\\nmodels.WedenotetheensembleofSPWCFandSPCSEastheinternalmodel,andthe\\nensemble of SPWE and SPSE as the external model. The ensemble of the internal\\nand the external models is the novel framework CSP. In practice, for words with\\nreliable word embeddings, i.e., high-frequency words, we can use the integration of\\nthe internal and the external models; for words with extremely low frequencies (e.g.,\\nhaving no reliable word embeddings), we can just use the internal model and ignore\\nthe external model, because the external information is noisy in this case. Figure6.11\\nshows model ensembling in different scenarios. For the sake of comparison, we use\\nthe integration of SPWCF, SPCSE, SPWE, and SPSE as CSP in all experiments.\\nAnd two models are integrated by simple weighted addition.\\n6.3.4.5\\nCross-Lingual Sememe Prediction\\nMost languages do not have sememe-based linguistic KBs such as HowNet, which\\nprevents us from understanding and utilizing human languages to a greater extent.\\nTherefore, it is important to build sememe-based linguistic KBs for various lan-\\nguages.\\nTo address the issue of the high labor cost of manual annotation, Qi et al. [56]\\npropose a new task, cross-lingual lexical sememe prediction (CLSP) which aims to\\n6.3 Applications\\n151\\nautomatically predict lexical sememes for words in other languages. There are two\\ncritical challenges for CLSP:\\n(1) There is not a consistent one-to-one match between words in different lan-\\nguages. For example, English word \\xe2\\x80\\x9cbeautiful\\xe2\\x80\\x9d can refer to Chinese words of either\\nor\\n. Hence, we cannot simply translate HowNet into another language.\\nAnd how to recognize the semantic meaning of a word in other languages becomes\\na critical problem.\\n(2) Since there is a gap between the semantic meanings of words and sememes,\\nwe need to build semantic representations for words and sememes to capture the\\nsemantic relatedness between them.\\nTo tackle these challenges, Qi et al. [56] propose a novel model for CLSP, which\\naims to transfer sememe-based linguistic KBs from source language to target lan-\\nguage. Their model contains three modules: (1) monolingual word embedding learn-\\ning which is intended for learning semantic representations of words for source and\\ntarget languages, respectively; (2) cross-lingual word embedding alignment which\\naims to bridge the gap between the semantic representations of words in two lan-\\nguages; (3) sememe-based word embedding learning whose objective is to incorpo-\\nrate sememe information into word representations.\\nThey take Chinese as source language and English as the target language to\\nshow the effectiveness of their model. Experimental results show that the proposed\\nmodel could effectively predict lexical sememes for words with different frequencies\\nin other languages and their model has consistent improvements on two auxiliary\\nexperiments including bilingual lexicon induction and monolingual word similarity\\ncomputation by jointly learning the representations of sememes, words in source and\\ntarget languages.\\nThe model consists of three parts: monolingual word representation learning,\\ncross-lingual word embedding alignment, and sememe-based word representation\\nlearning. Hence, they de\\xef\\xac\\x81ne the objective function of our method corresponding to\\nthe three parts:\\nL = Lmono + Lcross + Lsememe.\\n(6.32)\\nHere, the monolingual term Lmono is designed for learning monolingual word\\nembeddings from nonparallel corpora for source and target languages, respectively.\\nThe cross-lingual term Lcross aims to align cross-lingual word embeddings in a\\nuni\\xef\\xac\\x81ed semantic space. And Lsememe can draw sememe information into word rep-\\nresentation learning and conduce to better word embeddings for sememe prediction.\\nIn the following paragraphs, we will introduce the three parts in detail.\\nMonolingualWordRepresentation.Monolingualwordrepresentationisrespon-\\nsible for explaining regularities in monolingual corpora of source and target lan-\\nguages. Since the two corpora are nonparallel, Lmono comprises two monolingual\\nsubmodels that are independent of each other:\\nLmono = L S\\nmono + L T\\nmono,\\n(6.33)\\nwhere the superscripts S and T denote source and target languages, respectively.\\n152\\n6\\nSememe Knowledge Representation\\nAs a common practice, the well-established Skip-gram model is chosen to obtain\\nmonolingual word embeddings. The Skip-gram model is aimed at maximizing the\\npredictive probability of context words conditioned on the centered word. Formally,\\ntaking the source side, for example, given a training word sequence {wS\\n1, . . . , wS\\nn},\\nSkip-gram model intends to minimize\\nL S\\nmono = \\xe2\\x88\\x92\\nn\\xe2\\x88\\x92K\\n\\x02\\nc=K+1\\n\\x02\\n\\xe2\\x88\\x92K\\xe2\\x89\\xa4k\\xe2\\x89\\xa4K,k\\xcc\\xb8=0\\nlog P(wS\\nc+k|wS\\nc ),\\n(6.34)\\nwhere K is the size of the sliding window. P(wS\\nc+k|wS\\nc ) stands for the predictive prob-\\nability of one of the context words conditioned on the centered word wS\\nc , formalized\\nby the following softmax function:\\nP(wS\\nc+k|wS\\nc ) =\\nexp(wS\\nc+k \\xc2\\xb7 wS\\nc )\\n\\x03\\nwSs \\xe2\\x88\\x88V S exp(wSs \\xc2\\xb7 wSc ),\\n(6.35)\\nin which V s indicates the word vocabulary of source language. L T\\nmono can be formu-\\nlated similarly.\\nCross-lingual Word Embedding Alignment. Cross-lingual word embedding\\nalignment aims to build a uni\\xef\\xac\\x81ed semantic space for the words in source and target\\nlanguages. Inspired by [69], the cross-lingual word embeddings are aligned with\\nsignals of a seed lexicon and self-matching.\\nFormally, Lcross is composed of two terms including alignment by seed lexicon\\nLseed and alignment by matching Lmatch:\\nLcross = \\xce\\xbbsLseed + \\xce\\xbbmLmatch,\\n(6.36)\\nwhere \\xce\\xbbs and \\xce\\xbbm are hyperparameters for controlling relative weightings of the two\\nterms.\\n(1) Alignment by Seed Lexicon\\nThe seed lexicon term Lseed encourages word embeddings of translation pairs in\\na seed lexicon D to be close, which can be achieved via an L2 regularizer:\\nLseed =\\n\\x02\\n\\xe2\\x9f\\xa8wSs ,wT\\nt \\xe2\\x9f\\xa9\\xe2\\x88\\x88D\\n\\xe2\\x88\\xa5wS\\ns \\xe2\\x88\\x92wT\\nt \\xe2\\x88\\xa52,\\n(6.37)\\nin which wS\\ns and wT\\nt indicate the words in source and target languages in the seed\\nlexicon, respectively.\\n(2) Alignment by Matching Mechanism\\nAs for the matching process, it is found on the assumption that each target word\\nshould be matched to a single source word or a special empty word, and vice versa.\\nThe goal of the matching process is to \\xef\\xac\\x81nd the matched source (target) word for each\\n6.3 Applications\\n153\\ntarget (source) word and maximize the matching probabilities for all the matched\\nword pairs. The loss of this part can be formulated as\\nLmatch = L T2S\\nmatch + L S2T\\nmatch,\\n(6.38)\\nwhere L T 2S\\nmatch is the term for target-to-source matching and L S2T\\nmatch is the term for\\nsource-to-target matching.\\nNext, a detailed explanation of target-to-source matching is given, and the source-\\nto-target matching is de\\xef\\xac\\x81ned in the same way. A latent variable mt \\xe2\\x88\\x88{0, 1, . . . , |V S|}\\n(t = 1, 2, . . . , |V T |) is \\xef\\xac\\x81rst introduced for each target word wT\\nt , where |V S| and |V T |\\nindicate the vocabulary size of source and target languages, respectively. Here, mt\\nspeci\\xef\\xac\\x81es the index of the source word that wT\\nt matches with, and mt = 0 signi\\xef\\xac\\x81es the\\nempty word is matched. Then we have m = {m1, m2, . . . , m|V T |}, and can formalize\\nthe target-to-source matching term:\\nL T 2S\\nmatch = \\xe2\\x88\\x92log P(C T |C S) = \\xe2\\x88\\x92log\\n\\x02\\nm\\nP(C T , m|C S),\\n(6.39)\\nwhere C T and C S denote the target and source corpus, respectively. Here, they simply\\nassume that the matching processes of target words are independent of each other.\\nTherefore, we have\\nP(C T , m|C S) =\\n\\x07\\nwT \\xe2\\x88\\x88C T\\nP(wT , m|C S) =\\n|V T |\\n\\x07\\nt=1\\nP(wT\\nt |wS\\nmt)c(wT\\nt ),\\n(6.40)\\nwhere wS\\nmt is the source word that wT\\nt matches with, and c(wT\\nt ) is the number of\\ntimes wT\\nt occurs in the target corpus.\\n6.3.5\\nOther Sememe-Guided Applications\\n6.3.5.1\\nChinese LIWC Lexicon Expansion\\nLinguistic Inquiry and Word Count (LIWC) [52] has been widely used for comput-\\nerized text analysis in social science. Not only can LIWC be used to analyze text\\nfor classi\\xef\\xac\\x81cation and prediction, but it has also been used to examine the underlying\\npsychological states of a writer or speaker. In the beginning, LIWC was developed\\nto address content analytic issues in experimental psychology. Nowadays, there is\\nan increasing number of applications across \\xef\\xac\\x81elds such as computational linguistics\\n[22], demographics [48], health diagnostics [11], and social relationship [32].\\nChinese is the most spoken language in the world, but we cannot use the original\\nLIWC to analyze Chinese text. Fortunately, Chinese LIWC [28] has been released\\n154\\n6\\nSememe Knowledge Representation\\nto \\xef\\xac\\x81ll the vacancy. In this part, we mainly focus on Chinese LIWC and using LIWC\\nto stand for Chinese LIWC if not otherwise speci\\xef\\xac\\x81ed.\\nWhile LIWC has been used in a variety of \\xef\\xac\\x81elds, its lexicon only contains less than\\n7,000 words. This is insuf\\xef\\xac\\x81cient because according to [39], there are at least 56,008\\ncommon words in Chinese. Moreover, LIWC lexicon does not consider emerging\\nwords and phrases on the Internet. Therefore, it is reasonable and necessary to\\nexpand the LIWC lexicon so that it is more accurate and comprehensive for sci-\\nenti\\xef\\xac\\x81c research. One way to expand LIWC lexicon is to annotate the new words\\nmanually. However, it is too time-consuming and often requires language expertise\\nto add new words. Hence, expanding LIWC lexicon automatically is proposed.\\nIn LIWC lexicon, words are labeled with different categories and categories form\\na certain hierarchy. Therefore, hierarchical classi\\xef\\xac\\x81cation algorithms can be naturally\\napplied to LIWC lexicon. Reference [15] proposes Hierarchical SVM (Support Vec-\\ntor Machine), which is a modi\\xef\\xac\\x81ed version of SVM based on the hierarchical problem\\ndecomposition approach. In [6], the authors presented a novel algorithm which can\\nbe used on both tree- and Directed Acyclic Graph (DAG)-structured hierarchies.\\nSome recent works [12, 33] attempted to use neural networks in the hierarchical\\nclassi\\xef\\xac\\x81cation.\\nHowever, these methods are often too generic without considering the special\\nproperties of words and LIWC lexicon. Many words and phrases have multiple\\nmeaningsandaretherebyclassi\\xef\\xac\\x81edintomultipleleafcategories.Thisisoftenreferred\\nto as polysemy. Additionally, many categories in LIWC are \\xef\\xac\\x81ne-grained, thus making\\nit more dif\\xef\\xac\\x81cult to distinguish them. To address these issues, we introduce several\\nmodels to incorporate sememe information when expanding the lexicon, which will\\nbe discussed after the introduction of the basic model.\\nBasic Decoder for Hierarchical Classi\\xef\\xac\\x81cation. First, we introduce the basic\\nmodel for Chinese LIWC lexicon expansion. The well-known Sequence-to-Sequence\\ndecoder [64] is exploited for hierarchical classi\\xef\\xac\\x81cation. The original Sequence-to-\\nSequence decoder is often trained to predict the next word wt with consideration of\\nall the previously predicted words {w1, . . . , wt\\xe2\\x88\\x921}. This is a useful feature since an\\nimportant difference between \\xef\\xac\\x82at multilabel classi\\xef\\xac\\x81cation and hierarchical classi\\xef\\xac\\x81-\\ncation is that there are explicit connections among hierarchical labels. This property\\nis utilized by transforming hierarchical labels into a sequence. Let Y denote the label\\nset and \\xcf\\x80: Y \\xe2\\x86\\x92Y denote the parent relationship where \\xcf\\x80(y) is the parent node of\\ny \\xe2\\x88\\x88Y. Given a word w, its labels form a tree structure hierarchy. We then choose\\neach path from the root node to the leaf node, and transform it into a sequence\\n{y1, y2, . . . , yL} where \\xcf\\x80(yi) = yi\\xe2\\x88\\x921, \\xe2\\x88\\x80i \\xe2\\x88\\x88[2, L] and L is the number of levels in\\nthe hierarchy. In this way, when the model predicts a label yi, it takes into consid-\\neration the probability of parent label sequence {y1,. . . ,yi\\xe2\\x88\\x921}. Formally, the decoder\\nde\\xef\\xac\\x81nes a probability over the label sequence:\\nP(y1, y2, . . . , yL) =\\nL\\n\\x07\\ni=1\\nP(yi|(y1, . . . , yi\\xe2\\x88\\x921), w).\\n(6.41)\\n6.3 Applications\\n155\\nA common approach for decoder is to use LSTM [27] so that each conditional\\nprobability is computed as\\nP(yi|(y1, . . . , yi\\xe2\\x88\\x921), w) = g(yi\\xe2\\x88\\x921, si) = oi \\xe2\\x8a\\x99tanh(hi),\\n(6.42)\\nwhere\\nhi = fi \\xe2\\x8a\\x99hi\\xe2\\x88\\x921 + ii \\xe2\\x8a\\x99\\xcb\\x9chi,\\n\\xcb\\x9chi = tanh(Wh[hi\\xe2\\x88\\x921; yi\\xe2\\x88\\x921] + bh),\\noi = Sigmoid(Wo[hi\\xe2\\x88\\x921; yi\\xe2\\x88\\x921] + bo),\\nzi = Sigmoid(Wz[hi\\xe2\\x88\\x921; yi\\xe2\\x88\\x921] + bz),\\nfi = Sigmoid(W f [hi\\xe2\\x88\\x921; yi\\xe2\\x88\\x921] + b f ),\\n(6.43)\\nwhere \\xe2\\x8a\\x99is an element-wise multiplication and hi is the ith hidden state of the RNN.\\nWh, Wo, Wz, W f are weights and bh, bo, bz, b f are biases. oi, zi, and fi are known\\nas output gate layer, input gate layer, and forget gate layer, respectively.\\nTo take advantage of word embeddings, the initial state h0 = w is de\\xef\\xac\\x81ned where\\nw represents the embedding of the word. In other words, the word embeddings are\\napplied as the initial state of the decoder.\\nSpeci\\xef\\xac\\x81cally, the inputs of our model are word embeddings and label embeddings.\\nFirst, raw words are transformed into word embeddings by an embedding matrix E \\xe2\\x88\\x88\\nR|V |\\xc3\\x97dw, where dw is the word embedding dimension. Then, at each time step, label\\nembeddings y are fed to the model, which is obtained by a label embedding matrix\\nY \\xe2\\x88\\x88R|Y|\\xc3\\x97dy, where dy is the label embedding dimension. Here word embeddings are\\npretrained and \\xef\\xac\\x81xed during training.\\nGenerally speaking, the decoder is expected to decode word labels hierarchically\\nbased on word embeddings. At each time step, the decoder will predict the current\\nlabel depending on previously predicted labels.\\nHierarchical Decoder with Sememe Attention. The basic decoder uses word\\nembeddings as the initial state, then predicts word labels hierarchically as sequences.\\nHowever, each word in the basic decoder model has only one representation. This\\nis insuf\\xef\\xac\\x81cient because many words are polysemous and many categories are \\xef\\xac\\x81ne-\\ngrained in the LIWC lexicon. It is dif\\xef\\xac\\x81cult to handle these properties using a single\\nreal-valued vector. Therefore, Zeng et al. [68] propose to incorporate sememe infor-\\nmation.\\nBecause different sememes represent different meanings of a word, they should\\nhave different weights when predicting word labels. Moreover, we believe that the\\nsame sememe should have different weights in different categories. Take the word\\napex in Fig.6.12, for example. The sememe location should have a relatively\\nhigher weight when the decoder chooses among the subclasses of relative. When\\nchoosing among the subclasses of PersonalConcerns, location should have\\na lower weight because it represents a relatively irrelevant sense vertex.\\n156\\n6\\nSememe Knowledge Representation\\n2\\n1\\nSense  (acme)\\nSense  (vertex)\\n(apex)\\n(Boundary)\\n(Location)\\n(Entity)\\n(Angular)\\n(Dot)\\n(Most)\\n(GreaterThanNormal)\\nhost\\nmodifier\\nbelong\\nmodifier\\ndegree\\nFig. 6.12 Example word apex and its senses and sememes in HowNet annotation\\nTo achieve these goals, the utilization of attention mechanism [1] is proposed\\nto incorporate sememe information when decoding the word label sequence. The\\nstructure of the model is illustrated in Fig.6.13.\\nSimilar to the basic decoder approach, word embeddings are applied as the initial\\nstate of the decoder. The primary difference is that the conditional probability is\\nde\\xef\\xac\\x81ned as\\nP(yi|(y1, . . . , yi\\xe2\\x88\\x921), w, ci) = g([yi\\xe2\\x88\\x921; ci], hi),\\n(6.44)\\nwhere ci is known as context vector. The context vector ci depends on a set of sememe\\nembeddings {x1, . . . , xN}, acquired by a sememe embedding matrix X \\xe2\\x88\\x88R|S|\\xc3\\x97ds,\\nwhere ds is the sememe embedding dimension.\\nTo be more speci\\xef\\xac\\x81c, the context vector ci is computed as a weighted sum of the\\nsememe embedding x j:\\nci =\\nN\\n\\x02\\nj=1\\n\\xce\\xb1i jx j.\\n(6.45)\\nThe weight \\xce\\xb1i j of each sememe embedding x j is de\\xef\\xac\\x81ned as\\n\\xce\\xb1i j =\\nexp(v \\xc2\\xb7 tanh(W1yi\\xe2\\x88\\x921 + W2x j))\\n\\x03N\\nk=1 exp(v \\xc2\\xb7 tanh(W1yi\\xe2\\x88\\x921 + W2xk))\\n,\\n(6.46)\\n6.3 Applications\\n157\\nWord\\nEmbedding\\nSememe\\nEmbedding\\nSememe\\nEmbedding\\nSememe\\nEmbedding\\nSoftmax\\nLabel 1\\n<GO>\\n<EOS>\\nLabel 1.1\\nLabel 1.1.1\\nLabel 1\\nLabel 1.1\\nLabel 1.1.1\\n\\xe2\\x80\\xa6\\nFig. 6.13 The architecture of sememe attention decoder with word embeddings as the initial state\\nwhere v \\xe2\\x88\\x88Ra is a trainable parameter, W1 \\xe2\\x88\\x88Ra\\xc3\\x97dy and W2 \\xe2\\x88\\x88Ra\\xc3\\x97ds are weight\\nmatrices, and a is the number of hidden units in attention model.\\nIntuitively, at each time step, the decoder chooses which sememes to pay atten-\\ntion to when predicting the current word label. In this way, different sememes can\\nhave different weights, and the same sememe can have different weights in differ-\\nent categories. With the support of sememe attention, the decoder can differentiate\\nmultiple meanings in a word and the \\xef\\xac\\x81ne-grained categories and thus can expand a\\nmore accurate and comprehensive lexicon.\\n6.4\\nSummary\\nIn this chapter, we \\xef\\xac\\x81rst give an introduction to the most well-known sememe knowl-\\nedge base, HowNet, which uses about 2, 000 prede\\xef\\xac\\x81ned sememes to annotate over\\n100, 000 Chinese and English words and phrases. Different from other linguistic\\nknowledge bases like WordNet, HowNet is based on the minimum semantics units\\n(sememes) and captures the compositional relations between sememes and words.\\nTo learn the representations of sememe knowledge, we elaborate on three models,\\nnamely Simple Sememe Aggregation model (SSA), Sememe Attention over Context\\nmodel (SAC), and Sememe Attention over Target model (SAT). These models not\\nonly learn the representations of sememes but also help improve the representations\\nofwords.Next,wedescribesomeapplicationsofsememeknowledge,includingword\\n158\\n6\\nSememe Knowledge Representation\\nrepresentation, semantic composition, and language modeling. We also detail how to\\nautomatically predict sememes for both monolingual and cross-lingual unannotated\\nwords.\\nFor further learning of sememe knowledge-based NLP, you can read the book\\nwritten by the authors of HowNet [18]. You can also \\xef\\xac\\x81nd more related papers in\\nthis paper list https://github.com/thunlp/SCPapers. You can use the open source API\\nOpenHowNet https://github.com/thunlp/OpenHowNet to access HowNet data.\\nIn the future, there are some research directions worth exploring:\\n(1) Utilizing Structures of Sememe Annotations. The sememe annotations in\\nHowNet are hierarchical, and sememes annotated to a word are actually organized\\nas a tree. However, existing studies still do not utilize the structural information of\\nsememes. Instead, in current methods, sememes are simply regarded as semantic\\nlabels. In fact, the structures of sememes also incorporate abundant semantic infor-\\nmation and will be helpful to the deep understanding of lexical semantics. Besides,\\nexisting sememe prediction studies also predict unstructured sememes only, and it\\nis an interesting task to conduct structured sememe predictions.\\n(2) Leveraging Sememes in Low-data Regimes. One of the most important and\\ntypical characteristics of sememes is that limited sememes can represent unlimited\\nsemantics, which can play an important and positive role in tackling the low-data\\nregimes. In word representation learning, the representations of low-frequency words\\ncan be improved by their sememes, which have been well learned with the high-\\nfrequency words they annotate. We believe sememes will be bene\\xef\\xac\\x81cial to other\\nlow-data regimes, e.g., low-resource language NLP tasks.\\n(3)BuildingSememeKnowledgeBasesforOtherLanguages.OriginalHowNet\\nannotates sememes for only two languages: Chinese and English. As far as we\\nknow, there are not sememe knowledge bases like HowNet in other languages. Since\\nHowNet and its sememe knowledge have been veri\\xef\\xac\\x81ed helpful for better understand-\\ning human languages, it will be of great signi\\xef\\xac\\x81cance to annotate sememes for words\\nand phrases in other languages. In the section, we have described a study on cross-\\nlingual sememe prediction. And we think it is promising to make efforts toward this\\ndirection.\\nReferences\\n1. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. In Proceedings of ICLR, 2015.\\n2. Michele Banko, Vibhu O Mittal, and Michael J Witbrock. Headline generation based on sta-\\ntistical translation. In Proceedings of ACL, 2000.\\n3. MarcoBaroniandRobertoZamparelli.Nounsarevectors,adjectivesarematrices:Representing\\nadjective-noun constructions in semantic space. In Proceedings of EMNLP, 2010.\\n4. Yoshua Bengio, R\\xc3\\xa9jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\\nlanguage model. Journal of Machine Learning Research, 3(Feb):1137\\xe2\\x80\\x931155, 2003.\\n5. Adam Berger and John Lafferty. Information retrieval as statistical translation. In Proceedings\\nof SIGIR, 1999.\\nReferences\\n159\\n6. Wei Bi and James T Kwok. Multi-label classi\\xef\\xac\\x81cation on tree-and dag-structured hierarchies.\\nIn Proceedings of ICML, 2011.\\n7. William Blacoe and Mirella Lapata. A comparison of vector-based representations for semantic\\ncomposition. In Proceedings of EMNLP-CoNLL, 2012.\\n8. Leonard Bloom\\xef\\xac\\x81eld. A set of postulates for the science of language. Language, 2(3):153\\xe2\\x80\\x93164,\\n1926.\\n9. Thorsten Brants, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean. Large language\\nmodels in machine translation. In Proceedings of EMNLP, 2007.\\n10. Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Fredrick Jelinek,\\nJohn D Lafferty, Robert L Mercer, and Paul S Roossin. A statistical approach to machine\\ntranslation. Computational Linguistics, 16(2):79\\xe2\\x80\\x9385, 1990.\\n11. Wilma Bucci and Bernard Maskit. Building a weighted dictionary for referential activity. Com-\\nputing Attitude and Affect in Text, pages 49\\xe2\\x80\\x9360, 2005.\\n12. Ricardo Cerri, Rodrigo C Barros, and Andr\\xc3\\xa9 CPLF De Carvalho. Hierarchical multi-label\\nclassi\\xef\\xac\\x81cation using local neural networks. Journal of Computer and System Sciences, 80(1):39\\xe2\\x80\\x93\\n56, 2014.\\n13. Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. A uni\\xef\\xac\\x81ed model for word sense representation\\nand disambiguation. In Proceedings of EMNLP, 2014.\\n14. Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huanbo Luan. Joint learning of\\ncharacter and word embeddings. In Proceedings of IJCAI, 2015.\\n15. Yangchi Chen, Melba M Crawford, and Joydeep Ghosh. Integrating support vector machines\\nin a hierarchical output space decomposition framework. In Proceedings of IGARSS, 2004.\\n16. Lei Dang and Lei Zhang. Method of discriminant for chinese sentence sentiment orientation\\nbased on hownet. Application Research of Computers, 4:43, 2010.\\n17. Zhendong Dong and Qiang Dong. Hownet-a hybrid language and knowledge resource. In\\nProceedings of NLP-KE, 2003.\\n18. Zhendong Dong and Qiang Dong. HowNet and the Computation of Meaning (With CD-Rom).\\nWorld Scienti\\xef\\xac\\x81c, 2006.\\n19. Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A\\nSmith. Retro\\xef\\xac\\x81tting word vectors to semantic lexicons. In Proceedings of NAACL-HLT, 2015.\\n20. Xianghua Fu, Guo Liu, Yanyan Guo, and Zhiqiang Wang. Multi-aspect sentiment analysis for\\nchinese online social reviews based on topic modeling and hownet lexicon. Knowledge-Based\\nSystems, 37:186\\xe2\\x80\\x93195, 2013.\\n21. Edward Grefenstette and Mehrnoosh Sadrzadeh. Experimental support for a categorical com-\\npositional distributional model of meaning. In Proceedings of EMNLP, 2011.\\n22. Justin Grimmer and Brandon M Stewart. Text as data: The promise and pitfalls of automatic\\ncontent analysis methods for political texts. Political analysis, 21(3):267\\xe2\\x80\\x93297, 2013.\\n23. Yihong Gu, Jun Yan, Hao Zhu, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Fen Lin, and Leyu\\nLin. Language modeling with sparse product of sememe experts. In Proceedings of EMNLP,\\npages 4642\\xe2\\x80\\x934651, 2018.\\n24. Djoerd Hiemstra. A linguistically motivated probabilistic model of information retrieval. In\\nProceedings of TPDL, 1998.\\n25. G. E Hinton. Products of experts. In Proceedings of ICANN, 1999.\\n26. Geoffrey E Hinton. Learning distributed representations of concepts. In Proceedings of CogSci,\\n1986.\\n27. Sepp Hochreiter and J\\xc3\\xbcrgen Schmidhuber. Long short-term memory. Neural Computation,\\n9(8):1735\\xe2\\x80\\x931780, 1997.\\n28. Chin-Lan Huang, CK Chung, Natalie Hui, Yi-Cheng Lin, Yi-Tai Seih, WC Chen, and JW Pen-\\nnebaker. The development of the chinese linguistic inquiry and word count dictionary. Chinese\\nJournal of Psychology, 54(2):185\\xe2\\x80\\x93201, 2012.\\n29. Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. Improving word\\nrepresentations via global context and multiple word prototypes. In Proceedings of ACL, 2012.\\n30. Huiming Jin, Hao Zhu, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Fen Lin, and Leyu Lin.\\nIncorporating chinese characters of words for lexical sememe prediction. In Proceedings of\\nACL, 2018.\\n160\\n6\\nSememe Knowledge Representation\\n31. Dan Jurafsky. Speech & language processing. 2000.\\n32. Ewa Kacewicz, James W Pennebaker, Matthew Davis, Moongee Jeon, and Arthur C Graesser.\\nPronoun use re\\xef\\xac\\x82ects standings in social hierarchies. Journal of Language and Social Psychol-\\nogy, 33(2):125\\xe2\\x80\\x93143, 2014.\\n33. Sanjeev Kumar Karn, Ulli Waltinger, and Hinrich Sch\\xc3\\xbctze. End-to-end trainable attentive\\ndecoder for hierarchical entity classi\\xef\\xac\\x81cation. Proceedings of EACL, 2017.\\n34. Slava Katz. Estimation of probabilities from sparse data for the language model component of a\\nspeech recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400\\xe2\\x80\\x93\\n401, 1987.\\n35. Thomas Kober, Julie Weeds, Jeremy Ref\\xef\\xac\\x81n, and David Weir. Improving sparse word repre-\\nsentations with distributional inference for semantic composition. In Proceedings of EMNLP,\\n2016.\\n36. Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recom-\\nmender systems. Computer, 42(8), 2009.\\n37. Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In\\nProceedings of NeurIPS, 2014.\\n38. Wei Li, Xuancheng Ren, Damai Dai, Yunfang Wu, Houfeng Wang, and Xu Sun. Sememe\\nprediction: Learning semantic knowledge from unstructured textual wiki descriptions. arXiv\\npreprint arXiv:1808.05437, 2018.\\n39. Xingjian Li et al. Lexicon of common words in contemporary chinese, 2008.\\n40. Qun Liu. Word similarity computing based on hownet. Computational linguistics and Chinese\\nlanguage processing, 7(2):59\\xe2\\x80\\x9376, 2002.\\n41. Shu Liu, Jingjing Xu, Xuancheng Ren, and Xu Sun. Evaluating semantic rationality of a\\nsentence: A sememe-word-matching neural network based on hownet. 2019.\\n42. Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher\\nPotts. Learning word vectors for sentiment analysis. In Proceedings of ACL, 2011.\\n43. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\\xef\\xac\\x81cient estimation of word\\nrepresentations in vector space. In Proceedings of ICLR, 2013.\\n44. Tomas Mikolov, Martin Kara\\xef\\xac\\x81\\xc3\\xa1t, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recur-\\nrent neural network based language model. In Proceedings of InterSpeech, 2010.\\n45. David RH Miller, Tim Leek, and Richard M Schwartz. A hidden markov model information\\nretrieval system. In Proceedings of SIGIR, 1999.\\n46. JeffMitchell andMirella Lapata.Vector-basedmodelsofsemantic composition.InProceedings\\nof ACL, 2008.\\n47. Jeff Mitchell and Mirella Lapata. Language models based on semantic composition. In Pro-\\nceedings of EMNLP, 2009.\\n48. Matthew L Newman, Carla J Groom, Lori D Handelman, and James W Pennebaker. Gen-\\nder differences in language use: An analysis of 14,000 text samples. Discourse Processes,\\n45(3):211\\xe2\\x80\\x93236, 2008.\\n49. Yilin Niu, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Improved word representation learn-\\ning with sememes. In Proceedings of ACL, 2017.\\n50. Francis Jeffry Pelletier. The principle of semantic compositionality. Topoi, 13(1):11\\xe2\\x80\\x9324, 1994.\\n51. Francis Jeffry Pelletier. Semantic Compositionality, volume 1. 2016.\\n52. James W Pennebaker, Roger J Booth, and Martha E Francis. Linguistic inquiry and word count:\\nLiwc [computer software]. Austin, TX: liwc. net, 2007.\\n53. Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\\nrepresentation. In Proceedings of EMNLP, 2014.\\n54. Jay M Ponte and W Bruce Croft. A language modeling approach to information retrieval. In\\nProceedings of SIGIR, 1998.\\n55. Fanchao Qi, Junjie Huang, Chenghao Yang, Zhiyuan Liu, Xiao Chen, Qun Liu, and Maosong\\nSun. Modeling semantic compositionality with sememe knowledge. In Proceedings of ACL,\\n2019.\\n56. Fanchao Qi, Yankai Lin, Maosong Sun, Hao Zhu, Ruobing Xie, and Zhiyuan Liu. Cross-lingual\\nlexical sememe prediction. In Proceedings of EMNLP, 2018.\\nReferences\\n161\\n57. Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive\\nsentence summarization. In Proceedings of EMNLP, 2015.\\n58. Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. Item-based collaborative\\n\\xef\\xac\\x81ltering recommendation algorithms. In Proceedings of WWW, 2001.\\n59. Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. Parsing with com-\\npositional vector grammars. In Proceedings of ACL, 2013.\\n60. Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic compo-\\nsitionality through recursive matrix-vector spaces. In Proceedings of EMNLP-CoNLL, 2012.\\n61. Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D. Manning, Andrew Y\\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over a senti-\\nment treebank. In Proceedings of EMNLP, 2013.\\n62. Jingguang Sun, Dongfeng Cai, Dexin Lv, and Yanju Dong. Hownet based chinese question\\nautomatic classi\\xef\\xac\\x81cation. Journal of Chinese Information Processing, 21(1):90\\xe2\\x80\\x9395, 2007.\\n63. Maosong Sun and Xinxiong Chen. Embedding for words and word senses based on human\\nannotatedknowledgebase:Acasestudyonhownet.JournalofChineseInformationProcessing,\\n30:1\\xe2\\x80\\x936, 2016.\\n64. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\\nnetworks. In Proceedings of NeurIPS, 2014.\\n65. DavidWeir,JulieWeeds,JeremyRef\\xef\\xac\\x81n,andThomasKober.Aligningpackeddependencytrees:\\na theory of composition for distributional semantics. Computational Linguistics, 42(4):727\\xe2\\x80\\x93\\n761, December 2016.\\n66. Ruobing Xie, Xingchi Yuan, Zhiyuan Liu, and Maosong Sun. Lexical sememe prediction via\\nword embeddings and matrix factorization. In Proceedings of IJCAI, 2017.\\n67. Binyong Yin. Quantitative research on Chinese morphemes. Studies of the Chinese Language,\\n5:338\\xe2\\x80\\x93347, 1984.\\n68. Xiangkai Zeng, Cheng Yang, Cunchao Tu, Zhiyuan Liu, and Maosong Sun. Chinese liwc\\nlexicon expansion via hierarchical classi\\xef\\xac\\x81cation of word embeddings with sememe attention.\\nIn Proceedings of AAAI, 2018.\\n69. Meng Zhang, Haoruo Peng, Yang Liu, Huan-Bo Luan, and Maosong Sun. Bilingual lexicon\\ninduction from non-parallel data with minimal supervision. In Proceedings of AAAI, 2017.\\n70. Yuntao Zhang, Ling Gong, and Yongcheng Wang. Chinese word sense disambiguation using\\nhownet. In Proceedings of ICNC,\\n71. Yu Zhao, Zhiyuan Liu, and Maosong Sun. Phrase type sensitive tensor indexing model for\\nsemantic composition. In Proceedings of AAAI, 2015.\\n72. Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. DAG-Structured long short-term memory\\nfor semantic compositionality. In Proceedings of NAACL-HLT, 2016.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 7\\nWorld Knowledge Representation\\nAbstract World knowledge representation aims to represent entities and relations\\nin the knowledge graph in low-dimensional semantic space, which have been widely\\nused in large knowledge-driven tasks. In this chapter, we \\xef\\xac\\x81rst introduce the concept\\nof the knowledge graph. Next, we introduce the motivations and give an overview\\nof the existing approaches for knowledge graph representation. Further, we discuss\\nseveral advanced approaches that aim to deal with the current challenges of knowl-\\nedge graph representation. We also review the real-world applications of knowledge\\ngraph representation, such as language modeling, question answering, information\\nretrieval, and recommender systems.\\n7.1\\nIntroduction\\nKnowledge Graph (KG), which is also named as Knowledge Base (KB), is a signif-\\nicant multi-relational dataset for modeling concrete entities and abstract concepts in\\nthe real world. It provides useful structured information and plays a crucial role in\\nlots of real-world applications such as web search and question answering. It is not\\nexaggerated to say that knowledge graphs teach us how to model the entities as well\\nas the relationships among them in this complicated real world.\\nTo encode knowledge into a real-world application, knowledge graph represen-\\ntation, which represents entities and relations in knowledge graphs with distributed\\nrepresentations, has been proposed and applied to various real-world arti\\xef\\xac\\x81cial intel-\\nligence \\xef\\xac\\x81elds including question answering, information retrieval, and dialogue sys-\\ntem. That is, knowledge graph representation learning plays a vital role as a bridge\\nbetween knowledge graphs and knowledge-driven tasks.\\nIn this section, we will introduce the concept of knowledge graph, several typical\\nknowledge graphs, knowledge graph representation learning, and several typical\\nknowledge-driven tasks.\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_7\\n163\\n164\\n7\\nWorld Knowledge Representation\\n7.1.1\\nWorld Knowledge Graphs\\nIn ancient times, knowledge was stored and inherited through books and letters\\nwritten on parchment or bamboo slip. With the Internet thriving in the twenty-\\xef\\xac\\x81rst\\ncentury, millions of thousands of messages have \\xef\\xac\\x82ooded into the World Wide Web,\\nand knowledge was transferred to the semi-structured textual information on the\\nweb. However, due to the information explosion, it is not easy to extract knowledge\\nwe want from the signi\\xef\\xac\\x81cant, noisy plain text on the Internet. To obtain knowledge\\neffectively, people notice that the world is not only made of strings but also made of\\nentities and relations. Knowledge Graph, which arranges structured multi-relational\\ndata of concrete entities and abstract concepts in the real world, is blooming in recent\\nyears and attracts wide attention in both academia and industry.\\nKGs are usually constructed from existing Semantic Web datasets in Resource\\nDescription Framework (RDF) with the help of manual annotation, while it can\\nalso be automatically enriched by extracting knowledge from large plain texts on\\nthe Internet. A typical KG usually contains two elements, including entities (i.e.,\\nconcrete entities and abstract concepts in the real world) and relations between\\nentities. It usually represents knowledge with large quantities of triple facts in the\\ntriple form of \\xe2\\x9f\\xa8head entity, relation, tail entity\\xe2\\x9f\\xa9abridged as \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9. For example,\\nWilliam Shakespeare is a famous English poet and playwright, who is widely\\nregarded as the greatest writer in the English language, and Romeo and Juliet\\nis one of his masterpieces. In knowledge graph, we will represent this knowledge as\\n\\xe2\\x9f\\xa8William Shakespeare, works_written, Romeo and Juliet\\xe2\\x9f\\xa9. Note\\nthat in the real world, the same head entity and relation may have multiple tail\\nentities (e.g., William Shakespeare also wrote Hamlet and A Midsummer\\nNight\\xe2\\x80\\x99s Dream), and reversely the same situation will happen when tail entity\\nand relation are \\xef\\xac\\x81xed. Even it is possible when both the head entity and tail entity are\\nmultiple (e.g., in relations like actor_in_movie). However, in KG, all knowl-\\nedge can be represented in triple facts regardless of the types of entities and relations.\\nThrough these triples, we can generate a huge directed graph whose nodes corre-\\nspond to entities and edges correspond to relations to model the real world. With the\\nwell-structured united knowledge representation, KGs are widely used in a variety\\nof applications to enhance their system performance.\\nThere are several KGs widely utilized nowadays in applications of information\\nretrieval and question answering. In this subsection, we will introduce some famous\\nKGs such as Freebase, DBpedia, Yago, and WordNet. In fact, there are also lots\\nof comparatively smaller KGs in speci\\xef\\xac\\x81c \\xef\\xac\\x81elds of knowledge functioned in vertical\\nsearch.\\n7.1.1.1\\nFreebase\\nFreebase is one of the most popular knowledge graphs in the world. It is a large\\ncommunity-curated database consisting of well-known people, places, and things,\\n7.1 Introduction\\n165\\nFig. 7.1 An example of search results in Freebase\\nwhich is composed of existing databases and its community members. Freebase\\nwas \\xef\\xac\\x81rst developed by Metaweb, an American software company, and ran since\\nMarch 2007. In July 2010, Metaweb was acquired by Google, and Freebase was\\ncombined to power up Google\\xe2\\x80\\x99s Knowledge Graph. In December 2014, the Freebase\\nteam of\\xef\\xac\\x81cially announced that the website, as well as the API of Freebase, would\\nbe shut down by June 30, 2015. While the data in Freebase would be transferred\\nto Wikidata, which is another collaboratively edited knowledge base operated by\\nWikimedia Foundation. Up to March 24, 2016, Freebase arranged 58,726,427 topics\\nand 3,197,653,841 facts.\\nFreebase contains well-structured data representing relationships between entities\\naswellastheattributesofentitiesintheformoftriplefacts(Fig.7.1).DatainFreebase\\nwas mainly harvested from various sources, including Wikipedia, Fashion Model\\nDirectory, NNDB, MusicBrainz, and so on. Moreover, the community members also\\ncontributed a lot to Freebase. Freebase is an open and shared database that aims to\\nconstruct a global database which encodes the world\\xe2\\x80\\x99s knowledge. It announced an\\nopen API, RDF endpoint, and a database dump for its users for both commercial and\\nnoncommercial use. As described by Tim O\\xe2\\x80\\x99Reilly, Freebase is the bridge between\\nthe bottom-up vision of Web 2.0 collective intelligence and the more structured world\\nof the Semantic Web.\\n7.1.1.2\\nDBpedia\\nDBpedia is a crowd-sourced community effort aiming to extract structured content\\nfrom Wikipedia and make this information accessible on the web. It was started by\\nresearchers at Free University of Berlin, Leipzig University and OpenLink Software,\\n166\\n7\\nWorld Knowledge Representation\\ninitially released to the public in January 2007. DBpedia allows users to ask semantic\\nqueries associated with Wikipedia resources, even including links to other related\\ndatasets, which makes it easier for us to fully utilize the massive amount of informa-\\ntion in Wikipedia in a novel and effective way. DBpedia is also an essential part of\\nthe Linked Data effort described by Tim Berners-Lee.\\nThe English version of DBpedia describes 4.58 million entities, out of which\\n4.22 million are classi\\xef\\xac\\x81ed in a consistent ontology, including 1,445,000 persons,\\n735,000 places, 411,000 creative works, 251,000 species, 241,000 organizations, and\\n6,000 diseases. There are also localized versions of DBpedia in 125 languages, all of\\nwhich contain 38.3 million entities. Besides, DBpedia also contains a great number of\\ninternal and external links, including 80.9 million links to Wikipedia categories, 41.2\\nmillion links to YAGO categories, 25.2 million links to images, and 29.8 million links\\nto external web pages. Moreover, DBpedia maintains a hierarchical, cross-domain\\nontology covering overall 685 classes, which has been manually created based on\\nthe commonly used infoboxes in Wikipedia.\\nDBpedia has several advantages over other KGs. First, DBpedia has a close con-\\nnection to Wikipedia and can automatically evolve as Wikipedia changes. It makes\\nthe update process of DBpedia more ef\\xef\\xac\\x81cient. Second, DBpedia is multilingual that\\nis convenient for users over the world with their native languages.\\n7.1.1.3\\nYAGO\\nYAGO, which is short for Yet Another Great Ontology, is a high-quality KG devel-\\nopedbyMaxPlanckInstituteforComputerScienceinSaarbru\\xc3\\xbcckeninitiallyreleased\\nin 2008. Knowledge in YAGO is automatically extracted from Wikipedia, WordNet,\\nand GeoNames, whose accuracy has been manually evaluated and proves a con-\\n\\xef\\xac\\x81rmed accuracy of 95%. YAGO is special not only because of the con\\xef\\xac\\x81dence value\\nevery fact possesses depending on the manual evaluation but also because that YAGO\\nis anchored in space and time, which can provide a spatial dimension or temporal\\ndimension to part of its entities.\\nCurrently, YAGO has more than 10 million entities, including persons, organi-\\nzations, and locations, with over 120 million facts about these entities. YAGO also\\ncombines knowledge extracted from Wikipedias of 10 different languages and clas-\\nsi\\xef\\xac\\x81es them into approximately 350,000 classes according to the Wikipedia category\\nsystem and the taxonomy of WordNet. YAGO has also joined the linked data project\\nand been linked to the DBpedia ontology and the SUMO ontology (Fig.7.2).\\n7.2\\nKnowledge Graph Representation\\nKnowledge Graphs provide us with a novel aspect to describe the world with entities\\nand triple facts, which attract growing attention from researchers. Large KGs such\\nas Freebase, DBpedia, and YAGO have been constructed and widely used in an\\nenormous amount of applications such as question answering and Web search.\\n7.2 Knowledge Graph Representation\\n167\\nFig. 7.2 An example of search results in YAGO\\nHowever, with KG size increasing, we are facing two main challenges: data spar-\\nsity and computational inef\\xef\\xac\\x81ciency. Data sparsity is a general problem in lots of\\n\\xef\\xac\\x81elds like social network analysis or interest mining. It is because that there are too\\nmany nodes (e.g., users, products, or entities) in a large graph, while too few edges\\n(e.g., relationships) between these nodes, since the number of relations of a node is\\nlimited in the real world. Computational ef\\xef\\xac\\x81ciency is another challenge we need to\\novercome with the increasing size of knowledge graphs.\\nTo tackle these problems, representation learning is introduced to knowledge rep-\\nresentation. Representation learning in KGs aims to project both entities and relations\\ninto a low-dimensional continuous vector space to get their distributed representa-\\ntions, whose performance has been con\\xef\\xac\\x81rmed in word representation and social rep-\\nresentation. Compared with the traditional one-hot representation, distributed repre-\\nsentation has much fewer dimensions, and thus lowers the computational complexity.\\nWhat is more, distributed representation can explicitly show the similarity between\\nentities through some distance calculated by the low-dimensional embeddings, while\\nall embeddings in one-hot representation are orthogonal, making it dif\\xef\\xac\\x81cult to tell\\nthe potential relations between entities.\\nWith the advantages above, knowledge graph representation learning is blooming\\nin knowledge applications, signi\\xef\\xac\\x81cantly improving the ability of KGs on the task\\nof knowledge completion, knowledge fusion, and reasoning. It is considered as the\\nbridge between knowledge construction, knowledge graphs, and knowledge-driven\\napplications. Up till now, a high number of methods have been proposed using a\\ndistributed representation for modeling knowledge graphs, with the learned knowl-\\nedge representations widely utilized in various knowledge-driven tasks like question\\nanswering, information retrieval, and dialogue system.\\n168\\n7\\nWorld Knowledge Representation\\nIn summary, Knowledge graph Representation Learning (KRL) aims to construct\\ndistributed knowledge representations for entities and relations, projecting knowl-\\nedge into low-dimensional semantic vector spaces. Recent years have witnessed sig-\\nni\\xef\\xac\\x81cant advances in knowledge graph representation learning with a large amount of\\nKRL methods proposed to construct knowledge representations, among which the\\ntranslation-based methods achieve state-of-the-art performance in many KG tasks,\\nwith a right balance in both effectiveness and ef\\xef\\xac\\x81ciency.\\nIn this section, we will \\xef\\xac\\x81rst describe the notations that we will use in KRL. Then,\\nwe will introduce TransE, which is the fundamental version of translation-based\\nmethods. Next, we will explore the various extension methods of TransE in detail.\\nAt last, we will take a brief look over other representation learning methods utilized\\nin modeling knowledge graphs.\\n7.2.1\\nNotations\\nFirst, we introduce the general notations used in the rest of this section. We use\\nG = (E, R, T ) to denote the whole KG, in which E = {e1, e2, . . . , e|E|} stands for\\nthe entity set, R = {r1,r2, . . . ,r|R|} stands for the relation set, and T stands for the\\ntriple set. |E| and |R| are the corresponding entity and relation numbers in their\\noverall sets. As stated above, we represent knowledge in the form of triple fact\\n\\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, where h \\xe2\\x88\\x88E means the head entity, t \\xe2\\x88\\x88E means the tail entity, and r \\xe2\\x88\\x88R\\nmeans the relation between h and t.\\n7.2.2\\nTransE\\nTransE [7] is a translation-based model for learning low-dimensional embeddings of\\nentities and relations. It projects entities as well as relations into the same semantic\\nembeddingspace,andthenconsidersrelationsastranslationsintheembeddingspace.\\nFirst, we will start with the motivations of this method, and then discuss the details\\nin how knowledge representations are trained under TransE. Finally, we will explore\\nthe advantages and disadvantages of TransE for a deeper understanding.\\n7.2.2.1\\nMotivation\\nThere are three main motivations behind the translation-based knowledge graph\\nrepresentation learning method. The primary motivation is that it is natural to con-\\nsider relationships between entities as translating operations. Through distributed\\nrepresentations, entities are projected to a low-dimensional vector space. Intuitively,\\nwe agree that a reasonable projection should map entities with similar semantic\\nmeanings to the same \\xef\\xac\\x81eld, while entities with different meanings should belong to\\n7.2 Knowledge Graph Representation\\n169\\ndistinct clusters in the vector space. For example, William Shakespeare and\\nJane Austen may be in the same cluster of writers, Romeo, and Juliet and\\nPride and Prejudice may be in another cluster of books. In this case, they\\nshare the same relation works_written, and the translations between writers and\\nbooks in the vector space are similar.\\nThe secondary motivation of TransE derives from the breakthrough in word repre-\\nsentation by Word2vec [49]. Word2vec proposes two simple models, Skip-gram and\\nCBOW, to learn word embeddings from large-scale corpora, signi\\xef\\xac\\x81cantly improv-\\ning the performance in word similarity and analogy. The word embeddings learned\\nby Word2vec have some interesting phenomena: if two word-pairs share the same\\nsemantic or syntactic relationships, their subtraction embeddings in each word pair\\nwill be similar. For instance, we have\\nw(king) \\xe2\\x88\\x92w(man) \\xe2\\x89\\x88w(queen) \\xe2\\x88\\x92w(woman),\\n(7.1)\\nwhich indicates that the latent semantic relation between king and man, which is\\nsimilar to the relation between queen and woman, is successfully embedded in\\nthe word representation. This approximate relation could be found not only with the\\nsemantic relations but also with the syntactic relations. We have\\nw(bigger) \\xe2\\x88\\x92w(big) \\xe2\\x89\\x88w(smaller) \\xe2\\x88\\x92w(small).\\n(7.2)\\nThe phenomenon found in word representation strongly implies that there may\\nexist an explicit method to represent relationships between entities as translating\\noperations in vector space.\\nThe last motivation comes from the consideration of computational complexity.\\nOn the one hand, a substantial increase in model complexity will result in high\\ncomputational costs and obscure model interpretability. Moreover, a complex model\\nmay lead to over\\xef\\xac\\x81tting. On the other hand, experimental results on model complexity\\ndemonstrate that the simpler models perform almost as good as more expressive mod-\\nels in most KG applications, in the condition that there are sizeable multi-relational\\ndataset and a relatively large amount of relations. As KG size increases, computa-\\ntional complexity becomes the primary challenge in the knowledge graph represen-\\ntation. The intuitive assumption of translation leads to a better trade-off between\\naccuracy and ef\\xef\\xac\\x81ciency.\\n7.2.2.2\\nMethodology\\nAs illustrated in Fig.7.3, TransE projects entities and relations into the same low-\\ndimensional space. All embeddings take values in Rd, where d is a hyperparameter\\nindicating the dimension of embeddings. With the translation assumption, for each\\ntriple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9in T , we want the summation embedding h + r to be the nearest\\nneighbor of tail embedding t. The score function of TransE is then de\\xef\\xac\\x81ned as follows:\\n170\\n7\\nWorld Knowledge Representation\\nFig. 7.3 The architecture of\\nTransE model [47]\\nh\\nt\\nr\\nE (h,r, t) = \\xe2\\x88\\xa5h + r \\xe2\\x88\\x92t\\xe2\\x88\\xa5.\\n(7.3)\\nMore speci\\xef\\xac\\x81cally, to learn such embeddings of entities and relations, TransE\\nformalizes a margin-based loss function with negative sampling as objective for\\ntraining. The pair-wise function is de\\xef\\xac\\x81ned as follows:\\nL =\\n\\x02\\n\\xe2\\x9f\\xa8h,r,t\\xe2\\x9f\\xa9\\xe2\\x88\\x88T\\n\\x02\\n\\xe2\\x9f\\xa8h\\xe2\\x80\\xb2,r\\xe2\\x80\\xb2,t\\xe2\\x80\\xb2\\xe2\\x9f\\xa9\\xe2\\x88\\x88T \\xe2\\x88\\x92\\nmax(\\xce\\xb3 + E (h,r, t)) \\xe2\\x88\\x92E (h\\xe2\\x80\\xb2,r\\xe2\\x80\\xb2, t\\xe2\\x80\\xb2), 0),\\n(7.4)\\nin which E (h,r, t) is the score of energy function for a positive triple (i.e., triple in\\nT ) and E (h\\xe2\\x80\\xb2,r\\xe2\\x80\\xb2, t\\xe2\\x80\\xb2) is that of a negative triple. The energy function E can be either\\nmeasured by L1 or L2 distance. \\xce\\xb3 > 0 is a hyperparameter of margin and a bigger\\n\\xce\\xb3 means a wider gap between positive and the corresponding negative scores. T \\xe2\\x88\\x92is\\nthe negative triple set with respect to T .\\nSince there are no explicit negative triples in knowledge graphs, we de\\xef\\xac\\x81ne T \\xe2\\x88\\x92as\\nfollows:\\nT \\xe2\\x88\\x92= {\\xe2\\x9f\\xa8h\\xe2\\x80\\xb2,r, t\\xe2\\x9f\\xa9|h\\xe2\\x80\\xb2 \\xe2\\x88\\x88E} \\xe2\\x88\\xaa{\\xe2\\x9f\\xa8h,r\\xe2\\x80\\xb2, t\\xe2\\x9f\\xa9|r\\xe2\\x80\\xb2 \\xe2\\x88\\x88R} \\xe2\\x88\\xaa{\\xe2\\x9f\\xa8h,r, t\\xe2\\x80\\xb2\\xe2\\x9f\\xa9|t\\xe2\\x80\\xb2 \\xe2\\x88\\x88E},\\n\\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9\\xe2\\x88\\x88T,\\n(7.5)\\nwhich means the negative triple set T \\xe2\\x88\\x92is composed of the positive triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9\\nwith head entity, relation, or tail entity randomly replaced by any other entities or\\nrelations in KG. Note that the new triple generated after replacement will not be\\nconsidered as a negative sample if it has already been in T .\\nTransE is optimized using mini-batch stochastic gradient descent (SGD), with\\nentities and relations randomly initialized. Knowledge completion, which is a link\\nprediction task aiming to predict the third element in a triple (could be either entity\\nor relation) with the given rest two elements, is designed to evaluate the learned\\nknowledge representations.\\n7.2 Knowledge Graph Representation\\n171\\n7.2.2.3\\nDisadvantages and Challenges\\nTransE is effective and ef\\xef\\xac\\x81cient and has shown its power on link prediction. However,\\nit still has several disadvantages and challenges to be further explored.\\nFirst, in knowledge completion, we may have multiple correct answers with the\\ngiven two elements in a triple. For instance, with the given head entity William\\nShakespeare and the relation works_written, we will get a list of master-\\npiecesincludingRomeo and Juliet,Hamletand A Midsummer Night\\xe2\\x80\\x99s\\nDream. These books share the same information in the writer while differing in many\\nother \\xef\\xac\\x81elds such as theme, background, and famous roles in the book. However,\\nwith the translation assumption in TransE, every entity has only one embedding\\nin all triples, which signi\\xef\\xac\\x81cantly limits the ability of TransE in knowledge graph\\nrepresentations. In [7], the authors categorize all relations into four classes, 1-to-1,\\n1-to-Many, Many-to-1, Many-to-Many, according to the cardinalities of their head\\nand tail arguments. A relation is considered as 1-to-1 if most heads appear with\\none tail, 1-to-Many if a head can appear with many tails, Many-to-1 if a tail can\\nappear with many heads, and Many-to-Many if multiple heads appear with multiple\\ntails. Statistics demonstrate that the 1-to-Many, Many-to-1, Many-to-Many relations\\noccupy a large proportion. TransE does well in 1-to-1, but it has issues when dealing\\nwith 1-to-Many, Many-to-1, Many-to-Many relations. Similarly, TransE may also\\nstruggle with re\\xef\\xac\\x82exive relations.\\nSecond, the translating operation is intuitive and effective, only considering the\\nsimple one-step translation, which may limit the ability to model KGs. Taking enti-\\nties as nodes and relations as edges, we can construct a huge knowledge graph\\nwith the triple facts. However, TransE focuses on minimizing the energy function\\nE (h,r, t) = \\xe2\\x88\\xa5h + r \\xe2\\x88\\x92t\\xe2\\x88\\xa5, which only utilize the one-step relation information in\\nknowledge graphs, regardless of the latent relationships located in long-distance\\npaths. For example, if we know the triple fact that \\xe2\\x9f\\xa8The forbidden city,\\nlocate_in, Beijing\\xe2\\x9f\\xa9and \\xe2\\x9f\\xa8Beijing, capital_of, China\\xe2\\x9f\\xa9, we can infer\\nthat The forbidden city locates in China. TransE can be further enhanced\\nwith the favor of multistep information.\\nThird, the representation and the dissimilarity function in TransE are oversimpli-\\n\\xef\\xac\\x81ed for the consideration of ef\\xef\\xac\\x81ciency. Therefore, TransE may not be capable enough\\nof modeling those complicated entities and relations in knowledge graphs. There still\\nexist challenges on how to balance the effectiveness and ef\\xef\\xac\\x81ciency, avoiding both\\nover\\xef\\xac\\x81tting and under\\xef\\xac\\x81tting.\\nBesides the disadvantages and challenges stated above, multisource information\\nsuch as textual information and hierarchical type/label information is of great sig-\\nni\\xef\\xac\\x81cance, which will be further discussed in the following.\\n172\\n7\\nWorld Knowledge Representation\\n7.2.3\\nExtensions of TransE\\nThere are lots of extension methods following TransE to address the challenges\\nabove. Speci\\xef\\xac\\x81cally, TransH, TransR, TransD, and TranSparse are proposed to solve\\nthe challenges in modeling 1-to-Many, Many-to-1, and Many-to-Many relations,\\nPTransE is proposed to encode long-distance information located in multistep paths,\\nand CTransR, TransA, TransG, and KG2E further extend the oversimpli\\xef\\xac\\x81ed model\\nof TransE. We will discuss these extension methods in detail.\\n7.2.3.1\\nTransH\\nWith distributed representation, entities are projected to the semantic vector space,\\nand similar entities tend to be in the same cluster. However, it seems that William\\nShakespeare should be in the neighborhood of Isaac Newton when talking\\nabout nationality, while it should be next to Mark Twain when talking about occu-\\npation. To accomplish this, we want entities to show different preferences in different\\nsituations, that is, to have multiple representations in different triples.\\nTo address the issue when modeling 1-to-Many, Many-to-1, Many-to-Many, and\\nre\\xef\\xac\\x82exive relations, TransH [77] enables an entity to have multiple representations\\nwhen involved in different relations. As illustrated in Fig.7.4, TransH proposes a\\nrelation-speci\\xef\\xac\\x81c hyperplane wr for each relation, and judge dissimilarities on the\\nhyperplane instead of the original vector space of entities. Given a triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9,\\nTransH \\xef\\xac\\x81rst projects h and t to the corresponding hyperplane wr to get the projection\\nh\\xe2\\x8a\\xa5and t\\xe2\\x8a\\xa5, and the translation vector r is used to connect h\\xe2\\x8a\\xa5and t\\xe2\\x8a\\xa5on the hyperplane.\\nThe score function is de\\xef\\xac\\x81ned as follows:\\nE (h,r, t) = \\xe2\\x88\\xa5h\\xe2\\x8a\\xa5+ r \\xe2\\x88\\x92t\\xe2\\x8a\\xa5\\xe2\\x88\\xa5,\\n(7.6)\\nin which we have\\nh\\xe2\\x8a\\xa5= h \\xe2\\x88\\x92w\\xe2\\x8a\\xa4\\nr hwr,\\nt\\xe2\\x8a\\xa5= t \\xe2\\x88\\x92w\\xe2\\x8a\\xa4\\nr twr,\\n(7.7)\\nwhere wr is a vector and \\xe2\\x88\\xa5wr\\xe2\\x88\\xa52 is restricted to 1. As for training, TransH also\\nminimizes the margin-based loss function with negative sampling which is similar\\nto TransE, and use mini-batch SGD to learn representations.\\n7.2.3.2\\nTransR/CTransR\\nTransH enables entities to have multiple representations in different relations with\\nthe favor of hyperplanes, while entities and relations are still restricted in the same\\nsemantic vector space, which may limit the ability for modeling entities and relations.\\nTransR [39] assumes that entities and relations should be arranged in distinct spaces,\\nthat is, entity space for all entities and relation space for each relation.\\n7.2 Knowledge Graph Representation\\n173\\nhr\\ntr\\nr\\nh\\nt\\nFig. 7.4 The architecture of TransH model [47]\\nAs illustrated in Fig.7.5, For a triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, h, t \\xe2\\x88\\x88Rk and r \\xe2\\x88\\x88Rd, TransR \\xef\\xac\\x81rst\\nprojects h and t from entity space to the corresponding relation space of r. That is\\nto say, every entity has a relation-speci\\xef\\xac\\x81c representation for each relation, and the\\ntranslating operation is processed in the speci\\xef\\xac\\x81c relation space. The energy function\\nof TransR is de\\xef\\xac\\x81ned as follows:\\nE (h,r, t) = \\xe2\\x88\\xa5hr + r \\xe2\\x88\\x92tr\\xe2\\x88\\xa5,\\n(7.8)\\nwhere hr and tr stand for the relation-speci\\xef\\xac\\x81c representation for h and tr in the\\ncorresponding relation space of r. The projection from entity space to relation space\\nis\\nhr = hMr,\\ntr = tMr,\\n(7.9)\\nwhere Mr \\xe2\\x88\\x88Rk\\xc3\\x97d is a projection matrix mapping entities from the entity space to\\nthe relation space of r. TransR also constrains the norms of the embeddings and has\\n\\xe2\\x88\\xa5h\\xe2\\x88\\xa52 \\xe2\\x89\\xa41, \\xe2\\x88\\xa5t\\xe2\\x88\\xa52 \\xe2\\x89\\xa41, \\xe2\\x88\\xa5r\\xe2\\x88\\xa52 \\xe2\\x89\\xa41, \\xe2\\x88\\xa5hr\\xe2\\x88\\xa52 \\xe2\\x89\\xa41, \\xe2\\x88\\xa5tr\\xe2\\x88\\xa52 \\xe2\\x89\\xa41. As for training, TransR shares\\nthe same margin-based score function as TransE.\\nFurthermore, the author found that some relations in knowledge graphs could\\nbe divided into a few sub-relations that give more precise information. The dif-\\nferences between those sub-relations can be learned from corresponding entity\\npairs. For instance, the relation location_contains has head-tail patterns like\\ncity-street, country-city, and even country-university, showing\\ndifferent attributes in cognition. With the sub-relations being considered, entities\\nmay be projected to more precise positions in the semantic vector space.\\nCluster-based TransR (CTransR), which is an enhanced version of TransR with the\\nsub-relations into consideration, is then proposed. More speci\\xef\\xac\\x81cally, for each relation\\nr, all entity pairs (h, t) are \\xef\\xac\\x81rst clustered into several groups. The clustering of entity\\npairs depends on the subtraction result of t \\xe2\\x88\\x92h, in which h and t are pretrained by\\nTransE. Next, we learn a distinct sub-relation vector rc for each cluster according to\\nthe corresponding entity pairs, and the original energy function is modi\\xef\\xac\\x81ed as\\n174\\n7\\nWorld Knowledge Representation\\nt\\nh\\ntr\\nhr\\nr\\nMr\\nMr\\nentity space\\nrelation space of r\\nFig. 7.5 The architecture of TransR model [47]\\nE (h,r, t) = \\xe2\\x88\\xa5hr + rc \\xe2\\x88\\x92tr\\xe2\\x88\\xa5+ \\xce\\xb1\\xe2\\x88\\xa5rc \\xe2\\x88\\x92r\\xe2\\x88\\xa5,\\n(7.10)\\nwhere \\xe2\\x88\\xa5rc \\xe2\\x88\\x92r\\xe2\\x88\\xa5wants the sub-relation vector rc not to be too distinct from the uni\\xef\\xac\\x81ed\\nrelation vector r.\\n7.2.3.3\\nTransD\\nTransH and TransR focus on the multiple representations of entities in different\\nrelations, improving the performance on knowledge completion and triple classi-\\n\\xef\\xac\\x81cation. However, both models only project entities according to the relations in\\ntriples, ignoring the diversity of entities. Moreover, the projection operation with\\nmatrix-vector multiplication leads to a higher computational complexity compared\\nto TransE, which is time consuming when applied on large-scale graphs. To address\\nthis problem, TransD [32] proposes a novel projection method with a dynamic map-\\nping matrix depending on both entity and relation, which takes the diversity of entities\\nas well as relations into consideration.\\nTransD de\\xef\\xac\\x81nes two vectors for each entity and relation, i.e., the original vector that\\nis also used in TransE, TransH, and TransR for distributed representation of entities\\nandrelations,andtheprojectionvectorthatisusedinconstructingprojectionmatrices\\nfor mapping entities from entity space to relation space. As illustrated in Fig.7.6,\\nTransD uses h, t, r to represent the original vectors, while hp, tp, and rp are used to\\nrepresent the projection vectors. There are two projection matrices Mrh, Mrt \\xe2\\x88\\x88Rm\\xc3\\x97n\\nused to project from entity space to relation space, and the projection matrices are\\ndynamically constructed as follows:\\nMrh = rph\\xe2\\x8a\\xa4\\np + Im\\xc3\\x97n,\\nMrt = rpt\\xe2\\x8a\\xa4\\np + Im\\xc3\\x97n,\\n(7.11)\\n7.2 Knowledge Graph Representation\\n175\\nt2\\nh1\\nh1r\\nMrhi\\nMrti\\nentity space\\nrelation space of r\\nt3\\nt1\\nh2\\nh3\\nt1r\\nh2r\\nt2r\\nt3r\\nh3r\\nFig. 7.6 The architecture of TransD model [47]\\nwhich means the projection vectors of entity and relation are combined to determine\\nthe dynamic projection matrix. The score function is then de\\xef\\xac\\x81ned as\\nE (h,r, t) = \\xe2\\x88\\xa5Mrhh + r \\xe2\\x88\\x92Mrtt\\xe2\\x88\\xa5.\\n(7.12)\\nThe projection matrices are initialized with identity matrices, and there are also\\nsome normalization constraints as in TransR.\\nTransD proposes a dynamic method to construct projection matrices with the\\nconsideration of diversity in both entities and relations, achieving better performance\\ncompared to existing methods in link prediction and triple classi\\xef\\xac\\x81cation. Moreover,\\nit lowers both computational and spatial complexity compared to TransR.\\n7.2.3.4\\nTranSparse\\nThe extension methods of TransE stated above focus on the multiple representa-\\ntions for entities in different relations and entity pairs. However, there are still two\\nchallenges ignored: (1) The heterogeneity. Relations in knowledge graphs differ in\\ngranularity. Some complex relations may link to many entity pairs, while some rela-\\ntively simple relations not. (2) The unbalance. Some relations may have more links\\nto head entities and fewer links to tail entities, and vice versa. The performance will\\nbe further improved if we consider these rather than merely treat all relations equally.\\nExisting methods like TransR build projection matrices for each relation, while\\nthese projection matrices have the same number of parameters, regardless of the vari-\\nety in the complexity of relations. TranSparse [33] is then proposed to address the\\nissues. The underlying assumption of TranSparse is that complex relations should\\nhave more parameters to learn while simple relations have fewer, where the complex-\\nity of a relation is judged from the number of triples or entities linked by the relation.\\n176\\n7\\nWorld Knowledge Representation\\nTo accomplish this, two models, i.e., TranSparse(share) and TranSparse(separate),\\nare proposed for avoiding over\\xef\\xac\\x81tting and under\\xef\\xac\\x81tting.\\nInspired by TransR, TranSparse(share) builds a projection matrix Mr(\\xce\\xb8r) for each\\nrelationr.Thisprojectionmatrixissparse,andthesparsedegree\\xce\\xb8r mainlydependson\\nthe number of entity pairs linked tor. Suppose Nr is the number of linked entity pairs,\\nN \\xe2\\x88\\x97\\nr represents the maximum number of Nr, and \\xce\\xb8min denotes the minimum sparse\\ndegree of projection matrix Mr that 0 \\xe2\\x89\\xa4\\xce\\xb8min \\xe2\\x89\\xa41. The sparse degree of relation r is\\nde\\xef\\xac\\x81ned as follows:\\n\\xce\\xb8r = 1 \\xe2\\x88\\x92(1 \\xe2\\x88\\x92\\xce\\xb8min)Nr/N \\xe2\\x88\\x97\\nr .\\n(7.13)\\nBoth head and tail entities share the same sparse projection matrix Mr(\\xce\\xb8r) in\\ntranslation. The score function is\\nE (h,r, t) = \\xe2\\x88\\xa5Mr(\\xce\\xb8r)h + r \\xe2\\x88\\x92Mr(\\xce\\xb8r)t\\xe2\\x88\\xa5.\\n(7.14)\\nDiffering from TranSparse(share), TranSparse(separate) builds two different\\nsparse matrices Mrh(\\xce\\xb8rh) and Mrt(\\xce\\xb8rt) for head and tail entities. The sparse degree\\n\\xce\\xb8rh (or \\xce\\xb8rt) then depends on the number of head (or tail) entities linked by relation\\nr. We have Nrh (or Nrt) to represent the number of head (or tail) entities, as well as\\nN \\xe2\\x88\\x97\\nrh (or N \\xe2\\x88\\x97\\nrt) to represent the maximum number of Nrh (or Nrt). And \\xce\\xb8min will also\\nbe set as the minimum sparse degree of projection matrices that 0 \\xe2\\x89\\xa4\\xce\\xb8min \\xe2\\x89\\xa41. We\\nhave\\n\\xce\\xb8rh = 1 \\xe2\\x88\\x92(1 \\xe2\\x88\\x92\\xce\\xb8min)Nrh/N \\xe2\\x88\\x97\\nrh,\\n\\xce\\xb8rt = 1 \\xe2\\x88\\x92(1 \\xe2\\x88\\x92\\xce\\xb8min)Nrt/N \\xe2\\x88\\x97\\nrt.\\n(7.15)\\nThe score function of TranSparse(separate) is\\nE (h,r, t) = \\xe2\\x88\\xa5Mrh(\\xce\\xb8rh)h + r \\xe2\\x88\\x92Mrt(\\xce\\xb8rt)t\\xe2\\x88\\xa5.\\n(7.16)\\nThrough the sparse projection matrix, TranSparse solves the heterogeneity and\\nthe unbalance simultaneously.\\n7.2.3.5\\nPTransE\\nThe extension models of TransE stated above are mainly focused on the challenge of\\nmultiple representations of entities in different scenarios. However, those extension\\nmodelsonlyconsiderthesimpleone-steppaths(i.e.,relation)intranslatingoperation,\\nignoring the rich global information located in the whole knowledge graphs. Consid-\\nering the multistep relational path is a potential method to utilize the global informa-\\ntion. For instance, if we notice the multistep relational path that \\xe2\\x9f\\xa8The forbidden\\ncity, locate_in, Beijing\\xe2\\x9f\\xa9\\xe2\\x86\\x92\\xe2\\x9f\\xa8Beijing, capital_of, China\\xe2\\x9f\\xa9, we can\\ninference with con\\xef\\xac\\x81dence that the triple \\xe2\\x9f\\xa8The forbidden city, locate_in,\\nChina\\xe2\\x9f\\xa9may exist. The relational path provides us with a powerful way to con-\\n7.2 Knowledge Graph Representation\\n177\\nstruct better knowledge graph representations and even get a better understanding of\\nknowledge reasoning.\\nThere are two main challenges when encoding the information in multistep rela-\\ntional paths. First, how to select reliable and meaningful relational paths among\\nenormous path candidates in KGs, since there are lots of relation sequence patterns\\nwhich do not indicate reasonable relationships. Let us just consider the relational\\npath \\xe2\\x9f\\xa8The forbidden city, locate_in, Beijing\\xe2\\x9f\\xa9\\xe2\\x86\\x92\\xe2\\x9f\\xa8Beijing, held,\\n2008 Summer Olympics\\xe2\\x9f\\xa9, it is hard to describe the relationship between The\\nforbidden city and 2008 Summer Olympics. Second, how to model\\nthose meaningful relational paths once we get them since it is dif\\xef\\xac\\x81cult to solve\\nthis composition semantic problem in relational paths.\\nPTransE [38] is then proposed to model the multistep relational paths. To select\\nmeaningful relational paths, the authors propose a Path-Constraint Resource Alloca-\\ntion (PCRA) algorithm to judge the relation path reliability. Suppose there is infor-\\nmation (or resource) in head entity h which will \\xef\\xac\\x82ow to tail entity t through some\\ncertain relational paths. The basic assumption of PCRA is that: the reliability of path\\n\\xe2\\x84\\x93depends on the resource amount that \\xef\\xac\\x81nally \\xef\\xac\\x82ows from head to tail. Formally, we\\nset \\xe2\\x84\\x93= (r1, . . . ,rl) for a certain path between h and t. The resource travels from h\\nto t and the path could be represented as S0/h\\nr1\\xe2\\x88\\x92\\xe2\\x86\\x92S1\\nr2\\xe2\\x88\\x92\\xe2\\x86\\x92. . .\\nrl\\xe2\\x88\\x92\\xe2\\x86\\x92Sl/t. For an entity\\nm \\xe2\\x88\\x88Si, the resource amount of m is de\\xef\\xac\\x81ned as follows:\\nR\\xe2\\x84\\x93(m) =\\n\\x02\\nn\\xe2\\x88\\x88Si\\xe2\\x88\\x921(\\xc2\\xb7,m)\\n1\\n|Si(n, \\xc2\\xb7)| R\\xe2\\x84\\x93(n),\\n(7.17)\\nwhere Si\\xe2\\x88\\x921(\\xc2\\xb7, m) indicates all direct predecessors of entity m along with relation ri in\\nSi\\xe2\\x88\\x921, and Si(n, \\xc2\\xb7) indicates all direct successors of n \\xe2\\x88\\x88Si\\xe2\\x88\\x921 with relation r. Finally,\\nthe resource amount of tail R\\xe2\\x84\\x93(t) is used to measure the reliability of \\xe2\\x84\\x93in the given\\ntriple \\xe2\\x9f\\xa8h, \\xe2\\x84\\x93, t\\xe2\\x9f\\xa9.\\nOnce we have learned the reliability and select those meaningful relational path\\ncandidates, the next challenge is how to model the meaning of those multistep paths.\\nPTransE proposes three types of composition operation, namely, Addition, Multipli-\\ncation, and recurrent neural networks, to get the representation l of \\xe2\\x84\\x93= (r1, . . . ,rl)\\nthrough those relations. The score function of the path triple \\xe2\\x9f\\xa8h, \\xe2\\x84\\x93, t\\xe2\\x9f\\xa9is de\\xef\\xac\\x81ned as\\nfollows:\\nE (h, \\xe2\\x84\\x93, t) = \\xe2\\x88\\xa5l \\xe2\\x88\\x92(t \\xe2\\x88\\x92h)\\xe2\\x88\\xa5\\xe2\\x89\\x88\\xe2\\x88\\xa5l \\xe2\\x88\\x92r\\xe2\\x88\\xa5= E (\\xe2\\x84\\x93,r),\\n(7.18)\\nwhere r indicates the golden relation between h and t. Since PTransE wants to meet\\nthe assumption in TransE that r \\xe2\\x89\\x88t \\xe2\\x88\\x92h simultaneously, PTransE directly utilizes r\\nin training. The optimization objective of PTransE is\\nL =\\n\\x02\\n(h,r,t)\\xe2\\x88\\x88S\\n[L (h,r, t) + 1\\nZ\\n\\x02\\n\\xe2\\x84\\x93\\xe2\\x88\\x88P(h,t)\\nR(\\xe2\\x84\\x93|h, t)L (\\xe2\\x84\\x93,r)],\\n(7.19)\\n178\\n7\\nWorld Knowledge Representation\\nt2\\nh1\\n(a)\\nt1\\nh2\\nt3\\nr1\\nr2\\nt2\\nh1\\n(b)\\nt1\\nh2\\nt3\\nr1\\nr2\\nFig. 7.7 The architecture of TransA model [47]\\nwhere L (h,r, t) is the margin-based score function with E (h,r, t) and L (\\xe2\\x84\\x93,r) is\\nthe margin-based score function with E (\\xe2\\x84\\x93,r). The reliability R(\\xe2\\x84\\x93|h, t) of \\xe2\\x84\\x93in (h, \\xe2\\x84\\x93, t)\\nis well considered in the overall loss function.\\nBesides PTransE, similar ideas such as [21, 22] also consider the multistep rela-\\ntional paths on different tasks such as knowledge completion and question answer-\\ning successfully. These works demonstrate that there is plentiful information located\\nin multi-step relational paths, which could signi\\xef\\xac\\x81cantly improve the performance\\nof knowledge graph representation, and further explorations on more sophisticated\\nmodels for relational paths are still promising.\\n7.2.3.6\\nTransA\\nTransA [78] is proposed to solve the following problems in TransE and other\\nextensions: (1) TransE and its extensions only consider the Euclidean distance in\\ntheir energy functions, which seems to be less \\xef\\xac\\x82exible. (2) Existing methods regard\\neach dimension in the semantic vector space identically whatever the triple is, which\\nmay bring in errors when calculating dissimilarities. To solve these problems, as\\nillustrated in Fig.7.7, TransA replaces the in\\xef\\xac\\x82exible Euclidean distance with adaptive\\nMahalanobis distance, which is more adaptive and \\xef\\xac\\x82exible. The energy function of\\nTransA is as follows:\\nE (h,r, t) = (|h + r \\xe2\\x88\\x92t|)\\xe2\\x8a\\xa4Wr(|h + r \\xe2\\x88\\x92t|),\\n(7.20)\\nwhere Wr is a relation-speci\\xef\\xac\\x81c nonnegative symmetric matrix corresponding to the\\nadaptive matric. Note that the |h + r \\xe2\\x88\\x92t| stands for a nonnegative vector that each\\ndimension is the absolute value of the translating operation. We have\\n(|h + r \\xe2\\x88\\x92t|) \\xe2\\x89\\x9c(|h1 + r1 \\xe2\\x88\\x92t1|, |h2 + r2 \\xe2\\x88\\x92t2|, . . . |hn + rn \\xe2\\x88\\x92tn|).\\n(7.21)\\n7.2 Knowledge Graph Representation\\n179\\n7.2.3.7\\nKG2E\\nExisting translation-based models usually consider entities and relations as vectors\\nembeddedinlow-dimensionalsemanticspaces.However,asexplainedabove,entities\\nand relations in KGs are various with different granularities. Therefore, the margin\\nin the margin-based score function that is used to distinguish positive triples from\\nnegative triples should be more \\xef\\xac\\x82exible due to the diversity, and the uncertainties of\\nentities and relations should be taken into consideration.\\nTo solve this, KG2E [30] is proposed, introducing the multidimensional Gaus-\\nsian distributions to KG representations. As illustrated in Fig.7.8, KG2E represents\\neach entity and relation with a Gaussian distribution. Speci\\xef\\xac\\x81cally, the mean vector\\ndenotes the entity/relation\\xe2\\x80\\x99s central position, and the covariance matrix denotes its\\nuncertainties. To learn the Gaussian distributions for entities and relations, KG2E\\nalso follows the score function proposed in TransE. For a triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, the Gaussian\\ndistributions of entity and relation are de\\xef\\xac\\x81ned as follows:\\nh \\xe2\\x88\\xbcN (\\xce\\xbch, \\xce\\xa3h),\\nt \\xe2\\x88\\xbcN (\\xce\\xbct, \\xce\\xa3t),\\nr \\xe2\\x88\\xbcN (\\xce\\xbcr, \\xce\\xa3r).\\n(7.22)\\nNote that the covariances are diagonal for the consideration of ef\\xef\\xac\\x81ciency. KG2E\\nhypothesizes that the head and tail entity are independent with speci\\xef\\xac\\x81c relations, then\\nthe translation h \\xe2\\x88\\x92t could be de\\xef\\xac\\x81ned as\\nh \\xe2\\x88\\x92t = e \\xe2\\x88\\xbcN (\\xce\\xbch \\xe2\\x88\\x92\\xce\\xbct, \\xce\\xa3h + \\xce\\xa3t).\\n(7.23)\\nTo measure the dissimilarity between e and r, KG2E proposes two methods con-\\nsidering both asymmetric similarity and symmetric similarity.\\nThe asymmetric similarity is based on the KL divergence between e and r, which\\nis a straightforward method to measure the similarity between two probability dis-\\ntributions. The energy function is as follows:\\nE (h,r, t) = DKL(e\\xe2\\x88\\xa5r)\\n=\\n\\x03\\nx\\xe2\\x88\\x88Rke N (x; \\xce\\xbcr, \\xce\\xa3r) log N (x; \\xce\\xbce, \\xce\\xa3e)\\nN (x; \\xce\\xbcr, \\xce\\xa3r)dx\\n= 1\\n2\\n\\x04\\ntr(\\xce\\xa3\\xe2\\x88\\x921\\nr \\xce\\xa3r) + (\\xce\\xbcr \\xe2\\x88\\x92\\xce\\xbce)\\xe2\\x8a\\xa4\\xce\\xa3\\xe2\\x88\\x921\\nr (\\xce\\xbcr \\xe2\\x88\\x92\\xce\\xbce) \\xe2\\x88\\x92log det(\\xce\\xa3e)\\ndet(\\xce\\xa3r) \\xe2\\x88\\x92ke\\n\\x05\\n,\\n(7.24)\\nwhere tr(\\xce\\xa3) indicates the trace of \\xce\\xa3, and \\xce\\xa3\\xe2\\x88\\x921 indicates the inverse.\\nThe symmetric similarity is based on the expected likelihood or probability prod-\\nuct kernel. KE2G takes the inner product between Pe and Pr as the measurement of\\nsimilarity. The logarithm of energy function is\\n180\\n7\\nWorld Knowledge Representation\\nFig. 7.8 The architecture of\\nKG2E model [47]\\nBill Clinton\\nHillary Clinton\\nspouse\\nUSA\\nNationality\\nArkansas\\nBorn on\\nE (h,r, t) =\\n\\x03\\nx\\xe2\\x88\\x88Rke N (x; \\xce\\xbce, \\xce\\xa3e)N (x; \\xce\\xbcr, \\xce\\xa3r)dx\\n= log N (0; \\xce\\xbce \\xe2\\x88\\x92\\xce\\xbcr, \\xce\\xa3e + \\xce\\xa3r)\\n= 1\\n2\\n\\x06\\n(\\xce\\xbce \\xe2\\x88\\x92\\xce\\xbcr)\\xe2\\x8a\\xa4(\\xce\\xa3e + \\xce\\xa3r)\\xe2\\x88\\x921(\\xce\\xbce \\xe2\\x88\\x92\\xce\\xbcr) + log det(\\xce\\xa3e + \\xce\\xa3r) + ke log(2\\xcf\\x80)\\n\\x07\\n.\\n(7.25)\\nThe optimization objective of KG2E is also margin-based similar to TransE. Both\\nasymmetric and symmetric similarities are constrained by some regularization to\\navoid over\\xef\\xac\\x81tting:\\n\\xe2\\x88\\x80l \\xe2\\x88\\x88E \\xe2\\x88\\xaaR,\\n\\xe2\\x88\\xa5\\xce\\xbcl\\xe2\\x88\\xa52 \\xe2\\x89\\xa41,\\ncminI \\xe2\\x89\\xa4\\xce\\xa3l \\xe2\\x89\\xa4cmaxI,\\ncmin > 0.\\n(7.26)\\nFigure7.8 shows a brief example of representations in KG2E.\\n7.2.3.8\\nTransG\\nWe have discussed the problem of TransE in the session of TransR/CTransR that some\\nrelations in knowledge graphs such as location_contains or has_part may\\nhave multiple sub-meanings. These relations are more likely to be some combina-\\ntions that could be divided into several more precise relations. To address this issue,\\nCTransR is proposed with a preprocess of clustering for each relation r depending\\non the entity pairs (h, t). TransG [79] also focuses on this issue more elegantly by\\nintroducing a generative model. As illustrated in Fig.7.9, it assumes that different\\nsemantic component embeddings should follow a Gaussian Mixture Model. The\\ngenerative process is as follows:\\n1. For each entity e \\xe2\\x88\\x88E, TransG sets a standard normal distribution: \\xce\\xbce \\xe2\\x88\\xbcN (0, I).\\n2. For a triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, TransG uses Chinese Restaurant Process to automatically\\ndetect semantic components (i.e., sub-meanings in a relation): \\xcf\\x80r,n \\xe2\\x88\\xbcCRP(\\xce\\xb2).\\n7.2 Knowledge Graph Representation\\n181\\nt2\\nh\\n(b)\\nt1\\nt3\\nr\\nr\\nt4\\nt5\\nt2\\nh\\n(a)\\nt1\\nt3\\nr\\nt4\\nt5\\nFig. 7.9 The architecture of TransG model [47]\\n3. Drawtheheadembeddingtoformastandardnormaldistribution: h \\xe2\\x88\\xbcN (\\xce\\xbch, \\xcf\\x83 2\\nh I).\\n4. Draw the tail embedding to form a standard normal distribution: t \\xe2\\x88\\xbcN (\\xce\\xbct, \\xcf\\x83 2\\nt I).\\n5. Draw the relation embedding for this semantic component: \\xce\\xbcr,n = t \\xe2\\x88\\x92h \\xe2\\x88\\xbc\\nN (\\xce\\xbct \\xe2\\x88\\x92\\xce\\xbch, (\\xcf\\x83 2\\nh + \\xcf\\x83 2\\nt )I).\\n\\xce\\xbc is the mean embedding and \\xcf\\x83 is the variance. Finally, the score function is\\nE (h,r, t) \\xe2\\x88\\x9d\\nNr\\n\\x02\\nn=1\\n\\xcf\\x80r,nN (\\xce\\xbct \\xe2\\x88\\x92\\xce\\xbch, (\\xcf\\x83 2\\nh + \\xcf\\x83 2\\nt )I),\\n(7.27)\\nin which Nr is the number of semantic components of r, and \\xcf\\x80r,n is the weight of ith\\ncomponent generated by the Chinese Restaurant Process.\\nFigure7.9 shows the advantages of the generative Gaussian Mixture Model.\\n7.2.3.9\\nManifoldE\\nKG2E and TransG introduce Gaussian distributions to knowledge graph represen-\\ntation learning, improving the \\xef\\xac\\x82exibility and diversity with the various forms of\\nentity and relation representation. However, TransE and its most extensions view the\\ngolden triples as almost points in the low-dimensional vector space, following the\\nassumption of translation. This point assumption may lead to two problems: being\\nan ill-posed algebraic system and being over-strict with the geometric form.\\nManifoldE [80] is proposed to address this issue, considering the possible position\\nof the golden candidate in vector space as a manifold instead of one point. The overall\\nscore function of ManifoldE is de\\xef\\xac\\x81ned as follows:\\nE (h,r, t) = \\xe2\\x88\\xa5M (h,r, t) \\xe2\\x88\\x92D2\\nr \\xe2\\x88\\xa52,\\n(7.28)\\n182\\n7\\nWorld Knowledge Representation\\nin which D2\\nr is a relation-speci\\xef\\xac\\x81c manifold parameter indicating the bias. Two kinds\\nofmanifoldsarethenproposedinManifoldE.ManifoldE(Sphere)isastraightforward\\nmanifold that supposes t should be located in the sphere which has h + r to be the\\ncenter and Dr to be the radius. We have\\nM (h,r, t) = \\xe2\\x88\\xa5h + r \\xe2\\x88\\x92t\\xe2\\x88\\xa52\\n2.\\n(7.29)\\nThe second manifold utilized is the hyperplane for it is much easier for two\\nhyperplanes to intersect. The function of ManifoldE(Hyperplane) is\\nM (h,r, t) = (h + rh)\\xe2\\x8a\\xa4(t + rt),\\n(7.30)\\nin which rh and rt represent the two relation embeddings. This indicates that for\\na triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, the tail entity t should locate in the hyperplane whose direction\\nis h + rh with the bias to be D2\\nr . Furthermore, ManifoldE(Hyperplane) considers\\nabsolute values in M (h,r, t) as |h + rh|\\xe2\\x8a\\xa4|t + rt| to double the solution number of\\npossible tails. For both manifolds, the author applies kernel forms on Reproducing\\nKernel Hilbert Space.\\n7.2.4\\nOther Models\\nTranslation-based methods such as TransE are simple but effective, whose power\\nhas been consistently veri\\xef\\xac\\x81ed on various tasks like knowledge graph completion and\\ntriple classi\\xef\\xac\\x81cation, achieving state-of-the-art performance. However, there are also\\nsome other representation learning methods performing well on knowledge graph\\nrepresentation. In this part, we will take a brief look at these methods as inspiration.\\n7.2.4.1\\nStructured Embeddings\\nStructured Embeddings (SE) [8] is a classical representation learning method for\\nKGs. In SE, each entity is projected to a d-dimensional vector space. SE designs two\\nrelation-speci\\xef\\xac\\x81c matrices Mr,1, Mr,2 \\xe2\\x88\\x88Rd\\xc3\\x97d for each relation r, projecting both head\\nand tail entities with these relation-speci\\xef\\xac\\x81c matrices when calculating the similarities.\\nThe score function of SE is de\\xef\\xac\\x81ned as follows:\\nE (h,r, t) = \\xe2\\x88\\xa5Mr,1h \\xe2\\x88\\x92Mr,2t\\xe2\\x88\\xa51,\\n(7.31)\\nin which both h and t are transformed into a relation-speci\\xef\\xac\\x81c vector space with\\nthose projection matrices. The assumption of SE is that the projected head and tail\\nembeddings should be as similar as possible according to the loss function. Different\\nfrom the translation-based methods, SE models entities as embeddings and relations\\n7.2 Knowledge Graph Representation\\n183\\nas projection matrices. In training, SE considers all triples in the training set and\\nminimizes the overall loss function.\\n7.2.4.2\\nSemantic Matching Energy\\nSemantic Matching Energy (SME) [5, 6] proposes a more complicated representation\\nlearning method. Differing from SE, SME considers both entities and relations as\\nlow-dimensional vectors. For a triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, h and r are combined using a projection\\nfunction g to get a new embedding lh,r, and the same with t and r to get lt,r. Next, a\\npoint-wise multiplication function is used on the two combined embeddings lh,r and\\nlt,r to get the score of this triple. SME proposes two different projection functions in\\nthe second step, among which the linear form is\\nE (h,r, t) = (M1h + M2r + b1)\\xe2\\x8a\\xa4(M3t + M4r + b2),\\n(7.32)\\nand the bilinear form is:\\nE (h,r, t) = ((M1h \\xe2\\x8a\\x99M2r) + b1)\\xe2\\x8a\\xa4((M3t \\xe2\\x8a\\x99M4r) + b2),\\n(7.33)\\nwhere \\xe2\\x8a\\x99is the element-wise (Hadamard) product. M1, M2, M3, M4 are weight\\nmatrices in the projection function, and b1 and b2 are the bias. Bordes et al. [6] is\\nbased on SME and improves the bilinear form with three-way tensors instead of\\nmatrices.\\n7.2.4.3\\nLatent Factor Model\\nLatent Factor Model (LFM) is proposed for modeling large multi-relational datasets.\\nLFM is based on a bilinear structure, which models entities as embeddings and\\nrelations as matrices. It could share sparse latent factors among different relations,\\nsigni\\xef\\xac\\x81cantly reducing the model and computational complexity. The score function\\nof LFM is de\\xef\\xac\\x81ned as follows:\\nE (h,r, t) = h\\xe2\\x8a\\xa4Mrt,\\n(7.34)\\nin which Mr is the representation of the relation r. Moreover, [92] proposes DIST-\\nMULT model, which restricts Mr to be a diagonal matrix. This enhanced model not\\nonly reduces the parameter number of LFM and thus lowers the model\\xe2\\x80\\x99s computa-\\ntional complexity, but also achieves better performance.\\n184\\n7\\nWorld Knowledge Representation\\n7.2.4.4\\nRESCAL\\nRESCAL is a knowledge graph representation learning method based on matrix\\nfactorization [54, 55]. In RESCAL, to represent all triple facts in knowledge graphs,\\nthe authors employ a three-way tensor \\xe2\\x88\\x92\\xe2\\x86\\x92\\nX \\xe2\\x88\\x88Rd\\xc3\\x97d\\xc3\\x97k in which d is the dimension of\\nentities and k is that of relations. In the three-way tensor \\xe2\\x88\\x92\\xe2\\x86\\x92\\nX , two modes stand for\\nthe head and tail entities while the third mode represents the relations. The entries of\\n\\xe2\\x88\\x92\\xe2\\x86\\x92\\nX are based on the existence of the corresponding triple facts. That is, \\xe2\\x88\\x92\\xe2\\x86\\x92\\nX i jm = 1 if\\nthe triple \\xe2\\x9f\\xa8ith entity, mth relation, jth entity\\xe2\\x9f\\xa9holds in the training set, and otherwise\\n\\xe2\\x88\\x92\\xe2\\x86\\x92\\nX i jm = 0 if the triple is nonexisting.\\nTo capture the inherent structure of all triples, a tensor factorization model named\\nRESCAL is then proposed. Suppose \\xe2\\x88\\x92\\xe2\\x86\\x92\\nX = {X1, . . . , Xk}, for each slice Xn, we have\\nthe following rank-r factorization:\\nXn \\xe2\\x89\\x88ARnA\\xe2\\x8a\\xa4,\\n(7.35)\\nwhere A \\xe2\\x88\\x88Rd\\xc3\\x97r stands for the r-dimensional entity representations, and Rn \\xe2\\x88\\x88Rr\\xc3\\x97r\\nrepresents the interactions of the r latent components for n-th relation. The assump-\\ntion in this factorization is similar to LFM, while RESCAL also optimizes the nonex-\\nisting triples where \\xe2\\x88\\x92\\xe2\\x86\\x92\\nX i jm = 0 instead of only considering the positive instances.\\nFollowing this tensor factorization assumption, the loss function of RESCAL is\\nde\\xef\\xac\\x81ned as follows:\\nL = 1\\n2\\n\\x08\\x02\\nn\\n\\xe2\\x88\\xa5Xn \\xe2\\x88\\x92ARnA\\xe2\\x8a\\xa4\\xe2\\x88\\xa52\\nF\\n\\t\\n+ 1\\n2\\xce\\xbb\\n\\x08\\n\\xe2\\x88\\xa5A\\xe2\\x88\\xa52\\nF +\\n\\x02\\nn\\n\\xe2\\x88\\xa5Rn\\xe2\\x88\\xa52\\nF\\n\\t\\n,\\n(7.36)\\nin which the second term is a regularization term and \\xce\\xbb is a hyperparameter.\\n7.2.4.5\\nHOLE\\nRESCAL works well with multi-relational data but suffers from high computational\\ncomplexity. To leverage both effectiveness and ef\\xef\\xac\\x81ciency, Holographic Embeddings\\n(HOLE) is proposed as an enhanced version of RESCAL [53].\\nHOLE employs an operation named circular correlation to generate compositional\\nrepresentations, which is similar to those holographic models of associative memory.\\nThe circular correlation operation \\xe2\\x8b\\x86: Rd \\xc3\\x97 Rd \\xe2\\x86\\x92Rd between two entities h and t\\nis as follows:\\nh \\xe2\\x8b\\x86ttk =\\nd\\xe2\\x88\\x921\\n\\x02\\ni=0\\nhit(k+i)mod d.\\n(7.37)\\nFigure7.10a also demonstrates a simple instance of this operation. The probability\\nof a triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9is then de\\xef\\xac\\x81ned as\\n7.2 Knowledge Graph Representation\\n185\\nhead\\ntail\\neh0\\nr\\neh1\\neh2\\net0\\net1\\net2\\nP(\\nr(h,t))\\n(a)HOLE\\nhead\\ntail\\neh0\\nr\\neh1\\neh2\\net0\\net1\\net2\\nP(\\nr(h,t))\\n(b)RESCAL\\nFig. 7.10 The architecture of RESCAL and HOLE models\\nP(\\xcf\\x86r(h, t) = 1) = Sigmoid(r\\xe2\\x8a\\xa4(h \\xe2\\x8b\\x86t)).\\n(7.38)\\nConsidering circular correlation brings in lots of advantages: (1) unlike other\\noperations like multiplication or convolution, circular correlation is noncommutative\\n(i.e., h \\xe2\\x8b\\x86t \\xcc\\xb8= t \\xe2\\x8b\\x86h), which is capable of modeling asymmetric relations in knowledge\\ngraphs. (2) Circular correlation has lower computational complexity compared to\\ntensor product in RESCAL. What\\xe2\\x80\\x99s more, the circular correlation could further speed\\nup with the help of Fast Fourier Transform (FFT), which is formalized as follows:\\nh \\xe2\\x8b\\x86t = F \\xe2\\x88\\x921(F(h) \\xe2\\x8a\\x99F(b)).\\n(7.39)\\n186\\n7\\nWorld Knowledge Representation\\nF(\\xc2\\xb7) and F(\\xc2\\xb7)\\xe2\\x88\\x921 represent the FFT and its inverse, while F(\\xc2\\xb7) denotes the complex\\nconjugate in Cd, and \\xe2\\x8a\\x99stands for the element-wise (Hadamard) product. Due to\\nFFT, the computational complexity of circular correlation is O(d log d), which is\\nmuch lower than that of tensor product.\\n7.2.4.6\\nComplex Embedding (ComplEx)\\nComplEx [70] employs an eigenvalue decomposition model, which makes use of\\ncomplex valued embeddings. The composition of complex embeddings can handle a\\nlarge variety of binary relations, among the symmetric and antisymmetric relations.\\nFormally, the log-odd of the probability that the fact \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9is true is\\nfr(h, t) = Sigmoid(Xhrt),\\n(7.40)\\nwhere fr(h, t) is expected to be 1 when (h,r, t) holds, otherwise \\xe2\\x88\\x921. Here, Xhrt is\\ncalculated as follows:\\nXhrt = Re(\\xe2\\x9f\\xa8r, h, t\\xe2\\x9f\\xa9)\\n= \\xe2\\x9f\\xa8Re(r), Re(h), Re(t)\\xe2\\x9f\\xa9+ \\xe2\\x9f\\xa8Re(r), Im(h), Im(t)\\xe2\\x9f\\xa9\\n\\xe2\\x88\\x92\\xe2\\x9f\\xa8Im(r), Re(h), Im(t)\\xe2\\x9f\\xa9\\xe2\\x88\\x92\\xe2\\x9f\\xa8Im(r), Im(h), Re(t)\\xe2\\x9f\\xa9,\\n(7.41)\\nwhere \\xe2\\x9f\\xa8x, y, z\\xe2\\x9f\\xa9= \\ni xi yizi denotes the trilinear dot product, Re(x) and Im(x) indi-\\ncate the real part and the imaginary part of the number x respectively. In fact, Com-\\nplEx can be viewed as an extension of RESCAL, which assigns complex embedding\\nof the entities and relations.\\nBesides, [29] has proved that HolE is mathematically equivalent to ComplEx\\nrecently.\\n7.2.4.7\\nConvolutional 2D Embeddings (ConvE)\\nConvE [16] uses 2D convolution over embeddings and multiple layers of nonlinear\\nfeatures to model knowledge graphs. It is the \\xef\\xac\\x81rst nonlinear model that signi\\xef\\xac\\x81cantly\\noutperforms previous linear models.\\nSpeci\\xef\\xac\\x81cally, ConvE uses convolutional and fully connected layers to model the\\ninteractions between input entities and relationships. After that, the obtained features\\nare \\xef\\xac\\x82attened, transformed through a fully connected layer, and the inner product is\\ntaken with all object entity vectors to generate a score for each triple.\\nFor each triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, ConvE de\\xef\\xac\\x81nes its score function as\\nfr(h, t) = f (vec( f ([\\xc2\\xafh; \\xc2\\xafr] \\xe2\\x88\\x97\\xcf\\x89))W)t,\\n(7.42)\\n7.2 Knowledge Graph Representation\\n187\\nwhere \\xe2\\x88\\x97denotes the convolution operator, and vec(\\xc2\\xb7) means compressing a matrix\\ninto a vector. r \\xe2\\x88\\x88Rk is a relation parameter depending on r, \\xc2\\xafh and \\xc2\\xafr denote a 2D\\nreshaping of h and r, respectively: if h, r \\xe2\\x88\\x88Rk, then \\xc2\\xafh, \\xc2\\xafr \\xe2\\x88\\x88Rka\\xc3\\x97kb, where k = kakb.\\nConvE can be seen as an improvement on HolE. Compared with HolE, it learns\\nmultiple layers of nonlinear features, and thus theoretically more expressive than\\nHolE.\\n7.2.4.8\\nRotation Embeddings (RotatE)\\nRotatE [67] de\\xef\\xac\\x81nes each relation as a rotation from the head entity to the tail entity in\\nthe complex vector space. Thus, it is able to model and infer various relation patterns,\\nincluding symmetry/antisymmetry, inversion, and composition. Formally, the score\\nfunction of the fact \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9of RotatE is de\\xef\\xac\\x81ned as\\nfr(h, t) = \\xe2\\x88\\xa5h \\xe2\\x8a\\x99r \\xe2\\x88\\x92t\\xe2\\x88\\xa5,\\n(7.43)\\nwhere \\xe2\\x8a\\x99denotes the element-wise (Hadamard) product, h, r, t \\xe2\\x88\\x88Ck and |ri| = 1.\\nRotatE is simple but achieves quite good performance. Compared with previous\\nwork, it is the \\xef\\xac\\x81rst model that is capable of modeling and inferring all the three\\nrelation patterns above.\\n7.2.4.9\\nNeural Tensor Network\\nSocher et al. [65] propose Neural Tensor Network (NTN) as well as Single Layer\\nModel (SLM), while NTN is an enhanced version of SLM. Inspired by the previous\\nattempts in KRL, SLM represents both entities and relations as low-dimensional\\nvectors, and also designs relation-speci\\xef\\xac\\x81c projection matrices to map entities from\\nentity space to relation space. Similar to SE, the score function of SLM is as follows:\\nE (h,r, t) = r\\xe2\\x8a\\xa4tanh(Mr,1h + Mr,2t),\\n(7.44)\\nwhere h, t \\xe2\\x88\\x88Rd represent head and tail embeddings, r \\xe2\\x88\\x88Rk represents relation\\nembedding, and Mr,1, Mr,2 \\xe2\\x88\\x88Rd\\xc3\\x97k stand for the relation-speci\\xef\\xac\\x81c matrices.\\nAlthough SLM has introduced relation embeddings as well as a nonlinear layer\\ninto the score function, the model representation capability is still restricted. Neural\\ntensor network is then proposed with tensors being introduced into the SLM frame-\\nwork. Besides the original linear neural network layer that projects entities to the\\nrelation space, NTN also adds another tensor-based neural layer which combines\\nhead and tail embeddings with a relation-speci\\xef\\xac\\x81c tensor, as illustrated in Fig.7.11.\\nThe score function of NTN is then de\\xef\\xac\\x81ned as follows:\\nE (h,r, t) = r\\xe2\\x8a\\xa4tanh(h\\xe2\\x8a\\xa4\\xe2\\x88\\x92\\xe2\\x86\\x92\\nMrt + Mr,1h + Mr,2t + br),\\n(7.45)\\n188\\n7\\nWorld Knowledge Representation\\nt\\nh\\nword space\\nentity space\\nr\\nScore\\nNeural \\nTensor\\nNetwork\\nFig. 7.11 The architecture of NTN model [47]\\nwhere \\xe2\\x88\\x92\\xe2\\x86\\x92\\nMr \\xe2\\x88\\x88Rd\\xc3\\x97d\\xc3\\x97k is a 3-way relation-speci\\xef\\xac\\x81c tensor, br is the bias, and Mr,1,\\nMr,2 \\xe2\\x88\\x88Rd\\xc3\\x97k is the relation-speci\\xef\\xac\\x81c matrices similar to SLM. Note that SLM is the\\nsimpli\\xef\\xac\\x81ed version of NTN if the tensor and bias are set to zero.\\nBesides the improvements in score function, NTN also attempts to utilize the\\nlatent textual information located in entity names and successfully achieves signif-\\nicant improvements. Differing from previous RL models that provide each entity\\nwith a vector, NTN represents each entity as the average of its entity name\\xe2\\x80\\x99s word\\nembeddings. For example, the entity Bengal tiger will be represented as the\\naverage word embeddings of Bengal and tiger. It is apparent that the entity\\nname will provide valuable information for understanding an entity, since Bengal\\ntiger may come from Bengal and be related to other tigers. Moreover, the number\\nof words is far less than that of entities. Therefore, using the average word embed-\\ndings of entity names will also lower the computational complexity and alleviate the\\nissue of data sparsity.\\nNTN utilizes tensor-based neural networks to model triple facts and achieves\\nexcellent successes. However, the overcomplicated method will lead to higher com-\\nputational complexity compared to other methods, and the vast number of parameters\\nwill limit the performance on rather sparse and large-scale KGs.\\n7.2.4.10\\nNeural Association Model (NAM)\\nNAM [43] adopts multilayer nonlinear activations in the deep neural network to\\nmodel the conditional probabilities between head and tail entities. NAM studies\\ntwo model structures Deep Neural Network (DNN) and Relation Modulated Neural\\nNetwork (RMNN).\\nNAM-DNN feeds the head and tail entities\\xe2\\x80\\x99 embeddings into an MLP with L fully\\nconnected layers, which is formalized as follows:\\n7.2 Knowledge Graph Representation\\n189\\nz(l) = Sigmoid(Mlz(l\\xe2\\x88\\x921) + b(l)), l = 1, . . . , L,\\n(7.46)\\nwhere z(0) = [h; r], M(l) and b(l) is the weight matrix and bias vector for the l-th\\nfully connected layer, respectively. And \\xef\\xac\\x81nally the score function of NAM-DNN is\\nde\\xef\\xac\\x81ned as\\nfr(h, t) = Sigmoid(t\\xe2\\x8a\\xa4z(L)).\\n(7.47)\\nDifferent from NAM-DNN, NAM-RMNN feds the relation embedding r into\\neach layer of the deep neural network as follows:\\nz(l) = Sigmoid(M(l)z(l\\xe2\\x88\\x921) + B(l)r), l = 1, . . . , L,\\n(7.48)\\nwhere z(0) = [h; r], M(l) and B(l) indicate the weight matrices. The score function\\nof NAM-RMNN is de\\xef\\xac\\x81ned as\\nfr(h, t) = Sigmoid(t\\xe2\\x8a\\xa4z(L) + B(l+1)r).\\n(7.49)\\n7.3\\nMultisource Knowledge Graph Representation\\nWe are living in a complicated pluralistic real world, in which we can get information\\nthrough all senses and learn knowledge not only from structured knowledge graphs\\nbut also from plain texts, categories, images, and videos. This cross-modal infor-\\nmation is considered as multisource information. Besides the structured knowledge\\ngraph which is well utilized in previous KRL methods, we will introduce the other\\nkinds of KRL methods utilizing multisource information:\\n1. Plain text is one of the most common information we deliver, receive, and\\nanalyze every day. There are vast amounts of plain texts we possess remaining to be\\ndetected, in which the signi\\xef\\xac\\x81cant knowledge that structured knowledge graphs may\\nnot include locates. Entity description is a special kind of textual information that\\ndescribes the corresponding entity within a few sentences or a short paragraph. Usu-\\nally, entity descriptions are maintained by some knowledge graphs (i.e., Freebase)\\nor could be automatically extracted from huge databases like Wikipedia.\\n2. Entity type is another important structured information for building knowledge\\nrepresentations. To learn new objects within our prior knowledge systems, human\\nbeings tend to systemize those objects into existing categories. An entity type is usu-\\nally represented with hierarchical structures, which consist of different granularities\\nof entity subtypes. It is natural that entities in the real world usually have multiple\\nentity types. Most of the existing famous knowledge graphs own their customized\\nhierarchical structures of entity types.\\n3. Images provide intuitive visual information to describe what the entity looks\\nlike, which is con\\xef\\xac\\x81rmed to be the most signi\\xef\\xac\\x81cant information we receive and process\\nevery day. The latent information located in images helps a lot, especially when\\ndealing with concrete entities. For instance, we may \\xef\\xac\\x81nd out the potential relationship\\n190\\n7\\nWorld Knowledge Representation\\nbetween Cherry and Plum (there are both plants belonging to Rosaceae) from\\ntheir appearances. Images could be downloaded from websites, and there are also\\nsubstantial image datasets like ImageNet.\\nMultisourceinformationlearningprovidesanovelmethodtolearnknowledgerep-\\nresentations not only from the internal information of structured knowledge graphs\\nbut also from the external information of plain texts, hierarchical types, and images.\\nMoreover, the exploration in multisource information learning helps to further under-\\nstand human cognition with all senses in the real world. The cross-modal represen-\\ntations learned based on knowledge graphs will also provide possible relationships\\nbetween different kinds of information.\\n7.3.1\\nKnowledge Graph Representation with Texts\\nTextual information is one of the most common and widely used information these\\ndays. There are large plain texts generated every day on the web and easy to be\\nextracted. Words are compressed symbols of our thoughts and can provide the con-\\nnections between entities, which are of great signi\\xef\\xac\\x81cance in KRL.\\n7.3.1.1\\nKnowledge Graph and Text Joint Embedding\\nWang et al. [76] attempt to utilize textual information by jointly embedding entities,\\nrelations, and words into the same low-dimensional continuous vector space. Their\\njoint model contains three parts, namely, the knowledge model, the text model, and\\nthe alignment model. More speci\\xef\\xac\\x81cally, the knowledge model is learned based on the\\ntriple facts in KGs by translation-based models, while the text model is learned based\\non the concurrences of words in the large corpus by Skip-gram. As for the alignment\\nmodel, two methods are proposed utilizing Wikipedia anchors and entity names. The\\nmain idea of alignment by Wikipedia anchors is replacing the word-word pair (w, v)\\nwith the word-entity pair (w, ev) according to the anchors in Wiki pages, while the\\nmain idea of alignment by entity names is replacing the entities in original triple\\n\\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9with the corresponding entity names \\xe2\\x9f\\xa8wh,r, t\\xe2\\x9f\\xa9, \\xe2\\x9f\\xa8h,r, wt\\xe2\\x9f\\xa9, and \\xe2\\x9f\\xa8wh,r, wt\\xe2\\x9f\\xa9.\\nModeling entities and words into the same vector space are capable of encoding\\nboth information in knowledge graphs and that in plain texts, while the performance\\nof this joint model depends on the completeness of Wikipedia anchors and may suffer\\nfrom the weak interactions merely based on entity names. To address this issue,\\n[101] proposes a new joint embedding based on [76] and improves the alignment\\nmodel with entity descriptions into consideration, assuming that entities should be\\nsimilar to all words in their descriptions. These joint models learn knowledge and text\\njoint embeddings, improving evaluation performance in both word and knowledge\\nrepresentations.\\n7.3 Multisource Knowledge Graph Representation\\n191\\nCNN/\\nCBOW\\ndescription of head\\nw1\\nw2\\nwn\\nw1\\nw2\\nwn\\ndescription of tail\\nhead\\nrelation\\n+\\n=\\n+\\n=\\ntail\\nCNN/\\nCBOW\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nFig. 7.12 The architecture of DKRL model\\n7.3.1.2\\nDescription-Embodied Knowledge Graph Representation\\nAnother way of utilizing textual information is directly constructing knowledge rep-\\nresentations from entity descriptions instead of merely considering the alignments.\\nXie et al. [82] proposes Description-embodied Knowledge Graph Representation\\nLearning (DKRL) that provides two kinds of knowledge representations: the \\xef\\xac\\x81rst is\\nthe structure-based representation hS and tS, which can directly represent entities\\nwidely used in previous methods, and the second is the description-based represen-\\ntation hD and tD which derives from entity descriptions. The energy function derives\\nfrom translation-based framework:\\nE (h,r, t) = \\xe2\\x88\\xa5hS + r \\xe2\\x88\\x92tS\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5hS + r \\xe2\\x88\\x92tD\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5hD + r \\xe2\\x88\\x92tS\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5hD + r \\xe2\\x88\\x92tD\\xe2\\x88\\xa5.\\n(7.50)\\nThe description-based representation is constructed via CBOW or CNN encoders\\nthat encode rich textual information from plain texts into knowledge representations.\\nThe architecture of DKRL is shown in Fig.7.12.\\nCompared to conventional translation-based methods, the two types of entity\\nrepresentations in DKRL are constructed with both structural information and textual\\ninformation, and thus could get better performance in knowledge graph completion\\nand type classi\\xef\\xac\\x81cation. Besides, DKRL could represent an entity even if it is not in\\nthe training set, as long as there are a few sentences to describe the entity. As their\\nmillions of new entities come up every day, DKRL is capable of handling zero-shot\\nlearning.\\n192\\n7\\nWorld Knowledge Representation\\n7.3.2\\nKnowledge Graph Representation with Types\\nEntity types, which serve as a kind of category information of entities and are usu-\\nally arranged with hierarchical structures, could provide structured information to\\nunderstand entities in KRL better.\\n7.3.2.1\\nType-Constraint Knowledge Graph Representation\\nKrompa\\xc3\\x9f et al. [36] take type information as type constraints, and improves exist-\\ning methods like RESCAL and TransE via type constraints. It is intuitive that in a\\nparticular relation, the head or tail entities should belong to some speci\\xef\\xac\\x81c types. For\\nexample, the head entities of the relation write_books should be a human (or\\nmore precisely an author), and the tail entities should be a book.\\nSpeci\\xef\\xac\\x81cally, in RESCAL, the original factorization Xr \\xe2\\x89\\x88ARrA\\xe2\\x8a\\xa4is modi\\xef\\xac\\x81ed to\\nX\\xe2\\x80\\xb2\\nr \\xe2\\x89\\x88A[headr,:]RrA\\xe2\\x8a\\xa4\\n[tailr,:],\\n(7.51)\\nin which headr, tailr are the set of entities \\xef\\xac\\x81tting the type constraints of head or tail\\nand X\\xe2\\x80\\xb2r is a sparse adjacency matrix of shape |headr| \\xc3\\x97 |tailr|. In the enhanced ver-\\nsion, only the entities that \\xef\\xac\\x81t type constraints will be considered during factorization.\\nIn TransE, type constraints are utilized in negative sampling. The margin-based\\nscore functions of translation-based methods need negative instances, which are\\ngenerated through randomly replacing head or tail entities with another entity in\\ntriples. With type constraints, the negative samples are chosen by\\nh\\xe2\\x80\\xb2 \\xe2\\x88\\x88E[headr] \\xe2\\x8a\\x86E,\\nt\\xe2\\x80\\xb2 \\xe2\\x88\\x88E[tailr] \\xe2\\x8a\\x86E,\\n(7.52)\\nwhere E[headr] is the subset of entities following type constraints for head in relation\\nr, and E[tailr] is that for tail.\\n7.3.2.2\\nType-Embodied Knowledge Graph Representation\\nConsidering type information as constraints is simple but effective, while the per-\\nformance is still limited. Instead of merely viewing type information as type con-\\nstraints, Xie et al. [83] propose Type-embodied Knowledge Graph Representation\\nLearning (TKRL), utilizing hierarchical-type structures to instruct the construction\\nof projection matrices. Inspired by TransR that every entity should have multiple\\nrepresentations in different scenarios, the energy function of TKRL is de\\xef\\xac\\x81ned as\\nfollows:\\nE (h,r, t) = \\xe2\\x88\\xa5Mrhh + r \\xe2\\x88\\x92Mrtt\\xe2\\x88\\xa5,\\n(7.53)\\n7.3 Multisource Knowledge Graph Representation\\n193\\nh\\nh\\nt\\nt\\nhch\\n(m)\\nhch\\nhch\\ntct\\nRHE\\nWHE\\nr\\ntct\\nr\\nMct\\n(m)\\nMch\\n(m-1)\\nMct\\n(m-1)\\nMct\\n(1)\\nMct\\n(1)\\n\\xe2\\x88\\x91i\\xce\\xb2iMCh\\n(1)\\n\\xe2\\x88\\x91i\\xce\\xb2iMCt\\n(1)\\nFig. 7.13 The architecture of TKRL model\\nin which Mrh and Mrt are two projection matrices for h and t that depend on their\\ncorresponding hierarchical types in this triple. Two hierarchical-type encoders are\\nproposed to learn the projection matrices, regarding all subtypes in the hierarchy\\nas projection matrices, in which Recursive Hierarchy Encoder is based on matrix\\nmultiplication, while Weighted Hierarchy Encoder is based on matrix summation:\\nMRH Ec =\\nm\\n\\x0b\\ni=1\\nMc(i) = Mc(1)Mc(2) . . . Mc(m),\\n(7.54)\\nMW H Ec =\\nm\\n\\x02\\ni=1\\n\\xce\\xb2iMc(i) = \\xce\\xb21Mc(1) + \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 + \\xce\\xb2mMc(m),\\n(7.55)\\nwhere Mc(i) stands for the projection matrix of the ith subtype of the hierarchical\\ntype c, \\xce\\xb2i is the corresponding weight of the subtype. Figure7.13 demonstrates a\\nsimple illustration of TKRL. Taking RHE, for instance, given an entity William\\nShakespeare, it is \\xef\\xac\\x81rst projected to a rather general sub-type space like human,\\nand then sequentially projected to a more precise subtype like author or English\\nauthor. Moreover, TKRL also proposes an enhanced soft-type constraint to alle-\\nviate the problems caused by type information incompleteness.\\n194\\n7\\nWorld Knowledge Representation\\nArmet\\nSuit of \\narmour\\nhas part\\nFig. 7.14 Examples of entity images [81]\\n7.3.3\\nKnowledge Graph Representation with Images\\nImages could provide intuitive visual information of their corresponding entities\\xe2\\x80\\x99\\noutlook, which may give signi\\xef\\xac\\x81cant hints suggesting some latent attributes of entities\\nfrom certain aspects. For instance, Fig.7.14 demonstrates some examples of entity\\nimages of their corresponding entities Suit of armour and Armet. The left\\nside shows the triple facts that \\xe2\\x9f\\xa8Suit of armour, has_a_part, Armet\\xe2\\x9f\\xa9, and\\nsurprisingly, we can infer this knowledge directly from the images.\\n7.3.3.1\\nImage-Embodied Knowledge Graph Representation\\nXie et al. [81] propose Image-embodied Knowledge Graph Representation Learning\\n(IKRL) to take visual information into consideration when constructing knowledge\\nrepresentations. Inspired by the multiple entity representations in [82], IKRL also\\nproposes the image-based representation hI and tI besides the original structure-\\nbased representation, and jointly learn both two types of entity representations simul-\\ntaneously within the translation-based framework.\\nE (h,r, t) = \\xe2\\x88\\xa5hS + r \\xe2\\x88\\x92tS\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5hS + r \\xe2\\x88\\x92tI\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5hI + r \\xe2\\x88\\x92tS\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5hI + r \\xe2\\x88\\x92tI\\xe2\\x88\\xa5.\\n(7.56)\\nMore speci\\xef\\xac\\x81cally, IKRL \\xef\\xac\\x81rst constructs the image representations for all entity\\nimages with neural networks, and then project these image representations from\\nimage space to entity space via a projection matrix. Since most entities may have\\nmultiple images with different qualities, IKRL selects the more informative and\\ndiscriminative images via an attention-based method. The evaluation results of\\nIKRL not only con\\xef\\xac\\x81rm the signi\\xef\\xac\\x81cance of visual information in understanding\\n7.3 Multisource Knowledge Graph Representation\\n195\\n-\\n-\\n\\xe2\\x89\\x88\\ndresser\\ndrawer\\npianoforte\\nkeyboard\\n-\\n\\xe2\\x89\\x88\\n-\\ncat (Felidae)\\ntiger\\ntoothed whale\\ndolphin\\nw(part_of)\\nw(hypernym)\\n\\xe2\\x89\\x88\\n\\xe2\\x89\\x88\\nFig. 7.15 An example of semantic regularities in word space [81]\\nentities but also show the possibility of a joint heterogeneous semantic space.\\nMoreover, the authors also \\xef\\xac\\x81nd some interesting semantic regularities such as\\nw(man) \\xe2\\x88\\x92w(king) \\xe2\\x89\\x88w(woman) \\xe2\\x88\\x92w(queen) found in word space, which are\\nshown in Fig.7.15.\\n7.3.4\\nKnowledge Graph Representation with Logic Rules\\nTypicalknowledgegraphsstoreknowledgeintheformoftriplefactswithonerelation\\nlinkingtwoentities.MostexistingKRLmethodsonlyconsidertheinformationwithin\\ntriple facts separately, ignoring the possible interactions and correlations between dif-\\nferent triples. Logic rules, which are certain kinds of summaries deriving from human\\nbeings\\xe2\\x80\\x99 prior knowledge, could help us with knowledge inference and reasoning. For\\ninstance, if we know the triple fact that \\xe2\\x9f\\xa8Beijing, is_capital_of, China\\xe2\\x9f\\xa9,\\nwe can easily infer with high con\\xef\\xac\\x81dence that \\xe2\\x9f\\xa8Beijing, located_in, China\\xe2\\x9f\\xa9,\\nsince we know the logic rule that the relation is_capital_of \\xe2\\x87\\x92located_in.\\nSome works are focusing on introducing logic rules to knowledge acquisition and\\ninference, among which Markov Logic Networks are intuitively utilized to address\\nthis challenge [3, 58, 75]. The path-based TransE [38] stated above also implicitly\\nconsiders the latent logic rules between different relations via relation paths.\\n7.3.4.1\\nKALE\\nKALE is a translation-based KRL method that jointly learns knowledge representa-\\ntions with logic rules [24]. The joint learning consists of two parts, namely, the triple\\nmodeling and the rule modeling. For triple modeling, KALE follows the translation\\nassumption with minor alteration in scoring function as follows:\\n196\\n7\\nWorld Knowledge Representation\\nE (h,r, t) = 1 \\xe2\\x88\\x92\\n1\\n3\\n\\xe2\\x88\\x9a\\nd\\n\\xe2\\x88\\xa5h + r \\xe2\\x88\\x92t\\xe2\\x88\\xa5,\\n(7.57)\\nin which d stands for the dimension of knowledge embeddings. E (h,r, t) takes value\\nin [0, 1] for the convenience of joint learning.\\nFor the newly added rule modeling, KALE uses the t-norm fuzzy logics proposed\\nin [25] that represent the truth value of a complex formula with the truth values of its\\nconstituents. Specially, KALE focuses on two typical types of logic rules. The \\xef\\xac\\x81rst is\\n\\xe2\\x88\\x80h, t : \\xe2\\x9f\\xa8h,r1, t\\xe2\\x9f\\xa9\\xe2\\x87\\x92\\xe2\\x9f\\xa8h,r2, t\\xe2\\x9f\\xa9(e.g., given \\xe2\\x9f\\xa8Beijing, is_capital_of, China\\xe2\\x9f\\xa9,\\nwecaninferthat\\xe2\\x9f\\xa8Beijing,located_in,China\\xe2\\x9f\\xa9).KALErepresentsthescoring\\nfunction of this logic rule f1 via speci\\xef\\xac\\x81c t-norm based logical connectives as follows:\\nE ( f1) = E (h,r1, t)E (h,r2, t) \\xe2\\x88\\x92E (h,r1, t) + 1.\\n(7.58)\\nThesecondis\\xe2\\x88\\x80h, e, t : \\xe2\\x9f\\xa8h,r1, e\\xe2\\x9f\\xa9\\xe2\\x88\\xa7\\xe2\\x9f\\xa8e,r2, t\\xe2\\x9f\\xa9\\xe2\\x87\\x92\\xe2\\x9f\\xa8h,r3, t\\xe2\\x9f\\xa9(e.g.,given\\xe2\\x9f\\xa8Tsinghua,\\nlocated_in, Beijing\\xe2\\x9f\\xa9) and \\xe2\\x9f\\xa8Beijing, located_in, China\\xe2\\x9f\\xa9, we can infer\\nthat \\xe2\\x9f\\xa8Tsinghua, located_in, China\\xe2\\x9f\\xa9). And KALE de\\xef\\xac\\x81nes the second scoring\\nfunction as\\nE ( f2) = E (h,r1, e)E (e,r2, t)E (h,r3, t) \\xe2\\x88\\x92E (h,r1, e)E (e,r2, t) + 1.\\n(7.59)\\nThe joint training contains all positive formulae, including triple facts as well as\\nlogic rules. Note that for the consideration of logic rule qualities, KALE ranks all\\npossible logic rules by their truth values with pretrained TransE and manually \\xef\\xac\\x81lters\\nsome rules ranked at the top.\\n7.4\\nApplications\\nRecent years have witnessed the great thrive in knowledge-driven arti\\xef\\xac\\x81cial intelli-\\ngence, such as QA systems and chatbot. AI agents are expected to accurately and\\ndeeply understand user demands, and then appropriately and \\xef\\xac\\x82exibly give responses\\nand solutions. Such kind of work cannot be done without certain forms of knowledge.\\nTo introduce knowledge to AI agents, researchers \\xef\\xac\\x81rst extract knowledge from\\nheterogeneous information like plain texts, images, and structured knowledge bases.\\nThese various kinds of heterogeneous information are then fused and stored with\\ncertain structures like knowledge graphs. Next, the knowledge is projected to a low-\\ndimensional semantic space following some KRL methods. And \\xef\\xac\\x81nally, these learned\\nknowledge representations are utilized in various knowledge applications like infor-\\nmation retrieval and dialogue system. Figure7.16 demonstrates a brief pipeline of\\nknowledge-driven applications from scratch.\\nFrom the illustration, we can observe that knowledge graph representation learn-\\ning is the critical component in the whole knowledge-driven application\\xe2\\x80\\x99s pipeline.\\nIt bridges the gap between knowledge graphs that store knowledge and knowledge\\n7.4 Applications\\n197\\nHamlet is a tragedy\\nwritten by Willianm\\nShakespeare at an\\nuncertain date between\\n1599 and 1602,...\\nheterogeneous\\ninformation\\nknowledge graph\\nknowledge\\nconstruction\\nknowledge \\nrepresentation\\nKRL\\nmethods\\n embedding\\nmodels\\n knowledge\\napplications\\n information\\nretrieval\\n question\\nanswering\\n dialogue\\nsystem\\nFig. 7.16 An illustration of knowledge-driven applications\\napplications that use knowledge. Knowledge representations with distributed meth-\\nods, compared to those with symbolic methods, are able to solve the data sparsity and\\nmodeling the similarities between entities and relations. Moreover, embedding-based\\nmethods are convenient to be used with deep learning methods and are naturally \\xef\\xac\\x81t\\nfor the combination with heterogeneous information.\\nIn this section, we will introduce possible applications of knowledge represen-\\ntations mainly from two aspects. First, we will introduce the usage of knowledge\\nrepresentations for knowledge-driven applications, and then we will show the power\\nof knowledge representations for knowledge extraction and construction.\\n7.4.1\\nKnowledge Graph Completion\\nKnowledge graph completion aims to build structured knowledge bases by extract-\\ning knowledge from heterogeneous sources such as plain texts, existing knowledge\\nbases, and images. Knowledge construction consists of several subtasks like relation\\nextraction and information extraction, making the fundamental step in the whole\\nknowledge-driven framework.\\nRecently, automatic knowledge construction has attracted considerable attention\\nsince it is incredibly time consuming and labor intensive to deal with enormous\\nexisting and new information. In the following section, we will introduce some\\nexplorations on neural relation extraction, and concentrate on the combination of\\nknowledge representations.\\n7.4.1.1\\nKnowledge Representations for Relation Extraction\\nRelation extraction focuses on predicting the correct relation between two entities\\ngiven a short plain text containing the two entities. Generally, all relations to predict\\nare prede\\xef\\xac\\x81ned, which is different to open information extraction. Entities are usually\\nmarked with named entity recognition systems or extracted according to anchor texts,\\nor automatically generated via distance supervision [50].\\n198\\n7\\nWorld Knowledge Representation\\nKG\\nFact\\nText\\nWord\\nPosition\\nEncoder\\nPlaceOfBirth\\nwas\\nborn\\nin\\n[Mark Twain]\\nMark Twain\\nh\\n+\\n=\\nr\\nt\\n,\\n,\\n[Florida]\\nFlorida\\nFig. 7.17 The architecture of joint representation learning framework for knowledge acquisition\\nConventional methods for relation extraction and classi\\xef\\xac\\x81cation are mainly based\\non statistical machine learning, which strongly depends on the qualities of extracted\\nfeatures. Zeng et al. [96] \\xef\\xac\\x81rst introduce CNN to relation classi\\xef\\xac\\x81cation and achieve\\ngreat improvements. Lin et al. [40] further improves neural relation extraction models\\nwith attention-based models over instances.\\nHan et al. [27, 28] propose a novel joint representation learning framework for\\nknowledge acquisition. The key idea is that the joint model learns knowledge and text\\nrepresentations within a uni\\xef\\xac\\x81ed semantic space via KG-text alignments. Figure7.17\\nshows the brief framework of the KG-text joint model. For the text part, the sen-\\ntence with two entities Mark Twain and Florida is regarded as the input for\\na CNN encoder, and the output of CNN is considered to be the latent relation\\nplace_of_birth of this sentence. While for the KG part, entity and relation\\nrepresentations are learned via translation-based methods. The learned representa-\\ntions of KG and text parts are aligned during training. This work is the \\xef\\xac\\x81rst attempt\\nto encode knowledge representations from existing KGs to knowledge construc-\\ntion tasks and achieves improvements in both knowledge completion and relation\\nextraction.\\n7.4 Applications\\n199\\nFig. 7.18 The architecture of KNET model\\n7.4.2\\nKnowledge-Guided Entity Typing\\nEntity typing is the task of detecting semantic types for a named entity (or entity men-\\ntion) in plain text. For example, given a sentence Jordan played 15 seasons\\nin the NBA,entitytypingaimstoinferthat Jordaninthissentenceisa person,\\nan athlete, and even a basketball player. Entity typing is important for\\nnamed entity disambiguation since it can narrow down the range of candidates for an\\nentity mention [10]. Moreover, entity typing also bene\\xef\\xac\\x81ts massive Natural Language\\nProcessing (NLP) tasks such as relation extraction [46], question answering [90],\\nand knowledge base population [9].\\nConventional named entity recognition models [69, 73] typically classify entity\\nmentions into a small set of coarse labels (e.g., person, organization,\\nlocation, and others). Since these entity types are too coarse grained for many\\nNLP tasks, a number of works [15, 41, 94, 95] have been proposed to introduce a\\nmuch larger set of \\xef\\xac\\x81ne-grained types, which are typically subtypes of those coarse-\\ngrained types. Previous \\xef\\xac\\x81ne-grained entity typing methods usually derive features\\nusing NLP tools such as POS tagging and parsing, and inevitably suffer from error\\npropagation. Dong et al. [18] make the \\xef\\xac\\x81rst attempt to explore deep learning in entity\\ntyping. The method only employs word vectors as features, discarding complicated\\nfeature engineering. Shimaoka et al. [63] further introduce the attention scheme into\\nneural models for \\xef\\xac\\x81ne-grained entity typing.\\nNeural models have achieved state-of-the-art performance for \\xef\\xac\\x81ne-grained entity\\ntyping. However, these methods face the following nontrivial challenges:\\n(1) Entity-Context Separation. Existing methods typically encode context\\nwords without utilizing crucial correlations between entity and context. How-\\never, it is intuitive that the importance of words in the context for entity typ-\\n200\\n7\\nWorld Knowledge Representation\\ning is signi\\xef\\xac\\x81cantly in\\xef\\xac\\x82uenced by which entity mentions we concern about. For\\nexample, in a sentence In 1975, Gates and Paul Allen co-founded\\nMicrosoft, which became the world\\xe2\\x80\\x99s largest PC software\\ncompany, the word company is much more important for determining the type of\\nMicrosoft than for the type of Gates.\\n(2) Entity-Knowledge Separation. Existing methods only consider text informa-\\ntion of entity mentions for entity typing. In fact, Knowledge Graphs (KGs) provide\\nrich and effective additional information for determining entity types. For example,\\nin the above sentence In 1975, Gates ... Microsoft ... company,\\neven if we have no type information of Microsoft in KG, entities similar to\\nMicrosoft (such as IBM) will also provide supplementary information.\\nIn order to address the issues of entity-context separation and entity-knowledge\\nseparation, we propose Knowledge-guided Attention (KNET) Neural Entity Typing.\\nAs illustrated in Fig.7.18, KNET mainly consists of two parts. Firstly, KNET builds\\na neural network, including a Long Short-Term Memory (LSTM) and a fully con-\\nnected layer, to generate context and named entity representations. Secondly, KNET\\nintroduces knowledge attention to emphasize those critical words and improve the\\nquality of context representations. Here we introduce the knowledge attention in\\ndetail.\\nKnowledge graphs provide rich information about entities in the form of triples\\n\\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, where h and t are entities and r is the relation between them. Many KRL\\nworks have been devoted to encoding entities and relations into real-valued semantic\\nvector space based on triple information in KGs. KRL provides us with an ef\\xef\\xac\\x81cient\\nway to exploit KG information for entity typing.\\nKNET employs the most widely used KRL method TransE to obtain entity embed-\\nding e for each entity e. During the training scenario, it is known that the entity men-\\ntion m indicates the corresponding e in KGs with embedding e, and hence, KNET\\ncan directly compute knowledge attention as follows:\\n\\xce\\xb1KA\\ni\\n= f\\n\\x08\\neWKA\\n\\x0c\\xe2\\x88\\x92\\xe2\\x86\\x92\\nhi\\n\\xe2\\x86\\x90\\xe2\\x88\\x92\\nhi\\n\\r\\t\\n,\\n(7.60)\\nwhere WKA is a bilinear parameter matrix, and aKA\\ni\\nis the attention weight for the\\nith word.\\nKnowledge Attention in Testing. The challenge is that, in the testing scenario,\\nwe do not know the corresponding entity in the KG of a certain entity mention. A\\nsolution is to perform entity linking, but it will introduce linking errors. Besides,\\nin many cases, KGs may not contain the corresponding entities for many entity\\nmentions.\\nTo address this challenge, we build an additional text-based representation for\\nentities in KGs during training. Concretely, for an entity e and its context sentence\\ns, we encode its left and right context into cl and cr using an one-directional LSTM,\\nand further learn the text-based representation \\xcb\\x86e as follows:\\n7.4 Applications\\n201\\n\\xcb\\x86e = tanh\\n\\xe2\\x8e\\x9b\\n\\xe2\\x8e\\x9dW\\n\\xe2\\x8e\\xa1\\n\\xe2\\x8e\\xa3\\nm\\ncl\\ncr\\n\\xe2\\x8e\\xa4\\n\\xe2\\x8e\\xa6\\n\\xe2\\x8e\\x9e\\n\\xe2\\x8e\\xa0,\\n(7.61)\\nwhere W is the parameter matrix, and m is the mention representation. Note that,\\nLSTM used here is different from those in context representation in order to prevent\\ninterference. In order to bridge text-based and KG-based representations, in the\\ntraining scenario, we simultaneously learn \\xcb\\x86e by putting an additional component in\\nthe objective function:\\nOKG(\\xce\\xb8) = \\xe2\\x88\\x92\\n\\x02\\ne\\n\\xe2\\x88\\xa5e \\xe2\\x88\\x92\\xcb\\x86e\\xe2\\x88\\xa52.\\n(7.62)\\nIn this way, in the testing scenario, we can directly use Eq.7.61 to obtain the corre-\\nsponding entity representation and compute knowledge attention using Eq.7.60.\\n7.4.3\\nKnowledge-Guided Information Retrieval\\nThe emergence of large-scale knowledge graphs has motivated the development of\\nentity-oriented search, which utilizes knowledge graphs to improve search engines.\\nRecent progresses in entity-oriented search include better text representations with\\nentity annotations [61, 85], richer ranking features [14], entity-based connections\\nbetween query and documents [45, 84], and soft-match query and documents through\\nknowledgegraphrelationsorembeddings[19, 88].Theseapproachesbringinentities\\nand semantics from knowledge graphs and have greatly improved the effectiveness\\nof feature-based search systems.\\nAnother frontier of information retrieval is the development of neural ranking\\nmodels (neural-IR). Deep learning techniques have been used to learn distributed\\nrepresentations of queries and documents that capture their relevance relations\\n(representation-based) [62], or to model the query-document relevancy directly from\\ntheir word-level interactions (interaction-based) [13, 23, 87]. Neural-IR approaches,\\nespecially the interaction-based ones, have greatly improved the ranking accuracy\\nwhen large-scale training data are available [13].\\nEntity-oriented search and neural-IR push the boundary of search engines from\\ntwo different aspects. Entity-oriented search incorporates human knowledge from\\nentities and knowledge graph semantics. It has shown promising results on feature-\\nbased ranking systems. On the other hand, neural-IR leverages distributed repre-\\nsentations and neural networks to learn more sophisticated ranking models form\\nlarge-scale training data. Entity-Duet Neural Ranking Model (EDRM), as shown in\\nFig.7.19, incorporates entities in interaction-based neural ranking models. EDRM\\n\\xef\\xac\\x81rst learns the distributed representations of entities using their semantics from\\nknowledge graphs: descriptions and types. Then it follows a recent state-of-the-art\\nentity-oriented search framework, the word-entity duet [86], and matches documents\\nto queries with both bag-of-words and bag-of-entities. Instead of manual features,\\n202\\n7\\nWorld Knowledge Representation\\nObama\\nfamily\\ntree\\nObama\\nDescription\\nType\\nFamily Tree\\nDescription\\nType\\nAttention\\nCNN\\nCNN\\nQuery\\nDocument\\nUnigrams\\nBigrams\\nTrigrams\\nEnriched-entity\\nEmbedding\\nInteraction Matrix\\nSoft Match Feature\\nN-gram\\nEmbedding\\nEnriched-entity\\nEmbedding\\nKernel\\nPooling\\nFinal\\nRanking\\nScore\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nvwq\\nveq\\nvwd\\nved\\nMww\\nMew\\nMwe\\nMee\\nM\\n\\xce\\xa6 (M)\\nFig. 7.19 The architecture of EDRM model\\nEDRM uses interaction-based neural models [13] to match the query and documents\\nwith word-entity duet representations. As a result, EDRM combines entity-oriented\\nsearch and the interaction-based neural-IR; it brings the knowledge graph semantics\\nto neural-IR and enhances entity-oriented search with neural networks.\\n7.4.3.1\\nInteraction-Based Ranking Models\\nGiven a query q and a document d, interaction-based models \\xef\\xac\\x81rst build the word-\\nlevel translation matrix between q and d. The translation matrix describes word-\\npairs similarities using word correlations, which are captured by word embedding\\nsimilarities in interaction-based models.\\nTypically, interaction-based ranking models \\xef\\xac\\x81rst map each word w in q and d to\\nan L-dimensional embedding vw.\\nvw = Embw(w).\\n(7.63)\\nIt then constructs the interaction matrix M based on query and document embed-\\ndings. Each element Mi j in the matrix, compares the ith word in q and the jth word\\nin d, e.g., using the cosine similarity of word embeddings:\\nMi j = cos(vwq\\ni , vwd\\nj ).\\n(7.64)\\nWith the translation matrix describing the term level matches between query and\\ndocuments, the next step is to calculate the \\xef\\xac\\x81nal ranking score from the matrix. Many\\napproaches have been developed in interaction-based neural ranking models, but in\\ngeneral, that would include a feature extractor on M and then one or several ranking\\nlayers to combine the features to the ranking score.\\n7.4 Applications\\n203\\n7.4.3.2\\nSemantic Entity Representation\\nEDRM incorporates the semantic information about an entity from the knowledge\\ngraphs into its representation. The representation includes three embeddings: entity\\nembedding, description embedding, and type embedding, all in L dimension and are\\ncombined to generate the semantic representation of the entity.\\nEntity Embedding uses an L-dimensional embedding layer Embe to get the entity\\nembedding e for e:\\nve = Embe(e).\\n(7.65)\\nDescription Embedding encodes an entity description which contains m words\\nand explains the entity. EDRM \\xef\\xac\\x81rst employs the word embedding layer Embv to\\nembed the description word v to v. Then it combines all embeddings in the text to\\nan embedding matrix V. Next, it leverages convolutional \\xef\\xac\\x81lters to slide over the text\\nand compose the l length n-gram as g j\\ne:\\ng j\\ne = ReLU(WCNN \\xc2\\xb7 V j: j+h\\nw\\n+ bCNN),\\n(7.66)\\nwhere WCNN and bCNN are two parameters of the convolutional \\xef\\xac\\x81lter.\\nThen we use max pooling after the convolution layer to generate the description\\nembedding vdes\\ne :\\nvdes\\ne\\n= max(g1\\ne, ..., g j\\ne, ..., gm\\ne ).\\n(7.67)\\nType Embedding encodes the categories of entities. Each entity e has n kinds of\\ntypes Fe = { f1, ..., f j, ..., fn}. EDRM \\xef\\xac\\x81rst gets the f j embedding v f j through the\\ntype embedding layer Embtype:\\nvemb\\nf j\\n= Embtype(e).\\n(7.68)\\nThen EDRM utilizes an attention mechanism to combine entity types to the type\\nembedding vtype\\ne\\n:\\nvtype\\ne\\n=\\nn\\n\\x02\\nj\\n\\xce\\xb1 jv f j,\\n(7.69)\\nwhere \\xce\\xb1 j is the attention score, calculated as:\\n\\xce\\xb1 j =\\nexp(y j)\\n\\nn\\nl exp(yl),\\n(7.70)\\ny j =\\n\\x08\\x02\\ni\\nWbowvti\\n\\t\\n\\xc2\\xb7 v f j,\\n(7.71)\\n204\\n7\\nWorld Knowledge Representation\\nwhere y j is the dot product of the query or document representation and type embed-\\nding f j. We leverage bag-of-words for query or document encoding. Wbow is a\\nparameter matrix.\\nCombination. The three embeddings are combined by a linear layer to generate\\nthe semantic representation of the entity:\\nvsem\\ne\\n= vemb\\ne\\n+ We[vdes\\ne ; vtype\\ne\\n]\\xe2\\x8a\\xa4+ be,\\n(7.72)\\nin which We is an L \\xc3\\x97 2L matrix and be is an L-dimensional vector.\\n7.4.3.3\\nNeural Entity-Duet Framework\\nWord-entity duet [86] is a recently developed framework in entity-oriented search. It\\nutilizes the duet representation of bag-of-words and bag-of-entities to match question\\nq and document d with handcrafted features. This work introduces it to neural-IR.\\nThey \\xef\\xac\\x81rst construct bag-of-entities qe and de with entity annotation as well as\\nbag-of-words qw and dw for q and d. The duet utilizes a four-way interaction: query\\nwords to document words (qw-dw), query words to documents entities (qw-de), query\\nentities to document words (qe-dw), and query entities to document entities (qe-de).\\nInstead of features, EDRM uses a translation layer that calculates the similarity\\nbetween a pair of query-document terms: (vi\\nwq or vi\\neq) and (v j\\nwd or v j\\ned). It constructs the\\ninteraction matrix M = {Mww, Mwe, Mew, Mee}. And Mww, Mwe, Mew, Mee denote\\ninteractions of qw-dw, qw-de, qe-dw, qe-de respectively. And elements in them are\\nthe cosine similarities of corresponding terms:\\nMi j\\nww = cos(vi\\nwq, v j\\nwd); Mi j\\nee = cos(vi\\neq, v j\\ned)\\nMi j\\new = cos(vi\\neq, v j\\nwd); Mi j\\nwe = cos(vi\\nwq, v j\\ned).\\n(7.73)\\nThe \\xef\\xac\\x81nal ranking feature \\r(M) is a concatenation of four cross matches (\\xcf\\x86(M)):\\n\\r(M) = [\\xcf\\x86(Mww); \\xcf\\x86(Mwe); \\xcf\\x86(Mew); \\xcf\\x86(Mee)],\\n(7.74)\\nwhere the \\xcf\\x86 can be any function used in interaction-based neural ranking models.\\nThe entity-duet presents an effective way to crossly match query and document\\nin entity and word spaces. In EDRM, it introduces the knowledge graph semantics\\nrepresentations into neural-IR models.\\nThe duet translation matrices provided by EDRM can be plugged into any standard\\ninteraction-basedneuralrankingmodelssuchasK-NRM[87]andConv-KNRM[13].\\nWith suf\\xef\\xac\\x81cient training data, the whole model is optimized end-to-end with back-\\npropagation. During the process, the integration of the knowledge graph semantics,\\nentity embedding, description embeddings, type embeddings, and matching with\\nentities is learned jointly with the ranking neural network.\\n7.4 Applications\\n205\\n7.4.4\\nKnowledge-Guided Language Models\\nKnowledge is an important external information for language modeling. It is because\\nthe statistical co-occurrences cannot instruct the generation of all kinds of knowledge,\\nespecially for those named entities with low frequencies. Researchers try to incorpo-\\nrate external knowledge into language models for better performance on generation\\nand representation.\\n7.4.4.1\\nNKLM\\nLanguage models aim to learn the probability distribution over sequences of words,\\nwhich is a classical and essential NLP task widely studied. Recently, sequence to\\nsequence neural models (seq2seq) are blooming and widely utilized in sequential\\ngenerative tasks like machine translation [68] and image caption generation [72].\\nHowever, most seq2seq models have signi\\xef\\xac\\x81cant limitations when modeling and using\\nbackground knowledge.\\nTo address this problem, Ahn et al. [1] propose a Neural Knowledge Language\\nModel (NKLM) that considers knowledge provided by knowledge graphs when\\ngenerating natural language sequences with RNN language models. The key idea is\\nthat NKLM has two ways to generate a word. The \\xef\\xac\\x81rst is the same way as conventional\\nseq2seq models that generate a \\xe2\\x80\\x9cvocabulary word\\xe2\\x80\\x9d according to the probabilities of\\nsoftmax, and the second is to generate a \\xe2\\x80\\x9cknowledge word\\xe2\\x80\\x9d according to the external\\nknowledge graphs.\\nSpeci\\xef\\xac\\x81cally, the NKLM model takes LSTM as the framework of generating\\n\\xe2\\x80\\x9cvocabulary word\\xe2\\x80\\x9d. For external knowledge graph information, NKLM denotes the\\ntopic knowledge as K = {a1, . . . a|K |}, in which ai represents the entities (i.e.,\\nnamed as \\xe2\\x80\\x9ctopic\\xe2\\x80\\x9d in [1]) that appear in the same triple of a certain entity. At each step\\nt, NKLM takes both \\xe2\\x80\\x9cvocabulary word\\xe2\\x80\\x9d wv\\nt\\xe2\\x88\\x921 and \\xe2\\x80\\x9cknowledge word\\xe2\\x80\\x9d wo\\nt\\xe2\\x88\\x921 as well as\\nthe fact at\\xe2\\x88\\x921 predicted at step t \\xe2\\x88\\x921 as the inputs of LSTM. Next, the hidden state of\\nLSTM ht is combined with the knowledge context e to get the fact key kt via an MLP\\nmodule. The knowledge context ek derives from the mean embeddings of all related\\nfacts of fact k. The fact key kt is then used to extract the most appropriate fact at from\\nthe corresponding topic knowledge. And \\xef\\xac\\x81nally, the selected fact at is combined with\\nhidden state ht to predict (1) both \\xe2\\x80\\x9cvocabulary word\\xe2\\x80\\x9d wv\\nt and \\xe2\\x80\\x9cknowledge word\\xe2\\x80\\x9d wo\\nt ,\\nand (2) which word to generate at this step. The architecture of NKLM is shown in\\nFig.7.20.\\nThe NKLM model explores a novel neural model that combines the symbolic\\nknowledge information in external knowledge graphs with seq2seq language models.\\nHowever, the topic of knowledge is given when generating natural languages, which\\nmakes NKLM less practical and scalable for more general free talks. Nevertheless,\\nwe still believe that it is promising to encode knowledge into language models with\\nsuch methods.\\n206\\n7\\nWorld Knowledge Representation\\nFig. 7.20 The architecture\\nof NKLM model\\nTopic Knowledge\\ncopy\\nfact search\\nxt\\na1\\na2\\na3\\na4\\n\\xe2\\x80\\xa6\\naN\\nNaF\\no1\\no2\\no3\\no4\\n\\xe2\\x80\\xa6\\noN\\nht\\ne\\nkt\\nat\\nwt\\nv\\nwt\\no\\nht-1\\nat-1\\nwv\\nt-1\\nwo\\nt-1\\nLSTM\\nzt\\n7.4.4.2\\nERNIE\\nPretrained language models like BERT [17] have a strong ability to represent\\nlanguage information from text. With rich language representation, pretrained\\nmodels obtain state-of-the-art results on various NLP applications. However, the\\nexisting pretrained language models rarely consider incorporating external knowl-\\nedge to provide related background information for better language understanding.\\nFor example, given a sentence Bob Dylan wrote Blowin\\xe2\\x80\\x99 in the Wind\\nand Chronicles: Volume One,\\nwithout\\nknowing\\nBlowin\\xe2\\x80\\x99 in the\\nWind and Chronicles: Volume One are song and book respectively, it\\nis dif\\xef\\xac\\x81cult to recognize the two occupations of Bob Dylan, i.e., songwriter\\nand writer.\\nTo enhance language representation models with external knowledge, Zhang et\\nal. [100] propose an enhanced language representation model with informative enti-\\nties (ERNIE). Knowledge Graphs (KGs) are important external knowledge resources,\\nand they think informative entities in KGs can be the bridge to enhance language\\nrepresentation with knowledge. ERNIE considers overcoming two main challenges\\nfor incorporating external knowledge: Structured Knowledge Encoding and Hetero-\\ngeneous Information Fusion.\\nFor extracting and encoding knowledge information, ERNIE \\xef\\xac\\x81rstly recognizes\\nnamed entity mentions in text and then aligns these mentions to their corresponding\\nentities in KGs. Instead of directly using the graph-based facts in KGs, ERNIE\\nencodes the graph structure of KGs with knowledge embedding algorithms like\\nTransE [7], and then takes the informative entity embeddings as input. Based on the\\nalignments between text and KGs, ERNIE integrates entity representations in the\\nknowledge module into the underlying layers of the semantic module.\\n7.4 Applications\\n207\\nbob\\ndylan\\nwrote\\n\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\n1962\\nMulti-Head Attention\\nMulti-Head Attention \\nInformation Fusion\\n\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\nToken Input\\nEntity Input\\nToken Output\\nEntity Output\\nBob Dylan wrote Blowin\\xe2\\x80\\x99 in the Wind in 1962\\nblow\\nMulti-Head\\nAttention\\nFeed\\nForward\\nNx\\nMulti-Head\\nAttention\\nInformation\\nFusion\\nToken Input\\nMulti-Head\\nAttention\\nEntity Input\\nMx\\nToken Output\\nEntity Output\\nBlowin\\xe2\\x80\\x99 in the Wind\\nBob Dylan\\nAggregator\\nTransformer\\nAggregator\\n(a) Model Achitecture\\n(b) Aggregator\\nK-Encoder\\nT-Encoder\\nw1\\ni-1\\nw2\\ni-1\\nw3\\ni-1\\nw4\\ni-1\\nwn\\ni-1\\ne1\\ni-1\\ne2\\ni-1\\nw1\\ni-1\\nw2\\ni-1\\nw3\\ni-1\\nw4\\ni-1\\nwn\\ni-1\\ne1\\ni-1\\ne2\\ni-1\\ne1\\ni-1\\ne2\\ni-1\\n~\\n~\\n~\\n~\\n~\\n~\\n~\\n~\\n~\\n\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\nw1\\ni\\nw2\\ni\\nw3\\ni\\nw4\\ni\\nwn\\ni\\ne1\\ni\\ne2\\ni\\ne1\\ni\\ne2\\ni\\nFig. 7.21 The architecture of ERNIE model\\nSimilar to BERT, ERNIE adopts the masked language model and the next sen-\\ntence prediction as the pretraining objectives. Besides, for the better fusion of textual\\nand knowledge features, ERNIE uses a new pretraining objective (denoising entity\\nauto-encoder) by randomly masking some of the named entity alignments in the\\ninput text and training to select appropriate entities from KGs to complete the align-\\nments. Unlike the existing pre-trained language representation models only utilizing\\nlocal context to predict tokens, these objectives require ERNIE to aggregate both\\ncontext and knowledge facts for predicting both tokens and entities, and lead to a\\nknowledgeable language representation model.\\nFigure7.21 is the overall architecture. The left part shows that ERNIE consists\\nof two encoders (T-Encoder and K-Encoder), where T-Encoder is stacked by several\\nclassical transformer layers and K-Encoder is stacked by the new aggregator layers\\ndesigned for knowledge integration. The right part is the detail of the aggregator layer.\\nIn the aggregator layer, the input token embeddings and entity embeddings from the\\npreceding aggregator are fed into two multi-head self-attention, respectively. Then,\\nthe aggregator adopts an information fusion layer for the mutual integration of the\\ntoken and entity sequence and computes the output embedding for each token and\\nentity.\\nERNIE explores how to incorporate knowledge information into language repre-\\nsentation models. The experimental results demonstrate that ERNIE has more pow-\\nerful abilities of both denoising distantly supervised data and \\xef\\xac\\x81ne-tuning on limited\\ndata than BERT.\\n7.4.4.3\\nKALM\\nPre-trained language models can do many tasks without supervised training data, like\\nreading comprehension, summarization, and translation [60]. However, traditional\\nlanguage models are unable to ef\\xef\\xac\\x81ciently model entity names observed in text. To\\n208\\n7\\nWorld Knowledge Representation\\nsolve this problem, Liu et al. [42] propose a new language model architecture, called\\nKnowledge-Augmented Language Model (KALM), to use the entity types of words\\nfor better language modeling.\\nKALM is a language model with the option to generate words from a set of entities\\nfrom a knowledge database. An individual word can either come from a general\\nword dictionary as in the traditional language model or be generated as a name of an\\nentity from a knowledge database. The training objectives just supervise the output\\nand ignore the decision of the word type. Entities in the knowledge database are\\npartitioned by type and they use the database to build the types of words. According\\nto the context observed so far, the model decides whether the word is a general term\\nor a named entity in a given type. Thus, KALM learns to predict whether the context\\nobserved is indicative of a named entity and what tokens are likely to be entities of\\na given type.\\nWith the language modeling, KALM learns a named entity recognizer without any\\nexplicit supervision by using only plain text and the potential types of words. And,\\nit achieves a comparable performance with the state-of-the-art supervised methods.\\n7.4.5\\nOther Knowledge-Guided Applications\\nKnowledge enables AI agents to understand, infer, and address user demands, which\\nisessentialinmostknowledge-drivenapplicationslikeinformationretrieval,question\\nanswering, and dialogue system. The behavior of AI agents will be more reasonable\\nand accurate with the favor of knowledge representations. In the following subsec-\\ntions, we will introduce the great improvements made by knowledge representation\\nin question answering.\\n7.4.5.1\\nKnowledge-Guided Question Answering\\nQuestion answering aims to give correct answers according to users\\xe2\\x80\\x99 questions,\\nwhich needs the capabilities of both natural language understanding of questions\\nand inference on answer selection. Therefore, combining knowledge with question\\nanswering is a straightforward application for knowledge representations. Most con-\\nventional question answering systems directly utilize knowledge graphs as certain\\ndatabases, ignoring the latent relationships between entities and relations. Recently,\\nwith the thriving in deep learning, explorations have focused on neural models for\\nunderstanding questions and even generating answers.\\nConsideringthe\\xef\\xac\\x82exibilityanddiversityofgeneratedanswersinnaturallanguages,\\nYin et al. [93] propose a neural Generative Question Answering model (GENQA),\\nwhich explores on generating answers to simple factoid questions in natural lan-\\nguages. Figure7.22 demonstrates the work\\xef\\xac\\x82ow of GENQA. First, a bidirectional\\nRNN is regarded as the Interpreter to transform question q from natural language\\nto compressed representation Hq. Next, Enquirer takes HQ as the key to rank rel-\\n7.4 Applications\\n209\\nFig. 7.22 The architecture\\nof GENQA model\\nQ:\\xe2\\x80\\x9cHow tall is Yao Ming?\\xe2\\x80\\x9d\\nA:\\xe2\\x80\\x9cHe is 2.29m  and visible from space\\xe2\\x80\\x9d\\nAnswerer\\nGenerator\\nEnquirer\\ninterpreter\\nLong-term Memory\\n(Knowledge-base)\\nAtt.Model\\nShort-term Mernory\\nevant triples facts of q in knowledge graphs and retrieves possible entities in rq.\\nFinally, Answerer combines Hq and rq to generate answers in the form of natural\\nlanguages. Similar to [1], at each step, Answerer \\xef\\xac\\x81rst decides whether to generate\\ncommon words or knowledge words according to a logistic regression model. For\\ncommon words, Answerer acts in the same way as RNN decoders with Hq selected\\nby attention-based methods. As for knowledge words, Answerer directly generates\\nentities with higher ranks.\\nThere are gradually more efforts focusing on encoding knowledge representations\\ninto knowledge-driven tasks like information retrieval and dialogue systems. How-\\never, how to \\xef\\xac\\x82exibly and effectively combine knowledge with AI agents remains to\\nbe explored in the future.\\n7.4.5.2\\nKnowledge-Guided Recommendation System\\nDue to the rapid growth of web information, recommendation systems have been\\nplaying an essential role in the web application. The recommendation system aims\\nto predict the \\xe2\\x80\\x9crating\\xe2\\x80\\x9d or \\xe2\\x80\\x9cpreference\\xe2\\x80\\x9d that users may give to items. And since KGs\\ncan provide rich information, including both structured and unstructured data, rec-\\nommendation systems have utilized more and more knowledge from KGs to enrich\\ntheir contexts.\\nCheekula et al. [11] explore to utilize the hierarchical knowledge from the DBpe-\\ndia category structure in the recommendation system and employs the spreading\\nactivation algorithm to identify entities of interest to the user. Besides, Passant [56]\\nmeasures the semantic relatedness of the artist entity in a KG to build music recom-\\nmendation systems. However, most of these systems mainly investigate the problem\\nby leveraging the structure of KGs. Recently, with the development of representation\\n210\\n7\\nWorld Knowledge Representation\\nlearning, [98] proposes to jointly learn the latent representations in a collaborative\\n\\xef\\xac\\x81ltering recommendation system as well as entities\\xe2\\x80\\x99 representations in KGs.\\nExcept the tasks stated above, there are gradually more efforts focusing on encod-\\ning knowledge graph representations into other tasks such as dialogue system [37,\\n103], entity disambiguation [20, 31], knowledge graph alignment [12, 102], depen-\\ndency parsing [35], etc. Moreover, the idea of KRL has also motivated the research\\non visual relation extraction [2, 99] and social relation extraction [71].\\n7.5\\nSummary\\nIn this chapter, we \\xef\\xac\\x81rst introduce the concept of the knowledge graph. Knowledge\\ngraph contains both entities and the relationships among them in the form of triple\\nfacts, providing an effective way of human beings learning and understanding the\\nreal world. Next, we introduce the motivations of knowledge graph representation,\\nwhich is considered as a useful and convenient method for a large amount of data and\\nis widely explored and utilized in multiple knowledge-based tasks and signi\\xef\\xac\\x81cantly\\nimproves the performance. And we describe existing approaches for knowledge\\ngraph representation. Further, we discuss several advanced approaches that aim to\\ndeal with the current challenges of knowledge graph representation. We also review\\nthe real-world applications of knowledge graph representation such as language\\nmodeling, question answering, information retrieval, and recommendation systems.\\nFor further understanding of knowledge graph representation, you can \\xef\\xac\\x81nd more\\nrelated papers in this paper list https://github.com/thunlp/KRLPapers. There are also\\nsome recommended surveys and books including:\\n\\xe2\\x80\\xa2 Bengio et al. Representation learning: A review and new perspectives [4].\\n\\xe2\\x80\\xa2 Liu et al. Knowledge representation learning: A review [47].\\n\\xe2\\x80\\xa2 Nickel et al. A review of relational machine learning for knowledge graphs [52].\\n\\xe2\\x80\\xa2 Wang et al. Knowledge graph embedding: A survey of approaches and applications\\n[74].\\n\\xe2\\x80\\xa2 Ji et al. A survey on knowledge graphs: representation, acquisition and applications\\n[34].\\nIn the future, for better knowledge graph representation, there are some directions\\nrequiring further efforts:\\n(1) Utilizing More Knowledge. Current KRL approaches focus on represent-\\ning triple-based knowledge from world knowledge graphs such as Freebase, Wiki-\\ndata, etc. In fact, there are various kinds of knowledge in the real world such as\\nfactual knowledge, event knowledge, commonsense knowledge, etc. What\\xe2\\x80\\x99s more,\\nthe knowledge is stored with different formats, such as attributions, quanti\\xef\\xac\\x81er, text,\\nand so on. The researchers have formed a consensus that utilizing more knowledge\\nis a potential way toward more interpretable and intelligent NLP. Some existing\\nworks [44, 82] have made some preliminary attempts of utilizing more knowledge\\n7.5 Summary\\n211\\nin KRL. Beyond these works, is it possible to represent different knowledge in a\\nuni\\xef\\xac\\x81ed semantic space, which can be easily applied in downstream NLP tasks?\\n(2) Performing Deep Fusion of knowledge and language. There is no doubt\\nthat the joint learning of knowledge and language information can further bene\\xef\\xac\\x81t\\ndownstream NLP tasks. Existing works [76, 89, 97] have preliminarily veri\\xef\\xac\\x81ed the\\neffectiveness of joint learning. Recently, ERINE [100] and KnowBERT [57] further\\nprovide us a novel perspective to fuse knowledge and language in pretraining. Soares\\net al. [64] learn the relational similarity in text with the guidance of KGs, which is\\nalso a pioneer of knowledge fusion. Besides designing novel pretraining objectives,\\nwe could also design novel model architectures for downstream tasks, which are\\nmore suitable to utilize KRL, such as memory-based models [48, 91] and graph\\nnetwork-based models [66]. Nevertheless, it still remains an unsolved problem for\\neffectively performing the deep fusion of knowledge and language.\\n(3)OrientingHeterogeneousModalities.WiththefastdevelopmentoftheWorld\\nWide Web, the data size of audios, images, and videos on the Web have become\\nlarger and larger, which are also important resources for KRL besides texts. Some\\npioneer works [51, 81] explore to learn knowledge representations on a multi-modal\\nknowledge graph, but are still preliminary attempts. Intuitively, audio and visual\\nknowledge can provide complementary information, which bene\\xef\\xac\\x81ts related NLP\\ntasks. To the best of our knowledge, there still lacks research on applying multi-modal\\nKRL in downstream tasks. How to ef\\xef\\xac\\x81ciently and effectively integrate multi-modal\\nknowledge is becoming a critical and challenging problem for KRL.\\n(4) Exploring Knowledge Reasoning. Most of the existing KRL methods rep-\\nresent knowledge information in low-dimensional semantic space, which is feasible\\nfor the computation of complex knowledge graphs in neural-based NLP models.\\nAlthough bene\\xef\\xac\\x81ting from the usability of low-dimensional embeddings, KRL cannot\\nperform explainable reasoning such as symbolic rules, which is of great importance\\nfor downstream NLP tasks. Recently, there has been increasing interest in the com-\\nbination of embedding methods and symbolic reasoning methods [26, 59], aiming at\\ntaking both advantages of them. Beyond these works, there remain lots of unsolved\\nproblems for developing better knowledge reasoning ability for KRL.\\nReferences\\n1. Sungjin Ahn, Heeyoul Choi, Tanel P\\xc3\\xa4rnamaa, and Yoshua Bengio. A neural knowledge lan-\\nguage model. arXiv preprint arXiv:1608.00318, 2016.\\n2. Stephan Baier, Yunpu Ma, and Volker Tresp. Improving visual relationship detection using\\nsemantic modeling of scene descriptions. In Proceedings of ISWC, 2017.\\n3. Islam Beltagy and Raymond J Mooney. Ef\\xef\\xac\\x81cient markov logic inference for natural language\\nsemantics. In Proceedings of AAAI Workshop, 2014.\\n4. Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and\\nnew perspectives. TPAMI, 35(8):1798\\xe2\\x80\\x931828, 2013.\\n5. Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. Joint learning of words\\nand meaning representations for open-text semantic parsing. In Proceedings of AISTATS,\\n2012.\\n212\\n7\\nWorld Knowledge Representation\\n6. Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. A semantic matching\\nenergy function for learning with multi-relational data. Machine Learning, 94(2):233\\xe2\\x80\\x93259,\\n2014.\\n7. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana\\nYakhnenko. Translating embeddings for modeling multi-relational data. In Proceedings of\\nNeurIPS, 2013.\\n8. Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. Learning structured\\nembeddings of knowledge bases. In Proceedings of AAAI, 2011.\\n9. Andrew Carlson, Justin Betteridge, Richard C Wang, Estevam R Hruschka Jr, and Tom M\\nMitchell. Coupled semi-supervised learning for information extraction. In Proceedings of\\nWSDM, 2010.\\n10. Mohamed Chabchoub, Michel Gagnon, and Amal Zouaq. Collective disambiguation and\\nsemantic annotation for entity linking and typing. In Proceedings of SWEC, 2016.\\n11. Siva Kumar Cheekula, Pavan Kapanipathi, Derek Doran, Prateek Jain, and Amit P Sheth.\\nEntity recommendations using hierarchical knowledge bases. 2015.\\n12. Muhao Chen, Yingtao Tian, Mohan Yang, and Zaniolo Carlo. Multilingual knowledge graph\\nembeddings for cross-lingual knowledge alignment. In Proceedings of IJCAI, 2017.\\n13. Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks\\nfor soft-matching n-grams in ad-hoc search. In Proceedings of WSDM, 2018.\\n14. Jeffrey Dalton, Laura Dietz, and James Allan. Entity query feature expansion using knowledge\\nbase links. In Proceedings of SIGIR, 2014.\\n15. Luciano Del Corro, Abdalghani Abujabal, Rainer Gemulla, and Gerhard Weikum. Finet:\\nContext-aware \\xef\\xac\\x81ne-grained named entity typing. In Proceedings of EMNLP, 2015.\\n16. Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional\\n2d knowledge graph embeddings. In Proceedings of AAAI, 2018.\\n17. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019.\\n18. Li Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu. A hybrid neural model for type\\nclassi\\xef\\xac\\x81cation of entity mentions. In Proceedings of IJCAI, 2015.\\n19. Faezeh Ensan and Ebrahim Bagheri. Document retrieval model through semantic linking. In\\nProceedings of WSDM, 2017.\\n20. Wei Fang, Jianwen Zhang, Dilin Wang, Zheng Chen, and Ming Li. Entity disambiguation by\\nknowledge and text jointly embedding. In Proceedings of CoNLL, 2016.\\n21. Alberto Garc\\xc3\\xada-Dur\\xc3\\xa1n, Antoine Bordes, and Nicolas Usunier. Composing relationships with\\ntranslations. In Proceedings of EMNLP, 2015.\\n22. Kelvin Gu, John Miller, and Percy Liang. Traversing knowledge graphs in vector space. In\\nProceedings of EMNLP, 2015.\\n23. Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. Semantic matching by non-linear\\nword transportation for information retrieval. In Proceedings of CIKM, 2016.\\n24. Shu Guo, Quan Wang, Lihong Wang, Bin Wang, and Li Guo. Jointly embedding knowledge\\ngraphs and logical rules. In Proceedings of EMNLP, 2016.\\n25. Petr H\\xc3\\xa1jek. Metamathematics of fuzzy logic, volume 4. Springer Science & Business Media,\\n1998.\\n26. Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. Embedding\\nlogical queries on knowledge graphs. In Proceedings of NIPS, 2018.\\n27. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge\\nfor knowledge graph completion. arXiv preprint arXiv:1611.04125, 2016.\\n28. Xu Han, Zhiyuan Liu, and Maosong Sun. Neural knowledge acquisition via mutual attention\\nbetween knowledge graph and text. In Proceedings of AAAI, pages 4832\\xe2\\x80\\x934839, 2018.\\n29. Katsuhiko Hayashi and Masashi Shimbo. On the equivalence of holographic and complex\\nembeddings for link prediction. In Proceedings of ACL, 2017.\\n30. Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. Learning to represent knowledge graphs\\nwith gaussian embedding. In Proceedings of CIKM, 2015.\\nReferences\\n213\\n31. Hongzhao Huang, Larry Heck, and Heng Ji. Leveraging deep neural networks and knowledge\\ngraphs for entity disambiguation. arXiv preprint arXiv:1504.07678, 2015.\\n32. Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. Knowledge graph embedding\\nvia dynamic mapping matrix. In Proceedings of ACL, 2015.\\n33. Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao. Knowledge graph completion with adaptive\\nsparse transfer matrix. In Proceedings of AAAI, 2016.\\n34. Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S Yu. A survey on knowl-\\nedge graphs: Representation, acquisition and applications. arXiv preprint arXiv:2002.00388,\\n2020.\\n35. A-Yeong Kim, Hyun-Je Song, Seong-Bae Park, and Sang-Jo Lee. A re-ranking model for\\ndependency parsing with knowledge graph embeddings. In Proceedings of IALP, 2015.\\n36. Denis Krompa\\xc3\\x9f, Stephan Baier, and Volker Tresp. Type-constrained representation learning\\nin knowledge graphs. In Proceedings of ISWC, 2015.\\n37. Phong Le, Marc Dymetman, and Jean-Michel Renders. Lstm-based mixture-of-experts for\\nknowledge-aware dialogues. In Proceedings of ACL Workshop, 2016.\\n38. Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, and Song Liu. Modeling\\nrelation paths for representation learning of knowledge bases. In Proceedings of EMNLP,\\n2015.\\n39. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation\\nembeddings for knowledge graph completion. In Proceedings of AAAI, 2015.\\n40. Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Neural relation\\nextraction with selective attention over instances. In Proceedings of ACL, 2016.\\n41. Xiao Ling and Daniel S Weld. Fine-grained entity recognition. In Proceedings of AAAI, 2012.\\n42. Angli Liu, Jingfei Du, and Veselin Stoyanov. Knowledge-augmented language model and its\\napplication to unsupervised named-entity recognition. In Proceedings of NAACL, 2019.\\n43. Quan Liu, Hui Jiang, Andrew Evdokimov, Zhen-Hua Ling, Xiaodan Zhu, Si Wei, and\\nYu Hu. Probabilistic reasoning via deep learning: Neural association models. arXiv preprint\\narXiv:1603.07704, 2016.\\n44. Quan Liu, Hui Jiang, Zhen-Hua Ling, Xiaodan Zhu, Si Wei, and Yu Hu. Commonsense\\nknowledge enhanced embeddings for solving pronoun disambiguation problems in winograd\\nschema challenge. arXiv preprint arXiv:1611.04146, 2016.\\n45. Xitong Liu and Hui Fang. Latent entity space: A novel retrieval approach for entity-bearing\\nqueries. Information Retrieval Journal, 18(6):473\\xe2\\x80\\x93503, 2015.\\n46. Yang Liu, Kang Liu, Liheng Xu, Jun Zhao, et al. Exploring \\xef\\xac\\x81ne-grained entity type constraints\\nfor distantly supervised relation extraction. In Proceedings of COLING, 2014.\\n47. Zhiyuan Liu, Maosong Sun, Yankai Lin, and Ruobing Xie. Knowledge representation learn-\\ning: a review. JCRD, 53(2):247\\xe2\\x80\\x93261, 2016.\\n48. Todor Mihaylov and Anette Frank. Knowledgeable reader: Enhancing cloze-style reading\\ncomprehension with external commonsense knowledge. In Proceedings of ACL, 2018.\\n49. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\\xef\\xac\\x81cient estimation of word\\nrepresentations in vector space. In Proceedings of ICLR, 2013.\\n50. Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. Distant supervision for relation\\nextraction without labeled data. In Proceedings of ACL-IJCNLP, 2009.\\n51. Hatem Mousselly-Sergieh, Teresa Botschen, Iryna Gurevych, and Stefan Roth. A multimodal\\ntranslation-based approach for knowledge graph representation learning. In Proceedings of\\nJCLCS, 2018.\\n52. Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of\\nrelational machine learning for knowledge graphs. 2015.\\n53. Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. Holographic embeddings of\\nknowledge graphs. In Proceedings of AAAI, 2016.\\n54. Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective\\nlearning on multi-relational data. In Proceedings of ICML, 2011.\\n55. Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. Factorizing yago: scalable machine\\nlearning for linked data. In Proceedings of WWW, 2012.\\n214\\n7\\nWorld Knowledge Representation\\n56. Alexandre Passant. dbrec\\xe2\\x80\\x93music recommendations using dbpedia. In Proceedings of ISWC,\\n2010.\\n57. Matthew E. Peters, Mark Neumann, Robert L Logan, Roy Schwartz, Vidur Joshi, Sameer\\nSingh, and Noah A. Smith. Knowledge enhanced contextual word representations. In Pro-\\nceedings of EMNLP-IJCNLP, 2019.\\n58. Jay Pujara, Hui Miao, Lise Getoor, and William W Cohen. Knowledge graph identi\\xef\\xac\\x81cation.\\nIn Proceedings of ISWC, 2013.\\n59. Meng Qu and Jian Tang. Probabilistic logic neural networks for reasoning. In Proceedings of\\nNIPS, 2019.\\n60. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\\nLanguage models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.\\n61. Hadas Raviv, Oren Kurland, and David Carmel. Document retrieval using entity-based lan-\\nguage models. In Proceedings of SIGIR, 2016.\\n62. Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gr\\xc3\\xa9goire Mesnil. A latent semantic\\nmodelwithconvolutional-poolingstructureforinformationretrieval.In ProceedingsofCIKM,\\n2014.\\n63. Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. An attentive neural\\narchitecture for \\xef\\xac\\x81ne-grained entity type classi\\xef\\xac\\x81cation. In Proceedings of AKBC Workshop,\\n2016.\\n64. Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. Matching the\\nBlanks: Distributional similarity for relation learning. In Proceedings of ACL, pages 2895\\xe2\\x80\\x93\\n2905, 2019.\\n65. Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural\\ntensor networks for knowledge base completion. In Proceedings of NeurIPS, 2013.\\n66. Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and\\nWilliam Cohen. Open domain question answering using early fusion of knowledge bases and\\ntext. In Proceedings of EMNLP, 2018.\\n67. Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embed-\\nding by relational rotation in complex space. In Proceedings of ICLR, 2019.\\n68. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\\nnetworks. In Proceedings of NeurIPS, 2014.\\n69. Erik F Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task:\\nLanguage-independent named entity recognition. In Proceedings of HLT-NAACL, 2003.\\n70. Th\\xc3\\xa9o Trouillon, Johannes Welbl, Sebastian Riedel, \\xc3\\x89ric Gaussier, and Guillaume Bouchard.\\nComplex embeddings for simple link prediction. In Proceedings of ICML, 2016.\\n71. Cunchao Tu, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Transnet: translation-based\\nnetwork representation learning for social relation extraction. In Proceedings of IJCAI, 2017.\\n72. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural\\nimage caption generator. In Proceedings of CVPR, 2015.\\n73. Nina Wacholder, Yael Ravin, and Misook Choi. Disambiguation of proper names in text. In\\nProceedings of ANLP, 1997.\\n74. Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph embedding: A survey\\nof approaches and applications. TKDE, 29(12):2724\\xe2\\x80\\x932743, 2017.\\n75. Quan Wang, Bin Wang, and Li Guo. Knowledge base completion using embeddings and rules.\\nIn Proceedings of IJCAI, 2015.\\n76. Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph and text jointly\\nembedding. In Proceedings of EMNLP, pages 1591\\xe2\\x80\\x931601, 2014.\\n77. Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by\\ntranslating on hyperplanes. In Proceedings of AAAI, 2014.\\n78. Han Xiao, Minlie Huang, Yu Hao, and Xiaoyan Zhu. Transa: An adaptive approach for\\nknowledge graph embedding. arXiv preprint arXiv:1509.05490, 2015.\\n79. Han Xiao, Minlie Huang, Yu Hao, and Xiaoyan Zhu. Transg: A generative mixture model for\\nknowledge graph embedding. arXiv preprint arXiv:1509.05488, 2015.\\nReferences\\n215\\n80. Han Xiao, Minlie Huang, and Xiaoyan Zhu. From one point to a manifold: Knowledge graph\\nembedding for precise link prediction. In Proceedings of IJCAI, 2016.\\n81. Ruobing Xie, Zhiyuan Liu, Tat-seng Chua, Huanbo Luan, and Maosong Sun. Image-embodied\\nknowledge representation learning. In Proceedings of IJCAI, 2016.\\n82. Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. Representation learning\\nof knowledge graphs with entity descriptions. In Proceedings of AAAI, 2016.\\n83. Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Representation learning of knowledge graphs\\nwith hierarchical types. In Proceedings of IJCAI, 2016.\\n84. Chenyan Xiong and Jamie Callan. EsdRank: Connecting query and documents through exter-\\nnal semi-structured data. In Proceedings of CIKM, 2015.\\n85. Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. Bag-of-entities representation for ranking.\\nIn Proceedings of ICTIR, 2016.\\n86. Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. Word-entity duet representations for docu-\\nment ranking. In Proceedings of SIGIR, 2017.\\n87. Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end\\nneural ad-hoc ranking with kernel pooling. In Proceedings of SIGIR, 2017.\\n88. Chenyan Xiong, Russell Power, and Jamie Callan. Explicit semantic ranking for academic\\nsearch via knowledge graph embedding. In Proceedings of WWW, 2017.\\n89. Jiacheng Xu, Xipeng Qiu, Kan Chen, and Xuanjing Huang. Knowledge graph representation\\nwith jointly structural and textual encoding. In Proceedings of IJCAI, 2017.\\n90. Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, and Gerhard Weikum. Robust question\\nanswering over the web of linked data. In Proceedings of CIKM, 2013.\\n91. Bishan Yang and Tom Mitchell. Leveraging knowledge bases in LSTMs for improving\\nmachine reading. In Proceedings of ACL, 2017.\\n92. Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities\\nand relations for learning and inference in knowledge bases. In Proceedings of ICLR, 2015.\\n93. Jun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang, Hang Li, and Xiaoming Li. Neural gener-\\native question answering. In Proceedings of IJCAI, 2016.\\n94. Dani Yogatama, Daniel Gillick, and Nevena Lazic. Embedding methods for \\xef\\xac\\x81ne grained entity\\ntype classi\\xef\\xac\\x81cation. In Proceedings of ACL, 2015.\\n95. Mohamed Amir Yosef, Sandro Bauer, Johannes Hoffart, Marc Spaniol, and Gerhard Weikum.\\nHYENA: Hierarchical type classi\\xef\\xac\\x81cation for entity names. In Proceedings of COLING, 2012.\\n96. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation classi\\xef\\xac\\x81cation\\nvia convolutional deep neural network. In Proceedings of COLING, 2014.\\n97. Dongxu Zhang, Bin Yuan, Dong Wang, and Rong Liu. Joint semantic relevance learning with\\ntext data and graph knowledge. In Proceedings of ACL-IJCNLP, 2015.\\n98. Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. Collaborative\\nknowledge base embedding for recommender systems. In Proceedings of SIGKDD, 2016.\\n99. HanwangZhang,ZawlinKyaw,Shih-FuChang,andTat-SengChua.Visualtranslationembed-\\nding network for visual relation detection. In Proceedings of CVPR, 2017.\\n100. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie:\\nEnhanced language representation with informative entities. In Proceedings of ACL, 2019.\\n101. Huaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan, and Zheng Chen. Aligning knowledge\\nand text embeddings by entity descriptions. In Proceedings of EMNLP, 2015.\\n102. Hao Zhu, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Iterative entity alignment via joint\\nknowledge embeddings. In Proceedings of IJCAI, 2017.\\n103. Wenya Zhu, Kaixiang Mo, Yu Zhang, Zhangbin Zhu, Xuezheng Peng, and Qiang Yang.\\nFlexible end-to-end dialogue system for knowledge grounded conversation. arXiv preprint\\narXiv:1709.04264, 2017.\\n216\\n7\\nWorld Knowledge Representation\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 8\\nNetwork Representation\\nAbstract Network representation learning aims to embed the vertexes in a network\\ninto low-dimensional dense representations, in which similar vertices in the net-\\nwork should have \\xe2\\x80\\x9cclose\\xe2\\x80\\x9d representations (usually measured by cosine similarity or\\nEuclidean distance of their representations). The representations can be used as the\\nfeature of vertices and applied to many network study tasks. In this chapter, we will\\nintroduce network representation learning algorithms in the past decade. Then we\\nwill talk about their extensions when applied to various real-world networks. Finally,\\nwe will introduce some common evaluation tasks of network representation learning\\nand relevant datasets.\\n8.1\\nIntroduction\\nAs a natural way to represent objects and their relationships, the network is ubiqui-\\ntous in our daily lives. The rapid development of social networks like Facebook and\\nTwitter encourage researchers to design effective and ef\\xef\\xac\\x81cient algorithms on network\\nstructure. A key problem of network study is how to represent the network informa-\\ntion properly. Traditional representations of networks are usually high dimensional\\nand sparse, which becomes a weakness when people apply statistical learning to\\nnetworks. With the development of machine learning, feature learning of vertices in\\na network is becoming an emerging task. Therefore, network representation learn-\\ning algorithms turn network information into low-dimensional dense real-valued\\nvectors, which can be used as input for existing machine learning algorithms. For\\nexample, the representations of vertices can be fed to a classi\\xef\\xac\\x81er like Support Vector\\nMachine (SVM) for the vertex classi\\xef\\xac\\x81cation task. Also, the representations can be\\nused for visualization by taking the representations as points in Euclidean space. In\\nthis section, we will formalize the network representation learning problem.\\nThe original version of this chapter was revised: The \\xef\\xac\\x81rst paragraph in Section 8.3 was updated.\\nThe correction to this chapter can be found at https://doi.org/10.1007/978-981-15-5573-2_12.\\n\\xc2\\xa9 The Author(s) 2020, corrected publication 2023\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_8\\n217\\n218\\n8\\nNetwork Representation\\nFig. 8.1 A visualization of vertex embeddings learned by DeepWalk model [93]\\nDenote a network as G = (V, E) where V is the vertex set and E is the edge set.\\nAn edge e = (vi, v j) \\xe2\\x88\\x88E where vi, v j \\xe2\\x88\\x88V is a directed edge from vertex vi to v j.\\nThe outdegree of vertex vi is de\\xef\\xac\\x81ned as degO(vi) = |{v j|(vi, v j) \\xe2\\x88\\x88E}|. Similarly,\\nthe indegree of vertex vi is degI(vi) = |{v j|(v j, vi) \\xe2\\x88\\x88E}|. For undirected network,\\nwe have deg(vi) = degO(vi) = degI(vi). Taking social network as an example, a\\nvertex represents a user and an edge represents the friendship between two users.\\nThe indegree and outdegree represent the number of followers and followees of a\\nuser, respectively.\\nAdjacency matrix A \\xe2\\x88\\x88R|V |\\xc3\\x97|V | is a matrix where Ai j = 1 if (vi, v j) \\xe2\\x88\\x88E and\\nAi j = 0 otherwise. We can easily generalize adjacency matrix to weighted network\\nby setting Ai j to the weight of edge (vi, v j). The adjacency matrix is a simple\\nand straightforward representation of the network. Each row of adjacency matrix\\nA denotes the relationship between a vertex and other vertices and can be seen as\\nthe representation of the corresponding vertex.\\nThoughconvenientandstraightforward,therepresentationoftheadjacencymatrix\\nsuffers from the scalability problem. Adjacency matrix A takes |V | \\xc3\\x97 |V | space to\\nstore, and it is usually unacceptable when |V | grows large. Also, the adjacency matrix\\nis very sparse, which means most of its entries are zeros. The data sparsity makes\\ndiscrete algorithms applicable, but it is still hard to develop ef\\xef\\xac\\x81cient algorithms for\\nstatistic learning [93].\\nTherefore, people come up with the idea to learn low-dimensional dense rep-\\nresentations for vertices in a network. Formally, the goal of network representation\\nlearning is to learn a real-valued vector v \\xe2\\x88\\x88Rd for vertex v \\xe2\\x88\\x88V where dimension d is\\nmuch smaller than the number of vertices |V |. The idea is that similar vertices should\\nhave close representations as shown in Fig.8.1. Network representation learning can\\nbe unsupervised or semi-supervised. The representations are automatically learned\\nwithout feature engineering and can be further used for speci\\xef\\xac\\x81c tasks like classi-\\n\\xef\\xac\\x81cations once they are learned. These representations are low dimensional, which\\nenables ef\\xef\\xac\\x81cient algorithms to be designed over the representations without consid-\\nering the network structure itself. We will discuss more details about the evaluation\\nof network representations later in this chapter.\\n8.2 Network Representation\\n219\\n8.2\\nNetwork Representation\\nIn this section, we will introduce several kinds of network representation learning\\nalgorithms in detail.\\n8.2.1\\nSpectral Clustering Based Methods\\nSpectral clustering based methods are a group of algorithms that compute \\xef\\xac\\x81rst k\\neigenvectors or singular vectors of an af\\xef\\xac\\x81nity matrix, such as adjacency or Lapla-\\ncian matrix of the network. These methods depend heavily on the construction of the\\naf\\xef\\xac\\x81nity matrix. The evaluation result of different af\\xef\\xac\\x81nity matrices varies a lot. Gener-\\nally speaking, spectral clustering based methods have a high complexity because the\\ncomputations of eigenvectors and singular vectors have a nonlinear time complexity.\\nOn the other hand, spectral clustering based methods need to save an af\\xef\\xac\\x81nity\\nmatrix in the memory during the computation. Thus the space complexity cannot be\\nignored, either. These disadvantages limit the large-scale and online generalization of\\nthese methods. Now we will present several algorithms based on spectral clustering.\\nLocally Linear Embedding (LLE) [98] assumes that the representations of vertices\\nare sampled from a manifold. More speci\\xef\\xac\\x81cally, LLE supposes that the representa-\\ntions of a vertex and its neighbors lie in a locally linear patch of the manifold. That\\nis to say, a vertex\\xe2\\x80\\x99s representation can be approximated by a linear combination of\\nthe representation of its neighbors. LLE uses the linear combination of neighbors to\\nreconstruct the center vertex. Formally, the reconstruction error of all vertices can be\\nexpressed as\\nL (W, V) =\\n|V |\\n\\x02\\ni=1\\n\\x03\\x03\\x03\\x03\\x03\\x03\\nvi \\xe2\\x88\\x92\\n|V |\\n\\x02\\nj=1\\nWi jv j\\n\\x03\\x03\\x03\\x03\\x03\\x03\\n2\\n,\\n(8.1)\\nwhere V \\xe2\\x88\\x88R|V |\\xc3\\x97d is the vertex embedding matrix and Wi j is the contribution coef-\\n\\xef\\xac\\x81cient of vertex v j to vi. LLE enforces Wi j = 0 if vi and v j are not connected,\\ni.e., (vi, v j) /\\xe2\\x88\\x88E. Further, the summation of a row of matrix W is set to 1, i.e.,\\n\\x04|V |\\nj=1 Wi j = 1.\\nEquation8.1 is solved by alternatively optimizing weight matrix W and represen-\\ntation V. The optimization over W can be solved as a least-squares problem. The\\noptimization over representation V leads to the following optimization problem:\\nL (W, V) =\\n|V |\\n\\x02\\ni=1\\n\\x03\\x03\\x03\\x03\\x03\\x03\\nvi \\xe2\\x88\\x92\\n|V |\\n\\x02\\nj=1\\nWi jv j\\n\\x03\\x03\\x03\\x03\\x03\\x03\\n2\\n,\\n(8.2)\\n220\\n8\\nNetwork Representation\\ns.t.\\n|V |\\n\\x02\\ni=1\\nvi = 0,\\n(8.3)\\nand |V |\\xe2\\x88\\x921\\n|V |\\n\\x02\\ni=1\\nv\\xe2\\x8a\\xa4\\ni vi = Id,\\n(8.4)\\nwhere Id denotes d \\xc3\\x97 d identity matrix. The conditions Eqs.8.3 and 8.4 ensure\\nthe uniqueness of the solution. The \\xef\\xac\\x81rst condition enforces the center of all vertex\\nembeddings to zero point and the second condition guarantees different coordinates\\nhave the same scale, i.e., equal contribution to the reconstruction error.\\nThe optimization problem can be formulated as the computation of eigenvectors\\nof matrix (I|V | \\xe2\\x88\\x92W\\xe2\\x8a\\xa4)(I|V | \\xe2\\x88\\x92W), which is an easily solvable eigenvalue problem.\\nMore details can be found in the note [22].\\nLaplacian Eigenmap [8] algorithm simply follows the idea that the representations\\nof two connected vertices should be close. Speci\\xef\\xac\\x81cally, the \\xe2\\x80\\x9ccloseness\\xe2\\x80\\x9d is measured\\nby the square of Euclidean distance. We use D to denote diagonal degree matrix\\nwhere D is a |V | \\xc3\\x97 |V | diagonal matrix and the ith diagonal entry Dii is the degree\\nof vertex vi. The Laplacian matrix L of a graph is de\\xef\\xac\\x81ned as the difference of diagonal\\nmatrix D and adjacency matrix A, i.e., L = D \\xe2\\x88\\x92A.\\nLaplacian Eigenmap algorithm wants to minimize the following cost function:\\nL (V) =\\n\\x02\\n{i, j|(vi,v j)\\xe2\\x88\\x88E}\\n\\xe2\\x88\\xa5vi \\xe2\\x88\\x92v j\\xe2\\x88\\xa52,\\n(8.5)\\ns.t. V\\xe2\\x8a\\xa4DV = Id.\\n(8.6)\\nThe cost function is the summation of square loss of all connected vertex pairs\\nand the condition prevents the trivial all-zero solution caused by arbitrary scale.\\nEquation8.5 can be reformulated in matrix form as\\nV\\xe2\\x88\\x97= arg\\nmin\\nV\\xe2\\x8a\\xa4DV=Id\\ntr(V\\xe2\\x8a\\xa4LV).\\n(8.7)\\nAlgebraic knowledge tells us that the optimal solution V\\xe2\\x88\\x97of Eq.8.7 is the cor-\\nresponding eigenvectors of d smallest nonzero eigenvalues of Laplacian matrix L.\\nNotethat theLaplacianEigenmapalgorithmcanbeeasilygeneralizedtotheweighted\\ngraph.\\nBoth LLE and Laplacian Eigenmap have a symmetric cost function which indi-\\ncates that both algorithms cannot be applied to the directed graph. Directed Graph\\nEmbedding (DGE) [17] was proposed to generalize Laplacian Eigenmap.\\nFor both directed and undirected graph, we can de\\xef\\xac\\x81ne a transition probability\\nmatrix P \\xe2\\x88\\x88R|V |\\xc3\\x97|V |, where Pi j denotes the probability that vertex vi walks to v j.\\n8.2 Network Representation\\n221\\nTable 8.1 Applicability of LLE, Laplacian Eigenmap, and DGE algorithms on undirected,\\nweighted, and directed graph\\nAlgorithm\\nCapability\\nUndirected\\nWeighted\\nDirected\\nLLE\\n\\xe2\\x9c\\x93\\n\\xe2\\x80\\x93\\n\\xe2\\x80\\x93\\nLaplacian Eigenmap\\n\\xe2\\x9c\\x93\\n\\xe2\\x9c\\x93\\n\\xe2\\x80\\x93\\nDGE\\n\\xe2\\x9c\\x93\\n\\xe2\\x9c\\x93\\n\\xe2\\x9c\\x93\\nThe transition matrix de\\xef\\xac\\x81nes a Markov random walk through the graph. We denote\\nthe stationary value of vertex vi as \\xcf\\x80i where \\x04\\ni \\xcf\\x80i = 1. The stationary distribution\\nof random walk is commonly used in many ranking algorithms such as PageRank.\\nDGE designs a new cost function which emphasizes the important vertices, which\\nhave a higher stationary value:\\nL (V) =\\n|V |\\n\\x02\\ni=1\\n\\xcf\\x80i\\n|V |\\n\\x02\\nj=1\\nPi j\\xe2\\x88\\xa5vi \\xe2\\x88\\x92v j\\xe2\\x88\\xa52.\\n(8.8)\\nBy denoting M = diag(\\xcf\\x801, \\xcf\\x802, . . . , \\xcf\\x80|V |), the cost function Eq.8.8 can be refor-\\nmulated as\\nL (V) = 2tr(V\\xe2\\x8a\\xa4BV),\\n(8.9)\\ns.t. V\\xe2\\x8a\\xa4MV = Id,\\n(8.10)\\nwhere\\nB = M \\xe2\\x88\\x92MP \\xe2\\x88\\x92P\\xe2\\x8a\\xa4M\\n2\\n.\\n(8.11)\\nThe condition Eq.8.10 is added to remove an arbitrary scaling factor. Similar to\\nLaplacian Eigenmap, the optimization problem can also be solved as a generalized\\neigenvector problem.\\nFor comparisons between the above three network embedding learning algo-\\nrithms, we conclude the following table to illustrate their applicability (Table8.1).\\nUnlike previous works which minimize the distance between vertex representa-\\ntions, Tang and Liu [112] introduces modularity [85] into the cost function instead.\\nModularity is a measurement which characterizes how far the graph is away from\\na uniform random graph. Given graph G = (V, E), we assume that vertices V are\\ndivided into k nonoverlapping communities. By \\xe2\\x80\\x9cuniform random graph\\xe2\\x80\\x9d, we mean\\nvertices connect to each other based on a uniform distribution given their degrees.\\nThen the expected edges between vi and v j is deg(vi) deg(vj)\\n2|E|\\n. Then the modularity of a\\ngraph Q is de\\xef\\xac\\x81ned as\\n222\\n8\\nNetwork Representation\\nQ =\\n1\\n2|E|\\n\\x02\\ni, j\\n\\x05\\nAi j \\xe2\\x88\\x92deg(vi) deg(v j)\\n2|E|\\n\\x06\\n\\xce\\xb4(vi, v j),\\n(8.12)\\nwhere \\xce\\xb4(vi, v j) = 1 if vi and v j belong to the same community and \\xce\\xb4(vi, v j) = 0\\notherwise. A larger modularity indicates that the subgraphs inside communities are\\ndenser, which follows the intuition that a community is a dense well-connected\\ncluster. Then the problem is to \\xef\\xac\\x81nd a partition that maximizes the modularity Q.\\nHowever, a hard clustering on modularity maximization is proved to be NP hard.\\nTherefore, they relax the problem to a soft case. Let d \\xe2\\x88\\x88Z|V |\\n+ denotes the degree of\\nall vertices and 1 \\xe2\\x88\\x88{0, 1}|V |\\xc3\\x97k denotes the community indicator matrix where\\n1i j =\\n\\x07\\n1\\nif vertex i belongs to community j,\\n0\\notherwise.\\n(8.13)\\nThen we de\\xef\\xac\\x81ne modularity matrix B as\\nB = A \\xe2\\x88\\x92ddT\\n2|E|,\\n(8.14)\\nand modularity Q can be reformulated as\\nQ =\\n1\\n2|E|tr(1\\xe2\\x8a\\xa4B1).\\n(8.15)\\nBy relaxing 1 to a continuous matrix, it has been proved that the optimal solution\\n1 is the top-k eigenvectors of modularity matrix B [84].\\nAs an alternatively cost function, Tang and Liu also proposed another algorithm\\n[113] by optimizing over normalized cut of the graph. Similarly, the algorithm turns\\nto the computation of top-k eigenvectors of normalized graph Laplacian \\x08L:\\n\\x08L = D\\xe2\\x88\\x921\\n2 L D\\xe2\\x88\\x921\\n2 = I \\xe2\\x88\\x92D\\xe2\\x88\\x921\\n2 AD\\xe2\\x88\\x921\\n2 .\\n(8.16)\\nThen the community indicator matrix 1 is taken as a k-dimensional vertex repre-\\nsentation.\\nToconcludespectralclusteringmethodsfornetworkrepresentationlearning,these\\nmethods often de\\xef\\xac\\x81ne a cost function that is linear or quadratic to the vertex embed-\\nding. Then they reformulate the cost function as a matrix form and \\xef\\xac\\x81gure out that\\nthe optimal solutions are eigenvectors of a particular matrix according to algebra\\nknowledge. The major drawback of spectral clustering methods is the complexity:\\nthe computation of eigenvectors for large-scale matrices is both time consuming and\\nspace consuming.\\n8.2 Network Representation\\n223\\n8.2.2\\nDeepWalk\\nAs shown in previous subsections, accurate computation of the optimal solution, such\\nas eigenvector computation, is not very ef\\xef\\xac\\x81cient for large-scale problems. Meantime,\\nneural network approaches have proved their effectiveness in many areas such as\\nnatural language and image processing. Though the gradient descent method cannot\\nalways guarantee an optimal solution of the neural network models, the implemen-\\ntation and learning of neural networks are relatively fast, and they usually have good\\nperformances. On the other hand, neural network models can let people get rid of\\nfeature engineering and are mostly data driven. Thus, the exploration of the neural\\nnetwork approach on representation learning is becoming an emerging task.\\nDeepWalk [93] proposes a novel approach that introduces deep learning tech-\\nniques into network representation learning for the \\xef\\xac\\x81rst time. The bene\\xef\\xac\\x81ts of model-\\ning truncated random walks instead of the adjacency matrix are twofold: \\xef\\xac\\x81rst, random\\nwalks need only local information and thus enable discrete and online algorithms on\\nit while modeling of adjacency matrix may need to store everything in memory and\\nthus be space consuming; second, modeling random walks can alleviate the variance\\nand uncertainty of modeling original binary adjacency matrix. We will look insight\\ninto DeepWalk in the next subsection.\\nUnsupervised representation learning algorithms have been widely studied and\\napplied in the natural language processing area. The authors show that the vertex fre-\\nquency in short random walks also follows the power law as words in documents do.\\nShowing the connection between vertex to the word and random walks to sentences,\\nthe authors adapted a well-known word representation learning algorithm word2vec\\n[80] into vertex representation learning. Now, we will introduce DeepWalk algo-\\nrithms in detail.\\nGiven graph G = (V, E), we denote a random walk started at vertex vi as \\xe2\\x84\\x93vi.\\nWe use \\xe2\\x84\\x93k\\nvi to represent the kth vertex in the random walk \\xe2\\x84\\x93vi. The next vertex \\xe2\\x84\\x93k+1\\nvi\\nis generated by uniformly random selection from neighbors of vertex \\xe2\\x84\\x93k\\nvi. Random\\nwalk sequences have been used for many network analysis tasks, such as similarity\\nmeasurement and community detection [2, 32].\\nDeepWalk follows the idea of language modeling to model short random walk\\nsequences. That is to estimate the likelihood of observing vertex vi given all previous\\nvertices in the random walk:\\nP(vi|(v1, v2, . . . , vi\\xe2\\x88\\x921)).\\n(8.17)\\nTo the extent of vertex representation learning, we turn to predict vertex vi given\\nthe representations of all previous vertices:\\nP(vi|(v1, v2, . . . , vi\\xe2\\x88\\x921)).\\n(8.18)\\nA relaxation of this formula in language modeling turns to use vertex vi to predict\\nits neighboring vertices vi\\xe2\\x88\\x92w, . . . , vi\\xe2\\x88\\x921, vi+1, . . . , vi+w where w is the window size.\\n224\\n8\\nNetwork Representation\\nThis part of model is named as Skip-gram model in word embedding learning. The\\nneighboring vertices are also called context vertices of the center vertex. As another\\nsimpli\\xef\\xac\\x81cation, DeepWalk ignores the order and offset of the vertices and thus predict\\nvi\\xe2\\x88\\x92w and vi\\xe2\\x88\\x921 in the same way. The optimization function of a single vertex of a\\nrandom walk can be formulated as\\nmin\\nv \\xe2\\x88\\x92log P({vi\\xe2\\x88\\x92w, . . . , vi\\xe2\\x88\\x921, vi+1, . . . , vi+w}|vi).\\n(8.19)\\nBased on independent assumption, the loss function can be rewritten as\\nmin\\nv\\nw\\n\\x02\\nk=\\xe2\\x88\\x92w,k\\xcc\\xb8=0\\n\\xe2\\x88\\x92log P(vi+k|vi).\\n(8.20)\\nThe overall loss function can be obtained by adding up over every vertex in every\\nrandom walk.\\nNow we talk about how to predict a single vertex v j given center vertex vi. In\\nDeepWalk, each vertex vi has two representations with the same dimension: ver-\\ntex representation vi \\xe2\\x88\\x88Rd and context representation ci \\xe2\\x88\\x88Rd. The probability of\\nprediction P(v j|vi) is de\\xef\\xac\\x81ned by a softmax function over all vertices:\\nP(v j|vi) =\\nexp(vic\\xe2\\x8a\\xa4\\nj )\\n\\x04|V |\\nk=1 exp(vic\\xe2\\x8a\\xa4\\nk )\\n.\\n(8.21)\\nHere we come to the parameter learning phase of DeepWalk. We \\xef\\xac\\x81rst present the\\npseudocode of the DeepWalk framework in Algorithm 8.1.\\nAlgorithm 8.1 DeepWalk algorithm\\nGiven graph G = (V, E), window size w, embedding size d, walks per vertex n and walk length l\\nfor i = 1, 2, . . . , n do\\nfor vi \\xe2\\x88\\x88V do\\n\\xe2\\x84\\x93vi =RandomWalk(G, vi,l)\\nSkip-gram(V, \\xe2\\x84\\x93vi , w)\\nend for\\nend for\\nwhere RandomWalk(G, vi,l) generates a random walk rooted at vi with length l and\\nSkip-gram(V, \\xe2\\x84\\x93vi , w) function is de\\xef\\xac\\x81ned in Algorithm 8.2, where \\xce\\xb1l is the learning\\nrate of stochastic gradient descent.\\nNote that the parameter updating rule V = V \\xe2\\x88\\x92\\xce\\xb1l \\xe2\\x88\\x82J\\n\\xe2\\x88\\x82V in Skip-gram has a com-\\nplexity of O(|V |) because in the computation of the gradient of P(vk|v j) (as shown\\nin Eq.8.21), the denominator has |V | terms to compute. This complexity is unac-\\nceptable for large-scale networks.\\n8.2 Network Representation\\n225\\nAlgorithm 8.2 Skip-gram(R, Wvi , w)\\nfor v j \\xe2\\x88\\x88\\xe2\\x84\\x93vi do\\nfor vk \\xe2\\x88\\x88\\xe2\\x84\\x93vi [ j \\xe2\\x88\\x92w : j + w] do\\nif vk \\xcc\\xb8= v j then\\nJ(V) = \\xe2\\x88\\x92log P(vk|V j)\\nV = V \\xe2\\x88\\x92\\xce\\xb1l \\xe2\\x88\\x82J\\n\\xe2\\x88\\x82V\\nend if\\nend for\\nend for\\nTable 8.2 Analogy of DeepWalk and word2vec\\nMethod\\nObject\\nInput\\nOutput\\nWord2vec\\nWord\\nSentence\\nWord embedding\\nDeepWalk\\nVertex\\nRandom walk\\nVertex embedding\\nTo address this problem, people proposed Hierarchical Softmax as a variant of\\noriginal softmax function. The core idea is to map the vertices to a balanced binary\\ntree, where each vertex corresponds to a leaf of the tree. Then the prediction of a\\nvertex turns to the prediction of the path from the root to the corresponding leaf.\\nAssume that the path from root to vertex vk is denoted by a sequence of tree nodes\\nb1, b2 . . . , b\\xe2\\x8c\\x88log |V |\\xe2\\x8c\\x89and then we have\\nlog P(vk|v j) =\\n\\xe2\\x8c\\x88log |V |\\xe2\\x8c\\x89\\n\\x02\\ni=1\\nlog P(bi|v j).\\n(8.22)\\nA logistic function can easily implement a binary decision on a tree node. Hence,\\nthe time complexity reduces to O(log |V |) from O(|V |). We can accelerate the\\nalgorithm by using Huffman coding to map frequent vertices to the tree nodes that\\nare close to the root. We can also use negative sampling which is used in word2vec\\nto replace hierarchical softmax for speeding up.\\nSo far, we have \\xef\\xac\\x81nished the introduction of the DeepWalk algorithm. Deep-\\nWalk introduces ef\\xef\\xac\\x81cient deep learning techniques into network embedding learning.\\nTable8.2 gives an analogy between DeepWalk and Word2vec. DeepWalk outper-\\nforms traditional network representation learning methods on network classi\\xef\\xac\\x81cation\\ntasks and is also ef\\xef\\xac\\x81cient for large-scale networks. Besides, the generation of random\\nwalks can be generalized to nonrandom walk, such as the information propagation\\nstreams. In the next subsection, we will give a detailed proof to demonstrate the\\ncorrelation between DeepWalk and matrix factorization.\\n226\\n8\\nNetwork Representation\\n8.2.2.1\\nMatrix Factorization Comprehension of DeepWalk\\nPerozzi et al. introduced the Skip-gram model into the study of social network for\\nthe \\xef\\xac\\x81rst time, and designed an algorithm named DeepWalk [93] for learning vertex\\nrepresentation on a graph. In this subsection, we prove that the DeepWalk algorithm\\nwith Skip-gram and softmax model is actually factoring a matrix M where each\\nentry Mi j is the logarithm of the average probability that vertex vi randomly walks\\nto vertex v j in \\xef\\xac\\x81x steps. We will explain it later.\\nSince the Skip-gram model does not consider the offset of context vertex and\\npredict context vertices independently, we can regard the random walks as a set of\\nvertex-context pairs. The useful information on random walks is the co-occurrence\\nof vertex pairs inside a window. Given network G = (V, E), we suppose that vertex-\\ncontext set D is generated from random walks, where each piece of D is a vertex-\\ncontext pair (v, c). Let V be the set of nodes, and VC be the set of context nodes. In\\nmost cases, V = VC.\\nConsider a vertex-context pair (v, c):\\nN(v,c) denotes the number of times (v, c) appears in D. Nv = \\x04\\nc\\xe2\\x80\\xb2\\xe2\\x88\\x88VC N(v,c\\xe2\\x80\\xb2) and\\nNc = \\x04\\nv\\xe2\\x80\\xb2\\xe2\\x88\\x88V N(v\\xe2\\x80\\xb2,c) denotes the number of times v and c appears in D. Note that\\n|D| = \\x04\\nv\\xe2\\x80\\xb2\\xe2\\x88\\x88V\\n\\x04\\nc\\xe2\\x80\\xb2\\xe2\\x88\\x88VC N(v\\xe2\\x80\\xb2,c\\xe2\\x80\\xb2).\\nA context vertex c \\xe2\\x88\\x88VC is represented by a d-dimension vector c \\xe2\\x88\\x88Rd and C\\nis a |VC| \\xc3\\x97 d matrix, where row j is vector cj. Our goal is to \\xef\\xac\\x81gure out a matrix\\nM = VC\\xe2\\x8a\\xa4.\\nPerozzi et al. implemented the DeepWalk algorithm with the Skip-gram and Hier-\\narchical Softmax model. Note that Hierarchical Softmax is a variant of softmax for\\nspeeding the training time. In this subsection, we give proofs for both negative sam-\\npling and softmax with the Skip-gram model.\\nNegative sampling approximately maximizes the probability of softmax function\\nby randomly choosing k negative samples from the context set. Levy and Goldberg\\nshowed that Skip-gram with the Negative Sampling model (SGNS) is implicitly fac-\\ntorizing a word-context matrix [69] by assuming that dimensionality d is suf\\xef\\xac\\x81ciently\\nlarge. In other words, we can assign each product v \\xc2\\xb7 c a value independent of the\\nothers.\\nIn SGNS model, we have\\nP((v, c) \\xe2\\x88\\x88D) = Sigmoid(v \\xc2\\xb7 c) =\\n1\\n1 + e\\xe2\\x88\\x92v\\xc2\\xb7c .\\n(8.23)\\nSuppose we choose k negative samples for each vertex-context pair (v, c) accord-\\ning to the distribution PD(cN) =\\nNcN\\n|D| . Then, the objective function for SGNS can be\\nwritten as\\n8.2 Network Representation\\n227\\nO =\\n\\x02\\nv\\xe2\\x88\\x88V\\n\\x02\\nc\\xe2\\x88\\x88VC\\nN(v,c)(log Sigmoid(v \\xc2\\xb7 c) + kEcN \\xe2\\x88\\xbcPD[log Sigmoid(\\xe2\\x88\\x92v \\xc2\\xb7 c)])\\n=\\n\\x02\\nv\\xe2\\x88\\x88V\\n\\x02\\nc\\xe2\\x88\\x88VC\\nN(v,c) log Sigmoid(v \\xc2\\xb7 c) + k\\n\\x02\\nv\\xe2\\x88\\x88V\\nNv\\n\\x02\\ncN \\xe2\\x88\\x88VC\\nNcN\\n|D| log Sigmoid(\\xe2\\x88\\x92v \\xc2\\xb7 c)\\n=\\n\\x02\\nv\\xe2\\x88\\x88V\\n\\x02\\nc\\xe2\\x88\\x88VC\\nN(v,c) log Sigmoid(v \\xc2\\xb7 c) + kNv\\nNc\\n|D| log Sigmoid(\\xe2\\x88\\x92v \\xc2\\xb7 c).\\n(8.24)\\nDenote x = v \\xc2\\xb7 c. By solving \\xe2\\x88\\x82O\\n\\xe2\\x88\\x82x = 0, we have\\nv \\xc2\\xb7 c = x = log N(v,c)|D|\\nNvNc\\n\\xe2\\x88\\x92log k.\\n(8.25)\\nThus we have Mi j = log\\nN(vi ,c j )\\n|D|\\nNvi\\n|D|\\nNc j\\n|D|\\n\\xe2\\x88\\x92log k. Mi j can be interpreted as Point-wise\\nMutual Information(PMI) of vertex-context pair (vi, c j) shifted by log k.\\nSince both negative sampling and hierarchical softmax are variants of softmax,\\nwe pay more attention to the softmax model and give a further discussion on it. We\\nalso assume that the values of v \\xc2\\xb7 c are independent.\\nIn softmax model,\\nP((v, c) \\xe2\\x88\\x88D) =\\nev\\xc2\\xb7c\\n\\x04\\nc\\xe2\\x80\\xb2\\xe2\\x88\\x88VC ev\\xc2\\xb7c\\xe2\\x80\\xb2 .\\n(8.26)\\nAnd the objective function is\\nO =\\n\\x02\\nv\\xe2\\x88\\x88V\\n\\x02\\nc\\xe2\\x88\\x88VC\\nN(v,c) log\\nev\\xc2\\xb7c\\n\\x04\\nc\\xe2\\x80\\xb2\\xe2\\x88\\x88VC ev\\xc2\\xb7c\\xe2\\x80\\xb2 .\\n(8.27)\\nAfter extracting all terms associated to v \\xc2\\xb7 c as O(v, c), we have\\nO(v, c) = N(v,c) log\\nev\\xc2\\xb7c\\n\\x04\\nc\\xe2\\x80\\xb2\\xe2\\x88\\x88VC,c\\xe2\\x80\\xb2\\xcc\\xb8=c ev\\xc2\\xb7c\\xe2\\x80\\xb2 + ev\\xc2\\xb7c +\\n\\x02\\n\\xcb\\x9cc\\xe2\\x88\\x88VC,\\xcb\\x9cc\\xcc\\xb8=c\\nN(v,\\xcb\\x9cc) log\\nev\\xc2\\xb7\\xcb\\x9cc\\n\\x04\\nc\\xe2\\x80\\xb2\\xe2\\x88\\x88VC,c\\xe2\\x80\\xb2\\xcc\\xb8=c ev\\xc2\\xb7c\\xe2\\x80\\xb2 + ev\\xc2\\xb7c .\\n(8.28)\\nNote that O =\\n1\\n|VC|\\n\\x04\\nv\\xe2\\x88\\x88V\\n\\x04\\nc\\xe2\\x88\\x88VC O(v, c). Denote x = v \\xc2\\xb7 c. By solving \\xe2\\x88\\x82O\\n\\xe2\\x88\\x82x = 0\\nfor all such x, we have\\nv \\xc2\\xb7 c = x = log N(v,c)\\nNv\\n+ bv,\\n(8.29)\\nwhere bv can be any real constant since it will be canceled when we compute\\nP((v, c) \\xe2\\x88\\x88D). Thus, we have Mi j = log\\nN(vi ,c j )\\nN(vi ) + bvi. We will discuss what Mi j\\nrepresents in next section.\\n228\\n8\\nNetwork Representation\\nIt is clear that the method of sampling vertex-context pairs, i.e., random walks\\ngeneration, will affect matrix M. In this section, we will discuss Nv\\n|D|,\\nNc\\n|D| and N(v,c)\\nNv\\nbased on an ideal sampling method for DeepWalk algorithm.\\nAssume the graph is connected and undirected, and the window size is w. The\\nsampling algorithm is illustrated in Algorithm 8.3. We can easily generalize this\\nsampling method to the directed graph by only adding (RWi, RW j) into D.\\nAlgorithm 8.3 Ideal vertex-context pair sampling algorithm\\nGenerate an in\\xef\\xac\\x81nite long random walk \\xe2\\x84\\x93.\\nDenote \\xe2\\x84\\x93i as the vertex on position i of \\xe2\\x84\\x93, where i = 0, 1, 2, . . .\\nfor i = 0, 1, 2, . . . do\\nfor j \\xe2\\x88\\x88[i + 1, i + w] do\\nadd (\\xe2\\x84\\x93i, \\xe2\\x84\\x93j) into D\\nadd (\\xe2\\x84\\x93j, \\xe2\\x84\\x93i) into D\\nend for\\nend for\\nEach appearance of vertex i will be recorded 2w times in D for undirected graph\\nand w times for directed graph. Thus, we can \\xef\\xac\\x81gure out that\\nNvi\\n|D| is the frequency of\\nvi that appears in the random walk, which is exactly the PageRank value of vi. Also\\nnote that\\nN(vi ,v j )\\nNvi /2w is the expectation times that v j is observed in left/right w neighbors\\nof vi.\\nDenote the transition matrix in PageRank algorithm be P. More formally, let\\ndeg(vi) be the degree of vertex i. Pi j =\\n1\\ndeg(vi) if (i, j) \\xe2\\x88\\x88E and Pi j = 0 otherwise.\\nWe use ei to denote a |V |-dimension row vector, where all entries are zero except\\nthe ith entry is 1.\\nSuppose that we start a random walk from vertex i and use ei to denote the\\ninitial state. Then eiP is the distribution over all the vertices where jth entry is the\\nprobability that vertex vi walks to vertex v j. Hence, jth entry of eiPw is the probability\\nthat vertex vi walks to vertex v j at exactly w steps. Thus [ei(P + P2 + \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 + Pw)] j\\nis the expectation times that v j appears in right w neighbors of vi.\\nHence\\nN(vi,v j)\\nNvi/2w = 2[ei(P + P2 + \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 + Pw)] j,\\nN(vi,v j)\\nNvi\\n= [ei(P + P2 + \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 + Pw)] j\\nw\\n.\\n(8.30)\\nThis equality also holds for a directed graph.\\nBy setting bvi = log 2w for all i, Mi j = log\\nN(vi ,v j )\\nNvi /2w is logarithm of the expectation\\ntimes that v j appears in left/right w neighbors of vi.\\nBy setting bvi = 0 for all i, Mi j = log\\nN(vi ,v j )\\nNvi\\n= log [ei(A+A2+\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7+Aw)] j\\nw\\nis logarithm\\nof the average probability that vertex vi randomly walks to vertex v j in w steps.\\n8.2 Network Representation\\n229\\n8.2.2.2\\nDiscussion\\nSo far we have seen many different network representation learning algorithms and\\nwe can \\xef\\xac\\x81gure out some patterns that how network representation methods share.\\nThen we will move forward and see how these patterns match some recent network\\nembedding algorithms.\\nMost network representation algorithms try to reconstruct a data matrix generated\\nfrom the graph with vertex embeddings. The simplest matrix would be the adjacency\\nmatrix. However, recovering the adjacency matrix may not be the best choice. First,\\nreal-world networks are mostly very sparse which means O(|E|) = O(|V |). There-\\nfore, the adjacency matrix will be very sparse as well. Though the sparseness enables\\nan ef\\xef\\xac\\x81cient algorithm, it can harm the performance of vertex representation learning\\nbecause of the de\\xef\\xac\\x81ciency of useful information. Second, the adjacency matrix may\\nbe noisy and sensitive. A single missing link can completely change the correlation\\nbetween two vertices.\\nHence people seek to \\xef\\xac\\x81nd an alternative matrix to replace the adjacency matrix\\nthough implicitly. Take DeepWalk as an example, DeepWalk models the following\\nmatrix based on matrix factorization comprehension of DeepWalk:\\nM = P + P2 + \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 + Pw,\\n(8.31)\\nwhere\\nPi j =\\n\\x07\\n1/deg(vi)\\nif (vi, v j) \\xe2\\x88\\x88E,\\n0\\notherwise.\\n(8.32)\\nCompared with the adjacency matrix A, the matrix M modeled by DeepWalk is\\nmuch denser. Furthermore, the window size parameter w can adjust the density: a\\nlarger window size models a denser matrix but will slow down the algorithm. Hence,\\nthe window size w works as a harmonic factor to balance ef\\xef\\xac\\x81ciency and effectiveness.\\nOn the other hand, the matrix M can alleviate the noises in the adjacency matrix.\\nConsider two similar vertices vi and v j, even though the edge between them is\\nmissing, they can still have many co-occurrences by appearing inside a window size\\nof the same random walks.\\nIn a real-world application, direct computation of M may have a high time com-\\nplexity when window size w grows. Thus, it is essential to choose a proper w. How-\\never, window size w is a discrete parameter, and thus the matrix M may grow from\\ntoo sparse to too dense by changing w by 1. Here, we can see another bene\\xef\\xac\\x81t of\\nrandom walks. Random walks used by DeepWalk serve as Monte Carlo simulations\\nfor approximating matrix M. The more random walks you walk, the more likely you\\ncan approximate the matrix.\\nAfter we choose a matrix to model, we need to correlate the matrix entry with\\nvertex representations pairs. There are two widely used measurements of vertices\\npairs: Euclidean distance and inner product. Assume that we want to model the entry\\nMi j given vertex representations vi and v j, we can employ\\n230\\n8\\nNetwork Representation\\nMi j = f (\\xe2\\x88\\xa5vi \\xe2\\x88\\x92v j\\xe2\\x88\\xa52),\\nMi j = f (vi \\xc2\\xb7 v j),\\n(8.33)\\nwhere function f can be any reasonable matching functions such as sigmoid function\\nor linear function for our propose. Actually, the inner product vi \\xc2\\xb7 v j is used more\\nwidely and would correspond to equivalent matrix factorization methods.\\nThe next phase is to design a proper loss function between Mi j and f (vi \\xc2\\xb7 v j).\\nSeveral loss functions such as square loss and hinge loss can be employed. You can\\nalso design a generative model and maximize the likelihood of matrix M.\\nThe \\xef\\xac\\x81nal step of a network representation learning algorithm would be parameter\\nlearning. The most frequently used parameter learning method would be Stochastic\\nGradient Descent (SGD). Other variants of SGD such as AdaGrad and AdaDelta can\\nmake the learning phase converge faster. In the next subsection, we will see some\\nrecent network representation learning algorithms which follow DeepWalk. We will\\n\\xef\\xac\\x81nd that their models can match all these phases above and have some innovations\\non building matrix M, modifying function f , and changing loss function.\\n8.2.3\\nMatrix Factorization Based Methods\\nWe will focus on two network representation learning algorithms LINE and GraRep\\n[13, 111] in this subsection. They both follow the framework introduced in the last\\nsubsection.\\n8.2.3.1\\nLINE\\nTang et al. [111] proposed a network embedding model named as LINE. LINE algo-\\nrithm can handle large-scale networks with arbitrary types: (un)directed or weighted.\\nTo model the interaction between vertices, LINE models \\xef\\xac\\x81rst-order proximity which\\nis represented by observed links and second-order proximity which is determined by\\nshared neighbors but not links between vertices.\\nBefore we introduce the details of the algorithm, we can move one step back\\nand see how the idea works. The modeling of \\xef\\xac\\x81rst-order proximity, i.e., observed\\nlinks, is the modeling of the adjacency matrix. As we said in the last subsection,\\nthe adjacency matrix is usually too sparse. Hence the modeling of second-order\\nproximity, i.e., vertices with shared neighbors, can serve as complement information\\nto enrich the adjacency matrix and make it denser. The enumeration of all vertex\\npairs which have common neighbors is time consuming. Thus, it is necessary to\\ndesign a sampling phase to handle large-scale networks. The sampling phase works\\nlike Monte Carlo simulation to approximate the ideal matrix.\\n8.2 Network Representation\\n231\\nNow we only have two questions: how to de\\xef\\xac\\x81ne \\xef\\xac\\x81rst-order and second-order\\nproximity and how to de\\xef\\xac\\x81ne the loss function. In other words, it is equal to how to\\nde\\xef\\xac\\x81ne M and loss function.\\nFirst-order proximity between vertex u and v is de\\xef\\xac\\x81ned as the weight wuv on\\nedge (u, v). If there is no edge between vertex u and v, then the \\xef\\xac\\x81rst-order proximity\\nbetween them is 0.\\nSecond-order proximity between vertex u and v is de\\xef\\xac\\x81ned as the similarity\\nbetween their neighborhood network. Let pu = (wu,1, . . . , wu,|V |) denote the \\xef\\xac\\x81rst-\\norder proximity between vertex u and all other vertices. Then the second-order prox-\\nimity between u and v is de\\xef\\xac\\x81ned as the similarity of pu and pv. If they have no shared\\nneighbors, then the second-order proximity is zero.\\nThen we can introduce LINE model more speci\\xef\\xac\\x81cally. The joint probability\\nbetween vi and v j is\\np1(vi, v j) =\\n1\\n1 + exp(\\xe2\\x88\\x92vi \\xc2\\xb7 v j),\\n(8.34)\\nwhere vi and v j are d-dimensional row vectors which indicate the representations\\nof vertex vi and v j.\\nTo supervise the probabilities, empirical probability is de\\xef\\xac\\x81ned as \\xcb\\x86p1(i, j) = wi j\\nW ,\\nwhere W = \\x04\\n(vi,v j)\\xe2\\x88\\x88E wi j. Thus our goal is to \\xef\\xac\\x81nd vertex embeddings to approximate\\nwi j\\nW with\\n1\\n1+exp(\\xe2\\x88\\x92vi\\xc2\\xb7v j). Following the idea in last subsection, it is equivalent to say\\nvi \\xc2\\xb7 v j = Mi j = \\xe2\\x88\\x92log( W\\nwi j \\xe2\\x88\\x921).\\nThe loss function between joint probability p1 and its empirical probability \\xcb\\x86p1 is\\nL1 = DKL( \\xcb\\x86p1 || p1),\\n(8.35)\\nwhere DKL(\\xc2\\xb7 || \\xc2\\xb7) is KL-divergence of two probability distributions.\\nOn the other hand, we de\\xef\\xac\\x81ne the probability that vertex v j appears in vi\\xe2\\x80\\x99s context:\\np2(v j|vi) =\\nexp(c j \\xc2\\xb7 vi)\\n\\x04|V |\\nk=1 exp(ck \\xc2\\xb7 vi)\\n.\\n(8.36)\\nSimilarly, the empirical probability is de\\xef\\xac\\x81ned as \\xcb\\x86p2(v j|vi) = wi j\\ndi\\nwhere di =\\n\\x04\\nk wik and the loss function is\\nL2 =\\n\\x02\\ni\\ndi DKL( \\xcb\\x86p2(\\xc2\\xb7, vi) || p2(\\xc2\\xb7, vi)).\\n(8.37)\\nThe \\xef\\xac\\x81rst-order and second-order proximity embeddings are trained separately,\\nand we concatenate the embeddings together after the training phase as vertex rep-\\nresentations.\\n232\\n8\\nNetwork Representation\\n8.2.3.2\\nGraRep\\nNow we turn to another network representation learning algorithm, GraRep, which\\ndirectly follows the proof of matrix factorization form of DeepWalk. Recall that\\nwe prove DeepWalk is actually factorizing a matrix M where M = log A+A2+\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7+Aw\\nw\\n.\\nGraRep algorithm can be divided into 3 steps:\\n\\xe2\\x80\\xa2 Get k-step transition probability matrix Ak for each k = 1, 2, . . . , K.\\n\\xe2\\x80\\xa2 Get each k-step representation.\\n\\xe2\\x80\\xa2 Concatenate all k-step representations.\\nGraRep uses a simple idea, i.e., SVD decomposition on Ak, in the second step to\\nget embeddings. As K gets large, the matrix M gets denser and thus outputs a better\\nrepresentation. However, this algorithm is not very ef\\xef\\xac\\x81cient especially when K gets\\nlarge.\\n8.2.4\\nStructural Deep Network Methods\\nDifferent from previous methods that use a shallow neural network model to char-\\nacterize the network representations, Structural Deep Network Embedding (SDNE)\\n[125] employs the deeper neural model to model the nonlinearity between vertex\\nembeddings. As shown in Fig.8.2, the whole model can be divided into two parts:\\n(1) the \\xef\\xac\\x81rst part is supervised by Laplacian Eigenmaps, which models the \\xef\\xac\\x81rst-order\\nproximity; (2) the second part is unsupervised deep neural autoencoder which char-\\nacterizes the second-order proximity. Finally, the algorithm takes the intermediate\\nlayer which is used for the supervised part as the network representation.\\nFirst, we will give a brief introduction to deep neural autoencoder. A neural\\nautoencoder requires that the output vector should be as similar to the input vector.\\nGenerally speaking, the output cannot be the same with the input vector because\\nthe dimension of intermediate layers of the autoencoder is much smaller than that\\nof the input and output layer. That is to say, a deep autoencoder \\xef\\xac\\x81rst compresses\\nthe input into a low-dimensional intermediate vector and then tries to reconstruct\\nthe original input vector from the low-dimensional intermediate vector. Once the\\ndeep autoencoder is trained, we can say that the intermediate layer is an excellent\\nlow-dimensional representation of the original inputs since we can recover the input\\nvector from it.\\nMore formally, we assume the input vector is xi. Then the hidden representation\\nof each layer is de\\xef\\xac\\x81ned as\\ny(1)\\ni\\n= Sigmoid(W(1)xi + b(1)),\\ny(k)\\ni\\n= Sigmoid(W(k)y(k\\xe2\\x88\\x921)\\ni\\n+ b(k)), k = 2, 3 . . . ,\\n(8.38)\\n8.2 Network Representation\\n233\\nUnsupervised Component\\nLocal structure preserved cost\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nUnsupervised Component\\nLocal structure preserved cost\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nxi\\nyi\\n(1)\\nyi\\n(k)\\nyi\\n(1)\\nxi\\n^\\n^\\nxj\\nyj\\n(1)\\nyj\\n(k)\\nyj\\n(1)\\nxj\\n^\\n^\\nLaplacian\\nEigenmaps\\nparameter sharing\\nparameter sharing\\nSupervised Component\\n(Global structure\\npreserved cost)\\nVertex i\\nVertex j\\nFig. 8.2 The architecture of structural deep network embedding model\\nwhere W(k) and b(k) are weighted matrix and bias vector of kth layer. We assume\\nthat the hidden representation of the Kth layer has the minimum dimension. After\\nobtaining y(K)\\ni\\n, we can get the output \\xcb\\x86xi by reversing the calculation process. Then the\\noptimization objective of autoencoder is to minimize the difference between input\\nvector xi and output vector \\xcb\\x86xi:\\nL (W, b) =\\nn\\n\\x02\\ni=1\\n\\xe2\\x88\\xa5\\xcb\\x86xi \\xe2\\x88\\x92xi\\xe2\\x88\\xa52,\\n(8.39)\\nwhere n is the number of input instances.\\nBack to the network representation problem, SDNE applies the autoencoder to\\nevery vertex. The input vector xi of each vertex vi is de\\xef\\xac\\x81ned as follows: if vertex vi\\nand v j are connected, then the jth entry xi j > 0, otherwise xi j = 0. For unweighed\\ngraph, if vertex (vi, v j) \\xe2\\x88\\x88E, xi j = 1. Then the intermediate layer y(K)\\ni\\ncan be seen\\nas the low-dimension representation of vertex vi. Also note that there are much more\\nzero entries in input vectors than positive entries due to the sparity of real-world\\nnetwork. Therefore, the loss of positive entries should be emphasized. Therefore, the\\n\\xef\\xac\\x81nal optimization objective of second proximity modeling can be written as\\nL2nd =\\n|V |\\n\\x02\\ni=1\\n\\xe2\\x88\\xa5(\\xcb\\x86xi \\xe2\\x88\\x92xi) \\xe2\\x8a\\x99bi\\xe2\\x88\\xa52,\\n(8.40)\\n234\\n8\\nNetwork Representation\\nwhere \\xe2\\x8a\\x99denotes element-wise multiplication and bi j = 1 if xi j = 0 while bi j =\\n\\xce\\xb2 > 1 if xi j > 0.\\nWe have introduced the unsupervised part modeled by deep autoencoder. Now we\\nturn to the supervised part. The supervised part simply requires that the representation\\nof connected vertices should be close to each other. Thus, the loss function of this\\npart is\\nL1st =\\n|V |\\n\\x02\\ni, j=1\\nxi j\\xe2\\x88\\xa5y(K)\\ni\\n\\xe2\\x88\\x92y(K)\\nj\\n\\xe2\\x88\\xa52.\\n(8.41)\\nFinally, the overall loss function included regularization term is\\nL = L2nd + \\xce\\xb1L1st + \\xce\\xbbLreg,\\n(8.42)\\nwhere \\xce\\xb1 and \\xce\\xbb are harmonic hyperparameter and regularization loss Lreg is the sum\\nof the square of all parameters. The model can be optimized by back-propagation\\nin a standard neural network way. After the training process, y(K)\\ni\\nis taken as the\\nrepresentation of vertex vi.\\n8.2.5\\nExtensions\\n8.2.5.1\\nNetwork Representation with Internal Information\\nAsymmetric Transitivity Preserving Network Representation. Existing network\\nrepresentation learning algorithms mostly focus on an undirected graph. Most of\\nthe methods cannot handle the directed graph well because they do not accurately\\ncharacterize the asymmetric property. High-Order Proximity preserved Embedding\\n(HOPE) [89] is proposed to preserve high-order proximities of large-scale graphs and\\ncapture the asymmetric transitivity. The algorithm further derives a general formula-\\ntion that covers multiple popular high-order proximity measurements and provides\\nan approximate algorithm with an upper bound of RMSE (Root Mean Squared Error).\\nNetwork embedding assumes that the more and the shorter paths from vi to v j,\\nthe more similar should be their representation vectors. In particular, the algorithm\\nassigns two vectors, i.e., source and target vectors for each vertex. We denote adja-\\ncency matrix as A and the user representations as U = [Us, Ut], where Us \\xe2\\x88\\x88R|V |\\xc3\\x97d\\nand Ut \\xe2\\x88\\x88R|V |\\xc3\\x97d are source and target vertex embeddings, respectively. We de\\xef\\xac\\x81ne\\na high-order proximity matrix as S, where Si j is the proximity between vi and v j.\\nThen our goal is to approximate the matrix S with the product of Us and Ut. The\\noptimization objective can be written as\\nmin\\nUs,Ut \\xe2\\x88\\xa5S \\xe2\\x88\\x92UsUt \\xe2\\x8a\\xa4\\xe2\\x88\\xa52\\nF.\\n(8.43)\\n8.2 Network Representation\\n235\\nMany high-order proximity measurements which characterize the asymmetric\\ntransitivity share a general formulation which can be used for the approximation of\\nthe proximities:\\nS = M\\xe2\\x88\\x921\\ng Ml,\\n(8.44)\\nwhere Mg and Ml are both polynomials of matrices. Now we will take three com-\\nmonly used high-order proximity measurements to illustrate the formula.\\n\\xe2\\x80\\xa2 Katz Index Katz Index is a weighted summation over the path set between two\\nvertices. The computation of the Katz Index can be written recurrently:\\nS := \\xce\\xb2 AS + \\xce\\xb2 A,\\n(8.45)\\nwhere the decay parameter \\xce\\xb2 represents how fast the weight decreases when the\\nlength of paths grows.\\n\\xe2\\x80\\xa2 Rooted PageRank For rooted PageRank, Si j is the probability that a random walk\\nfrom vertex vi will locate at v j in the stable state. The formula can be written as\\nS := \\xce\\xb1SP + (1 \\xe2\\x88\\x92\\xce\\xb1)I,\\n(8.46)\\nwhere \\xce\\xb1 is the probability that a random walk returns to its start point and P is the\\ntransition matrix.\\n\\xe2\\x80\\xa2 Common Neighbors Si j is the number of vertexes which is the target of an edge\\nfrom vi and the source of an edge to v j. The matrix S can be expressed as\\nS = A2.\\n(8.47)\\nFor the three high-order proximity measurements introduced above, we summa-\\nrize their equivalent form S = M\\xe2\\x88\\x921\\ng Ml in the following table (Table8.3).\\nA simple idea of approximating S with the product of matrices is SVD decom-\\nposition. However, the direct computation of SVD decomposition of matrix S has a\\ncomplexity of O(|V |3). By writing matrix S as M\\xe2\\x88\\x921\\ng Ml, we do not need to compute\\nmatrix S directly. Instead, we can do JDGSVD decomposition on Mg and Ml inde-\\npendently and then use their results to derive the decomposition of S. The complexity\\nreduces to |E|d2 for each iteration of JDGSVD.\\nCommunity Preserving Network Representation. While previous methods aim\\nat preserving the microscopic structure of a network such as \\xef\\xac\\x81rst- and second-order\\nTable 8.3 General formula for high-order proximity measurements\\nMeasurement\\nMg\\nMl\\nKatz index\\nI \\xe2\\x88\\x92\\xce\\xb2 A\\n\\xce\\xb2 A\\nRooted PageRank\\nI \\xe2\\x88\\x92\\xce\\xb1P\\n(1 \\xe2\\x88\\x92\\xce\\xb1)I\\nCommon neighbors\\nI\\nA2\\n236\\n8\\nNetwork Representation\\nproximities. Wang et al. [127] proposed Modularized Nonnegative Matrix Factor-\\nization (M-NMF), which encodes the mesoscopic community structure information\\ninto the network representations. The basic idea is to consider the modularity as part\\nof the optimization function. Recall that the modularity is formulated in Eq.8.15 and\\nS is the community indicator matrix. Then the loss function of modularity part is to\\nminimize \\xe2\\x88\\x92tr(S\\xe2\\x8a\\xa4BS).\\nSimilar to previous methods, M-NMF also factorizes an af\\xef\\xac\\x81nity matrix which\\nencodes \\xef\\xac\\x81rst-order and second-order proximities. Speci\\xef\\xac\\x81cally, M-NMF takes adja-\\ncency matrix A as the \\xef\\xac\\x81rst-order proximity matrix A1 and computes the cosine\\nsimilarity of corresponding rows of adjacency matrix A as the second-order prox-\\nimity matrix A2. M-NMF uses a mixture of A1 and A2 as the similarity matrix. To\\nconclude, the overall optimization function of M-NMF is\\nmin\\nM,U,S,C\\n\\x03\\x03A1 + \\xce\\xb7A2 \\xe2\\x88\\x92MU\\xe2\\x8a\\xa4\\x03\\x032\\nF + \\xce\\xb1\\n\\x03\\x03S \\xe2\\x88\\x92UC\\xe2\\x8a\\xa4\\x03\\x032\\nF \\xe2\\x88\\x92\\xce\\xb2tr(S\\xe2\\x8a\\xa4BS),\\n(8.48)\\nwhereS \\xe2\\x88\\x88R|V |\\xc3\\x97k, M, U\\xe2\\x88\\x88R|V |\\xc3\\x97m, C\\xe2\\x88\\x88Rk\\xc3\\x97m, Mi j, Ui j, Si j, Ci j \\xe2\\x89\\xa50, \\xe2\\x88\\x80i\\xe2\\x88\\x80j, tr(S\\xe2\\x8a\\xa4S) =\\n|V | and \\xce\\xb1, \\xce\\xb2, \\xce\\xb7 > 0 are harmonic hyperparameters. Subscript F denotes Frobenius\\nnorm. Here similarity matrix A1 + \\xce\\xb7A2 is factorized into two nonnegative matrices\\nM and U. Then community representation matrix C in the second term bridges the\\nmatrix factorization part and the modularity part.\\nA concurrent algorithm Community-enhanced NRL (CNRL) [116, 117] is a\\npipeline algorithm that learns node-community assignment at \\xef\\xac\\x81rst and then reforms\\nthe DeepWalk algorithm to incorporate community information. Speci\\xef\\xac\\x81cally, in the\\n\\xef\\xac\\x81rst phase, CNRL made an analogy between community detection and topic model-\\ning. Then CNRL started by generating random walks and fed these vertex sequences\\ninto Latent Dirichlet Allocation (LDA) algorithm. By taking a vertex as a word and a\\ntopic as a community, CNRL can get a soft-assignment of vertex-community mem-\\nbership. Then in the second phase, both the embedding of a center node and the\\nembedding of its community are used to predict the neighborhood vertices in the\\nrandom walk sequences. The illustration \\xef\\xac\\x81gure is shown in Fig.8.3.\\n8.2.5.2\\nNetwork Representation with External Information\\nNetwork Representation with Text Information. We will present the network\\nembedding algorithm TADW, which further generalizes the matrix factorization\\nframework to take advantage of text information. Text-Associated DeepWalk\\n(TADW) [136] incorporates text features of vertices into network representation\\nlearning under the framework of matrix factorization. The matrix factorization view\\nof DeepWalk enables the introduction of text information into matrix factorization for\\nnetwork representation learning. Figure8.4 shows the main idea of TADW: factorize\\nvertex af\\xef\\xac\\x81nity matrix M \\xe2\\x88\\x88R|V |\\xc3\\x97|V | into the product of three matrices: W \\xe2\\x88\\x88Rk\\xc3\\x97|V |,\\nH \\xe2\\x88\\x88Rk\\xc3\\x97 ft, and text features T \\xe2\\x88\\x88R ft\\xc3\\x97|V |. Then TADW concatenates W and HT as\\n2k-dimensional representations of vertices.\\n8.2 Network Representation\\n237\\nVetex\\nembeddings\\nSliding window\\nCommunity\\nembeddings\\nCommunity\\nembedding\\nAssigned\\ncommunities\\n1\\n1\\n2 \\n2 \\n3 \\n3 \\n4 \\n4 \\n5\\n5\\n6\\n6\\n7\\n7\\nVetex sequence\\nRandom walks\\non a network\\nCommunity 1\\nCommunity 2\\nCommunity 3\\nFig. 8.3 The architecture of community preserving network embedding model\\nFig. 8.4 The architecture of\\ntext-associated DeepWalk\\nmodel\\n\\xc3\\x97\\n\\xc3\\x97\\nV\\nV\\nV\\nk\\nM\\nWT\\nH\\nT\\nft\\nThen the question is how to build vertex af\\xef\\xac\\x81nity matrix M and how to extract\\ntext feature T from the text information. Following the proof of matrix factorization\\nform of DeepWalk, TADW set vertex af\\xef\\xac\\x81nity matrix M to a tradeoff between speed\\nand accuracy: factorize the matrix M = (A + A2)/2 where A is the row-normalized\\nadjacency matrix. For text feature matrix T, TADW \\xef\\xac\\x81rst constructs the TF-IDF matrix\\nfrom the text and then reduces the dimension of the TF-IDF matrix to 200 via SVD\\ndecomposition.\\n238\\n8\\nNetwork Representation\\nReconstructed\\nEdge Vector\\nEdge Autoencoder\\nLabel#2\\nLabel#5\\nu\\nl\\nu\\nv\\nv\\nTranslation\\nMechanism\\nBinary Edge\\nVertor\\nEdge with\\nMulti-labels\\nFig. 8.5 The architecture of TransNet model\\nFormally, the model of TADW minimizes the following optimization function:\\nmin\\nW,H \\xe2\\x88\\xa5M \\xe2\\x88\\x92W\\xe2\\x8a\\xa4HT\\xe2\\x88\\xa52\\nF + \\xce\\xbb\\n2(\\xe2\\x88\\xa5W\\xe2\\x88\\xa52\\nF + \\xe2\\x88\\xa5H\\xe2\\x88\\xa52\\nF),\\n(8.49)\\nwhere \\xce\\xbb is the regularization factor. The optimization of parameters are processed\\nby updating W and H iteratively via conjugate gradient descent.\\nTransNet. Most existing NRL methods neglect the semantic information of edges\\nand simplify the edge as a binary or continuous value. TransNet algorithm [119]\\nconsiders the label information on the edges instead of nodes. In particular, TransNet\\nis based on translation mechanism shown in Fig.8.5.\\nIn the settings of TransNet, each edge has a number of binary labels on it. Then the\\nloss function of TransNet consists of two parts: one part is the translation loss which\\nmeasures the distance between u + e and v where u, e, v stand for the embeddings\\nof head vertex, edge, and tail vertex; another part is the reconstruction loss of the\\nautoencoder which encodes the labels of an edge into its embedding e and restore\\nthe labels from the embedding. After the learning phase, we can compute the edge\\nembedding by subtracting two vertices and use the decoder part of the autoencoder\\nto predict the labels of an unobserved edge.\\n8.2 Network Representation\\n239\\nSemi-supervised Network Representation. In this part, we introduce several\\nsemi-supervised network representation learning methods that are applied to het-\\nerogeneous networks. All methods learn vertex embeddings and their classi\\xef\\xac\\x81cation\\nlabels simultaneously.\\n(1) LSHM The \\xef\\xac\\x81rst algorithm LSHM (Latent Space Heterogeneous Model) [52],\\nfollows the manifold assumption which assumes that two connected nodes tend to\\nhave similar node embeddings. Thus, the regularization loss which forces connected\\nnodes to have similar representations can be formulated as\\n\\x02\\ni, j\\nwi j\\xe2\\x88\\xa5vi \\xe2\\x88\\x92v j\\xe2\\x88\\xa52,\\n(8.50)\\nwhere wi j is the weight of edge (vi, v j).\\nAs a semi-supervised representation learning algorithm, LHSM also needs to\\npredict the classi\\xef\\xac\\x81cation labels for unlabeled vertices. To train the classi\\xef\\xac\\x81ers, LHSM\\ncomputes the loss of observed labels as\\n\\x02\\ni\\n\\xce\\x94( f\\xce\\xb8(vi), yi),\\n(8.51)\\nwhere f\\xce\\xb8(vi) is the predicted label for vertex vi, yi is the observed label for vi and\\n\\xce\\x94(\\xc2\\xb7, \\xc2\\xb7) is the loss function between predicted label and ground truth label. Speci\\xef\\xac\\x81-\\ncally, f\\xce\\xb8(\\xc2\\xb7) is a linear function and \\xce\\x94(\\xc2\\xb7, \\xc2\\xb7) is set to hinge loss.\\nFinally, the objective function is\\nL (V, \\xce\\xb8) =\\n\\x02\\ni\\n\\xce\\x94( f\\xce\\xb8(vi), yi) + \\xce\\xbb\\n\\x02\\ni, j\\nwi j\\xe2\\x88\\xa5vi \\xe2\\x88\\x92v j\\xe2\\x88\\xa52,\\n(8.52)\\nwhere \\xce\\xbb is a harmonic hyperparameter. The algorithm is optimized via stochastic\\ngradient descent.\\n(2) node2vec Node2vec [38] modi\\xef\\xac\\x81es DeepWalk by changing the generation\\nof random walks. As shown in previous subsections, DeepWalk generates rooted\\nrandom walks by choosing the next vertex according to a uniform distribution, which\\ncould be improved by using a well-designed random walk generation strategy.\\nNode2vec \\xef\\xac\\x81rst considers two extreme cases of vertex visiting sequences: Breadth-\\nFirst Search (BFS) and Depth-First Search (DFS). By restricting the search to nearby\\nnodes, BFS characterizes the nearby neighborhoods of center vertices and obtains\\na microscopic view of the neighborhood of every node. Vertices in the sampled\\nneighborhoods of BFS tend to repeat many times, which can reduce the variance in\\ncharacterizing the distribution of neighboring vertices of the source node. In contrast,\\nthesamplednodesinDFSre\\xef\\xac\\x82ectamacro-viewoftheneighborhoodwhichisessential\\nin inferring communities based on homophily.\\nNode2vec designs a neighborhood sampling strategy which can smoothly inter-\\npolate between BFS and DFS. More speci\\xef\\xac\\x81cally, consider a random walk that just\\nwalks through edge (t, v) and now stays at vertex v. The walk evaluates the transition\\n240\\n8\\nNetwork Representation\\nprobabilities of edge (v, x) to decide the next step. Node2vec sets the unnormalized\\ntransition probability to \\xcf\\x80vx = \\xce\\xb1pq(t, x) \\xc2\\xb7 wvx, where\\n\\xce\\xb1pq(t, x) =\\n\\xe2\\x8e\\xa7\\n\\xe2\\x8e\\xa8\\n\\xe2\\x8e\\xa9\\n1\\np if dtx = 0,\\n1 if dtx = 1,\\n1\\nq if dtx = 2,\\n(8.53)\\nand dtx denotes the shortest path distance between vertices t and x. p and q are\\nparameters that guide the random walk and control how fast the walk explores and\\nleaves the neighborhood of starting vertex. A low p will increase the probability of\\nrevisiting a vertex and make the random walk focus on local neighborhoods while a\\nlowq will encourage the random walk to explore further vertices. After the generation\\nof the random walks, the rest of the algorithm is almost the same as that of DeepWalk.\\n(3) MMDW Max-Margin DeepWalk (MMDW) [118] utilizes the max-margin\\nstrategy in SVM to generalize DeepWalk algorithm for semi-supervised learning.\\nSpeci\\xef\\xac\\x81cally, MMDW employs the matrix factorization form of DeepWalk proved\\nin TADW [136] and further add the max-margin constraint which requires that the\\nembeddings of nodes from different labels should be far from each other. The opti-\\nmization function can be written as\\nmin\\nX,Y,W,\\xce\\xbe L =\\nmin\\nX,Y,W,\\xce\\xbe LDW + 1\\n2\\xe2\\x88\\xa5W\\xe2\\x88\\xa52 + C\\nT\\n\\x02\\ni=1\\n\\xce\\xbei,\\ns.t. w\\xe2\\x8a\\xa4\\nli xi \\xe2\\x88\\x92w\\xe2\\x8a\\xa4\\nj xi \\xe2\\x89\\xa5e j\\ni \\xe2\\x88\\x92\\xce\\xbei, \\xe2\\x88\\x80i, j,\\n(8.54)\\nwhere W = [w1, w2, . . . , wm]T is the weight matrix of SVM, \\xce\\xbe is the slack variables,\\ne j\\ni = 1 if li \\xcc\\xb8= j and e j\\ni = 0 otherwise, and LDW is the matrix factorization form\\nDeepWalk loss function:\\nLDW = \\xe2\\x88\\xa5M \\xe2\\x88\\x92X\\xe2\\x8a\\xa4Y\\xe2\\x88\\xa52\\n2 + \\xce\\xbb\\n2(\\xe2\\x88\\xa5X\\xe2\\x88\\xa52\\n2 + \\xe2\\x88\\xa5Y\\xe2\\x88\\xa52\\n2),\\n(8.55)\\nwhich is introduced in previous sections.\\nFigure8.6 shows the visualization result of the DeepWalk and MMDW algorithm\\non the Wiki dataset [103]. We can see that the embeddings of nodes from different\\nclasses are more separable with the help of semi-supervised max-margin represen-\\ntation learning.\\n(4)PTEAnotheralgorithmcalledPTE(PredictiveTextEmbedding)[110]focuses\\non text network such as the bibliography network where a paper is a vertex, and\\nthe citation relationship between papers forms the edges. PTE considers network\\nstructure together with plain text and observed vertex labels. PTE proposes a semi-\\nsupervised framework to learn vertex representation and predict unobserved vertex\\nlabels.\\n8.2 Network Representation\\n241\\nFig. 8.6 A visualization of t-SNE 2D representations on Wiki dataset (left: DeepWalk, right:\\nMMDW) [118]\\nA text network is divided into three bipartite networks: word-word, word-\\ndocument, and word-label networks. We will introduce the de\\xef\\xac\\x81nition of the three\\nnetworks in more detail.\\nFor the word-word network, the weight wi j of the edge between word vi and v j is\\nde\\xef\\xac\\x81ned as the number of times that the two words co-occur in the same context win-\\ndows. For word-document network, the weight wi j between word vi and document\\nd j is de\\xef\\xac\\x81ned as the number of times vi appears in document d j. For the word-label\\nnetwork, the weight wi j of the edge between word vi and class c j is de\\xef\\xac\\x81ned as:\\nwi j = \\x04\\nd:ld= j ndi, where ndi is the term frequency of word vi in document d, and ld\\nis the class label of document d.\\nThen following previous work LINE, given bipartite network G = (VA \\xe2\\x88\\xaaVB, E),\\nthe conditional probability of generating vi \\xe2\\x88\\x88VA from v j \\xe2\\x88\\x88VB is de\\xef\\xac\\x81ned as\\nP(vi|v j) =\\nexp(v j \\xc2\\xb7 vi)\\n\\x04|V |\\nk=1 exp(vk \\xc2\\xb7 vi)\\n.\\n(8.56)\\nSimilar to LINE model, the loss function is de\\xef\\xac\\x81ned as the KL-divergence between\\nempirical distribution and conditional distribution. The optimization objective can\\nbe further formulated as\\nL = \\xe2\\x88\\x92\\n\\x02\\n(vi,v j)\\xe2\\x88\\x88E\\nwi j log P(vi|v j).\\n(8.57)\\n242\\n8\\nNetwork Representation\\nThen the \\xef\\xac\\x81nal objective can be obtained by summing all three bipartite networks:\\nLpte = Lww + Lwd + Lwl,\\n(8.58)\\nwhere\\nLww = \\xe2\\x88\\x92\\n\\x02\\n(vi,v j)\\xe2\\x88\\x88Eww\\nwi j log P(vi|v j),\\n(8.59)\\nLwd = \\xe2\\x88\\x92\\n\\x02\\n(vi,v j)\\xe2\\x88\\x88Ewd\\nwi j log P(vi|d j),\\n(8.60)\\nLwl = \\xe2\\x88\\x92\\n\\x02\\n(vi,v j)\\xe2\\x88\\x88Ewl\\nwi j log P(vi|l j).\\n(8.61)\\nThen the optimization can be done by stochastic gradient descent.\\n8.2.5.3\\nTask-Speci\\xef\\xac\\x81c Network Representation\\nNetwork Representation for Community Detection. As shown in spectral cluster-\\ning methods, people make their effort to learn community indicator matrix based on\\nmodularity and normalized graph cut. The continuous community indicator matrix\\ncan be seen as a k-dimensional vertex representation, where k is the number of com-\\nmunities. Note that modularity and graph cut is de\\xef\\xac\\x81ned for nonoverlapping commu-\\nnities. By alternating a cost function for overlapping communities, the idea can also\\nwork for overlapping community detection. In this subsection, we will introduce sev-\\neral community detection algorithms. These community detection algorithms start\\nby learning a k-dimensional nonnegative vertex-community af\\xef\\xac\\x81nity matrix and then\\nderive a hard community assignment for vertices based on the matrix. Therefore, the\\nkey procedure of these algorithms can be regarded as an unsupervised k-dimensional\\nnonnegative vertex embedding learning.\\nBIGCLAM [140] is an overlapping community detection method. It assumes that\\nmatrix F \\xe2\\x88\\x88R|V |\\xc3\\x97k is the user-community af\\xef\\xac\\x81nity matrix, where Fvc is the strength\\nbetween vertex v and community c. Matrix F is nonnegative and Fvc = 0 indicates\\nno af\\xef\\xac\\x81liation. BIGCLAM builds a generative model by modeling the probability that\\nvertexvi connectsv j givenuser-communityaf\\xef\\xac\\x81nitymatrixF.Morespeci\\xef\\xac\\x81cally,given\\nmatrix F, BIGCLAM generates an edge between vertex vi and v j with a probability\\nP(vi, v j) = 1 \\xe2\\x88\\x92exp(\\xe2\\x88\\x92Fvi \\xc2\\xb7 Fv j),\\n(8.62)\\nwhere Fvi is the corresponding row of matrix F for vertex vi and can be seen as the\\nrepresentation of vi. Note that the probability P(vi, v j) has an increasing relationship\\nwith Fvi \\xc2\\xb7 F\\xe2\\x8a\\xa4\\nv j = \\x04\\nc Fvi,cFv j,c, which indicates that the more communities a pair of\\nnodes shared, the more likely they are connected.\\n8.2 Network Representation\\n243\\nFor the case that Fvi \\xc2\\xb7 Fv j = 0, BIGCLAM adds a background probability \\xcf\\xb5 =\\n2|E|\\n|V |(|V |\\xe2\\x88\\x921) to the pair of nodes to avoid a zero probability.\\nThen BIGCLAM tries to maximize the log-likelihood of the graph G = (V, E):\\nO(F) =\\n\\x02\\ni, j:(vi,v j)\\xe2\\x88\\x88E\\nlog P(vi, v j) +\\n\\x02\\ni, j:(vi,v j)/\\xe2\\x88\\x88E\\nlog(1 \\xe2\\x88\\x92P(vi, v j)),\\n(8.63)\\nwhich can be reformulated as\\nO(F) =\\n\\x02\\ni, j:(vi,v j)\\xe2\\x88\\x88E\\nlog(1 \\xe2\\x88\\x92exp(\\xe2\\x88\\x92Fvi \\xc2\\xb7 Fv j)) \\xe2\\x88\\x92\\n\\x02\\ni, j:(vi,v j)/\\xe2\\x88\\x88E\\nFvi \\xc2\\xb7 Fv j.\\n(8.64)\\nThe parameters F are learned by projected gradient descent. Note that the train-\\ning objective can be regarded as a variant of nonnegative matrix factorization. The\\nmaximization of log-likelihood function is an approximation of adjacency matrix A\\nby FF\\xe2\\x8a\\xa4. Compared with L2-norm loss function, the gradient of Eq.8.64 can be com-\\nputed more ef\\xef\\xac\\x81ciently for a sparse matrix A which is the most case in the real-world\\ndataset.\\nThe model can also be generalized to asymmetric case [141]. That is to replace\\nEq.8.62 by\\nP(vi, v j) = 1 \\xe2\\x88\\x92exp(\\xe2\\x88\\x92Fvi \\xc2\\xb7 Hv j),\\n(8.65)\\nwhere H is another matrix that has the same size with the matrix F. The generative\\nmodel can also consider attributes of vertices by adding attribute terms to Eq.8.62\\n[79].\\n8.2.5.4\\nNetwork Representation for Visualization\\nDifferent from previous algorithms that focus on machine learning tasks, the algo-\\nrithms introduced in this subsection are designed for visualization. As a commonly\\nused data structure, the visualization of networks is an important task. The dimen-\\nsions of representations of vertices are usually 2 or 3 to draw the graph.\\nRepresentation learning for network visualization generally follows the following\\naesthetic criteria [30]:\\n\\xe2\\x80\\xa2 Distribute the vertices evenly in the frame.\\n\\xe2\\x80\\xa2 Minimize edge crossings.\\n\\xe2\\x80\\xa2 Make edge lengths uniform.\\n\\xe2\\x80\\xa2 Re\\xef\\xac\\x82ect inherent symmetry.\\n\\xe2\\x80\\xa2 Conform to the frame.\\nFollowing these criteria, graph visualization algorithms build a force-directed\\ngraph drawing framework. The basic assumption is that there is a spring between\\neach pair of vertices. Then the optimization objective is to minimize the energy of\\nthe graph according to Hooke\\xe2\\x80\\x99s law:\\n244\\n8\\nNetwork Representation\\nE =\\n\\x02\\ni, j\\n1\\n2ki j(\\xe2\\x88\\xa5vi \\xe2\\x88\\x92v j\\xe2\\x88\\xa5\\xe2\\x88\\x92li j)2,\\n(8.66)\\nwhere ki j is spring constant, vi is the position of vertex vi and li j is the length of\\nshortest path between vertex vi and v j. The intuition is straightforward: close vertices\\nshould have close positions in the drawing. Several algorithms have been proposed\\nto improve this framework [34, 54, 60] by changing the setting of spring constant ki j\\nor the energy function. The parameters can be easily learned via gradient descent.\\n8.2.5.5\\nEmbedding Enhancement via High-Order Proximity\\nApproximation\\nYang et al. [137] summarize several existing NRL methods into a uni\\xef\\xac\\x81ed two-step\\nframework, including proximity matrix construction and dimension reduction. They\\nconclude that an NRL method can be improved by exploring higher order proximities\\nwhen building the proximity matrix. Then they propose Network Embedding Update\\n(NEU) algorithm, which implicitly approximates higher order proximities with the-\\noretical approximation bound and can be applied to any NRL methods to enhance\\ntheir performances. NEU can make a consistent and signi\\xef\\xac\\x81cant improvement over\\nsome NRL methods with almost negligible running time.\\nThe two-step framework is summarized as follows:\\nStep 1: Proximity Matrix Construction. Compute a proximity matrix M \\xe2\\x88\\x88\\nR|V |\\xc3\\x97|V |, which encodes the information of k-order proximity matrix where k =\\n1, 2 . . . , K. For example, M = 1\\nK A + 1\\nK A2 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 + 1\\nK AK stands for an average com-\\nbination of k-order proximity matrix for k = 1, 2 . . . , K. The proximity matrix M is\\nusually represented by a polynomial of normalized adjacency matrix A of degree K,\\nand we denote the polynomial as f (A) \\xe2\\x88\\x88R|V |\\xc3\\x97|V |. Here the degree K of polynomial\\nf (A) corresponds to the maximum order of proximities encoded in the proximity\\nmatrix. Note that the storage and computation of proximity matrix M doesn\\xe2\\x80\\x99t nec-\\nessarily take O(|V |2) time because we only need to save and compute the nonzero\\nentries.\\nStep 2: Dimension Reduction. Find network embedding matrix V \\xe2\\x88\\x88R|V |\\xc3\\x97d and\\ncontext embedding C \\xe2\\x88\\x88R|V |\\xc3\\x97d so that the product VC\\xe2\\x8a\\xa4approximates proximity\\nmatrix M. Here different algorithms may employ different distance functions to\\nminimize the distance between M and VC\\xe2\\x8a\\xa4. For example, we can naturally use the\\nnorm of matrix M \\xe2\\x88\\x92VC\\xe2\\x8a\\xa4to measure the distance and minimize it.\\nSpectral Clustering, DeepWalk, and GraRep can be formalized into the two-step\\nframework. Now we focus on the \\xef\\xac\\x81rst step and study how to de\\xef\\xac\\x81ne the right proximity\\nmatrix for NRL.\\nWe summarize the comparisons among Spectral Clustering (SC), DeepWalk, and\\nGraRep in Table8.4 and conclude the following observations.\\n8.2 Network Representation\\n245\\nTable 8.4 Comparisons among three NRL methods\\nSC\\nDeepWalk\\nGraRep\\nProximity matrix\\nL\\n\\x04K\\nk=1\\nAk\\nK\\nAk, k = 1 . . . K\\nComputation\\nAccurate\\nApproximate\\nAccurate\\nScalability\\nYes\\nYes\\nNo\\nPerformance\\nLow\\nMiddle\\nHigh\\nObservation 8.1 Modeling higher order and accurate proximity matrix can improve\\nthe quality of network representation. In other words, NRL can bene\\xef\\xac\\x81t from exploring\\na polynomial proximity matrix f (A) of a higher degree.\\nFrom the development of NRL methods, it can be seen that DeepWalk outperforms\\nSpectral Clustering because DeepWalk considers higher order proximity matrices,\\nand the higher order proximity matrices can provide complementary information for\\nlower order proximity matrices. GraRep outperforms DeepWalk because GraRep\\naccurately calculates the k-order proximity matrix rather than approximating it by\\nMonte Carlo simulation as DeepWalk does.\\nObservation 8.2 Accurate computation of high-order proximity matrix is not fea-\\nsible for large-scale networks.\\nThe major drawback of GraRep is the computation complexity of calculating the\\naccurate k-order proximity matrix. In fact, the computation of high-order proximity\\nmatrix takes O(|V |2) time and the time complexity of SVD decomposition also\\nincreases as k-order proximity matrix gets dense when k grows. In summary, the\\ntime complexity of O(|V |2) is too expensive to handle large-scale networks.\\nThe \\xef\\xac\\x81rst observation provides the motivation to explore higher order proximity\\nmatrices in NRL models, but the second observation indicates that an accurate infer-\\nence of higher order proximity matrices isn\\xe2\\x80\\x99t acceptable. Therefore, how to learn\\nnetwork embeddings from approximate higher order proximity matrices ef\\xef\\xac\\x81ciently\\nbecomes important. To be more ef\\xef\\xac\\x81cient, the network representations which encode\\nthe information of lower order proximity matrices can be used as our basis to avoid\\nrepeated computations. The problem is formalized below.\\nProblem Formalization. Assume that we have normalized adjacency matrix A\\nas the \\xef\\xac\\x81rst-order proximity matrix, network embedding V, and context embedding C,\\nwhere V, C \\xe2\\x88\\x88R|V |\\xc3\\x97d. Suppose that the embeddings V and C are learned by the above\\nNRL framework which indicates that the product VC\\xe2\\x8a\\xa4approximates a polynomial\\nproximity matrix f (A) of degree K. The goal is to learn a better representation V\\xe2\\x80\\xb2\\nand C\\xe2\\x80\\xb2, which approximates a polynomial proximity matrix g(A) with higher degree\\nthan f (A). Also, the algorithm should be ef\\xef\\xac\\x81cient in the linear time of |V |. Note\\nthat the lower bound of time complexity is O(|V |d) which is the size of embedding\\nmatrix R.\\nThere is a simple, ef\\xef\\xac\\x81cient, and effective iterative updating algorithm to solve the\\nabove problem.\\n246\\n8\\nNetwork Representation\\nMethod. Given hyperparameter \\xce\\xbb \\xe2\\x88\\x88(0, 1\\n2], normalized adjacency matrix A, we\\nupdate V and C as follows:\\nV\\xe2\\x80\\xb2 = V + \\xce\\xbbAV,\\nC\\xe2\\x80\\xb2 = C + \\xce\\xbbA\\xe2\\x8a\\xa4C.\\n(8.67)\\nThe time complexity of computing AV and A\\xe2\\x8a\\xa4C is O(|V |d) because matrix A\\nis sparse and has O(|V |) nonzero entries. Thus the overall time complexity of one\\niteration of operation (Eq.8.67) is O(|V |d).\\nRecall that product of previous embedding V and C approximates polynomial\\nproximity matrix f (A) of degree K. It can be proved that the algorithm can learn\\nbetter embeddings V\\xe2\\x80\\xb2 and C\\xe2\\x80\\xb2, where the product V\\xe2\\x80\\xb2C\\xe2\\x80\\xb2\\xe2\\x8a\\xa4approximates a polynomial\\nproximity matrix g(A) of degree K + 2 bounded by matrix in\\xef\\xac\\x81nite norm.\\nTheorem Denote the network and context embedding by V and C, and suppose\\nthat the approximation between VC\\xe2\\x8a\\xa4and proximity matrix M = f (A) is bounded\\nby r = \\xe2\\x88\\xa5f (A) \\xe2\\x88\\x92VC\\xe2\\x8a\\xa4\\xe2\\x88\\xa5\\xe2\\x88\\x9eand f (\\xc2\\xb7) is a polynomial of degree K. Then the product\\nof updated embeddings V\\xe2\\x80\\xb2 and C\\xe2\\x80\\xb2 from Eq.8.67 approximates a polynomial g(A) =\\nf (A) + 2\\xce\\xbbAf (A) + \\xce\\xbb2 A2 f (A) of degree K + 2 with approximation bound r\\xe2\\x80\\xb2 =\\n(1 + 2\\xce\\xbb + \\xce\\xbb2)r \\xe2\\x89\\xa49\\n4r.\\nProof Assume that S = f (A) \\xe2\\x88\\x92VC\\xe2\\x8a\\xa4and thus r = \\xe2\\x88\\xa5S\\xe2\\x88\\xa5\\xe2\\x88\\x9e.\\n\\xe2\\x88\\xa5g(A) \\xe2\\x88\\x92V\\xe2\\x80\\xb2C\\xe2\\x80\\xb2\\xe2\\x8a\\xa4\\xe2\\x88\\xa5\\xe2\\x88\\x9e= \\xe2\\x88\\xa5g(A) \\xe2\\x88\\x92(V + \\xce\\xbbAV)(C\\xe2\\x8a\\xa4+ \\xce\\xbbC\\xe2\\x8a\\xa4A)\\xe2\\x88\\xa5\\xe2\\x88\\x9e\\n= \\xe2\\x88\\xa5g(A) \\xe2\\x88\\x92VC\\xe2\\x8a\\xa4\\xe2\\x88\\x92\\xce\\xbbAVC\\xe2\\x8a\\xa4\\xe2\\x88\\x92\\xce\\xbbVC\\xe2\\x8a\\xa4A \\xe2\\x88\\x92\\xce\\xbb2 AVC\\xe2\\x8a\\xa4A\\xe2\\x88\\xa5\\xe2\\x88\\x9e\\n= \\xe2\\x88\\xa5S + \\xce\\xbbAS + \\xce\\xbbSA + \\xce\\xbb2 ASA\\xe2\\x88\\xa5\\xe2\\x88\\x9e\\n\\xe2\\x89\\xa4\\xe2\\x88\\xa5S\\xe2\\x88\\xa5\\xe2\\x88\\x9e+ \\xce\\xbb\\xe2\\x88\\xa5A\\xe2\\x88\\xa5\\xe2\\x88\\x9e\\xe2\\x88\\xa5S\\xe2\\x88\\xa5\\xe2\\x88\\x9e+ \\xce\\xbb\\xe2\\x88\\xa5S\\xe2\\x88\\xa5\\xe2\\x88\\x9e\\xe2\\x88\\xa5A\\xe2\\x88\\xa5\\xe2\\x88\\x9e+ \\xce\\xbb2\\xe2\\x88\\xa5S\\xe2\\x88\\xa5\\xe2\\x88\\x9e\\xe2\\x88\\xa5A\\xe2\\x88\\xa52\\n\\xe2\\x88\\x9e\\n= r + 2\\xce\\xbbr + \\xce\\xbb2r,\\n(8.68)\\nwhere the second last equality replaces g(A) and f (A) \\xe2\\x88\\x92VC\\xe2\\x8a\\xa4by the de\\xef\\xac\\x81nitions\\nof g(A) and S and the last equality uses the fact that \\xe2\\x88\\xa5A\\xe2\\x88\\xa5\\xe2\\x88\\x9e= maxi\\n\\x04\\nj |Ai j| = 1\\nbecause the summation of each row of A equals to 1.\\nIn the experimental settings, it is assumed that the weight of lower order proxim-\\nities should be larger than higher order proximities because they are more directly\\nrelated to the original network. Therefore, given g(A) = f (A) + 2\\xce\\xbbAf (A) + \\xce\\xbb2 A2\\nf (A), we have 1 \\xe2\\x89\\xa52\\xce\\xbb \\xe2\\x89\\xa5\\xce\\xbb2 > 0 which indicates that \\xce\\xbb \\xe2\\x88\\x88(0, 1\\n2]. The proof indicates\\nthat the updated embedding can implicitly approximate a polynomial g(A) of 2 more\\ndegrees within 9\\n4 times matrix in\\xef\\xac\\x81nite norm of previous embeddings.\\n\\xe2\\x96\\xa0\\nAlgorithm. The update Eq.8.67 can be further generalized in two directions. First\\nwe can update embeddings V and C according to Eq.8.69:\\nV\\xe2\\x80\\xb2 = V + \\xce\\xbb1A V + \\xce\\xbb2 A (A V),\\nC\\xe2\\x80\\xb2 = C + \\xce\\xbb1A\\xe2\\x8a\\xa4C + \\xce\\xbb2 A\\xe2\\x8a\\xa4(A\\xe2\\x8a\\xa4C).\\n(8.69)\\n8.2 Network Representation\\n247\\nThe time complexity is still O(|V |d) but Eq.8.69 can obtain higher proximity\\nmatrix approximation than Eq.8.67 in one iteration. More complex update formulas\\nthat explore further higher proximities than Eq.8.69 can also be applied but Eq.8.69\\nis used in current experiments as a cost-effective choice.\\nAnother direction is that the update equation can be processed for T rounds to\\nobtain higher proximity approximation. However, the approximation bound would\\ngrow exponentially as the number of rounds T grows and thus the update cannot be\\ndonein\\xef\\xac\\x81nitely.NotethattheupdateoperationofV andCarecompletelyindependent.\\nTherefore, only updating network embedding V is enough for NRL. The above\\nalgorithm (NEU) avoids an accurate computation of high-order proximity matrix\\nbut can yield network embeddings that actually approximate high-order proximities.\\nHence, this algorithm can improve the quality of network embeddings ef\\xef\\xac\\x81ciently.\\nIntuitively, Eqs.8.67 and 8.69 allow the learned embeddings to further propagate to\\ntheir neighbors. Hence, the proximities of longer distances between vertices will be\\nembedded.\\n8.2.6\\nApplications\\nIn this part, we will introduce common applications for network representation learn-\\ning and their evaluation metrics.\\n8.2.6.1\\nMulti-label Classi\\xef\\xac\\x81cation\\nA multi-label classi\\xef\\xac\\x81cation task is the most widely used network representation\\nlearning evaluation task. The representations of vertices are considered as vertex\\nfeatures and applied to classi\\xef\\xac\\x81ers to predict vertex labels. More formally, we assume\\nthat there are K labels in total. The vertex-label relationship can be expressed as a\\nbinary matrix M \\xe2\\x88\\x88{0, 1}|V |\\xc3\\x97K where Mi j = 1 indicates that vertex vi has jth label\\nand Mi j = 0 otherwise. Speci\\xef\\xac\\x81cally, for the multiclass classi\\xef\\xac\\x81cation problem, each\\nvertex has exactly one label, which means there is only an \\xe2\\x80\\x9c1\\xe2\\x80\\x9d in each row of matrix\\nM. For the evaluation task, we set a training ratio which indicates how much percent\\nof vertices have observed labels. Then our goal is to predict the labels for the vertices\\nin the test set.\\nFor unsupervised network representation learning algorithms, the labels of the\\ntraining set are not used for embedding learning. The network representations are\\nfed to classi\\xef\\xac\\x81ers like SVM or logistic regression. Each label will have its classi-\\n\\xef\\xac\\x81er. For semi-supervised learning methods, they take the observed vertex labels into\\naccount in the representation learning period. These algorithms will have their spe-\\nci\\xef\\xac\\x81c classi\\xef\\xac\\x81ers for label prediction.\\nOnce the label prediction is done, we can move to compute the evaluation met-\\nrics. For multiclass classi\\xef\\xac\\x81cation, we assume that the number of correctly predicted\\nvertices is |Vr|. Then the classi\\xef\\xac\\x81cation accuracy is de\\xef\\xac\\x81ned as the ratio of correctly\\n248\\n8\\nNetwork Representation\\npredicted vertices which can be formulated as |Vr|/|V |. For multi-label classi\\xef\\xac\\x81ca-\\ntion, the precision, recall, and F1 are the most popular metrics, which are computed\\nas follows:\\nPrecision = Ncorrectly predicted labels\\nNpredicted labels\\n,\\nRecall = Ncorrectly predicted labels\\nNunobserved labels\\n,\\nF1-Score = 2Precision \\xc3\\x97 Recall\\nPrecision + Recall .\\n(8.70)\\n8.2.6.2\\nLink Prediction\\nLink prediction is another important evaluation task for network representation learn-\\ning because a good network embedding should have the ability to model the af\\xef\\xac\\x81nity\\nbetween vertices. For evaluation, we randomly pick up edges as training set and leave\\nthe rest as test set. Cross-validation can also be employed for training and testing.\\nTo make link prediction given the vertex representations, we \\xef\\xac\\x81rst need to evaluate\\nthe strength of a pair of vertices. The strength between two vertices is evaluated\\nby computing the similarity between their representations. This similarity is usually\\ncomputed by cosine similarity, inner product, or square loss, which depends on the\\nalgorithm. For example, if an algorithm uses \\xe2\\x88\\xa5Vi \\xe2\\x88\\x92C j\\xe2\\x88\\xa52\\n2 in their objective function,\\nthen square loss should be used to measure the similarity between vertex represen-\\ntations. Then after we get the similarity of all unobserved links, we can rank them\\nfor link prediction. There are two signi\\xef\\xac\\x81cant metrics for link prediction: area under\\nthe receiver operating characteristic curve (AUC) and precision.\\nAUC. The AUC value is the probability that a randomly chosen missing link has\\na higher score than a randomly chosen nonexistent link. For implementation, we\\nrandomly select a missing link and a nonexistent link and compare their similarity\\nscore. Assume that there are n1 times that missing link having a higher score and n2\\ntimes they have the same score among n independent comparisons. Then the AUC\\nvalue is\\nAUC = n1 + 0.5n2\\nn\\n.\\n(8.71)\\nNote that for a random network representation, the AUC value should be 0.5.\\nPrecision. Given the ranking of all the non-observed links, we predict the links\\nwith top-L highest score as predicted ones. Assume that there are Lr links that are\\nmissing links, then the precision is de\\xef\\xac\\x81ned as Lr/L.\\n8.2.6.3\\nCommunity Detection\\nFor the network representation based community detection algorithm, we \\xef\\xac\\x81rst need\\nto convert the nonnegative vertex representation into the hard assignment of commu-\\n8.2 Network Representation\\n249\\nnities. Assume that we have network representation matrix V \\xe2\\x88\\x88R+|V |\\xc3\\x97k where row i\\nof V is the nonnegative embedding of vertex vi. For community detection, we regard\\neach dimension of the embeddings as a community. That is to say, Vi j denotes the\\naf\\xef\\xac\\x81nity between vertex vi and community c j. For each column of matrix V, we set\\na threshold \\xce\\x94 and the vertices with af\\xef\\xac\\x81nity score higher than the threshold will be\\nconsidered as a member of the corresponding community. The threshold can be set\\nin various ways. For example, we can set \\xce\\xb4 so that a vertex belongs to a community c\\nif the node is connected to other members of c with an edge probability higher than\\n1/N: [140]\\n1\\nN \\xe2\\x89\\xa41 \\xe2\\x88\\x92exp(\\xe2\\x88\\x92\\xce\\x942),\\n(8.72)\\nwhich indicates that \\xce\\x94 =\\n\\x0c\\n\\xe2\\x88\\x92log(1 \\xe2\\x88\\x921/N).\\nFor evaluation metrics, we have two choices: modularity and matching score.\\nModularity. Recall that the modularity of a graph Q is de\\xef\\xac\\x81ned as\\nQ =\\n1\\n2|E|\\n\\x02\\ni, j\\n\\x05\\nAi j \\xe2\\x88\\x92deg(vi)deg(v j)\\n2|E|\\n\\x06\\n\\xce\\xb4(vi, v j),\\n(8.73)\\nwhere \\xce\\xb4(vi, v j) = 1 if vi and v j belong to the same community and \\xce\\xb4(vi, v j) = 0\\notherwise. A larger modularity indicates a better community detection algorithm.\\nMatching Score. This is a more sophisticated evaluation metric for community\\ndetection. To compare a set of ground truth communities C\\xe2\\x88\\x97to a set of detected\\ncommunities C, we \\xef\\xac\\x81rst need to match each detected community to the most similar\\nground truth community. On the other side, we also \\xef\\xac\\x81nd the most similar detected\\ncommunity for each ground truth community. Then the \\xef\\xac\\x81nal performance is evaluated\\nby the average of both sides:\\n1\\n2|C\\xe2\\x88\\x97|\\n\\x02\\nc\\xe2\\x88\\x97\\ni \\xe2\\x88\\x88C\\xe2\\x88\\x97\\nmax\\nc j\\xe2\\x88\\x88C \\xce\\xb4(c\\xe2\\x88\\x97\\ni , c j) +\\n1\\n2|C|\\n\\x02\\nc j\\xe2\\x88\\x88C\\nmax\\nc\\xe2\\x88\\x97\\ni \\xe2\\x88\\x88C\\xe2\\x88\\x97\\xce\\xb4(c\\xe2\\x88\\x97\\ni , c j),\\n(8.74)\\nwhere \\xce\\xb4(c\\xe2\\x88\\x97\\ni , c j) is a similarity measurement of ground truth community c\\xe2\\x88\\x97\\ni and\\ndetected community c j, such as Jaccard similarity. The score is between 0 and 1,\\nwhere 1 indicates a perfect matching of ground truth communities.\\n8.2.6.4\\nRecommender System\\nRecommender systems aim at recommending items (e.g., products, movies, or loca-\\ntions) for users and cover a wide range of applications. In many cases, an application\\ncomes with an associated social network between users. Now we will present an\\nexample to show how to use the idea of network representation for building recom-\\nmender systems in location-based social networks.\\n250\\n8\\nNetwork Representation\\n(a) Friendship Network\\n(b) User Trajectory\\nFig. 8.7 An illustrative example for the data in LBSNs: a Link connections represent the friendship\\nbetween users. b A trajectory generated by a user is a sequence of chronologically ordered check-in\\nrecords [138]\\nThe accelerated growth of mobile trajectories in location-based services brings\\nvaluable data resources to understand users\\xe2\\x80\\x99 moving behaviors. Apart from recording\\nthe trajectory data, another major characteristic of these location-based services is\\nthat they also allow the users to connect whomever they like or are interested in. As\\nshown in Fig.8.7, a combination of social networking and location-based services\\nis called as Location-Based Social Networks (LBSN). As shown in [21], locations\\nthat are frequently visited by socially related persons tend to be correlated, which\\nindicates the close association between social connections and trajectory behaviors\\nof users in LBSNs. In order to better analyze and mine LBSN data, we need to have\\na comprehensive view to analyze and mine the information from the two aspects,\\ni.e., the social network and mobile trajectory data.\\nSpeci\\xef\\xac\\x81cally, JNTM [138] is proposed to model both social networks and mobile\\ntrajectories jointly. The model consists of two components: the construction of social\\nnetworks and the generation of mobile trajectories. First, JNTM adopts a network\\nembedding method for the construction of social networks where a networking rep-\\nresentation can be derived for a user. Secondly, JNTM considers four factors that\\nin\\xef\\xac\\x82uence the generation process of mobile trajectories, namely, user visit prefer-\\nence, in\\xef\\xac\\x82uence of friends, short-term sequential contexts, and long-term sequential\\ncontexts. Then JNTM uses real-valued representations to encode the four factors and\\nset two different user representations to model the \\xef\\xac\\x81rst two factors: a visit interest\\nrepresentation and a network representation. To characterize the last two contexts,\\nJNTM employs the RNN and GRU models to capture the sequential relatedness in\\nmobile trajectories at different levels, i.e., short term or long term. Finally, the two\\ncomponents are tied by sharing user network representations. The overall model is\\nillustrated in Fig.8.8.\\n8.2 Network Representation\\n251\\nLayer 1\\n(Output)\\nLayer 2\\n(Representation)\\nLayer 3\\n(Deeper Neural Network)\\nGRU\\nRNN\\nFriendship\\nUser Interest\\nLong-term Context\\nShort-term Context\\nNetwork G\\nTrajectory T\\nFig. 8.8 The architecture of JNTM model\\n8.2.6.5\\nInformation Diffusion Prediction\\nInformation diffusion prediction is an important task which studies how information\\nitems spread among users. The prediction of information diffusion, also known as\\ncascade prediction, has been studied over a wide range of applications, such as\\nproduct adoption [67], epidemiology [124], social networks [63], and the spread of\\nnews and opinions [68].\\nAs shown in Fig.8.9, microscopic diffusion prediction aims at guessing the next\\ninfected user, while macroscopic diffusion prediction estimates the total numbers of\\ninfected users during the diffusion process. Also, an underlying social graph among\\nusers will be available when information diffusion occurs on a social network service.\\nThe social graph will be considered as additional structural inputs for diffusion\\nprediction.\\nFOREST [139] is the \\xef\\xac\\x81rst work to address both microscopic and macroscopic\\npredictions. As shown in Fig.8.10, FOREST proposes a structural context extrac-\\nFig. 8.9 Illustrative examples for microscopic next infected user prediction (left) and macroscopic\\ncascade size prediction (right) [139]\\n252\\n8\\nNetwork Representation\\nFig. 8.10 An illustrative example of structural context extraction of the orange node by neighbor\\nsampling and feature aggregation [139]\\ntion algorithm that was originally introduced for accelerating graph convolutional\\nnetworks [41] to build an RNN-based microscopic cascade model. For each user\\nv, we \\xef\\xac\\x81rst sample Z users {u1, u2 . . . , uZ} from v and its neighbors N (v). Then\\nwe update its feature vector by aggregating the neighborhood features. The updated\\nuser feature vector encodes structural information by aggregating features from v\\xe2\\x80\\x99s\\n\\xef\\xac\\x81rst-order neighbors. The operation can also be processed recursively to explore a\\nlarger neighborhood of user v. Empirically, a two-step neighborhood exploration is\\ntime ef\\xef\\xac\\x81cient and enough to give promising results.\\nFOREST further incorporates the ability of macroscopic prediction, i.e., esti-\\nmating the eventual size of a cascade into the model by reinforcement learning. The\\nmethod can be divided into four steps: (a) encode observed K users by a microscopic\\ncascade model; (b) enable the microscopic cascade model to predict the size of a cas-\\ncade by cascade simulations; (c) use Mean-Square Log-Transformed Error (MSLE)\\nas the supervision signal for macroscopic predictions; and (d) employ a reinforce-\\nment learning framework to update parameters through policy gradient algorithm.\\nThe overall work\\xef\\xac\\x82ow is illustrated in Fig.8.11.\\n8.3\\nGraph Neural Networks\\nWe now give a short introduction to Graph Neural Networks for NRL, partially based\\non our review [161] and tutorial [162] whose publishing agreement allows the authors\\nto reuse these contents.\\n8.3 Graph Neural Networks\\n253\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nh1\\nh2\\nhK\\nMicroscopic Cascade Model\\nu1\\nu2\\nuK\\n(a) Feed observed K users into\\nmicroscopic cascade model\\n<STOP>\\n(b) cascade simulations by sampling\\n(d) Update parameters by\\npolicy gradients\\n(c) MSLE\\nReward\\nFig. 8.11 The work\\xef\\xac\\x82ow of adopting microscopic cascade model for macroscopic size prediction\\nby reinforcement learning\\n8.3.1\\nMotivations\\nGraph Neural Networks (GNNs) are deep learning based methods that operate on\\ngraph domain. Due to its convincing performance and high interpretability, GNN\\nhas been a widely applied graph analysis method recently. In this subsection, we will\\nillustrate the fundamental motivations of graph neural networks.\\nIn recent years, CNNs [65] have made breakthroughs in various machine learning\\nareas, especially in the area of computer vision, and started the revolution of deep\\nlearning [64]. CNNs are capable of extracting multiscale localized features and these\\nfeatures are used to generate more expressive representations. As we are going deeper\\ninto CNNs and graphs, we found the keys of CNNs: local connection, shared weights,\\nand the use of multilayer [64]. These are also of great importance in solving problems\\nof graph domain, because (1) graphs are the most typical locally connected structure,\\n(2) shared weights reduce the computational cost compared with traditional spectral\\ngraph theory [23], and (3) multilayer structure is the key to deal with hierarchical\\npatterns, which captures the features of various sizes. However, CNNs can only\\noperate on regular Euclidean data like images (2D grid) and text (1D sequence)\\nwhile these data structures can be regarded as instances of graphs. Therefore, it is\\nstraightforward to think of \\xef\\xac\\x81nding the generalization of CNNs to graphs. As shown\\nin Fig.8.12, it is hard to de\\xef\\xac\\x81ne localized convolutional \\xef\\xac\\x81lters and pooling operators,\\nwhich hinders the transformation of CNN from Euclidean domain to non-Euclidean\\ndomain.\\nThe other motivation comes from network embedding [12, 24, 37, 42, 149]. In\\nthe \\xef\\xac\\x81eld of graph analysis, traditional machine learning approaches usually rely on\\nhand-engineered features and are limited by its in\\xef\\xac\\x82exibility and high cost. Follow-\\ning the idea of representation learning and the success of word embedding [81],\\nDeepWalk [93], which is regarded as the \\xef\\xac\\x81rst graph embedding method based on\\n254\\n8\\nNetwork Representation\\nFig. 8.12 Left: image in Euclidean space. Right: graph in non-Euclidean space [155]\\nrepresentation learning, applies Skip-gram model [81] on the generated random\\nwalks. Similar approaches such as node2vec [38], LINE [111], and TADW [136]\\nalso achieved breakthroughs. However, these methods suffer from two severe draw-\\nbacks [42]. First, no parameters are shared between nodes in the encoder, which leads\\nto computational inef\\xef\\xac\\x81ciency, since it means the number of parameters grows linearly\\nwith the number of nodes. Second, the direct embedding methods lack the ability of\\ngeneralization, which means they cannot deal with dynamic graphs or generalize to\\nnew graphs.\\nBased on CNNs and network embedding, Graph Neural Networks (GNNs) are\\nproposed to collectively aggregate information from graph structure. Thus, they can\\nmodel input and/or output consisting of elements and their dependency. Further, the\\ngraph neural networks can simultaneously model the diffusion process on the graph\\nwith the RNN kernel.\\nIn the rest of this section, we will \\xef\\xac\\x81rst introduce several typical variants of graph\\nneural networks such as Graph Convolutional Networks (GCNs), Graph Attention\\nNetworks (GATs), and Graph Recurrent Networks (GRNs). Then we will introduce\\nseveral extensions to the original model and \\xef\\xac\\x81nally, we will give some examples of\\napplications that utilize graph neural networks.\\n8.3.2\\nGraph Convolutional Networks\\nGraph Convolutional Networks (GCNs) aim to generalize convolutions to the graph\\ndomain. Advances in this direction are often categorized as spectral approaches and\\nspatial (nonspectral) approaches.\\n8.3 Graph Neural Networks\\n255\\n8.3.2.1\\nSpectral Approaches\\nSpectral approaches work with a spectral representation of the graphs.\\nSpectral Network. Bruna et al. [11] proposes the spectral network. The convolu-\\ntion operation is de\\xef\\xac\\x81ned in the Fourier domain by computing the eigendecomposition\\nof the graph Laplacian. The operation can be de\\xef\\xac\\x81ned as the multiplication of a signal\\nx \\xe2\\x88\\x88RN (a scalar for each node) with a \\xef\\xac\\x81lter g\\xce\\xb8 =diag(\\xce\\xb8) parameterized by \\xce\\xb8 \\xe2\\x88\\x88RN:\\ng\\xce\\xb8 \\xe2\\x8b\\x86x = Ug\\xce\\xb8(\\x0f)U T x,\\n(8.75)\\nwhere U is the matrix of eigenvectors of the normalized graph Laplacian L = IN \\xe2\\x88\\x92\\nD\\xe2\\x88\\x921\\n2 AD\\xe2\\x88\\x921\\n2 = U\\x0fU T (D is the degree matrix and A is the adjacency matrix of the\\ngraph), with a diagonal matrix of its eigenvalues \\x0f.\\nThis operation results in potentially intense computations and non-spatially local-\\nized \\xef\\xac\\x81lters. Henaff et al. [47] attempts to make the spectral \\xef\\xac\\x81lters spatially localized\\nby introducing a parameterization with smooth coef\\xef\\xac\\x81cients.\\nChebNet. Hammond et al. [43] suggests that g\\xce\\xb8(\\x0f) can be approximated by a\\ntruncated expansion in terms of Chebyshev polynomials Tk(x) up to Kth order. Thus,\\nthe operation is\\ng\\xce\\xb8 \\xe2\\x8b\\x86x \\xe2\\x89\\x88\\nK\\n\\x02\\nk=0\\n\\xce\\xb8kTk( \\xcb\\x9cL)x,\\n(8.76)\\nwith \\xcb\\x9cL = 2/\\xce\\xbbmax L \\xe2\\x88\\x92IN. \\xce\\xbbmax denotes the largest eigenvalue of L. \\xce\\xb8 \\xe2\\x88\\x88RK is now\\na vector of Chebyshev coef\\xef\\xac\\x81cients. The Chebyshev polynomials are de\\xef\\xac\\x81ned as\\nTk(x) = 2xTk\\xe2\\x88\\x921(x) \\xe2\\x88\\x92Tk\\xe2\\x88\\x922(x), with T0(x) = 1 and T1(x) = x. It can be observed\\nthat the operation is K-localized since it is a Kth-order polynomial in the Laplacian.\\nDefferrard et al. [28] proposes the ChebNet. It uses this K-localized convolution to\\nde\\xef\\xac\\x81ne a convolutional neural network, which could remove the need to compute the\\neigenvectors of the Laplacian.\\nGCN. Kipf and Welling [59] limits the layer-wise convolution operation to K = 1\\nto alleviate the problem of over\\xef\\xac\\x81tting on local neighborhood structures for graphs\\nwith very wide node degree distributions. It further approximates \\xce\\xbbmax \\xe2\\x89\\x882 and the\\nequation simpli\\xef\\xac\\x81es to\\ng\\xce\\xb8\\xe2\\x80\\xb2 \\xe2\\x8b\\x86x \\xe2\\x89\\x88\\xce\\xb8\\xe2\\x80\\xb2\\n0x + \\xce\\xb8\\xe2\\x80\\xb2\\n1 (L \\xe2\\x88\\x92IN) x = \\xce\\xb8\\xe2\\x80\\xb2\\n0x \\xe2\\x88\\x92\\xce\\xb8\\xe2\\x80\\xb2\\n1D\\xe2\\x88\\x921\\n2 AD\\xe2\\x88\\x921\\n2 x,\\n(8.77)\\nwith two free parameters \\xce\\xb8\\xe2\\x80\\xb2\\n0 and \\xce\\xb8\\xe2\\x80\\xb2\\n1. After constraining the number of parameters\\nwith \\xce\\xb8 = \\xce\\xb8\\xe2\\x80\\xb2\\n0 = \\xe2\\x88\\x92\\xce\\xb8\\xe2\\x80\\xb2\\n1, we can obtain the following expression:\\ng\\xce\\xb8 \\xe2\\x8b\\x86x \\xe2\\x89\\x88\\xce\\xb8\\n\\r\\nIN + D\\xe2\\x88\\x921\\n2 AD\\xe2\\x88\\x921\\n2\\n\\x0e\\nx.\\n(8.78)\\nNote that stacking this operator could lead to numerical instabilities and\\nexploding/vanishing\\ngradients,\\n[59]\\nintroduces\\nthe\\nrenormalization\\ntrick:\\n256\\n8\\nNetwork Representation\\nIN + D\\xe2\\x88\\x921\\n2 AD\\xe2\\x88\\x921\\n2 \\xe2\\x86\\x92\\xcb\\x9cD\\xe2\\x88\\x921\\n2 \\xcb\\x9cA \\xcb\\x9cD\\xe2\\x88\\x921\\n2 , with \\xcb\\x9cA = A + IN and \\xcb\\x9cDii = \\x04\\nj \\xcb\\x9cAi j. Finally, [59]\\ngeneralizes the de\\xef\\xac\\x81nition to a signal X \\xe2\\x88\\x88RN\\xc3\\x97C with C input channels and F \\xef\\xac\\x81lters\\nfor feature maps as follows:\\nH = f ( \\xcb\\x9cD\\xe2\\x88\\x921\\n2 \\xcb\\x9cA \\xcb\\x9cD\\xe2\\x88\\x921\\n2 XW),\\n(8.79)\\nwhere W \\xe2\\x88\\x88RC\\xc3\\x97F is a matrix of \\xef\\xac\\x81lter parameters, H \\xe2\\x88\\x88RN\\xc3\\x97F is the convolved signal\\nmatrix and f (\\xc2\\xb7) is the activation function.\\nThe GCN layer can be stacked for multiple times so that we have the equation:\\nH(t) = f ( \\xcb\\x9cD\\xe2\\x88\\x921\\n2 \\xcb\\x9cA \\xcb\\x9cD\\xe2\\x88\\x921\\n2 H(t\\xe2\\x88\\x921)W(t\\xe2\\x88\\x921)),\\n(8.80)\\nwhere the superscripts t and t \\xe2\\x88\\x921 denote the layers of the matrices, the initial matrix\\nH(0) could be X. After L layers, we can use the \\xef\\xac\\x81nal embedding matrix H(L) and a\\nreadout function to get the \\xef\\xac\\x81nal output matrix Z:\\nZ = Readout(H (L)),\\n(8.81)\\nwhere the readout function can be any machine learning methods, such as MLP.\\nFinally, as a semi-supervised algorithm, GCN uses the feature matrix at the top\\nlayer Z which has the same dimension with the total number of labels to predict the\\nlabels of all observed labels. The loss function can be written as\\nL = \\xe2\\x88\\x92\\n\\x02\\nl\\xe2\\x88\\x88yL\\n\\x02\\nf\\nYl f ln Zl f ,\\n(8.82)\\nwhere yL is the set of node indices that have observed labels. Figure8.13 shows the\\nalgorithm of GCN.\\nAnswer:green\\noutput layer\\ninput layer\\nhidden\\nlayers\\nC\\nF\\nx1\\nZ1\\nZ2\\nZ3\\nZ4\\nY1\\nY4\\nx2\\nx3\\nx4\\nFig. 8.13 The architecture of graph convolutional network model\\n8.3 Graph Neural Networks\\n257\\n8.3.2.2\\nSpatial Approaches\\nIn all of the spectral approaches mentioned above, the learned \\xef\\xac\\x81lters depend on\\nthe Laplacian eigenbasis, which depends on the graph structure, that is, a model\\ntrained on a speci\\xef\\xac\\x81c structure could not be directly applied to a graph with a different\\nstructure.\\nSpatial approaches de\\xef\\xac\\x81ne convolutions directly on the graph, operating on spa-\\ntially close neighbors. The major challenge of spatial approaches is de\\xef\\xac\\x81ning the con-\\nvolution operation with differently sized neighborhoods and maintaining the local\\ninvariance of CNNs.\\nNeural FPs. Duvenaud et al. [31] uses different weight matrices for nodes with\\ndifferent degrees\\nx(t) = h(t\\xe2\\x88\\x921)\\nv\\n+\\n|Nv|\\n\\x02\\ni=1\\nh(t\\xe2\\x88\\x921)\\ni\\n,\\nh(t)\\nv\\n= f (W(t)\\n|Nv|x(t)),\\n(8.83)\\nwhere W(t)\\n|Nv| is the weight matrix for nodes with degree |Nv| at layer t. And the main\\ndrawback of the method is that it cannot be applied to large-scale graphs with more\\nnode degrees.\\nIn the following description of other models, we use h(t)\\nv\\nto denote the hidden\\nstate of node v at layer t. Nv denotes the neighbor set of node v and |Nv| denotes the\\nsize of the set.\\nDCNN. Atwood and Towsley [4] proposes the Diffusion-Convolutional Neural\\nNetworks (DCNNs). Transition matrices are used to de\\xef\\xac\\x81ne the neighborhood for\\nnodes in DCNN. For node classi\\xef\\xac\\x81cation, it has\\nH = f\\n\\r\\nWc \\xe2\\x8a\\x99\\xe2\\x88\\x92\\xe2\\x86\\x92\\nP X\\n\\x0e\\n,\\n(8.84)\\nwhere \\xe2\\x8a\\x99is the element-wise multiplication and X is an N \\xc3\\x97 F matrix of input\\nfeatures. \\xe2\\x88\\x92\\xe2\\x86\\x92\\nP is an N \\xc3\\x97 K \\xc3\\x97 N tensor which contains the power series {P, P2,\\xe2\\x80\\xa6,\\nPK} of matrix P. And P is the degree-normalized transition matrix from the graphs\\nadjacency matrix A. Each entity is transformed to a diffusion-convolutional rep-\\nresentation, which is a K \\xc3\\x97 F matrix de\\xef\\xac\\x81ned by K hops of graph diffusion over\\nF features. And then it will be de\\xef\\xac\\x81ned by a K \\xc3\\x97 F weight matrix and a nonlin-\\near activation function f . Finally H (which is N \\xc3\\x97 K \\xc3\\x97 F) denotes the diffusion\\nrepresentations of each node in the graph.\\nDGCN. Zhuang and Ma [158] proposes the Dual Graph Convolutional Network\\n(DGCN) to consider the local consistency and global consistency of graphs jointly. It\\nuses two convolutional networks to capture the local/global consistency and adopts\\nan unsupervised loss to ensemble them. The \\xef\\xac\\x81rst convolutional network is the same\\nas Eq.8.80. And the second network replaces the adjacency matrix with Positive\\nPoint-wise Mutual Information (PPMI) matrix:\\n258\\n8\\nNetwork Representation\\nH(t) = f (D\\n\\xe2\\x88\\x921\\n2\\nP X P D\\n\\xe2\\x88\\x921\\n2\\nP H (t\\xe2\\x88\\x921)W),\\n(8.85)\\nwhere X P is the PPMI matrix and DP is the diagonal degree matrix of X P.\\nGraphSAGE. Hamilton et al. [41] proposes the GraphSAGE, a general induc-\\ntive framework. The framework generates embeddings by sampling and aggregating\\nfeatures from a node\\xe2\\x80\\x99s local neighborhood.\\nh(t)\\nNv = AGGREGATE(t)({h(t\\xe2\\x88\\x921)\\nu\\n, \\xe2\\x88\\x80u \\xe2\\x88\\x88Nv}),\\nh(t)\\nv\\n= f (W(t)[h(t\\xe2\\x88\\x921)\\nv\\n; h(t)\\nNv]).\\n(8.86)\\nHowever, [41] does not utilize the full set of neighbors in Eq.8.86 but a \\xef\\xac\\x81xed-\\nsize set of neighbors by uniformly sampling. And [41] suggests three aggregator\\nfunctions.\\n\\xe2\\x80\\xa2 Mean aggregator. It could be viewed as an approximation of the convolutional\\noperation from the transductive GCN framework [59], so that the inductive version\\nof the GCN variant could be derived by\\nh(t)\\nv\\n= f\\n\\x0f\\nW \\xc2\\xb7 MEAN\\n\\x0f\\n{h(t\\xe2\\x88\\x921)\\nv\\n} \\xe2\\x88\\xaa{h(t\\xe2\\x88\\x921)\\nu\\n|\\xe2\\x88\\x80u \\xe2\\x88\\x88Nv}\\n\\x10\\x10\\n.\\n(8.87)\\nThe mean aggregator is different from other aggregators because it does not per-\\nform the concatenation operation which concatenates ht\\xe2\\x88\\x921\\nv\\nand ht\\nNv in Eq.8.86. It\\ncan be viewed as a form of \\xe2\\x80\\x9cskip connection\\xe2\\x80\\x9d [46] and can achieve better perfor-\\nmance.\\n\\xe2\\x80\\xa2 LSTM aggregator. Hamilton et al. [41] also uses an LSTM-based aggregator which\\nhas a larger expressive capability. However, LSTMs process inputs in a sequential\\nmanner so that they are not permutation invariant. Hamilton et al. [41] adapts\\nLSTMs to operate on an unordered set by permutating node\\xe2\\x80\\x99s neighbors.\\n\\xe2\\x80\\xa2 Pooling aggregator. In the pooling aggregator, each neighbor\\xe2\\x80\\x99s hidden state is fed\\nthrough a fully connected layer and then a max-pooling operation is applied to the\\nset of the node\\xe2\\x80\\x99s neighbors.\\nh(t)\\nNv = max({ f (Wpoolh(t\\xe2\\x88\\x921)\\nu\\n+ b), \\xe2\\x88\\x80u \\xe2\\x88\\x88Nv}).\\n(8.88)\\nNote that any symmetric functions could be used in place of the max-pooling\\noperation here.\\nOther methods. There are still many other spatial methods. The PATCHY-SAN\\nmodel [86] \\xef\\xac\\x81rst extracts exactly k nodes for each node and normalizes them. Then\\nthe convolutional operation is applied to the normalized neighborhood. LGCN [35]\\nleverages CNNs as aggregators. It performs max-pooling on nodes\\xe2\\x80\\x99 neighborhood\\nmatrices to get top-k feature elements and then applies 1-D CNN to compute hid-\\nden representations. Monti et al. [82] proposes a spatial-domain model (MoNet)\\non non-Euclidean domains which could generalize several previous techniques.\\n8.3 Graph Neural Networks\\n259\\nThe Geodesic CNN (GCNN) [78] and Anisotropic CNN (ACNN) [10] on manifolds\\nor GCN [59] and DCNN [4] on graphs could be formulated as particular instances\\nof MoNet. Our readers can refer to their papers for more details.\\n8.3.3\\nGraph Attention Networks\\nThe attention mechanism has been successfully used in many sequence-based tasks\\nsuch as machine translation [5, 36, 121], machine reading [19], etc. Many works\\nfocus on generalizing the attention mechanism to the graph domain.\\nGAT. Velickovic et al. [122] proposes a Graph Attention Network (GAT) which\\nincorporates the attention mechanism into the propagation step. Speci\\xef\\xac\\x81cally, it uses\\nthe self-attention strategy and each node\\xe2\\x80\\x99s hidden state is computed by attending\\nover its neighbors.\\nVelickovic et al. [122] de\\xef\\xac\\x81nes a single graph attentional layer and constructs\\narbitrary graph attention networks by stacking this layer. The layer computes the\\ncoef\\xef\\xac\\x81cients in the attention mechanism of the node pair (i, j) by:\\n\\xce\\xb1i j =\\nexp\\n\\r\\nLeakyReLU\\n\\r\\na\\xe2\\x8a\\xa4[Wh(t\\xe2\\x88\\x921)\\ni\\n; Wh(t\\xe2\\x88\\x921)\\nj\\n]\\n\\x0e\\x0e\\n\\x04\\nk\\xe2\\x88\\x88Ni exp\\n\\r\\nLeakyReLU\\n\\r\\na\\xe2\\x8a\\xa4[Wh(t\\xe2\\x88\\x921)\\ni\\n; Wh(t\\xe2\\x88\\x921)\\nk\\n]\\n\\x0e\\x0e,\\n(8.89)\\nwhere \\xce\\xb1i j is the attention coef\\xef\\xac\\x81cient of node j to i. W \\xe2\\x88\\x88RF\\xe2\\x80\\xb2\\xc3\\x97F is the weight matrix\\nof a shared linear transformation which applied to every node, a \\xe2\\x88\\x88R2F\\xe2\\x80\\xb2 is the weight\\nvector. It is normalized by a softmax function and the LeakyReLU nonlinearity (with\\nnegative input slop 0.2) is applied.\\nThen the \\xef\\xac\\x81nal output features of each node can be obtained by (after applying a\\nnonlinearity f ):\\nh(t)\\ni\\n= f\\n\\xe2\\x8e\\x9b\\n\\xe2\\x8e\\x9d\\x02\\nj\\xe2\\x88\\x88Ni\\n\\xce\\xb1i jWh(t\\xe2\\x88\\x921)\\nj\\n\\xe2\\x8e\\x9e\\n\\xe2\\x8e\\xa0.\\n(8.90)\\nMoreover, the layer utilizes the multi-head attention similarly to [121] to stabilize\\nthe learning process. It applies K independent attention mechanisms to compute the\\nhidden states and then concatenates their features(or computes the average), resulting\\nin the following two output representations:\\nh(t)\\ni\\n= \\xe2\\x88\\xa5K\\nk=1 f\\n\\xe2\\x8e\\x9b\\n\\xe2\\x8e\\x9d\\x02\\nj\\xe2\\x88\\x88Ni\\n\\xce\\xb1k\\ni jWkh(t\\xe2\\x88\\x921)\\nj\\n\\xe2\\x8e\\x9e\\n\\xe2\\x8e\\xa0,\\n(8.91)\\nh(t)\\ni\\n= f\\n\\xe2\\x8e\\x9b\\n\\xe2\\x8e\\x9d1\\nK\\nK\\n\\x02\\nk=1\\n\\x02\\nj\\xe2\\x88\\x88Ni\\n\\xce\\xb1k\\ni jWkh(t\\xe2\\x88\\x921)\\nj\\n\\xe2\\x8e\\x9e\\n\\xe2\\x8e\\xa0,\\n(8.92)\\n260\\n8\\nNetwork Representation\\nwhere \\xce\\xb1k\\ni j is normalized attention coef\\xef\\xac\\x81cient computed by the kth attention mecha-\\nnism, \\xe2\\x88\\xa5is the concatenation operation.\\nThe attention architecture in [122] has several properties: (1) the computation of\\nthe node-neighbor pairs is parallelizable thus the operation is ef\\xef\\xac\\x81cient; (2) it can\\ndeal with nodes that have different degrees by assigning reasonable weights to their\\nneighbors; (3) it can be applied to the inductive learning problems easily.\\nGAAN. Besides GAT, Gated Attention Network (GAAN) [150] also uses the\\nmulti-head attention mechanism. However, it uses a self-attention mechanism to\\ngather information from different heads to replace the average operation of GAT.\\n8.3.4\\nGraph Recurrent Networks\\nSeveral works are attempting to use the gate mechanism like GRU [20] or LSTM [48]\\nin the propagation step to release the limitations induced by the vanilla GNN architec-\\nture and improve the effectiveness of the long-term information propagation across\\nthe graph. We call these methods Graph Recurrent Networks (GRNs) and we will\\nintroduce some variants of GRNs in this subsection.\\nGGNN.Lietal.[72]proposesthegatedgraphneuralnetwork(GGNN)whichuses\\nthe Gate Recurrent Units (GRU) in the propagation step. It follows the computation\\nsteps from recurrent neural networks for a \\xef\\xac\\x81xed number of L steps, then it back-\\npropagates through time to compute gradients.\\nSpeci\\xef\\xac\\x81cally, the basic recurrence of the propagation model is\\na(t)\\nv\\n=A\\xe2\\x8a\\xa4\\nv [h(t\\xe2\\x88\\x921)\\n1\\n. . . h(t\\xe2\\x88\\x921)\\nN\\n]\\xe2\\x8a\\xa4+ b,\\nz(t)\\nv\\n= Sigmoid\\n\\x0f\\nWza(t)\\nv + Uzh(t\\xe2\\x88\\x921)\\nv\\n\\x10\\n,\\nr(t)\\nv\\n= Sigmoid\\n\\x0f\\nWra(t)\\nv + Urh(t\\xe2\\x88\\x921)\\nv\\n\\x10\\n,\\n(8.93)\\n\\x08h(t)\\nv\\n= tanh\\n\\x0f\\nWa(t)\\nv + U\\n\\x0f\\nr(t)\\nv \\xe2\\x8a\\x99h(t\\xe2\\x88\\x921)\\nv\\n\\x10\\x10\\n,\\nh(t)\\nv\\n=\\n\\x0f\\n1 \\xe2\\x88\\x92z(t)\\nv\\n\\x10\\n\\xe2\\x8a\\x99h(t\\xe2\\x88\\x921)\\nv\\n+ z(t)\\nv \\xe2\\x8a\\x99\\x08h(t)\\nv .\\nThe node v \\xef\\xac\\x81rst aggregates message from its neighbors, where Av is the sub-\\nmatrix of the graph adjacency matrix A and denotes the connection of node v with\\nits neighbors. Then the hidden state of the node is updated by the GRU-like function\\nusing the information from its neighbors and the hidden state from the previous\\ntimestep. a gathers the neighborhood information of node v, z and r are the update\\nand reset gates.\\nLSTMs are also used similarly as GRU through the propagation process based on\\na tree or a graph.\\nTree-LSTM. Tai et al. [109] proposes two extensions to the basic LSTM architec-\\nture: the Child-Sum Tree-LSTM and the N-ary Tree-LSTM. Like in standard LSTM\\nunits, each Tree-LSTM unit (indexed by v) contains input and output gates iv and ov,\\na memory cell cv and hidden state hv. The Tree-LSTM unit replaces the single forget\\n8.3 Graph Neural Networks\\n261\\ngate by a forget gate fvk for each child k, allowing node v to select information from\\nits children accordingly. The equations of the Child-Sum Tree-LSTM are\\n\\x08ht\\xe2\\x88\\x921\\nv\\n=\\n\\x02\\nk\\xe2\\x88\\x88Nv\\nht\\xe2\\x88\\x921\\nk\\n,\\nit\\nv = Sigmoid\\n\\r\\nWixt\\nv + Ui\\x08ht\\xe2\\x88\\x921\\nv\\n+ bi\\x0e\\n,\\nft\\nvk = Sigmoid\\n\\r\\nW f xt\\nv + U f ht\\xe2\\x88\\x921\\nk\\n+ b f \\x0e\\n,\\not\\nv = Sigmoid\\n\\r\\nWoxt\\nv + Uo\\x08ht\\xe2\\x88\\x921\\nv\\n+ bo\\x0e\\n,\\n(8.94)\\nut\\nv = tanh\\n\\r\\nWuxt\\nv + Uu\\x08ht\\xe2\\x88\\x921\\nv\\n+ bu\\x0e\\n,\\nct\\nv = it\\nv \\xe2\\x8a\\x99ut\\nv +\\n\\x02\\nk\\xe2\\x88\\x88Nv\\nft\\nvk \\xe2\\x8a\\x99ct\\xe2\\x88\\x921\\nk\\n,\\nht\\nv = ot\\nv \\xe2\\x8a\\x99tanh(ct\\nv),\\nwhere xt\\nv is the input vector at time t in the standard LSTM setting.\\nIn a speci\\xef\\xac\\x81c case, if each node\\xe2\\x80\\x99s number of children is at most K and these children\\ncan be ordered from 1 to K, then the N-ary Tree-LSTM can be applied. For node\\nv, ht\\nvk and ct\\nvk denote the hidden state and memory cell of its kth child at time t\\nrespectively. The transition equations are the following:\\nit\\nv = Sigmoid\\n\\r\\nWixt\\nv +\\nK\\n\\x02\\nl=1\\nUi\\nlht\\xe2\\x88\\x921\\nvl\\n+ bi\\x0e\\n,\\nft\\nvk = Sigmoid\\n\\r\\nW f xt\\nv +\\nK\\n\\x02\\nl=1\\nU f\\nklht\\xe2\\x88\\x921\\nvl\\n+ b f \\x0e\\n,\\not\\nv = Sigmoid\\n\\r\\nWoxt\\nv +\\nK\\n\\x02\\nl=1\\nUo\\nl ht\\xe2\\x88\\x921\\nvl\\n+ bo\\x0e\\n,\\n(8.95)\\nut\\nv = tanh\\n\\r\\nWuxt\\nv +\\nK\\n\\x02\\nl=1\\nUu\\nl ht\\xe2\\x88\\x921\\nvl\\n+ bu\\x0e\\n,\\nct\\nv = it\\nv \\xe2\\x8a\\x99ut\\nv +\\nK\\n\\x02\\nl=1\\nft\\nvl \\xe2\\x8a\\x99ct\\xe2\\x88\\x921\\nvl ,\\nht\\nv = ot\\nv \\xe2\\x8a\\x99tanh(ct\\nv).\\nCompared to the Child-Sum Tree-LSTM, the N-ary Tree-LSTM introduces sep-\\narate parameters for each child k. These parameters allow the model to learn more\\n\\xef\\xac\\x81ne-grained representations conditioning on each node\\xe2\\x80\\x99s children.\\nGraph LSTM. The two types of Tree-LSTMs can be easily adapted to the graph.\\nThe graph-structured LSTM in [148] is an example of the N-ary Tree-LSTM applied\\n262\\n8\\nNetwork Representation\\nto the graph. However, it is a simpli\\xef\\xac\\x81ed version since each node in the graph has at\\nmost 2 incoming edges (from its parent and sibling predecessor). Peng et al. [92]\\nproposes another variant of the Graph LSTM based on the relation extraction task.\\nThe main difference between graphs and trees is that edges of graphs have their\\nlabels, and [92] utilizes different weight matrices to represent different labels.\\nit\\nv = Sigmoid\\n\\r\\nWixt\\nv +\\n\\x02\\nk\\xe2\\x88\\x88Nv\\nUi\\nm(v,k)ht\\xe2\\x88\\x921\\nk\\n+ bi\\x0e\\n,\\nft\\nvk = Sigmoid\\n\\r\\nW f xt\\nv + U f\\nm(v,k)ht\\xe2\\x88\\x921\\nk\\n+ b f \\x0e\\n,\\not\\nv = Sigmoid\\n\\r\\nWoxt\\nv +\\n\\x02\\nk\\xe2\\x88\\x88Nv\\nUo\\nm(v,k)ht\\xe2\\x88\\x921\\nk\\n+ bo\\x0e\\n,\\n(8.96)\\nut\\nv = tanh\\n\\r\\nWuxt\\nv +\\n\\x02\\nk\\xe2\\x88\\x88Nv\\nUu\\nm(v,k)ht\\xe2\\x88\\x921\\nk\\n+ bu\\x0e\\n,\\nct\\nv = it\\nv \\xe2\\x8a\\x99ut\\nv +\\n\\x02\\nk\\xe2\\x88\\x88Nv\\nft\\nvk \\xe2\\x8a\\x99ct\\xe2\\x88\\x921\\nk\\n,\\nht\\nv = ot\\nv \\xe2\\x8a\\x99tanh(ct\\nv),\\nwhere m(v, k) denotes the edge label between node v and k.\\nBesides, [74] proposes a Graph LSTM network to address the semantic object\\nparsing task. It uses the con\\xef\\xac\\x81dence-driven scheme to adaptively select the starting\\nnode and determine the node updating sequence. It follows the same idea of general-\\nizing the existing LSTMs into the graph-structured data but has a speci\\xef\\xac\\x81c updating\\nsequence while the methods we mentioned above are agnostic to the order of nodes.\\nSentence LSTM. Zhang et al. [152] proposes the Sentence LSTM (S-LSTM) for\\nimproving text encoding. It converts text into a graph and utilizes the Graph LSTM\\nto learn the representation. The S-LSTM shows strong representation power in many\\nNLP problems.\\n8.3.5\\nExtensions\\nIn this subsection, we will talk about some extensions of graph neural networks.\\n8.3.5.1\\nSkip Connection\\nMany applications unroll or stack the graph neural network layer aiming to achieve\\nbetter results as more layers (i.e., k layers) make each node aggregate more informa-\\ntion from neighbors k hops away. However, it has been observed in many experiments\\nthat deeper models could not improve the performance and deeper models could even\\n8.3 Graph Neural Networks\\n263\\nperform worse [59]. This is mainly because more layers could also propagate the\\nnoisy information from an exponentially increasing number of expanded neighbor-\\nhood members.\\nA straightforward method to address the problem, the residual network [45], can\\nbe found from the computer vision community. Nevertheless, even with residual\\nconnections, GCNs with more layers do not perform as well as the 2-layer GCN on\\nmany datasets [59].\\nHighway Network. Rahimi et al. [96] borrows ideas from the highway net-\\nwork [159] and uses layer-wise gates to build a Highway GCN. The input of each\\nlayer is multiplied by the gating weights and then summed with the output:\\nT (h(t)) = Sigmoid\\n\\x0f\\nW(t)h(t) + b(t)\\x10\\n,\\nh(t+1) = h(t+1) \\xe2\\x8a\\x99T (h(t)) + h(t) \\xe2\\x8a\\x99(1 \\xe2\\x88\\x92T (h(t))).\\n(8.97)\\nBy adding the highway gates, the performance peaks at four layers in a speci\\xef\\xac\\x81c\\nproblem discussed in [96]. The Column Network (CLN) proposed in [94] also utilizes\\nthe highway network. However, it has a different function to compute the gating\\nweights.\\nJump Knowledge Network. Xu et al. [134] studies properties and resulting limi-\\ntations of neighborhood aggregation schemes. It proposes the Jump Knowledge Net-\\nwork which could learn adaptive, structure-aware representations. The Jump Knowl-\\nedge Network selects from all of the intermediate representations (which\"jump\" to\\nthe last layer) for each node at the last layer, which enables the model to select effec-\\ntive neighborhood information for each node. Xu et al. [134] uses three approaches\\nof concatenation, max-pooling, and LSTM-attention in the experiments to aggre-\\ngate information. The Jump Knowledge Network performs well on the experiments\\nin social, bioinformatics, and citation networks. It can also be combined with models\\nlike Graph Convolutional Networks, GraphSAGE, and Graph Attention Networks to\\nimprove their performance.\\n8.3.5.2\\nHierarchical Pooling\\nIn the area of computer vision, a convolutional layer is usually followed by a pooling\\nlayer to get more general features. Similar to these pooling layers, much work focuses\\non designing hierarchical pooling layers on graphs. Complicated and large-scale\\ngraphs usually carry rich hierarchical structures that are of great importance for\\nnode-level and graph-level classi\\xef\\xac\\x81cation tasks.\\nTo explore such inner features, Edge-Conditioned Convolution (ECC) [106]\\ndesigns its pooling module with the recursively downsampling operation. The down-\\nsampling method is based on splitting the graph into two components by the sign of\\nthe largest eigenvector of the Laplacian.\\n264\\n8\\nNetwork Representation\\nDIFFPOOL [144] proposes a learnable hierarchical clustering module by training\\nan assignment matrix in each layer:\\nS(l) = Softmax(GNNl,pool(A(l), V(l))),\\n(8.98)\\nwhere V(l) is node features and A(l) is coarsened adjacency matrix of layer l.\\n8.3.5.3\\nNeighborhood Sampling\\nThe original graph convolutional neural network has several drawbacks. Speci\\xef\\xac\\x81cally,\\nGCN requires the full graph Laplacian, which is computationally consuming for large\\ngraphs. Furthermore, the embedding of a node at layer L is computed recursively by\\nthe embeddings of all its neighbors at layer L \\xe2\\x88\\x921. Therefore, the receptive \\xef\\xac\\x81eld of a\\nsingle node grows exponentially with respect to the number of layers, so computing\\ngradient for a single node costs a lot. Finally, GCN is trained independently for a\\n\\xef\\xac\\x81xed graph, which lacks the ability for inductive learning.\\nGraphSAGE [41] is a comprehensive improvement of the original GCN. To\\nsolve the problems mentioned above, GraphSAGE replaced full graph Laplacian\\nwith learnable aggregation functions, which are crucial to perform message passing\\nand generalize to unseen nodes. As shown in Eq.8.86, they \\xef\\xac\\x81rst aggregate neighbor-\\nhood embeddings, concatenate with target node\\xe2\\x80\\x99s embedding, then propagate to the\\nnext layer. With learned aggregation and propagation functions, GraphSAGE could\\ngenerate embeddings for unseen nodes. Also, GraphSAGE uses neighbor sampling\\nto alleviate the receptive \\xef\\xac\\x81eld expansion.\\nPinSage [143] proposes importance-based sampling method. By simulating ran-\\ndom walks starting from target nodes, this approach chooses the top T nodes with\\nthe highest normalized visit counts.\\nFastGCN [16] further improves the sampling algorithm. Instead of sampling\\nneighbors for each node, FastGCN directly samples the receptive \\xef\\xac\\x81eld for each\\nlayer. FastGCN uses importance sampling, which the important factor is calculated\\nas below:\\nq(v) \\xe2\\x88\\x9d\\n1\\n|Nv|\\n\\x02\\nu\\xe2\\x88\\x88Nv\\n1\\n|Nu|.\\n(8.99)\\nAdapt. In contrast to \\xef\\xac\\x81xed sampling methods above, [51] introduces a parameter-\\nized and trainable sampler to perform layer-wise sampling conditioned on the former\\nlayer. Furthermore, this adaptive sampler could \\xef\\xac\\x81nd optimal sampling importance\\nand reduce variance simultaneously.\\n8.3 Graph Neural Networks\\n265\\n8.3.5.4\\nVarious Graph Types\\nIn the original GNN [101], the input graph consists of nodes with label information\\nand undirected edges, which is the simplest graph format. However, there are many\\nvariants of graphs in the world. In the following, we will introduce some methods\\ndesigned to model different kinds of graphs.\\nDirected Graphs. The \\xef\\xac\\x81rst variant of the graph is directed graphs. Undirected\\nedge which can be treated as two directed edges shows that there is a relation between\\ntwo nodes. However, directed edges can bring more information than undirected\\nedges. For example, in a knowledge graph where the edge starts from the head entity\\nand ends at the tail entity, the head entity is the parent class of the tail entity, which\\nsuggests we should treat the information propagation process from parent classes\\nand child classes differently. DGP [55] uses two kinds of the weight matrix, Wp\\nand Wc, to incorporate more precise structural information. The propagation rule is\\nshown as follows:\\nH(t) = f (D\\xe2\\x88\\x921\\np Ap f (D\\xe2\\x88\\x921\\nc AcH(t\\xe2\\x88\\x921)Wc)Wp),\\n(8.100)\\nwhere D\\xe2\\x88\\x921\\np Ap, D\\xe2\\x88\\x921\\nc Ac are the normalized adjacency matrix for parents and children,\\nrespectively.\\nHeterogeneous Graphs. The second variant of the graph is a heterogeneous\\ngraph, where there are several kinds of nodes. The simplest way to process the\\nheterogeneous graph is to convert the type of each node to a one-hot feature vector\\nwhich is concatenated with the original feature.\\nWhat\\xe2\\x80\\x99s more, GraphInception [151] introduces the concept of metapath into the\\npropagation on the heterogeneous graph. With metapath, we can group the neighbors\\naccording to their node types and distances. For each neighbor group, GraphInception\\ntreats it as a subgraph in a homogeneous graph to do propagation and concatenates\\nthe propagation results from different homogeneous graphs to do a collective node\\nrepresentation.Recently,[128]proposestheHeterogeneousgraphAttentionNetwork\\n(HAN) which utilizes node-level and semantic-level attention. And the model has\\nthe ability to consider node importance and metapaths simultaneously.\\nGraphs with Edge Information. In another variant of graph, each edge has\\nadditional information like the weight or the type of the edge. We list two ways to\\nhandle this kind of graphs:\\nFirstly, we can convert the graph to a bipartite graph where the original edges\\nalso become nodes and one original edge is split into two new edges which means\\nthere are two new edges between the edge node and begin/end nodes. The encoder\\nof G2S [7] uses the following aggregation function for neighbors:\\nh(t)\\nv\\n= f\\n\\x15\\n1\\n|Nv|\\n\\x02\\nu\\xe2\\x88\\x88Nv\\nWr\\n\\x0f\\nr(t)\\nv \\xe2\\x8a\\x99h(t\\xe2\\x88\\x921)\\nu\\n\\x10\\n+ br\\n\\x16\\n,\\n(8.101)\\nwhere Wr and br are the propagation parameters for different types of edges\\n(relations).\\n266\\n8\\nNetwork Representation\\nSecondly, we can adapt different weight matrices for the propagation of different\\nkinds of edges. When the number of relations is huge, r-GCN [102] introduces two\\nkinds of regularization to reduce the number of parameters for modeling amounts of\\nrelations: basis- and block-diagonal-decomposition. With the basis decomposition,\\neach Wr is de\\xef\\xac\\x81ned as follows:\\nWr =\\nB\\n\\x02\\nb=1\\n\\xce\\xb1rbMb.\\n(8.102)\\nHere each Wr is a linear combination of basis transformations Mb \\xe2\\x88\\x88Rdin\\xc3\\x97dout\\nwith coef\\xef\\xac\\x81cients \\xce\\xb1rb. In the block-diagonal decomposition, r-GCN de\\xef\\xac\\x81nes each Wr\\nthrough the direct sum over a set of low-dimensional matrices, which needs more\\nparameters than the \\xef\\xac\\x81rst one.\\nDynamic Graphs. Another variant of the graph is dynamic graph, which has\\na static graph structure and dynamic input signals. To capture both kinds of infor-\\nmation, DCRNN [71] and STGCN [147] \\xef\\xac\\x81rst collect spatial information by GNNs,\\nthen feed the outputs into a sequence model like sequence-to-sequence model or\\nCNNs. Differently, Structural-RNN [53] and ST-GCN [135] collect spatial and tem-\\nporal messages at the same time. They extend static graph structure with temporal\\nconnections so they can apply traditional GNNs on the extended graphs.\\n8.3.6\\nApplications\\nGraph neural networks have been explored in a wide range of problem domains across\\nsupervised, semi-supervised, unsupervised, and reinforcement learning settings. In\\nthis section, we simply divide the applications into three scenarios: (1) Structural\\nscenarios where the data has explicit relational structure, such as physical systems,\\nmolecular structures, and knowledge graphs; (2) Nonstructural scenarios where the\\nrelational structure is not explicit include image, text, etc; (3) Other application\\nscenarios such as generative models and combinatorial optimization problems. Note\\nthat we only list several representative applications instead of providing an exhaustive\\nlist. We further give some examples of GNNs in the task of fact veri\\xef\\xac\\x81cation and\\nrelation extraction. Figure8.14 illustrates some application scenarios of graph neural\\nnetworks.\\n8.3.6.1\\nStructural Scenarios\\nIn the following, we will introduce GNN\\xe2\\x80\\x99s applications in structural scenarios, where\\nthe data are naturally performed in the graph structure. For example, GNNs are\\nwidely being used in social network prediction [41, 59], traf\\xef\\xac\\x81c prediction [25, 96],\\nrecommender systems [120, 143], and graph representation [144]. Speci\\xef\\xac\\x81cally, we\\n8.3 Graph Neural Networks\\n267\\nFig. 8.14 Application scenarios of graph neural network [155]\\nare discussing how to model real-world physical systems with object-relationship\\ngraphs, how to predict the chemical properties of molecules and biological interaction\\nproperties of proteins and the applications of GNNs on knowledge graphs.\\nPhysics. Modeling real-world physical systems is one of the most fundamental\\naspects of understanding human intelligence. By representing objects as nodes and\\nrelations as edges, we can perform GNN-based reasoning about objects, relations,\\nand physics in a simpli\\xef\\xac\\x81ed but effective way.\\nBattaglia et al. [6] proposes Interaction Networks to make predictions and infer-\\nencesaboutvariousphysicalsystems.Objectsandrelationsare\\xef\\xac\\x81rstfedintothemodel\\nas input. Then the model considers the interactions and physical dynamics to predict\\nnew states. They separately model relation-centric and object-centric models, mak-\\ning it easier to generalize across different systems. In CommNet [107], interactions\\nare not modeled explicitly. Instead, an interaction vector is obtained by averaging all\\nother agents\\xe2\\x80\\x99 hidden vectors. VAIN [49] further introduced attentional methods into\\nthe agent interaction process, which preserves both the complexity advantages and\\ncomputational ef\\xef\\xac\\x81ciency as well.\\n268\\n8\\nNetwork Representation\\nVisual Interaction Networks [132] can make predictions from pixels. It learns a\\nstate code from two consecutive input frames for each object. Then, after adding\\ntheir interaction effect by an Interaction Net block, the state decoder converts state\\ncodes to the next step\\xe2\\x80\\x99s state.\\nSanchez-Gonzalez et al. [99] proposes a Graph Network based model which could\\neither perform state prediction or inductive inference. The inference model takes\\npartially observed information as input and constructs a hidden graph for implicit\\nsystem classi\\xef\\xac\\x81cation.\\nMolecular Fingerprints. Molecular \\xef\\xac\\x81ngerprints are feature vectors representing\\nmolecules, which are important in computer-aided drug design. Traditional molecu-\\nlar \\xef\\xac\\x81ngerprint discovering relies on heuristic methods which are hand-crafted. And\\nGNNs can provide more \\xef\\xac\\x82exible approaches for better \\xef\\xac\\x81ngerprints.\\nDuvenaud et al. [31] propose neural graph \\xef\\xac\\x81ngerprints (Neural FPs) that calculate\\nsubstructure feature vectors via GCN and sum to get overall representation. The\\naggregation function is introduced in Eq.8.83.\\nKearnes et al. [56] further explicitly models atom and atom pairs independently\\nto emphasize atom interactions. It introduces edge representation e(t)\\nuv instead of\\naggregation function, i.e., h(t)\\nNv = \\x04\\nu\\xe2\\x88\\x88Nv e(t)\\nuv. The node update function is\\nh(t+1)\\nv\\n= ReLU(W1[ReLU(W0h(t)\\nu ); h(t)\\nNv]),\\n(8.103)\\nwhile the edge update function is\\ne(t+1)\\nuv\\n= ReLU(W4[ReLU(W2e(t)\\nuv); ReLU(W3[h(t)\\nv ; h(t)\\nu ])]).\\n(8.104)\\nProtein Interface Prediction. Fout et al. [33] focuses on the task named protein\\ninterface prediction, which is a challenging problem with critical applications in\\ndrug discovery and design. The proposed GCN-based method, respectively, learns\\nligand and receptor protein residue representation and merges them for pair-wise\\nclassi\\xef\\xac\\x81cation.\\nGNN can also be used in biomedical engineering. With Protein-Protein Inter-\\naction Network, [97] leverages graph convolution and relation network for breast\\ncancer subtype classi\\xef\\xac\\x81cation. Zitnik et al. [160] also suggest a GCN-based model\\nfor polypharmacy side effects prediction. Their work models the drug and protein\\ninteraction network and separately deals with edges in different types.\\nKnowledge Graph. Hamaguchi et al. [40] utilizes GNNs to solve the Out-Of-\\nKnowledge-Base (OOKB) entity problem in Knowledge Base Completion (KBC).\\nThe OOKB entities in [40] are directly connected to the existing entities thus the\\nembeddings of OOKB entities can be aggregated from the existing entities. The\\nmethod achieves satisfying performance both in the standard KBC setting and the\\nOOKB setting.\\nWang et al. [130] utilize GCNs to solve the cross-lingual knowledge graph align-\\nment problem. The model embeds entities from different languages into a uni\\xef\\xac\\x81ed\\nembedding space and aligns them based on the embedding similarity.\\n8.3 Graph Neural Networks\\n269\\n8.3.6.2\\nNonstructural Scenarios\\nIn this section we will talk about applications on nonstructural scenarios such as\\nimage, text, programming source code [1, 72], and multi-agent systems [49, 58,\\n107]. We will only give a detailed introduction to the \\xef\\xac\\x81rst two scenarios due to the\\nlength limit. Roughly, there are two ways to apply the graph neural networks on\\nnonstructural scenarios: (1) Incorporate structural information from other domains\\nto improve the performance, for example, using information from knowledge graphs\\nto alleviate the zero-shot problems in image tasks; (2) Infer or assume the relational\\nstructure in the scenario and then apply the model to solve the problems de\\xef\\xac\\x81ned on\\ngraphs, such as the method in [152] which models text as graphs.\\nImage classi\\xef\\xac\\x81cation. Image classi\\xef\\xac\\x81cation is a fundamental and essential task\\nin the \\xef\\xac\\x81eld of computer vision, which attracts much attention and has many famous\\ndatasets like ImageNet [62]. Recent progress in image classi\\xef\\xac\\x81cation bene\\xef\\xac\\x81ts from big\\ndata and the strong power of GPU computation, which allows us to train a classi\\xef\\xac\\x81er\\nwithout extracting structural information from images. However, zero-shot and few-\\nshot learning become more and more popular in the \\xef\\xac\\x81eld of image classi\\xef\\xac\\x81cation,\\nbecause most models can achieve similar performance with enough data. There are\\nseveral works leveraging graph neural networks to incorporate structural information\\nin image classi\\xef\\xac\\x81cation.\\nFirst, knowledge graphs can be used as extra information to guide zero-shot recog-\\nnition classi\\xef\\xac\\x81cation [55, 129]. Wang et al. [129] builds a knowledge graph where\\neach node corresponds to an object category and takes the word embeddings of nodes\\nas input for predicting the classi\\xef\\xac\\x81er of different categories. As the over-smoothing\\neffect happens with the deep depth of convolution architecture, the 6-layer GCN used\\nin [129] will wash out much useful information in the representation. To solve the\\nsmoothing problem in the propagation of GCN, [55] uses single-layer GCN with a\\nlarger neighborhood, which includes both one-hop and multi-hop nodes in the graph.\\nAnd it proved effective in building a zero-shot classi\\xef\\xac\\x81er with existing ones.\\nExcept for the knowledge graph, the similarity between images in the dataset is\\nalso helpful for few-shot learning [100]. Satorras and Estrach [100] propose to build\\na weighted fully connected image network based on the similarity and do message\\npassing in the graph for few-shot recognition. As most knowledge graphs are large\\nfor reasoning, [77] selects some related entities to build a subgraph based on the\\nresult of object detection and apply GGNN to the extracted graph for prediction.\\nBesides, [66] proposes to construct a new knowledge graph where the entities are all\\nthe categories. And, they de\\xef\\xac\\x81ned three types of label relations: super-subordinate,\\npositive correlation, and negative correlation and propagate the con\\xef\\xac\\x81dence of labels\\nin the graph directly.\\nVisual reasoning. Computer vision systems usually need to perform reasoning\\nby incorporating both spatial and semantic information. So it is natural to generate\\ngraphs for reasoning tasks.\\nA typical visual reasoning task is Visual Question Answering (VQA), [114],\\nrespectively, constructs image scene graph and question syntactic graph. Then it\\napplies GGNN to train the embeddings for predicting the \\xef\\xac\\x81nal answer. Despite spatial\\n270\\n8\\nNetwork Representation\\nconnections among objects, [87] builds the relational graphs conditioned on the\\nquestions. With knowledge graphs, [83, 131] can perform \\xef\\xac\\x81ner relation exploration\\nand a more interpretable reasoning process.\\nOther applications of visual reasoning include object detection, interaction detec-\\ntion, and region classi\\xef\\xac\\x81cation. In object detection [39, 50], GNNs are used to calcu-\\nlate RoI features; In interaction detection [53, 95], GNNs are message-passing tools\\nbetween human and objects; In region classi\\xef\\xac\\x81cation [18], GNNs perform reasoning\\non graphs which connects regions and classes.\\nText Classi\\xef\\xac\\x81cation. Text classi\\xef\\xac\\x81cation is an essential and classical problem in\\nnatural language processing. The classical GCN models [4, 28, 41, 47, 59, 82] and\\nGAT model [122] are applied to solve the problem, but they only use the structural\\ninformation between the documents and they do not use much text information.\\nPeng et al. [91] propose a graph-CNN-based deep learning model. It \\xef\\xac\\x81rst turns\\ntexts to graph-of-words, then the graph convolution operations in [347] are used\\non the word graph. Zhang et al. [152] propose the Sentence LSTM to encode text.\\nThe whole sentence is represented in a single state which contains a global sentence-\\nlevel state and several substates for individual words. It uses the global sentence-level\\nrepresentation for classi\\xef\\xac\\x81cation tasks.\\nThese methods either view a document or a sentence as a graph of word nodes\\nor rely on the document citation relation to construct the graph. Yao et al. [142]\\nregard the documents and words as nodes to construct the corpus graph (hence\\nheterogeneous graph) and uses the Text GCN to learn embeddings of words and\\ndocuments. Sentiment classi\\xef\\xac\\x81cation could also be regarded as a text classi\\xef\\xac\\x81cation\\nproblem and a Tree-LSTM approach is proposed by [109].\\nSequence Labeling. As each node in GNNs has its hidden state, we can utilize\\nthe hidden state to address the sequence labeling problem if we consider every word\\nin the sentence as a node. Zhang et al. [152] utilize the Sentence LSTM to label the\\nsequence. It has conducted experiments on POS-tagging and NER tasks and achieves\\npromising performance.\\nSemantic role labeling is another task of sequence labeling. Marcheggiani and\\nTitov [76] propose a Syntactic GCN to solve the problem. The Syntactic GCN which\\noperates on the direct graph with labeled edges is a special variant of the GCN [59].\\nIt uses edge-wise gates that enable the model to regulate the contribution of each\\ndependency edge. The Syntactic GCNs over syntactic dependency trees are used as\\nsentence encoders to learn latent feature representations of words in the sentence.\\nMarcheggiani and Titov [76] also reveal that GCNs and LSTMs are functionally\\ncomplementary in the task.\\n8.3.6.3\\nOther Scenarios\\nBesides structural and nonstructural scenarios, there are some other scenarios where\\ngraph neural networks play an important role. In this subsection, we will introduce\\ngenerative graph models and combinatorial optimization with GNNs.\\n8.3 Graph Neural Networks\\n271\\nGenerative Models. Generative models for real-world graphs have drawn signif-\\nicant attention for its essential applications, including modeling social interactions,\\ndiscovering new chemical structures, and constructing knowledge graphs. As deep\\nlearning methods have a powerful ability to learn the implicit distribution of graphs,\\nthere is a surge in neural graph generative models recently.\\nNetGAN [104] is one of the \\xef\\xac\\x81rst works to build a neural graph generative model,\\nwhich generates graphs via random walks. It transformed the problem of graph\\ngeneration to the problem of walk generation, which takes the random walks from a\\nspeci\\xef\\xac\\x81c graph as input and trains a walk generative model using GAN architecture.\\nWhile the generated graph preserves essential topological properties of the original\\ngraph, the number of nodes is unable to change in the generating process, which is\\nas same as the original graph. GraphRNN [146] generate the adjacency matrix of a\\ngraph by generating the adjacency vector of each node step by step, which can output\\nrequired networks having different numbers of nodes.\\nInstead of generating adjacency matrix sequentially, MolGAN [27] predict a\\ndiscrete graph structure (the adjacency matrix) at once and utilizes a permutation-\\ninvariant discriminator to solve the node variant problem in the adjacency matrix.\\nBesides, it applies a reward network for RL-based optimization towards desired\\nchemical properties. What is more, [75] proposes constrained variational autoen-\\ncoders to ensure the semantic validity of generated graphs. Moreover, GCPN [145]\\nincorporates domain-speci\\xef\\xac\\x81c rules through reinforcement learning.\\nLi et al. [73] propose a model that generates edges and nodes sequentially and\\nutilize a graph neural network to extract the hidden state of the current graph, which\\nis used to decide the action in the next step during the sequential generative process.\\nCombinatorial optimization. Combinatorial optimization problems over graphs\\nare a set of NP-hard problems that attract much attention from scientists of all \\xef\\xac\\x81elds.\\nSome speci\\xef\\xac\\x81c problems like Traveling Salesman Problem (TSP) have got various\\nheuristic solutions. Recently, using a deep neural network for solving such problems\\nhas been a hotspot, and some of the solutions further leverage graph neural networks\\nbecause of their graph structure.\\nBello et al. [9] \\xef\\xac\\x81rst propose a deep learning approach to tackle TSP. Their method\\nconsistsoftwoparts:aPointerNetwork[123]forparameterizingrewardsandapolicy\\ngradient [108] module for training. This work has been proved to be comparable with\\ntraditional approaches. However, Pointer Networks are designed for sequential data\\nlike texts, while order-invariant encoders are more appropriate for such work.\\nKhalil et al. [57] and Kool and Welling [61] improve the above method by includ-\\ning graph neural networks. The former work \\xef\\xac\\x81rst obtains the node embeddings from\\nstructure2vec [26] then feeds them into a Q-learning module for making decisions.\\nThe latter one builds an attention-based encoder-decoder system. By replacing the\\nreinforcement learning module with an attention-based decoder, it is more ef\\xef\\xac\\x81cient\\nfor training. These work achieved better performance than previous algorithms,\\nwhich proved the representation power of graph neural networks.\\nNowaketal.[88]focusonQuadraticAssignmentProblem,i.e.,measuringthesim-\\nilarity of two graphs. The GNN-based model learns node embeddings for each graph\\nindependently and matches them using an attention mechanism. Even in situations\\n272\\n8\\nNetwork Representation\\nwhere traditional relaxation-based methods may perform not well, this model still\\nshows satisfying performance.\\n8.3.6.4\\nExample: GNNs for Fact Veri\\xef\\xac\\x81cation\\nDue to the rapid development of Information Extraction (IE), huge volumes of data\\nhave been extracted. How to automatically verify the data becomes a vital problem\\nfor various data-driven applications, e.g., knowledge graph completion [126] and\\nopen domain question answering [15]. Hence, many recent research efforts have\\nbeen devoted to Fact Veri\\xef\\xac\\x81cation (FV), which aims to verify given claims with the\\nevidence retrieved from plain text. More speci\\xef\\xac\\x81cally, given a claim, an FV system\\nis asked to label it as \\xe2\\x80\\x9cSUPPORTED\\xe2\\x80\\x9d, \\xe2\\x80\\x9cREFUTED\\xe2\\x80\\x9d, or \\xe2\\x80\\x9cNOT ENOUGH INFO\\xe2\\x80\\x9d,\\nwhich indicates that the evidence can support, refute, or is not suf\\xef\\xac\\x81cient for the claim.\\nAn example of the FV task is shown in Table8.5.\\nExisting FV methods formulate FV as a Natural Language Inference (NLI) [3]\\ntask. However, they utilize simple evidence combination methods such as concate-\\nnating the evidence or just dealing with each evidence-claim pair. These methods are\\nunable to grasp suf\\xef\\xac\\x81cient relational and logical information among the evidence. In\\nfact, many claims require to simultaneously integrate and reason over several pieces\\nof evidence for veri\\xef\\xac\\x81cation. As shown in Table8.5, for this particular example, we\\ncannot verify the given claim by checking any evidence in isolation. The claim can\\nbe veri\\xef\\xac\\x81ed only by understanding and reasoning over multiple evidence.\\nTable 8.5 A case of the claim that requires integrating multiple evidence to verify. The represen-\\ntation for evidence \\xe2\\x80\\x9c{DocName, LineNum}\\xe2\\x80\\x9d means the evidence is extracted from the document\\n\\xe2\\x80\\x9cDocName\\xe2\\x80\\x9d and of which the line number is LineNum\\nClaim:\\nAl Jardine is an American rhythm guitarist\\nTruth evidence:\\n{Al Jardine, 0}, {Al Jardine, 1}\\nRetrieved evidence:\\n{Al Jardine, 1}, {Al Jardine, 0}, {Al Jardine, 2}, {Al Jardine, 5}, {Jardine, 42}\\nEvidence:\\n(1) He is best known as the band\\xe2\\x80\\x99s rhythm guitarist, and for occasionally singing lead vocals\\non singles such as \\xe2\\x80\\x9cHelp Me, Rhonda\\xe2\\x80\\x9d (1965), \\xe2\\x80\\x9cThen I Kissed Her\\xe2\\x80\\x9d (1965), and \\xe2\\x80\\x9cCome Go with\\nMe\\xe2\\x80\\x9d (1978)\\n(2) Alan Charles Jardine (born September 3, 1942) is an American musician, singer and\\nsongwriter who co-founded the Beach Boys\\n(3) In 2010, Jardine released his debut solo studio album, A Postcard from California\\n(4) In 1988, Jardine was inducted into the Rock and Roll Hall of Fame as a member of the\\nBeach Boys\\n(5) Ray Jardine American rock climber, lightweight backpacker, inventor, author, and global\\nadventurer\\nLabel: SUPPORTED\\n8.3 Graph Neural Networks\\n273\\nTo integrate and reason over information from multiple pieces of evidence, [156]\\nproposes a graph-based evidence aggregating and reasoning (GEAR) framework.\\nSpeci\\xef\\xac\\x81cally, [156] \\xef\\xac\\x81rst builds a fully connected evidence graph and encourages\\ninformation propagation among the evidence. Then, GEAR aggregates the pieces\\nof evidence and adopts a classi\\xef\\xac\\x81er to decide whether the evidence can support,\\nrefute, or is not suf\\xef\\xac\\x81cient for the claim. Intuitively, by suf\\xef\\xac\\x81ciently exchanging and\\nreasoning over evidence information on the evidence graph, the proposed model can\\nmake the best of the information for verifying claims. For example, by delivering the\\ninformation \\xe2\\x80\\x9cLos Angeles County is the most populous county in the USA\\xe2\\x80\\x9d to \\xe2\\x80\\x9cthe\\nRodney King riots occurred in Los Angeles County\\xe2\\x80\\x9d through the evidence graph,\\nthe synthetic information can support \\xe2\\x80\\x9cThe Rodney King riots took place in the\\nmost populous county in the USA\\xe2\\x80\\x9d. Furthermore, we adopt an effective pretrained\\nlanguage representation model BERT [29] to better grasp both evidence and claim\\nsemantics.\\nZhou et al. [156] employ a three-step pipeline with components for document\\nretrieval, sentence selection, and claim veri\\xef\\xac\\x81cation to solve the task. In the document\\nretrieval and sentence selection stages, they simply follow the method from [44] since\\ntheir method has the highest score on evidence recall in the former FEVER-shared\\ntask. And they propose the GEAR framework in the \\xef\\xac\\x81nal claim veri\\xef\\xac\\x81cation stage.\\nThe full pipeline is illustrated in Fig.8.15.\\nGiven a claim and the retrieved evidence, GEAR \\xef\\xac\\x81rst utilizes a sentence encoder\\nto obtain representations for the claim and the evidence. Then it builds a fully con-\\nnected evidence graph and uses an Evidence Reasoning Network (ERNet) to prop-\\nagate information among evidence and reason over the graph. Finally, it utilizes an\\nevidence aggregator to infer the \\xef\\xac\\x81nal results.\\nSentence Encoder. Given an input sentence, GEAR employs BERT [29] as the\\nsentence encoder by extracting the \\xef\\xac\\x81nal hidden state of the [CLS] token as the\\nrepresentation, where [CLS] is the special classi\\xef\\xac\\x81cation token in BERT.\\nei = BERT (ei, c) ,\\nc = BERT (c) .\\n(8.105)\\nFig. 8.15 The pipeline used in [156]. The GEAR framework is illustrated in the claim veri\\xef\\xac\\x81cation\\nsection\\n274\\n8\\nNetwork Representation\\nEvidence Reasoning Network. To encourage the information propagation among\\nevidence, GEAR builds a fully connected evidence graph where each node indi-\\ncates a piece of evidence. It also adds self-loop to every node because each node\\nneeds the information from itself in the message propagation process. We use\\nht = {ht\\n1, ht\\n2, . . . , ht\\nN} to represent the hidden states of nodes at layer t. The ini-\\ntial hidden state of each evidence node h0\\ni is initialized by the evidence presentation:\\nh0\\ni = ei.\\nInspired by recent work on semi-supervised graph learning and relational rea-\\nsoning [59, 90, 122], Zhou et al. [156] propose an Evidence Reasoning Network\\n(ERNet) to propagate information among the evidence nodes. It \\xef\\xac\\x81rst uses an MLP\\nto compute the attention coef\\xef\\xac\\x81cients between a node i and its neighbor j ( j \\xe2\\x88\\x88Ni),\\nyi j = W(t\\xe2\\x88\\x921)\\n1\\n(ReLU(W(t\\xe2\\x88\\x921)\\n0\\n[h(t\\xe2\\x88\\x921)\\ni\\n; h(t\\xe2\\x88\\x921)\\nj\\n])),\\n(8.106)\\nwhere Ni denotes the set of neighbors of node i, W(t\\xe2\\x88\\x921)\\n0\\nand W(t\\xe2\\x88\\x921)\\n1\\nare weight\\nmatrices, and [\\xc2\\xb7; \\xc2\\xb7] denotes concatenation operation.\\nThen, it normalizes the coef\\xef\\xac\\x81cients using the softmax function\\n\\xce\\xb1i j = Softmax j(yi j) =\\nexp(yi j)\\n\\x04\\nk\\xe2\\x88\\x88Ni exp(yik).\\n(8.107)\\nFinally, the normalized attention coef\\xef\\xac\\x81cients are used to compute a linear com-\\nbination of the neighbor features and thus we obtain the features for node i at layer\\nt,\\nh(t)\\ni\\n=\\n\\x02\\nj\\xe2\\x88\\x88Ni\\n\\xce\\xb1i jh(t\\xe2\\x88\\x921)\\nj\\n.\\n(8.108)\\nBy stacking T layers of ERNet, [156] assumes that each evidence could grasp\\nenough information by communicating with other evidence.\\nEvidence Aggregator. Zhou et al. [156] employ an evidence aggregator to gather\\ninformation from different evidence nodes and obtain the \\xef\\xac\\x81nal hidden state o. The\\naggregator may utilize different aggregating strategies and [156] suggests three\\naggregators:\\nAttention Aggregator. Zhou et al. [156] use the representation of the claim c to\\nattend the hidden states of evidence and get the \\xef\\xac\\x81nal aggregated state o.\\ny j = W\\xe2\\x80\\xb2\\n1(ReLU(W\\xe2\\x80\\xb2\\n0[c; h\\xe2\\x8a\\xa4\\nj ])),\\n\\xce\\xb1 j = Softmax(y j) =\\nexp(y j)\\n\\x04N\\nk=1 exp(yk)\\n,\\no =\\nN\\n\\x02\\nk=1\\n\\xce\\xb1kh\\xe2\\x8a\\xa4\\nk .\\n(8.109)\\n8.3 Graph Neural Networks\\n275\\nMax Aggregator. The max aggregator performs the element-wise max operation\\namong hidden states.\\no = Max(h\\xe2\\x8a\\xa4\\n1 , h\\xe2\\x8a\\xa4\\n2 , . . . , h\\xe2\\x8a\\xa4\\nN).\\n(8.110)\\nMean Aggregator. The mean aggregator performs the element-wise mean opera-\\ntion among hidden states.\\no = Mean(h\\xe2\\x8a\\xa4\\n1 , h\\xe2\\x8a\\xa4\\n2 , . . . , h\\xe2\\x8a\\xa4\\nN).\\n(8.111)\\nOnce the \\xef\\xac\\x81nal state o is obtained, GEAR employs a one-layer MLP to get the \\xef\\xac\\x81nal\\nprediction l.\\nl = Softmax(ReLU(Wo + b)),\\n(8.112)\\nwhere W and b are parameters.\\nZhou et al. [156] conduct experiments on the large-scale benchmark dataset for\\nFact Extraction and VERi\\xef\\xac\\x81cation (FEVER) [115]. Experimental results show that\\nthe proposed framework outperforms recent state-of-the-art baseline systems. The\\nfurther case study indicates that the framework could better leverage multi-evidence\\ninformation and reason over the evidence for FV.\\n8.4\\nSummary\\nIn this chapter, we have introduced network representation learning, which turns\\nthe network structure information into the continuous vector space and make deep\\nlearning techniques possible on network data.\\nUnsupervised network representation learning comes \\xef\\xac\\x81rst during the development\\nof NRL. Spectral Clustering, DeepWalk, LINE, GraRep, and other methods utilize\\nthe network structure for vertex embedding learning. Afterward, TADW incorporates\\ntext information into NRL under the framework of matrix factorization. The NEU\\nalgorithm then moves one step forward and proposes a general method to improve\\nthe quality of any learned network embeddings. Other unsupervised methods also\\nconsider preserving speci\\xef\\xac\\x81c properties of the network topology, e.g., community and\\nasymmetry.\\nRecently, semi-supervised NRL algorithms have attracted much attention. This\\nkind of methods focus on a speci\\xef\\xac\\x81c task such as classi\\xef\\xac\\x81cation and use the labels of\\nthe training set to improve the quality of network embeddings. Node2vec, MMDW,\\nand many other methods including the family of Graph Neural Networks (GNNs)\\nare proposed for this end. Semi-supervised algorithms can achieve better results as\\nthey can take advantage of more information from the speci\\xef\\xac\\x81c task.\\nFor further understanding of network representation learning, you can also \\xef\\xac\\x81nd\\nmore related papers in this paper list https://github.com/thunlp/GNNPapers. There\\nare also some recommended surveys and books including the following:\\n276\\n8\\nNetwork Representation\\n\\xe2\\x80\\xa2 Cui et al. A survey on network embedding [24].\\n\\xe2\\x80\\xa2 Goyal and Ferrara. Graph embedding techniques, applications, and performance:\\nA survey [37].\\n\\xe2\\x80\\xa2 Zhang et al. Network representation learning: A survey [149].\\n\\xe2\\x80\\xa2 Wu et al. A comprehensive survey on graph neural networks [133].\\n\\xe2\\x80\\xa2 Zhou et al. Graph neural networks: A review of methods and applications [155].\\n\\xe2\\x80\\xa2 Zhang et al. Deep learning on graphs: A survey [154].\\nIn the future, for better network representation learning, some directions are\\nrequiring further efforts:\\n(1) More Complex and Realistic Networks. An intriguing direction would be\\nthe representation of learning on heterogeneous and dynamic networks where most\\nreal-world network data fall into this category. The vertices and edges in a heteroge-\\nneous network may belong to different types. Networks in real life are also highly\\ndynamic, e.g., the friendship between Facebook users may establish and disappear.\\nThese characteristics require the researchers to design speci\\xef\\xac\\x81c algorithms for them.\\nNetwork embedding learning on dynamic network structures is, therefore, an impor-\\ntant task. There have been some works proposed [14, 105] for much more complex\\nand realistic settings.\\n(2) Deeper Model Architectures. Conventional deep neural networks can stack\\nhundreds of layers to get better performance because the deeper structure has more\\nparameters and may improve the expressive power signi\\xef\\xac\\x81cantly. However, NRL and\\nGNNmodelsareusuallyshallow.Infact,mostofthemhavenomorethanthreelayers.\\nTaking GCN as an example, as experiments in [70] show, stacking multiple GCN\\nlayers will result in over-smoothing: the representations of all vertices will converge\\nto the same. Although some researchers have managed to tackle this problem [70,\\n125] to some extents, it remains to be a limitation of NRL. Designing deeper model\\narchitectures is an exciting challenge for future research, and will be a considerable\\ncontribution to the understanding of NRL.\\n(3) Scalability. Scalability determines whether an algorithm is able to be applied\\nto practical use. How to apply NRL methods in real-world web-scale scenarios such\\nas social networks or recommendation systems has been an essential problem for\\nmost network embedding algorithms. Scaling up NRL methods especially GNN\\nis dif\\xef\\xac\\x81cult because many core steps are computationally consuming in a big data\\nenvironment. For example, network data are not regular Euclidean, and each node\\nhas its own neighborhood structure. Therefore, batch tricks cannot be easily applied.\\nMoreover, computing graph Laplacian is also unfeasible when there are millions or\\neven billions of nodes and edges. Several works has proposed their solutions to this\\nproblem [143, 153, 157] and we are paying close attention to the progress.\\nReferences\\n277\\nReferences\\n1. Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent\\nprograms with graphs. In Proceedings of ICLR, 2018.\\n2. Reid Andersen, Fan Chung, and Kevin Lang. Local graph partitioning using pagerank vectors.\\nIn Proceedings of FOCS, 2006.\\n3. Gabor Angeli and Christopher D Manning. Naturalli: Natural logic inference for common\\nsense reasoning. In Proceedings of EMNLP, 2014.\\n4. James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Proceedings\\nof NeurIPS, 2016.\\n5. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\\njointly learning to align and translate. In Proceedings of ICLR, 2015.\\n6. Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction\\nnetworks for learning about objects, relations and physics. In Proceedings of NeurIPS, 2016.\\n7. Daniel Beck, Gholamreza Haffari, and Trevor Cohn. Graph-to-sequence learning using gated\\ngraph neural networks. In Proceedings of ACL, 2018.\\n8. Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embed-\\nding and clustering. In Proceedings of NeurIPS, volume 14, 2001.\\n9. Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural com-\\nbinatorial optimization with reinforcement learning. In Proceedings of ICLR, 2017.\\n10. Davide Boscaini, Jonathan Masci, Emanuele Rodol\\xc3\\xa0, and Michael Bronstein. Learning shape\\ncorrespondence with anisotropic convolutional neural networks. In Proceedings of NeurIPS,\\n2016.\\n11. Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Lecun. Spectral networks and locally\\nconnected networks on graphs. In Proceedings of ICLR, 2014.\\n12. Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of\\ngraph embedding: Problems, techniques, and applications. IEEE Transactions on Knowledge\\nand Data Engineering, 30(9):1616\\xe2\\x80\\x931637, 2018.\\n13. Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with\\nglobal structural information. In Proceedings of CIKM, 2015.\\n14. Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C Aggarwal, and Thomas S Huang.\\nHeterogeneous network embedding via deep architectures. In Proceedings of SIGKDD, 2015.\\n15. Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer\\nopen-domain questions. In Proceedings of the ACL, 2017.\\n16. Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks\\nvia importance sampling. In Proceedings of ICLR, 2018.\\n17. Mo Chen, Qiong Yang, and Xiaoou Tang. Directed graph embedding. In Proceedings of IJCAI,\\n2007.\\n18. Xinlei Chen, Lijia Li, Li Feifei, and Abhinav Gupta. Iterative visual reasoning beyond con-\\nvolutions. In Proceedings of CVPR, 2018.\\n19. Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for\\nmachine reading. In Proceedings of EMNLP, 2016.\\n20. Kyunghyun Cho, Bart van Merri\\xc3\\xabnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\\nHolger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder\\xe2\\x80\\x93\\ndecoder for statistical machine translation. In Proceedings of EMNLP, 2014.\\n21. Yoon-Sik Cho, Greg Ver Steeg, and Aram Galstyan. Socially relevant venue clustering from\\ncheck-in data. In Proceedings of KDD Workshop, 2013.\\n22. Wojciech Chojnacki and Michael J Brooks. A note on the locally linear embedding algorithm.\\nInternational Journal of Pattern Recognition and Arti\\xef\\xac\\x81cial Intelligence, 23(08):1739\\xe2\\x80\\x931752,\\n2009.\\n23. Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Math-\\nematical Soc., 1997.\\n24. Peng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. A survey on network embedding. IEEE\\nTransactions on Knowledge and Data Engineering, 2018.\\n278\\n8\\nNetwork Representation\\n25. Zhiyong Cui, Kristian Henrickson, Ruimin Ke, and Yinhai Wang. Traf\\xef\\xac\\x81c graph convolutional\\nrecurrent neural network: A deep learning framework for network-scale traf\\xef\\xac\\x81c learning and\\nforecasting. IEEE Transactions on Intelligent Transportation Systems, 2019.\\n26. Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for\\nstructured data. In Proceedings of ICML, 2016.\\n27. Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular\\ngraphs. arXiv preprint arXiv:1805.11973, 2018.\\n28. Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural net-\\nworks on graphs with fast localized spectral \\xef\\xac\\x81ltering. In Proceedings of NeurIPS, 2016.\\n29. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019.\\n30. Giuseppe Di Battista, Peter Eades, Roberto Tamassia, and Ioannis G Tollis. Algorithms for\\ndrawing graphs: an annotated bibliography. Computational Geometry, 4(5):235\\xe2\\x80\\x93282, 1994.\\n31. David K Duvenaud, Dougal Maclaurin, Jorge Aguileraiparraguirre, Rafael Gomezbombarelli,\\nTimothy D Hirzel, Alan Aspuruguzik, and Ryan P Adams. Convolutional networks on graphs\\nfor learning molecular \\xef\\xac\\x81ngerprints. In Proceedings of NeurIPS, 2015.\\n32. Francois Fouss, Alain Pirotte, Jean-Michel Renders, and Marco Saerens. Random-walk com-\\nputation of similarities between nodes of a graph with application to collaborative recommen-\\ndation. IEEE Transactions on Knowledge and Data Engineering, 19(3):355\\xe2\\x80\\x93369, 2007.\\n33. Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using\\ngraph convolutional networks. In Proceedings of NeurIPS, 2017.\\n34. Thomas MJ Fruchterman and Edward M Reingold. Graph drawing by force-directed place-\\nment. Software: Practice and Experience, 21(11):1129\\xe2\\x80\\x931164, 1991.\\n35. Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolu-\\ntional networks. In Proceedings of SIGKDD, 2018.\\n36. Jonas Gehring, Michael Auli, David Grangier, and Yann N Dauphin. A convolutional encoder\\nmodel for neural machine translation. In Proceedings of ACL, 2017.\\n37. Palash Goyal and Emilio Ferrara. Graph embedding techniques, applications, and perfor-\\nmance: A survey. Knowledge-Based Systems, 151:78\\xe2\\x80\\x9394, 2018.\\n38. Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Pro-\\nceedings of SIGKDD, 2016.\\n39. Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, and Jifeng Dai. Learning region features for\\nobject detection. In Proceedings of ECCV, 2018.\\n40. Takuo Hamaguchi, Hidekazu Oiwa, Masashi Shimbo, and Yuji Matsumoto. Knowledge trans-\\nfer for out-of-knowledge-base entities : A graph neural network approach. In Proceedings of\\nIJCAI, 2017.\\n41. Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large\\ngraphs. In Proceedings of NeurIPS, 2017.\\n42. William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Meth-\\nods and applications. IEEE Data(base) Engineering Bulletin, 40:52\\xe2\\x80\\x9374, 2017.\\n43. David K Hammond, Pierre Vandergheynst, and Remi Gribonval. Wavelets on graphs via\\nspectral graph theory. Applied and Computational Harmonic Analysis, 30(2):129\\xe2\\x80\\x93150, 2011.\\n44. Andreas Hanselowski, Hao Zhang, Zile Li, Daniil Sorokin, Benjamin Schiller, Claudia Schulz,\\nand Iryna Gurevych. Ukp-athene: Multi-sentence textual entailment for claim veri\\xef\\xac\\x81cation. In\\nProceedings of EMNLP, 2018.\\n45. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition. In Proceedings of CVPR, 2016.\\n46. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\\nnetworks. In Proceedings of ECCV, 2016.\\n47. Mikael Henaff, Joan Bruna, and Yann Lecun. Deep convolutional networks on graph-\\nstructured data. arXiv preprint arXiv:1506.05163, 2015.\\n48. Sepp Hochreiter and J\\xc3\\xbcrgen Schmidhuber. Long short-term memory. Neural Computation,\\n9(8):1735\\xe2\\x80\\x931780, 1997.\\nReferences\\n279\\n49. Yedid Hoshen. Vain: Attentional multi-agent predictive modeling. In Proceedings of NeurIPS,\\n2017.\\n50. Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\\ndetection. In Proceedings of CVPR, 2018.\\n51. Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast\\ngraph representation learning. In Proceedings of NeurIPS, 2018.\\n52. Yann Jacob, Ludovic Denoyer, and Patrick Gallinari. Learning latent representations of nodes\\nfor classifying in heterogeneous social networks. In Proceedings of WSDM, 2014.\\n53. Ashesh Jain, Amir R Zamir, Silvio Savarese, and Ashutosh Saxena. Structural-rnn: Deep\\nlearning on spatio-temporal graphs. In Proceedings of CVPR, 2016.\\n54. Tomihisa Kamada and Satoru Kawai. An algorithm for drawing general undirected graphs.\\nInformation Processing Letters, 31(1):7\\xe2\\x80\\x9315, 1989.\\n55. Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, and Eric P\\nXing. Rethinking knowledge graph propagation for zero-shot learning. In Proceedings of\\nCVPR, 2019.\\n56. Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecu-\\nlar graph convolutions: moving beyond \\xef\\xac\\x81ngerprints. Journal of computer-aided molecular\\ndesign, 30(8):595\\xe2\\x80\\x93608, 2016.\\n57. Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial\\noptimization algorithms over graphs. In Proceedings of NeurIPS, 2017.\\n58. Thomas Kipf, Ethan Fetaya, Kuanchieh Wang, Max Welling, and Richard S Zemel. Neural\\nrelational inference for interacting systems. In Proceedings of ICML, 2018.\\n59. Thomas N Kipf and Max Welling. Semi-supervised classi\\xef\\xac\\x81cation with graph convolutional\\nnetworks. In Proceedings of ICLR, 2017.\\n60. Stephen G Kobourov. Spring embedders and force directed graph drawing algorithms. arXiv\\npreprint arXiv:1201.3011, 2012.\\n61. WWM Kool and M Welling. Attention solves your tsp. arXiv preprint arXiv:1803.08475,\\n2018.\\n62. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classi\\xef\\xac\\x81cation with deep\\nconvolutional neural networks. In Proceedings of NeurIPS, 2012.\\n63. Theodoros Lappas, Evimaria Terzi, Dimitrios Gunopulos, and Heikki Mannila. Finding effec-\\ntors in social networks. In Proceedings of SIGKDD, 2010.\\n64. Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436,\\n2015.\\n65. Yann LeCun, L\\xc3\\xa9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning\\napplied to document recognition. Proceedings of the IEEE, 1998.\\n66. Chungwei Lee, Wei Fang, Chihkuan Yeh, and Yuchiang Frank Wang. Multi-label zero-shot\\nlearning with structured knowledge graphs. In Proceedings of CVPR, 2018.\\n67. Jure Leskovec, Lada A Adamic, and Bernardo A Huberman. The dynamics of viral marketing.\\nACM Transactions on the Web (TWEB), 1(1):5, 2007.\\n68. Jure Leskovec, Lars Backstrom, and Jon Kleinberg. Meme-tracking and the dynamics of the\\nnews cycle. In Proceedings of SIGKDD, 2009.\\n69. Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In\\nProceedings of NeurIPS, 2014.\\n70. Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional net-\\nworks for semi-supervised learning. In Proceedings of AAAI, 2018.\\n71. Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural\\nnetwork: Data-driven traf\\xef\\xac\\x81c forecasting. In Proceedings of ICLR, 2018.\\n72. Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S Zemel. Gated graph sequence\\nneural networks. In Proceedings of ICLR, 2016.\\n73. Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep\\ngenerative models of graphs. In Proceedings of ICLR, 2018.\\n74. Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin, and Shuicheng Yan. Semantic object\\nparsing with graph lstm. In Proceedings of ECCV, 2016.\\n280\\n8\\nNetwork Representation\\n75. Tengfei Ma, Jie Chen, and Cao Xiao. Constrained generation of semantically valid graphs via\\nregularizing variational autoencoders. In Proceedings of NeurIPS, 2018.\\n76. Diego Marcheggiani and Ivan Titov. Encoding sentences with graph convolutional networks\\nfor semantic role labeling. In Proceedings of EMNLP, 2017.\\n77. Kenneth Marino, Ruslan Salakhutdinov, and Abhinav Gupta. The more you know: Using\\nknowledge graphs for image classi\\xef\\xac\\x81cation. In Proceedings of CVPR, 2017.\\n78. Jonathan Masci, Davide Boscaini, Michael Bronstein, and Pierre Vandergheynst. Geodesic\\nconvolutional neural networks on riemannian manifolds. In Proceedings of ICCV workshops,\\n2015.\\n79. Julian J McAuley and Jure Leskovec. Learning to discover social circles in ego networks. In\\nProceedings of NeurIPS, 2012.\\n80. T Mikolov and J Dean. Distributed representations of words and phrases and their composi-\\ntionality. Proceedings of NeurIPS, 2013.\\n81. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\\xef\\xac\\x81cient estimation of word\\nrepresentations in vector space. In Proceedings of ICLR, 2013.\\n82. Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and\\nMichael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model\\ncnns. In Proceedings of CVPR, 2017.\\n83. Medhini Narasimhan, Svetlana Lazebnik, and Alexander Gerhard Schwing. Out of the box:\\nReasoning with graph convolution nets for factual visual question answering. In Proceedings\\nof NeurIPS, 2018.\\n84. Mark EJ Newman. Finding community structure in networks using the eigenvectors of matri-\\nces. Physical Review E, 74(3):036104, 2006.\\n85. Mark EJ Newman. Modularity and community structure in networks. Proceedings of the\\nNational Academy of Sciences, 103(23):8577\\xe2\\x80\\x938582, 2006.\\n86. Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural\\nnetworks for graphs. In Proceedings of ICML, 2016.\\n87. Will Norcliffebrown, Stathis Vafeias, and Sarah Parisot. Learning conditioned graph structures\\nfor interpretable visual question answering. In Proceedings of NeurIPS, 2018.\\n88. Alex Nowak, Soledad Villar, Afonso S Bandeira, and Joan Bruna. Revised note on learning\\nquadratic assignment with graph neural networks. In Proceedings of IEEE DSW 2018, 2018.\\n89. Mingdong Ou, Peng Cui, Jian Pei, and Wenwu Zhu. Asymmetric transitivity preserving graph\\nembedding. In Proceedings of SIGKDD, 2016.\\n90. Rasmus Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In Proceedings\\nof NeurIPS, 2018.\\n91. Hao Peng, Jianxin Li, Yu He, Yaopeng Liu, Mengjiao Bao, Lihong Wang, Yangqiu Song,\\nand Qiang Yang. Large-scale hierarchical text classi\\xef\\xac\\x81cation with recursively regularized deep\\ngraph-cnn. In Proceedings of WWW, 2018.\\n92. Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wentau Yih. Cross-\\nsentence n-ary relation extraction with graph lstms. Transactions of the Association for Com-\\nputational Linguistics, 5(1):101\\xe2\\x80\\x93115, 2017.\\n93. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social rep-\\nresentations. In Proceedings of SIGKDD, 2014.\\n94. TrangPham,TruyenTran,DinhPhung,andSvethaVenkatesh.Columnnetworksforcollective\\nclassi\\xef\\xac\\x81cation. In Proceedings of AAAI, 2017.\\n95. Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen, and Songchun Zhu. Learning\\nhuman-object interactions by graph parsing neural networks. In Proceedings of ECCV, 2018.\\n96. Afshin Rahimi, Trevor Cohn, and Timothy Baldwin. Semi-supervised user geolocation via\\ngraph convolutional networks. In Proceedings of ACL, 2018.\\n97. Sungmin Rhee, Seokjun Seo, and Sun Kim. Hybrid approach of relation network and localized\\ngraph convolutional \\xef\\xac\\x81ltering for breast cancer subtype classi\\xef\\xac\\x81cation. In Proceedings of IJCAI,\\n2018.\\n98. Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear\\nembedding. science, 290(5500):2323\\xe2\\x80\\x932326, 2000.\\nReferences\\n281\\n99. Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Ried-\\nmiller, Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for\\ninference and control. In Proceedings of ICML, 2018.\\n100. Victor Garcia Satorras and Joan Bruna Estrach. Few-shot learning with graph neural networks.\\nIn Proceedings of ICLR, 2018.\\n101. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfar-\\ndini. The graph neural network model. IEEE TNN 2009, 20(1):61\\xe2\\x80\\x9380, 2009.\\n102. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and\\nMax Welling. Modeling relational data with graph convolutional networks. In Proceedings of\\nESWC, 2018.\\n103. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-\\nRad. Collective classi\\xef\\xac\\x81cation in network data. AI magazine, 29(3):93\\xe2\\x80\\x9393, 2008.\\n104. Oleksandr Shchur, Daniel Zugner, Aleksandar Bojchevski, and Stephan Gunnemann. Netgan:\\nGenerating graphs via random walks. In Proceedings of ICML, 2018.\\n105. Chuan Shi, Binbin Hu, Wayne Xin Zhao, and S Yu Philip. Heterogeneous information network\\nembedding for recommendation. IEEE Transactions on Knowledge and Data Engineering,\\n31(2):357\\xe2\\x80\\x93370, 2018.\\n106. Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned \\xef\\xac\\x81lters in convolutional\\nneural networks on graphs. In Proceedings of CVPR, 2017.\\n107. Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backprop-\\nagation. In Proceedings of NeurIPS, 2016.\\n108. Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\\n2018.\\n109. Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representa-\\ntions from tree-structured long short-term memory networks. In Proceedings of ACL, 2015.\\n110. Jian Tang, Meng Qu, and Qiaozhu Mei. Pte: Predictive text embedding through large-scale\\nheterogeneous text networks. In Proceedings of SIGKDD, 2015.\\n111. Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-\\nscale information network embedding. In Proceedings of WWW, 2015.\\n112. Lei Tang and Huan Liu. Relational learning via latent social dimensions. In Proceedings of\\nSIGKDD, 2009.\\n113. Lei Tang and Huan Liu. Leveraging social media networks for classi\\xef\\xac\\x81cation. Data Mining\\nand Knowledge Discovery, 23(3):447\\xe2\\x80\\x93478, 2011.\\n114. Damien Teney, Lingqiao Liu, and Anton Van Den Hengel. Graph-structured representations\\nfor visual question answering. In Proceedings of CVPR, 2017.\\n115. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\\nlarge-scale dataset for fact extraction and veri\\xef\\xac\\x81cation. In Proceedings of NAACL-HLT, 2018.\\n116. Cunchao Tu, Hao Wang, Xiangkai Zeng, Zhiyuan Liu, and Maosong Sun. Community-\\nenhanced\\nnetwork\\nrepresentation\\nlearning\\nfor\\nnetwork\\nanalysis.\\narXiv\\npreprint\\narXiv:1611.06645, 2016.\\n117. Cunchao Tu, Xiangkai Zeng, Hao Wang, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun,\\nBo Zhang, and Leyu Lin. A uni\\xef\\xac\\x81ed framework for community detection and network rep-\\nresentation learning. IEEE Transactions on Knowledge and Data Engineering (TKDE),\\n31(6):1051\\xe2\\x80\\x931065, 2018.\\n118. Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, and Maosong Sun. Max-margin deepwalk: Dis-\\ncriminative learning of network representation. In Proceedings of IJCAI, 2016.\\n119. Cunchao Tu, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Transnet: translation-based\\nnetwork representation learning for social relation extraction. In Proceedings of IJCAI, 2017.\\n120. Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph convolutional matrix com-\\npletion. arXiv preprint arXiv:1706.02263, 2017.\\n121. Ashish Vaswani, Noam Shazeer, Niki Parmar, Llion Jones, Jakob Uszkoreit, Aidan N Gomez,\\nand Lukasz Kaiser. Attention is all you need. In Proceedings of NeurIPS, 2017.\\n122. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and\\nYoshua Bengio. Graph attention networks. In Proceedings of ICLR, 2018.\\n282\\n8\\nNetwork Representation\\n123. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Proceedings of\\nNeurIPS, 2015.\\n124. Jacco Wallinga and Peter Teunis. Different epidemic curves for severe acute respiratory syn-\\ndrome reveal similar impacts of control measures. American Journal of epidemiology, 2004.\\n125. Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings\\nof SIGKDD, 2016.\\n126. Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph embedding: A survey\\nof approaches and applications. TKDE, 29(12):2724\\xe2\\x80\\x932743, 2017.\\n127. Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, and Shiqiang Yang. Community\\npreserving network embedding. In Proceedings of AAAI, 2017.\\n128. Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Het-\\nerogeneous graph attention network. In Proceedings of WWW, 2019.\\n129. Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot recognition via semantic embed-\\ndings and knowledge graphs. In Proceedings of CVPR, 2018.\\n130. Zhichun Wang, Qingsong Lv, Xiaohan Lan, and Yu Zhang. Cross-lingual knowledge graph\\nalignment via graph convolutional networks. In Proceedings of EMNLP, 2018.\\n131. Zhouxia Wang, Tianshui Chen, Jimmy S J Ren, Weihao Yu, Hui Cheng, and Liang Lin. Deep\\nreasoning with knowledge graph for social relationship understanding. In Proceedings of\\nIJCAI, 2018.\\n132. Nicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu, and\\nAndrea Tacchetti. Visual interaction networks: Learning a physics simulator from video. In\\nProceedings of NeurIPS, 2017.\\n133. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu.\\nA comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019.\\n134. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Kenichi Kawarabayashi, and\\nStefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In\\nProceedings of ICML, 2018.\\n135. Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for\\nskeleton-based action recognition. In Proceedings of AAAI, 2018.\\n136. Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network repre-\\nsentation learning with rich text information. In Proceedings of IJCAI, 2015.\\n137. Cheng Yang, Maosong Sun, Zhiyuan Liu, and Cunchao Tu. Fast network embedding enhance-\\nment via high order proximity approximation. In Proceedings of IJCAI, 2017.\\n138. Cheng Yang, Maosong Sun, Wayne Xin Zhao, Zhiyuan Liu, and Edward Y Chang. A neural\\nnetwork approach to jointly modeling social networks and mobile trajectories. ACM Trans-\\nactions on Information Systems (TOIS), 35(4):36, 2017.\\n139. Cheng Yang, Jian Tang, Maosong Sun, Ganqu Cui, and Liu Zhiyuan. Multi-scale information\\ndiffusion prediction with reinforced recurrent networks. In Proceedings of IJCAI, 2019.\\n140. Jaewon Yang and Jure Leskovec. Overlapping community detection at scale: a nonnegative\\nmatrix factorization approach. In Proceedings of WSDM, 2013.\\n141. Jaewon Yang, Julian McAuley, and Jure Leskovec. Detecting cohesive and 2-mode commu-\\nnities indirected and undirected networks. In Proceedings of WSDM, 2014.\\n142. Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classi\\xef\\xac\\x81-\\ncation. In Proceedings of AAAI, 2019.\\n143. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure\\nLeskovec. Graph convolutional neural networks for web-scale recommender systems. In Pro-\\nceedings of SIGKDD, 2018.\\n144. Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec.\\nHierarchical graph representation learning with differentiable pooling. In Proceedings of\\nNeurIPS, 2018.\\n145. Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay S Pande, and Jure Leskovec. Graph convolutional\\npolicy network for goal-directed molecular graph generation. In Proceedings of NeurIPS,\\n2018.\\nReferences\\n283\\n146. Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. Graphrnn: Gen-\\nerating realistic graphs with deep auto-regressive models. In Proceedings of ICML, 2018.\\n147. Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks: A\\ndeep learning framework for traf\\xef\\xac\\x81c forecasting. In Proceedings of ICLR, 2018.\\n148. Victoria Zayats and Mari Ostendorf. Conversation modeling on reddit using a graph-structured\\nlstm. Transactions of the Association for Computational Linguistics, 6:121\\xe2\\x80\\x93132, 2018.\\n149. Daokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang. Network representation learning:\\nA survey. IEEE transactions on Big Data, 2018.\\n150. Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit Yan Yeung. Gaan:\\nGated attention networks for learning on large and spatiotemporal graphs. In Proceedings of\\nUAI, 2018.\\n151. Yizhou Zhang, Yun Xiong, Xiangnan Kong, Shanshan Li, Jinhong Mi, and Yangyong Zhu.\\nDeep collective classi\\xef\\xac\\x81cation in heterogeneous information networks. In Proceedings of\\nWWW, 2018.\\n152. Yue Zhang, Qi Liu, and Linfeng Song. Sentence-state lstm for text representation. In Pro-\\nceedings of ACL, 2018.\\n153. Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Maosong Sun, Zhichong Fang, Bo Zhang, and\\nLeyu Lin. Cosine: Compressive network embedding on large-scale information networks.\\narXiv preprint arXiv:1812.08972, 2018.\\n154. Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs: A survey. arXiv preprint\\narXiv:1812.04202, 2018.\\n155. Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng\\nLi, and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv\\npreprint arXiv:1812.08434, 2018.\\n156. JieZhou,XuHan,ChengYang,ZhiyuanLiu,LifengWang,ChangchengLi,andMaosongSun.\\nGEAR: Graph-based evidence aggregating and reasoning for fact veri\\xef\\xac\\x81cation. In Proceedings\\nof ACL 2019, 2019.\\n157. Zhaocheng Zhu, Shizhen Xu, Jian Tang, and Meng Qu. Graphvite: A high-performance cpu-\\ngpu hybrid system for node embedding. In Proceedings of WWW, 2019.\\n158. Chenyi Zhuang and Qiang Ma. Dual graph convolutional networks for graph-based semi-\\nsupervised classi\\xef\\xac\\x81cation. In Proceedings of WWW, 2018.\\n159. Julian G Zilly, Rupesh Kumar Srivastava, Jan Koutnik, and Jurgen Schmidhuber. Recurrent\\nhighway networks. In Proceedings of ICML, 2016.\\n160. Marinka Zitnik, Monica Agrawal, and Jure Leskovec. Modeling polypharmacy side effects\\nwith graph convolutional networks. Intelligent Systems in Molecular Biology, 34(13):258814,\\n2018.\\n161. Zhou Jie, Cui Ganqu, Zhang Zhengyan, Yang Cheng, Liu Zhiyuan, Wang Lifeng, Li\\nChangcheng, and Sun Maosong. Graph neural networks: A review of methods and appli-\\ncations. arXiv preprint arXiv:1812.08434, 2018.\\n162. Liu Zhiyuan and Zhou Jie. Introduction to graph neural networks. Morgan & Claypool Pub-\\nlishers, 2020.\\n284\\n8\\nNetwork Representation\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 9\\nCross-Modal Representation\\nAbstract Cross-modal representation learning is an essential part of representation\\nlearning, which aims to learn latent semantic representations for modalities including\\ntexts,audio,images,videos,etc.Inthischapter,we\\xef\\xac\\x81rstintroducetypicalcross-modal\\nrepresentation models. After that, we review several real-world applications related\\nto cross-modal representation learning including image captioning, visual relation\\ndetection, and visual question answering.\\n9.1\\nIntroduction\\nAs introduced in Wikipedia, a modality is the classi\\xef\\xac\\x81cation of a single independent\\nchannel of sensory input/output between a computer and a human. To be more\\ngeneral, modalities are different means of information exchange between human\\nbeings and the real world. The classi\\xef\\xac\\x81cation is usually based on the form in which\\ninformation is presented to a human. Typical modalities in the real world include\\ntexts, audio, images, videos, etc.\\nCross-modal representation learning is an important part of representation learn-\\ning. In fact, arti\\xef\\xac\\x81cial intelligence is inherently a multi-modal task [30]. Human beings\\nare exposed to multi-modal information every day, and it is normal for us to integrate\\ninformation from different modalities and make comprehensive judgments. Further-\\nmore, different modalities are not independent, but they have correlations more or\\nless. For example, the judgment of a syllable is made by not only the sound we hear\\nbut also the movement of the lips and tongue of the speaker we see. An experiment\\nin [48] shows that a voiced /ba/ with a visual /ga/ is perceived by most people as\\na /da/. Another example is human beings\\xe2\\x80\\x99 ability to consider the 2D image and 3D\\nscan of the same object together and reconstruct its structure: correlations between\\nimage and scan can be found based on the fact that a discontinuity of depth in the\\nscan usually indicates a sharp line in the image [52]. Inspired by this, it is natural\\nfor us to consider the possibility of combining inputs from multi-modalities in our\\narti\\xef\\xac\\x81cial intelligence systems and generate cross-modal representation.\\nNgiam et al. [52] explore the probability of merging multi-modalities into one\\nlearning task. The authors divide a typical machine learning task into three stages:\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_9\\n285\\n286\\n9\\nCross-Modal Representation\\nfeature learning, supervised learning, and prediction. And they further propose four\\nkinds of learning settings for multi-modalities:\\n(1) Single-modal learning: all stages are all done on just one modality.\\n(2) Multi-modal fusion: all stages are all done with all modalities available.\\n(3) Cross-modal learning: in the feature learning stage, all modalities are avail-\\nable, but in supervised learning and prediction, only one modality is used.\\n(4) Shared representation learning: in the feature learning stage, all modalities\\nare available. In supervised learning, only one modality is used, and in prediction, a\\ndifferent modality is used.\\nExperiments show promising results for these multi-modal tasks. When more\\nmodalities are provided (such as multi-modal fusion, cross-modal learning, and\\nshared representation learning), the performance of the system is generally better.\\nIn the following part of this chapter, we will \\xef\\xac\\x81rst introduce cross-modal represen-\\ntation models, which are fundamental parts of cross-modal representation learning\\nin NLP. And then, we will introduce several critical applications, such as image\\ncaptioning, visual relationship detection, and visual question answering.\\n9.2\\nCross-Modal Representation\\nCross-modal representation learning aims to build embeddings using information\\nfrom multiple modalities. Existing cross-modal representation models involving text\\nmodality can be generally divided into two categories: (1) [30, 77] try to fuse infor-\\nmation from different modalities into uni\\xef\\xac\\x81ed embeddings (e.g., visually grounded\\nword representations). (2) Researchers also try to build embeddings for different\\nmodalities in a common semantic space, which allows the model to compute cross-\\nmodal similarity. Such cross-modal similarity can be further utilized for downstream\\ntasks, such as zero-shot recognition [5, 14, 18, 53, 65] and cross-media retrieval [23,\\n55]. In this section, we will introduce these two kinds of cross-modal representation\\nmodels, respectively.\\n9.2.1\\nVisual Word2vec\\nComputing word embeddings is a fundamental task in representation learning for\\nnatural language processing. Typical word embedding models (like Word2vec [49])\\nare trained on a text corpus. These models, while being extremely successful, cannot\\ndiscoverimplicitsemanticrelatednessbetweenwordsthatcouldbeexpressedinother\\nmodalities. Kottur et al. [30] provide an example: even though eat and stare at\\nseem are unrelated from text, images might show that when people are eating\\nsomething they would also tend to stare at it. This implies that considering\\nother modalities when constructing word embeddings may help capture more implicit\\nsemantic relatedness.\\n9.2 Cross-Modal Representation\\n287\\nTwo\\nducks\\nin\\nwater\\nswim\\nwalk\\nsit\\nCNN\\nFig. 9.1 The architecture for word embedding with global visual context\\nVision, being one of the most critical modalities, has attracted attention from\\nresearchers seeking to improve word representation. Several models that incorporate\\nvisual information and improve word embeddings with vision have been proposed.\\nWe introduce two typical word representation models incorporating visual informa-\\ntion in the following.\\n9.2.1.1\\nWord Embedding with Global Visual Context\\nXu et al. [77] propose a model that makes a natural attempt to incorporate visual\\nfeatures. It claims that in most word representation models, only local context infor-\\nmation (e.g., trying to predict a word using neighboring words and phrases) is con-\\nsidered. Global text information (e.g., the topic of the passage), on the other hand,\\nis often neglected. This model extends a simple local context model by using visual\\ninformation as global features (see Fig.9.1).\\nThe input of the model is an image I and a sequence describing it. It is based\\non a simple local context language model: when we consider a certain word wt in\\na sequence, its local feature is the average of embeddings of words in a window,\\ni.e., {wt\\xe2\\x88\\x92k, . . . , wt\\xe2\\x88\\x921, wt+1, . . . , wt+k}. The visual feature is computed directly from\\nthe image I using a CNN and then used as the global feature. The local feature and\\nthe global feature are then concatenated into a vector f. The predicted probability\\nof a word wt (in this blank) is the softmax normalized product of f and the word\\nembedding wt:\\nowt = wT\\nt f,\\n(9.1)\\nP(wt|wt\\xe2\\x88\\x92k, . . . , wt\\xe2\\x88\\x921,wt+1, . . . , wt+k; I) =\\nexp(owt)\\n\\x02\\ni exp(owi).\\n(9.2)\\nThe model is optimized by maximizing the average of log probability:\\n288\\n9\\nCross-Modal Representation\\nL = 1\\nT\\nT\\xe2\\x88\\x92k\\n\\x03\\nt=k\\nlog P(wt|wt\\xe2\\x88\\x92k, . . . , wt\\xe2\\x88\\x921, wt+1, . . . , wt+k; I).\\n(9.3)\\nThe classi\\xef\\xac\\x81cation error will be back-propagated to local text vector (i.e., word\\nembeddings), visual vector, and all model parameters. This accomplishes jointly\\nlearning for a set of word embeddings, a language model, and the model used for\\nvisual encoding.\\n9.2.1.2\\nWord Embedding with Abstract Visual Scene\\nKottur et al. [30] also propose a neural model to capture \\xef\\xac\\x81ne-grained semantics from\\nvisual information. Instead of focusing on literal pixels, the abstract scene behind\\nthe vision is considered. The model takes a pair of the visual scene and a related\\nword sequence (I, w) as input. At each training step, a window is used upon the\\nword sequence w, forming a subsequence Sw. All the words in Sw will be fed into the\\ninput layer using one-hot encoding, and therefore the dimension of the input layer\\nis |V |, which is also the size of the vocabulary. The words are then transformed into\\ntheir embeddings, and the hidden layer is the average of all these embeddings. The\\nsize of the hidden layer is NH, which is also the dimension of the word embeddings.\\nThe hidden layer and the output layer are connected by a full connection matrix of\\ndimension NH \\xe2\\x88\\x97NK and a softmax function. The output layer can be regarded as\\na probability distribution over a discrete-valued function g(\\xc2\\xb7) of the visual scene I\\n(details will be given in the following paragraph). The entire model is optimized by\\nminimizing the objective function:\\nL = \\xe2\\x88\\x92log P(g(w)|Sw).\\n(9.4)\\nThe most important part of the model is the function g(\\xc2\\xb7). It maps the visual scene\\nI into the set {1, 2, . . . , NK}, which indicates what kind of abstract scene it is. In\\npractice, it is learned of\\xef\\xac\\x82ine using K-means clustering, and each cluster represents\\nthe semantics of one kind of visual scenes, consequently, the word sequence w, which\\nis designed to be related to the scene.\\n9.2.2\\nCross-Modal Representation for Zero-Shot Recognition\\nLarge-scale datasets partially support the success of deep learning methods. Even\\nthough the scales of datasets continue to grow larger, and more categories are\\ninvolved, the annotation of datasets is expensive and time-consuming. For many\\ncategories, there are very limited or even no instances, which restricts the scalability\\nof recognition systems.\\n9.2 Cross-Modal Representation\\n289\\nZero-shot recognition is proposed to solve the problem as mentioned above, which\\naims to classify instances of categories that have not been seen during training. Many\\nworksproposetoutilizecross-modalrepresentationforzero-shotimageclassi\\xef\\xac\\x81cation\\n[5, 14, 18, 53, 65]. Speci\\xef\\xac\\x81cally, image representation and category representation\\nare embedded into a common semantic space, where similarities between image and\\ncategory representations can serve for further classi\\xef\\xac\\x81cation. For example, in such a\\ncommon semantic space, the embedding of an image of cat is expected to be closer\\nto the embedding of category cat than the embedding of category truck.\\n9.2.2.1\\nDeep Visual-Semantic Embedding\\nThe challenge of zero-shot learning lies in the absence of instances of unseen cat-\\negories, which makes it challenging to obtain well-performed classi\\xef\\xac\\x81ers of unseen\\ncategories. Frome et al. [18] present a model that utilizes both labeled images and\\ninformation from the large-scale plain text for zero-shot image classi\\xef\\xac\\x81cation. They\\ntry to leverage semantic information from word embeddings and transfer it to image\\nclassi\\xef\\xac\\x81cation systems.\\nTheir model is motivated by the fact that word embeddings incorporate semantic\\ninformation of concepts or categories, which can be potentially utilized as classi-\\n\\xef\\xac\\x81ers of corresponding categories. Similar categories cluster well in semantic space.\\nFor example, in word embedding space, the nearest neighbors of the term tiger\\nshark are similar kinds of sharks, such as bull shark, blacktip shark,\\nsandbar shark, and oceanic whitetip shark. In addition, bound-\\naries between different clusters are clear. The aforementioned properties indicate\\nthat word embeddings can be further utilized as classi\\xef\\xac\\x81ers for recognition systems.\\nSpeci\\xef\\xac\\x81cally, the model \\xef\\xac\\x81rst pretrains word embeddings using the Skip-gram text\\nmodel on large-scale Wikipedia articles. For visual feature extraction, the model pre-\\ntrains a deep convolutional neural network for 1, 000 object categories on ImageNet.\\nThe pretrained word embeddings and the convolutional neural network are used to\\ninitialize the proposed Deep Visual-Semantic Embedding model (DeViSE).\\nTo train the proposed model, they replace the softmax layer of the pretrained\\nconvolutional neural network with a linear projection layer. The model is trained to\\npredict the word embeddings of categories for images using a hinge ranking loss:\\nL (I, y) =\\n\\x03\\nj\\xcc\\xb8=y\\nmax[0, \\xce\\xb3 \\xe2\\x88\\x92wyMI + w jMI],\\n(9.5)\\nwhere wy and w j are the learned word embeddings of the positive label and sampled\\nnegative label, respectively, I denotes the feature of the image obtained from the\\nconvolutional neural network, M is the trainable parameters in linear projection\\nlayer, and \\xce\\xb3 is a hyperparameter in hinge ranking loss. Given an image, the objective\\nrequires the model to produce a higher score for the correct label than randomly\\nchosen labels, where the score is de\\xef\\xac\\x81ned as the dot product of the projected image\\nfeature and word embedding of terms.\\n290\\n9\\nCross-Modal Representation\\nAt test time, given a test image, the score of each possible category is obtained\\nusing the same approach during training. Note that a crucial difference at test time is\\nthat the classi\\xef\\xac\\x81ers (word embeddings) are expanded to all possible categories, includ-\\ning unseen categories. Thus the model is capable of predicting unseen categories.\\nExperiment results show that DeViSE can make zero-shot predictions with more\\nsemantically reasonable errors, which means that even if the prediction is not exactly\\ncorrect, it is semantically related to the ground truth class. However, a drawback is\\nthat although the model can utilize semantic information in word embeddings to\\nmake zero-shot image classi\\xef\\xac\\x81cation, using word embeddings as classi\\xef\\xac\\x81ers restricts\\nthe \\xef\\xac\\x82exibility of the model, which results in inferior performance in the original\\n1, 000 categories compared to the original softmax classi\\xef\\xac\\x81er.\\n9.2.2.2\\nConvex Combination of Semantic Embeddings\\nInspired by DeViSE, [53] proposes a model ConSE that tries to utilize semantic\\ninformation from word embeddings for zero-shot classi\\xef\\xac\\x81cation. A vital difference\\nto DeViSE is that they obtain the semantic embedding of test image using a convex\\ncombination of word embeddings of seen categories. The score of the corresponding\\ncategory determines the weights of the composing word embeddings.\\nSpeci\\xef\\xac\\x81cally, they train a deep convolutional neural network on seen categories.\\nAt test time, given a test image I (possibly from unseen categories), they obtain\\nthe top T con\\xef\\xac\\x81dent predictions of seen categories, where T is a hyperparameter.\\nThen the semantic embedding f (I) of I is determined by the convex combination\\nof semantic embeddings of the top T con\\xef\\xac\\x81dent categories, which can be formally\\nde\\xef\\xac\\x81ned as follows:\\nf (I) = 1\\nZ\\nT\\n\\x03\\nt=1\\nP( \\xcb\\x86y0(I, t)|I) \\xc2\\xb7 w( \\xcb\\x86y0(I, t)),\\n(9.6)\\nwhere \\xcb\\x86y0(I, t) is the tth most con\\xef\\xac\\x81dent training label for I, w( \\xcb\\x86y0(I, t)) is the semantic\\nembedding (word embedding) of \\xcb\\x86y0(I, t), and Z is a normalization factor given by\\nZ =\\nT\\n\\x03\\nt=1\\nP( \\xcb\\x86y0(I, t)|I).\\n(9.7)\\nAfter obtaining the semantic embedding f (I), the score of the category m is given\\nby the cosine similarity of f (I) and w(m).\\nThe motivation of ConSE is that they assume novel categories can be mod-\\neled as the convex combination of seen categories. If the model is highly con\\xef\\xac\\x81-\\ndent about a prediction, (i.e., P( \\xcb\\x86y0(I, 1)|I) \\xe2\\x89\\x881), the semantic embedding f (I) will\\nbe close to w( \\xcb\\x86y0(I, 1)). If the predictions are ambiguous, (e.g., P(tiger|I) =\\n0.5, P(lion|I) = 0.5), the semantic embedding f (I) will be between w(lion)\\nand w(tiger). And they expect the semantic embedding f (I) = 0.5w(lion) +\\n9.2 Cross-Modal Representation\\n291\\n0.5w(tiger) to be close to the semantic embedding w(liger) (a hybrid cross\\nbetween lions and tigers).\\nAlthough ConSE and DeViSE share many similarities, there are also some crucial\\ndifferences. DeViSE replaces the softmax layer of the pretrained visual model with a\\nprojection layer, while ConSE preserves the softmax layer. ConSE does not need to\\nbe further trained and uses a convex combination of semantic embeddings to perform\\nzero-shot classi\\xef\\xac\\x81cation at test time. Experiment results show that ConSE outperforms\\nDeViSE on unseen categories, indicating better generalization capability. However,\\nthe performance of ConSE on seen categories is not as competitive as DeViSE and\\nthe original softmax classi\\xef\\xac\\x81er.\\n9.2.2.3\\nCross-Modal Transfer\\nSocher et al. [65] present a cross-modal representation model for zero-shot recogni-\\ntion. In their model, all word vectors are initialized with pretrained 50-dimensional\\nword vectors and are kept \\xef\\xac\\x81xed during training. Each image is represented by a vector\\nI constructed by a deep convolutional neural network. They \\xef\\xac\\x81rst project an image\\ninto semantic word spaces by minimizing\\nL (\\xce\\x98) =\\n\\x03\\ny\\xe2\\x88\\x88Ys\\n\\x03\\nI (i)\\xe2\\x88\\x88X y\\n\\xe2\\x88\\xa5wy \\xe2\\x88\\x92\\xce\\xb8(2) f (\\xce\\xb8(1)I(i))\\xe2\\x88\\xa52,\\n(9.8)\\nwhere Ys denotes the set of images\\xe2\\x80\\x99 classes which can be seen in training data,\\nX y denotes the set of images\\xe2\\x80\\x99 vectors of class y, wy denotes the word vector of\\nclass y, and \\xce\\x98 = (\\xce\\xb8(1), \\xce\\xb8(2)) denotes parameters of the 2-layer neural network with\\nf (\\xc2\\xb7) = tanh(\\xc2\\xb7) as activation function.\\nThey observe that instances from unseen categories are usually outliers of the\\ncomplete data manifold. Following this observation, they \\xef\\xac\\x81rst classify an instance\\ninto seen and unseen categories via outlier detection methods. Then the instance is\\nclassi\\xef\\xac\\x81ed using corresponding classi\\xef\\xac\\x81ers.\\nFormally, they marginalize a binary random variable V \\xe2\\x88\\x88{s, u} which denotes\\nwhether an instance belongs to seen categories or unseen categories separately, which\\nmeans probability is given as\\nP(y|I) =\\n\\x03\\nV \\xe2\\x88\\x88{s,u}\\nP(y|V, I)P(V |I).\\n(9.9)\\nFor seen image classes, they simply use softmax classi\\xef\\xac\\x81er to determine P(y|s, I),\\nwhile for unseen classes, they assume an isometric Gaussian distribution around each\\nof the novel class word vectors and assign classes based on their likelihood. To detect\\nnovelty, they calculate a Local Outlier Probability by Gaussian error function.\\n292\\n9\\nCross-Modal Representation\\n9.2.3\\nCross-Modal Representation for Cross-Media Retrieval\\nLearningcross-modalrepresentationfromdifferentmodalitiesinacommonsemantic\\nspace allows one to easily compute cross-modal similarities, which can facilitate\\nmany important cross-modal tasks, such as cross-media retrieval. With the rapid\\ngrowth of multimedia data such as text, image, video, and audio on the Internet, the\\nneed to retrieve information across different modalities has become stronger. Cross-\\nmedia retrieval is an important task in the multimedia area, which aims to perform\\nretrieval across different modalities such as text and image. For example, a user may\\nsubmit an image of a white horse, and retrieve relevant information from different\\nmodalities, such as textual descriptions of horses, and vice versa.\\nA signi\\xef\\xac\\x81cant challenge of cross-modal retrieval is the domain discrepancies\\nbetween different modalities. Besides, for a speci\\xef\\xac\\x81c area of interest, cross-modal data\\ncan be insuf\\xef\\xac\\x81cient, which limits the performance of existing cross-modal retrieval\\nmethods. Many works have focused on the challenges as mentioned above in cross-\\nmodal retrieval [23, 24].\\n9.2.3.1\\nCross-Modal Hybrid Transfer Network\\nHuang et al. [24] present a framework that tries to relieve the cross-modal data\\nsparsity problem by transfer learning. They propose to leverage knowledge from\\na large-scale single-modal dataset to boost the model training on the small-scale\\ndataset. The massive auxiliary dataset is denoted as the source domain, and the\\nsmall-scale dataset of interest is denoted as the target domain. In their work, they\\nadopt ImageNet [12], a large-scale image database as the source domain.\\nFormally, a training set consists of data from source domain Src = {I p\\ns , y p\\ns }P\\np=1\\nand target domain Tartr = {(I j\\ns , t j\\ns ), y j\\ns }J\\nj=1, where (I, t) is the image/text pair with\\nlabel y. Similarly, a test set can be denoted as Tarte = {(I m\\ns , tm\\ns ), ym\\ns }M\\nm=1. The goal\\nof their model is to transfer knowledge from Src to boost the model performance on\\nTarte for cross-media retrieval.\\nTheir model consists of a modal-sharing transfer subnetwork and a layer-sharing\\ncorrelation subnetwork. In modal-sharing transfer subnetwork, they adopt the con-\\nvolutional layers of AlexNet [32] to extract image features for source and target\\ndomains, and use word vectors to obtain text features. The image and text features\\npass through two fully connected layers, where single-modal and cross-modal knowl-\\nedge transfer are performed.\\nSingle-modal knowledge transfer aims to transfer knowledge from images in the\\nsource domain to images in the target domain. The main challenge is the domain\\ndiscrepancy between the two image datasets. They propose to solve the domain\\ndiscrepancy problem by minimizing the Maximum Mean Discrepancy (MMD) of\\nimage modality between the source and target domains. MMD is calculated in a\\nlayer-wise style in the fully connected layers. By minimizing MMD in reproduced\\nkernel Hilbert space, the image representations from source and target domains are\\n9.2 Cross-Modal Representation\\n293\\nencouraged to have the same distribution, so knowledge from images in the source\\ndomain is expected to transfer to images in the target domain. Besides, the image\\nencoder in the source domain is also \\xef\\xac\\x81ne-tuned by optimizing softmax loss on labeled\\nimage instances.\\nCross-modal knowledge transfer aims to transfer knowledge between image and\\ntext in the target domain. Text and image representations from an annotated pair\\nin the target domain are encouraged to be close to each other by minimizing their\\nEuclidean distance. The cross-modal transfer loss of image and text representations\\nis also computed in a layer-wise style in the fully connected layers. The domain\\ndiscrepancy between image and text modalities is expected to be reduced in high-\\nlevel layers.\\nIn layer-sharing correlation subnetwork, representations from modal-sharing\\ntransfer subnetwork in the target domain are fed into shared fully connected layers\\nto obtain the \\xef\\xac\\x81nal common representation for both image and text. As the parameters\\nare shared between two modalities, the last two fully connected layers are expected\\nto capture the cross-modal correlation. Their model also utilizes label information\\nin the target domain by minimizing softmax loss on labeled image/text pairs. After\\nobtaining the \\xef\\xac\\x81nal common representations, cross-media retrieval can be achieved\\nby simply computing the nearest neighbors in semantic space.\\n9.2.3.2\\nDeep Cross-Media Knowledge Transfer\\nAs an extension of [23, 24] also focuses on dealing with domain discrepancy and\\ninsuf\\xef\\xac\\x81cient cross-modal data for cross-media retrieval in speci\\xef\\xac\\x81c areas, Huang and\\nPeng [23] present a framework that transfers knowledge from a large-scale cross-\\nmediadataset(sourcedomain)toboostthemodelperformanceonanothersmall-scale\\ncross-media dataset (target domain).\\nA crucial difference from [24] is that the dataset in the source domain also consists\\nof image/text pairs with label annotations instead of a single-modal setting in [24].\\nSince both domains contain image and text media types, domain discrepancy comes\\nfrom the media-level discrepancy in the same media type, and correlation-level dis-\\ncrepancy in image/text correlation patterns between different domains. They propose\\nto transfer intra-media semantic and inter-media correlation knowledge by jointly\\nreducing domain discrepancies on media-level and correlation-level.\\nToextractthedistributedfeaturesfordifferentmediatypes,theyadoptVGG19[63]\\nfor image encoder and Word CNN [29] for text encoder. The two domains have the\\nsame architecture but do not share parameters. The extracted image/text features\\npass through two fully connected layers, respectively, where the media-level transfer\\nis performed. Similar to [24], they reduce domain discrepancies within the same\\nmodalities by minimizing Maximum Mean Discrepancy (MMD) between the source\\nand target domains. The MMD is computed in a layer-wise style to transfer knowl-\\nedge within the same modalities. They also minimize Euclidean distance between\\nimage/text representations pairs in both source and target domains to preserve the\\nsemantic information across modalities.\\n294\\n9\\nCross-Modal Representation\\nCorrelation-level transfer aims to reduce domain discrepancy in image/text corre-\\nlation patterns in different domains. In two domains, both image and text representa-\\ntions share the last two fully connected layers to obtain the common representation\\nfor each domain. They optimize layer-wise MMD loss between the shared fully con-\\nnected layers in different domains for correlation-level knowledge transfer, which\\nencourages source and target domains to have the same image/text correlation pat-\\nterns. Finally, both domains are trained with label information of image/text pairs.\\nNote that the source domain and target domain do not necessarily share the same\\nlabel set.\\nIn addition, they propose a progressive transfer mechanism, which is a curricu-\\nlum learning method aiming to promote the robustness of the model training. This is\\nachieved by selecting easy samples for model training in the early period, and grad-\\nually increases the dif\\xef\\xac\\x81culty during the training. The dif\\xef\\xac\\x81culty of training samples\\nis measured according to the bidirectional cross-media retrieval consistency.\\n9.3\\nImage Captioning\\nImage captioning is the task of automatically generating natural language descrip-\\ntions for images. It is a fundamental task in arti\\xef\\xac\\x81cial intelligence, which connects\\nnatural language processing and computer vision. Compared with other computer\\nvision tasks, such as image classi\\xef\\xac\\x81cation and object detection, image captioning\\nis signi\\xef\\xac\\x81cantly harder for two reasons: \\xef\\xac\\x81rst, not only objects but also relationships\\nbetween them have to be detected; second, besides basic judgments and classi\\xef\\xac\\x81cation,\\nnatural language sentences have to be generated.\\nTraditional methods for image captioning are usually using retrieval models or\\ngeneration models, of which the ability to generalize is comparatively weaker com-\\npared with that of novel deep neural network models. In this section, we will introduce\\nseveral typical models of both genres in the following.\\n9.3.1\\nRetrieval Models for Image Captioning\\nThe primary pipeline of retrieval models is (1) represent images and/or sentences\\nusing special features; (2) for new images and/or sentences, search for probable\\ncandidates according to the similarity of features.\\nLinking words to images has a rich history, and [50] (a retrieval model) is the \\xef\\xac\\x81rst\\nimage annotation system. This paper tries to build a keyword assigning system for\\nimages from labeled data. The pipeline is as follows:\\n(1) Image segmentation. Every image is divided into several parts, using the\\nsimplest rectangular division. The reason for doing so is that an image is typically\\nannotated with multiple labels, each of which often corresponds to only a part of it.\\nSegmentation would help reduce noises in labeling.\\n9.3 Image Captioning\\n295\\n(2) Feature extraction. Features of every part of the image are extracted.\\n(3) Clustering. Feature vectors of image segments are divided into several clusters.\\nEach cluster accumulates word frequencies and thereby calculates word likelihood.\\nConcretely,\\nP(wi|c j) =\\nP(c j|wi)P(wi)\\n\\x02\\nk P(c j|wk)P(wk) = n ji\\nN j\\n,\\n(9.10)\\nwhere n ji is the number of times word wi appears in cluster j, and N j is the number of\\ntimes that all words appear in cluster j. The calculation is based on using frequencies\\nas probabilities.\\n(4)Inference.Foranewimage,themodeldividesitintosegments,extractsfeatures\\nfor every part, and \\xef\\xac\\x81nally, aggregates keywords assigned to every part to obtain the\\n\\xef\\xac\\x81nal prediction.\\nThe key idea of this model is image segmentation. Take a landscape picture, for\\ninstance, there are two parts: mountain and sky, and both parts will be annotated\\nwith both labels. However, if another picture has two parts mountain and river,\\nthe two mountain parts would hopefully be in the same cluster and discover that\\nthey share the same label mountain. In this way, labels can be assigned to the\\ncorrect part of the image, and noises could be alleviated.\\nAnother typical retrieval model is proposed by [17], which can assign a linking\\nscore between an image and a sentence. An intermediate space of meaning calculates\\nthis score of linking. The representation of the meaning space is a triple in the form of\\n\\xe2\\x9f\\xa8object, action, scene \\xe2\\x9f\\xa9. Each slot of the triple has a \\xef\\xac\\x81nite discrete candidate set. The\\nproblem of mapping images and sentences into the meaning space involves solving\\na Markov random \\xef\\xac\\x81eld.\\nDifferent from the previous model, this system can do not only image caption,\\nbut also do the inverse, that is, given a sentence, the model provides certain probable\\nassociated images. At the inference stage, the image (sentence) is \\xef\\xac\\x81rst mapped to the\\nintermediate meaning space, then we search in the pool for the sentence (image) that\\nhas the best matching score.\\nAfter that, researchers also proposed a lot of retrieval models which consider\\ndifferent kinds of characteristics of the images, such as [21, 28, 34].\\n9.3.2\\nGeneration Models for Image Captioning\\nDifferent from the retrieval-based model, the basic pipeline of generation models is\\n(1) use computer vision techniques to extract image features, (2) generate sentences\\nfrom these features using methods such as language models or sentence templates.\\nKulkarni et al. [33] propose a system that makes a tight connection between the\\nparticular image and the sentence generating process. The model uses visual detectors\\nto detect speci\\xef\\xac\\x81c objects, as well as attributes of a single object and relationships\\nbetween multiple objects. Then it constructs a conditional random \\xef\\xac\\x81eld to incorporate\\nunary image potentials and higher order text potentials and thereby predicts labels\\n296\\n9\\nCross-Modal Representation\\nfor the image. Labels predicted by conditional random \\xef\\xac\\x81elds (CRF) is arranged as a\\ntriple, e.g., \\xe2\\x9f\\xa8\\xe2\\x9f\\xa8white, cloud\\xe2\\x9f\\xa9, in, \\xe2\\x9f\\xa8blue, sky\\xe2\\x9f\\xa9\\xe2\\x9f\\xa9.\\nThen sentences are generated according to the labels. There are two ways to build\\na sentence based on the triple skeleton. (1) The \\xef\\xac\\x81rst is to use an n-gram language\\nmodel. For example, when trying to decide whether or not to put a glue word x\\nbetween a pair of meaningful words (which means they are inside the triple) a and b,\\nthe probabilities \\xcb\\x86p(axb) and \\xcb\\x86p(ab) are compared for the decision. \\xcb\\x86p is the standard\\nlength-normalized probability of the n-gram language model. (2) The second is to\\nuse a set of descriptive language templates, which alleviates the problem of grammar\\nmistakes in the language model.\\nFurther, [16] proposes a novel framework to explicitly represent the relationship\\nbetween image structure and its caption sentence\\xe2\\x80\\x99s structure. The method, Visual\\nDependency Representation, detects objects in the image, and detects the relationship\\nbetween these objects based on the proposed Visual Dependency Grammar, which\\nincludes eight typical relations like beside or above. Then the image can be\\narranged as a dependency graph, where nodes are objects and edges are relations. This\\nimage dependency graph can be aligned with the syntactic dependency representation\\nof the caption sentence. The paper further provides four templates to generating\\ndescriptive sentences from the extracted dependency representation.\\nBesides these two typical works, there are massive generation models for image\\ncaptioning, such as [15, 35, 78].\\n9.3.3\\nNeural Models for Image Captioning\\nIn [33], it was claimed in 2011 that in image captioning tasks: Natural language\\ngeneration still remains an open research problem. Most previous work is based on\\nretrieval and summarization. From 2015, inspired by advances in neural language\\nmodel and neural machine translation, a number of end-to-end neural image caption-\\ning models based on the encoder-decoder system have been proposed. These new\\nmodels signi\\xef\\xac\\x81cantly improve the ability to generate natural language descriptions.\\n9.3.3.1\\nThe Basic Model\\nTraditional machine translation models typically stitch many subtasks together, such\\nas individual word translation and reordering, to perform sentence and paragraph\\ntranslation. Recent neural machine translation models, such as [8], use a single\\nencoder-decoder model, which can be optimized by stochastic gradient descent con-\\nveniently.Thetaskofimagecaptioningisinherentlyanalogoustomachinetranslation\\nbecause it can also be regarded as a translation task, where the source \\xe2\\x80\\x9clanguage\\xe2\\x80\\x9d\\nis an image. The encoders and decoders used for machine translations are typically\\nRNNs, which is a natural selection for sequences of words. For image captioning,\\nCNN is chosen to be the encoder, and RNN is still used as the decoder.\\n9.3 Image Captioning\\n297\\nInput Image\\no\\no\\no\\np\\np\\np\\nw1\\nw2\\nEND\\nw0\\nw1\\nwN\\n\\xe2\\x80\\xa6\\xe2\\x80\\xa6\\nCNN\\nLSTM\\nSoftmax\\nCorrect Caption Sentence s={w1, w2, \\xe2\\x80\\xa6, wN}\\nFig. 9.2 The architecture of encoder-decoder framework for image captioning\\nVinyals et al. [70] is the most typical model which uses encoder-decoder for\\nimage captioning (see Fig.9.2). Concretely, a CNN model is used to encode the\\nimage into a \\xef\\xac\\x81x length vector, which is believed to contain the necessary information\\nfor captioning. With this vector, an RNN language model is used to generate natural\\nlanguage descriptions, and this is the decoder. Here, the decoder is similar to the\\nLSTM used for machine translation. The \\xef\\xac\\x81rst unit takes the image vector as the\\ninput vector, and the rest units take the previous word embedding as input. Each unit\\noutputs a vector o and passes a vector to the next unit. o is further fed into a softmax\\nlayer, whose output p is the probability of each word within the vocabulary. The\\nways to deal with these calculated probabilities are different in training and testing:\\nTraining. These probabilities p are used to calculate the likelihood of the provided\\ndescription sentences. Considering the nature of RNNs, it is easy to model the joint\\nprobability into conditional probabilities.\\nlog P(s|I) =\\nN\\n\\x03\\nt=0\\nlog P(wt|I, w0, . . . , wt\\xe2\\x88\\x921),\\n(9.11)\\nwhere s = {w1, w2, ..., wN} is the sentence and its words, w0 is a special START\\ntoken, and I is the image. Stochastic gradient descent can thereby be performed to\\noptimize the model.\\nTesting. There are multiple approaches to generate sentences given an image.\\nThe \\xef\\xac\\x81rst one is called Sampling. For each step, the single word with the highest\\nprobability in p is chosen, and used as the input of the next unit until the END\\ntoken is generated or a maximal length is reached. The second one is called Beam\\nSearch. For each step (now the length of sentences is t), k best sentences are kept.\\nEach of them generates several new sentences of length t + 1, and again, only k new\\n298\\n9\\nCross-Modal Representation\\nFig. 9.3 An example of image captioning with attention mechanism\\nsentences are kept. Beam Search provides a better approximation for\\ns\\xe2\\x88\\x97= arg max\\ns\\nlog P(s|I).\\n(9.12)\\n9.3.3.2\\nVariants of the Basic Model\\nThe research on image captioning tightly follows that on machine translation.\\nInspired by [6], which uses attention mechanism in machine translation, [76] intro-\\nduces visual attention into the encoder-decoder image captioning model.\\nThe major bottleneck of [70] is the fact that information from the image is shown\\nto the LSTM decoder only at the \\xef\\xac\\x81rst decoding unit, which actually requires the\\nencoder to squeeze all useful information into one \\xef\\xac\\x81xed-length vector. In contrast,\\n[76] does not require such compression. The CNN encoder does not produce one\\nvector for the entire image; instead, it produces L region vectors Ii, each of which is\\nthe representation of a part of the image. At every step of decoding, the inputs include\\nstandard LSTM inputs (i.e., output and hidden state of last step ot\\xe2\\x88\\x921 and ht\\xe2\\x88\\x921), and\\nan input vector z from the encoder. Here, z is the weighted sum of image vectors\\nIi: z = \\x02\\ni \\xce\\xb1iIi, where \\xce\\xb1i is the weight computed from Ii and ht\\xe2\\x88\\x921. Throughout\\nthe training process, the model learns to focus on parts of the image for generating\\nthe next word by producing larger weights \\xce\\xb1 on more relevant parts, as shown in\\nFig.9.3.1\\nWhile the above paper uses soft attention for the image, [27] makes explicit\\nalignment between image fragments and sentence fragments before generating a\\ndescription for the image. In the \\xef\\xac\\x81rst stage, the alignment stage, sentence and image\\nfragments are aligned by being mapped to a shared space. Concretely, sentence\\nfragments (i.e., n consecutive words) are encoded using a bidirectional LSTM into\\nthe embeddings s, and image fragments (i.e., part of the image, and also the entire\\nimage) are encoded using a CNN into the embeddings I. The similarity score between\\nimage I and sentence s is computed as\\n1The example is obtained from the implementation of Yunjey Choi (https://github.com/yunjey/\\nshow-attend-and-tell).\\n9.3 Image Captioning\\n299\\nsim(I, s) =\\n\\x03\\nt\\xe2\\x88\\x88gs\\nmaxi\\xe2\\x88\\x88gI (0, I\\xe2\\x8a\\xa4\\ni st),\\n(9.13)\\nwhere gs is the sentence fragment set of sentence s, and gI is the image fragment set\\nof image I. The alignment is then optimized by minimizing the ranking loss L for\\nboth sentences and images:\\nL =\\n\\x03\\nI\\n\\x04\\x03\\ns\\nmax(0, sim(I, s) \\xe2\\x88\\x92sim(I, I) + 1) +\\n\\x03\\ns\\nmax(0, sim(s, I) \\xe2\\x88\\x92sim(I, I) + 1)\\n\\x05\\n.\\n(9.14)\\nThe assumption for this alignment procedure is similar to [50] (see Sect.9.3.1): all\\ndescription sentences are regarded as (possibly noisy) labels for every image section\\nand are based on the massive training data, the model would hopefully be trained to\\nalign caption sentences to their corresponding image fragments. The second stage is\\nsimilar to the basic model in [70], but the alignment results are used to provide more\\nprecise training data.\\nAs mentioned above, [76] makes the decoder have the ability to focus attention\\non the different parts of the image for different words. However, there are some\\nnonvisual words in the decoding process. For example, words such as the and of\\nare more dependent on semantic information than visual information. Furthermore,\\nwords such as phone followed by cell or meter before near the parking\\nare usually generated by the language model. To avoid the gradient of a nonvisual\\nword decreasing the effectiveness of visual attention, in the process of generating\\ncaptions, [43] adopts an adaptive attention model with a visual sentinel. At each time\\nstep, the model needs to determine that it depends on an image region or a visual\\nsentinel.\\nAdaptive attention model [43] uses attention in the process of generating a word\\nrather than updating the LSTM state; it utilizes \\xe2\\x80\\x9cvisual sentinel\" vector xt and image\\nregion vectors Ii. Here, xt is produced by the inputs and states of LSTM at time step\\nt, while Ii is provided from CNN encoder. Then the adaptive context vector \\xcb\\x86ct is the\\nweighted sum of L image region vectors Ii and visual sentinel xt:\\n\\xcb\\x86ct =\\nL\\n\\x03\\ni\\n\\xce\\xb1iIi + \\xce\\xb1L+1xt,\\n(9.15)\\nwhere \\xce\\xb1i are the weights computed by Ii, xt, and the LSTM hidden state ht. We\\nhave \\x02L+1\\ni=1 \\xce\\xb1i = 1. Finally the probability of a word in vocabulary at time t can be\\ncalculated as a residual form:\\npt = Softmax(Wp(\\xcb\\x86ct + ht)),\\n(9.16)\\nwhere Wp is a learned weight parameter.\\nMany existing image captioning models with attention allocate attention over\\nimage\\xe2\\x80\\x99s regions, whose size is often 7 \\xc3\\x97 7 or 14 \\xc3\\x97 14 decided by the last pooling\\n300\\n9\\nCross-Modal Representation\\nResize\\nLatent Channel\\nActivation\\nActivated Region\\nImage\\nHt\\nct\\nI\\nFig. 9.4 An example of the activated region of a latent channel\\nlayer in CNN encoder. Anderson et al. [2] \\xef\\xac\\x81rst calculate attention at the level of\\nobjects. It \\xef\\xac\\x81rst employs Faster R-CNN [58] which is trained on ImageNet [60] and\\nGenome [31] to predict attribute class, such as an open oven, green bottle, \\xef\\xac\\x82oral\\ndress, and so on. After that, it applies attention over valid bounding boxes to get\\n\\xef\\xac\\x81ne-grained attention for helping the caption generation.\\nBesides, [11] rethinks the form of latent states in image captioning, which usu-\\nally compresses two-dimensional visual feature maps encoded by CNN to a one-\\ndimensional vector as the input of the language model. They \\xef\\xac\\x81nd that the language\\nmodel with 2D states can preserve the spatial locality, which can link the input visual\\ndomain and output linguistic domain observed by visualizing the transformation of\\nhidden states.\\nWord embeddings and hidden states in [11] are 3D tensors of size C \\xc3\\x97 H \\xc3\\x97 W,\\nwhich means C channels, each of size H \\xc3\\x97 W. The encoded features maps will\\nbe directly inputted to the 2D language model instead of going through an average\\npooling layer. In the 2D language model, the convolution operator takes the place of\\nmatrix multiplication in the 1D model, and mean pooling will be used to generate\\nthe output word probability distribution from 2D hidden states. Figure9.4 shows\\nactivated region of a latent channel at the tth step. When we set a threshold for the\\nactivated regions, it is revealed that the special channels are associated with speci\\xef\\xac\\x81c\\nnouns in the decoding process, which help get a better understanding of the process\\nof generating captions.\\nTraditional methods train the caption model by maximizing the likelihood of\\ntraining examples, which forms a gap between the optimization objective and evalu-\\nating metrics. To alleviate the problem, [59] uses reinforcement learning to directly\\nmaximize the CIDEr metric [69]. CIDEr re\\xef\\xac\\x82ects the diversity of generated cap-\\ntions by giving high weights to the low-frequency n-grams in the training set, which\\ndemonstrates that people prefer detailed captions rather than universal ones, like a\\nboy is playing a game. To encourage the distinctiveness of captions, [10]\\nadopts contrastive learning. Their model learns to discriminate the caption of a given\\nimage and the caption of an alike image by maximizing the difference between\\nground truth positive pair and mismatch negative pair. The experiment shows that\\ncontrastive learning increases the diversity of captions signi\\xef\\xac\\x81cantly.\\nFurthermore, automatic evaluation metrics, such as BLEU [54], METEOR [13],\\nROUGE [38], CIDEr [69], SPICE [1], and so on, may neglect some novel expres-\\nsions restrained by the ground truth captions. To better evaluate the naturalness and\\ndiversity of captions, [9] proposes a framework based on Conditional Generative\\nAdversarial Networks, whose generator tries to achieve a higher score in the evalua-\\n9.3 Image Captioning\\n301\\ntor, while the evaluator tries to distinguish between the generated caption and human\\ndescriptions for a given image, as well as between the given image and the mismatch\\ndescription. The user study shows that the trained generator can generate natural and\\ndiverse captions than the model trained by maximum likelihood estimate, while the\\ntrained evaluator is more consistent with human\\xe2\\x80\\x99s evaluation.\\nBesides the works we introduced above, there are also a mass of variants of the\\nbasic encoder-decoder model such as [20, 26, 40, 45, 51, 71, 73].\\n9.4\\nVisual Relationship Detection\\nVisual relationship detection is the task of detecting objects in an image and under-\\nstanding the relationship between them. While detecting the objects is always based\\non semantic segmentation or object detection methods, such as R-CNN, understand-\\ning the relationship is the key challenge of this task. While detecting visual relation\\nwith image information is intuitive and effective [25, 62, 84], leveraging information\\nfrom language can further boost the model performance [37, 41, 82].\\n9.4.1\\nVisual Relationship Detection with Language Priors\\nLu et al. [41] propose a model that uses language priors to enhance the performance\\non infrequent relationships for which suf\\xef\\xac\\x81cient training instances are hard to obtain\\nsolely from images. The overall architecture is shown in Fig.9.5.\\nThey \\xef\\xac\\x81rst train a CNN to calculate the unnormalized relations\\xe2\\x80\\x99 probability\\nobtained from visual inputs by\\nRCNN\\nLanguage Module     f(R,W)\\nhat-ride-horse\\nhorse-pull-hat\\nperson-ride-horse\\nperson-wear-horse\\nVisual Module     V(R,\\xce\\xb8)\\nperson-wear-hat\\nhorse-wear-hat\\nperson-ride-horse\\n0.76\\n0.11\\n0.81\\nCNN\\n...\\n...\\nFig. 9.5 The architecture of visual relationship detection with language prior\\n302\\n9\\nCross-Modal Representation\\nPV (R\\xe2\\x9f\\xa8i, j,k\\xe2\\x9f\\xa9, \\xce\\x98|\\xe2\\x9f\\xa8O1, O2\\xe2\\x9f\\xa9) = Pi(O1)(z\\xe2\\x8a\\xa4\\nk CNN(O1, O2) + sk)Pj(O2),\\n(9.17)\\nwhere Pi(O j) denotes the probability that bounding box O j is entity i, and\\nC N N(O1, O2) is the joint feature of box O1 with box O2. \\xce\\x98 = {zk, sk} is the set of\\nparameters.\\nBesides, language prior is considered in this model by calculating the unnormal-\\nized probability that the entity pair \\xe2\\x9f\\xa8i, j\\xe2\\x9f\\xa9has the relation k:\\nPf (R, W) = r\\xe2\\x8a\\xa4\\nk [wi; w j] + bk,\\n(9.18)\\nwhere wi and w j are the word embeddings of the text of subject and object, respec-\\ntively, rk is the learned relational embedding of the relation k.\\nGiven the probabilities of a relation from visual and textual inputs, respectively,\\nthe authors combine them into the integrated probability of a relation. The \\xef\\xac\\x81nal\\nprediction is the one with maximal integrated probability:\\nR\\xe2\\x88\\x97= max\\nR\\nPV (R\\xe2\\x9f\\xa8i, j,k\\xe2\\x9f\\xa9|\\xe2\\x9f\\xa8O1, O2\\xe2\\x9f\\xa9)Pf (R, W).\\n(9.19)\\nThe rank of the ground truth relationship R with bounding boxes O1 and O2 is\\nmaximized using the following rank loss function:\\nC(\\xce\\x98, W) =\\n\\x03\\n\\xe2\\x9f\\xa8O1,O2\\xe2\\x9f\\xa9,R\\nmax{1 \\xe2\\x88\\x92PV (R, \\xce\\x98|\\xe2\\x9f\\xa8O1, O2\\xe2\\x9f\\xa9)Pf (R, W)\\n+\\nmax\\n\\xe2\\x9f\\xa8O\\xe2\\x80\\xb2\\n1,O\\xe2\\x80\\xb2\\n2\\xe2\\x9f\\xa9\\xcc\\xb8=\\xe2\\x9f\\xa8O1,O2\\xe2\\x9f\\xa9,R\\xe2\\x80\\xb2\\xcc\\xb8=R PV (R\\xe2\\x80\\xb2, \\xce\\x98|\\xe2\\x9f\\xa8O\\xe2\\x80\\xb2\\n1, O\\xe2\\x80\\xb2\\n2\\xe2\\x9f\\xa9)Pf (R\\xe2\\x80\\xb2, W), 0}.\\n(9.20)\\nIn addition to the loss that optimizes the rank of the ground truth relationships,\\nthe authors also propose two regularization functions for language priors. The \\xef\\xac\\x81nal\\nloss function of this model is de\\xef\\xac\\x81ned as\\nL = C(\\xce\\x98, W) + \\xce\\xbb1L(W) + \\xce\\xbb2K(W).\\n(9.21)\\nK(W) is a variance function to make the similar relationships\\xe2\\x80\\x99 corresponding\\nf (\\xc2\\xb7) function closer:\\nK(W) = Var{[Pf (R, W) \\xe2\\x88\\x92Pf (R\\xe2\\x80\\xb2, W)]2\\nd(R, R\\xe2\\x80\\xb2)\\n}, \\xe2\\x88\\x80R, R\\xe2\\x80\\xb2,\\n(9.22)\\nwhere d(R, R\\xe2\\x80\\xb2) is the sum of the cosine distances (in Word2vec space) between the\\ntwo objects and the predicates of the two relationships R and R\\xe2\\x80\\xb2.\\nL(W) is a function to encourage less-frequent relation to have a lower f () score.\\nWhen R occurs more frequently than R\\xe2\\x80\\xb2, we have\\nL(W) =\\n\\x03\\nR,R\\xe2\\x80\\xb2\\nmax{Pf (R\\xe2\\x80\\xb2, W) \\xe2\\x88\\x92Pf (R, W) + 1, 0}.\\n(9.23)\\n9.4 Visual Relationship Detection\\n303\\nCNN\\nObject Detection Module\\nRelation Prediction Module\\nclasseme\\nlocation\\nvisual\\nperson\\nelephant\\nperson\\nperson\\nride\\ntaller\\nnext to\\nwith\\nelephant\\nperson\\nelephant\\npants\\nperson\\nelephant\\nperson\\npants\\nbox\\nBilinear\\nInterpolation\\nconv\\nfeat\\nScaling\\nScaling\\nws\\nFeature\\nExtraction\\nLayer\\nwo\\nSoftmax\\n...\\nbox\\n...\\nFig. 9.6 The architecture of VTransE model\\n(a) A scene.\\nTable\\nTablecloth\\nGoose\\nApple\\nPorcelain\\nR: On\\nR: On\\nR: Inside\\nR: Inside\\nR: ?  Answer: On\\n(b) The corresponding scene graph.\\nFig. 9.7 An illustration for scene graph generation\\n9.4.2\\nVisual Translation Embedding Network\\nInspired by recent progress in knowledge representation learning, [82] proposes\\nVTransE, a visual translation embedding network. Objects and the relationship\\nbetween objects are modeled as TransE [7] like vector translation. VTransE \\xef\\xac\\x81rst\\nprojects subject and object into the same space as relation translation vector r \\xe2\\x88\\x88Rr.\\nSubject and object could be denoted as xs, xo \\xe2\\x88\\x88RM in the feature space, where\\nM \\xe2\\x89\\xabr. Similar to TransE relationship, VTransE establishes a relationship as\\nWsxs + r \\xe2\\x88\\xbcWoxo,\\n(9.24)\\nwhere Ws and Wo are projection matrices. The overall architecture is shown in\\nFig.9.6.\\n9.4.3\\nScene Graph Generation\\nLi et al. [37] further formulate visual relation detection as a scene graph generation\\ntask, where nodes correspond to objects and directed edges correspond to visual\\nrelations between objects, as shown in Fig.9.7.\\nThis formulation allows [37] to leverage different levels of context information,\\nsuch as information from objects, phrases (i.e., \\xe2\\x9f\\xa8subject, predicate, object\\xe2\\x9f\\xa9triples),\\n304\\n9\\nCross-Modal Representation\\nc)\\nb)\\na)\\nFig. 9.8 Dynamical graph construction. a The input image. b Object (bottom), phrase (middle),\\nand caption region (top) proposals. c The graph modeling connections between proposals. Some of\\nthe phrase boxes are omitted\\nand region captions, to boost the performance of visual relation detection. Speci\\xef\\xac\\x81-\\ncally, [37] proposes to construct a graph that aligns these three levels of information\\nand perform feature re\\xef\\xac\\x81nement via message passing, as shown in Fig.9.8. By leverag-\\ning complementary information from different levels, the performances of different\\ntasks are expected to be mutually improved.\\nDynamic Graph Construction. Given an image, they \\xef\\xac\\x81rst generate three kinds\\nof proposals that correspond to three kinds of nodes in the proposed graph structure.\\nThe proposals include object proposals, phrase proposals, and region proposals. The\\nobject and region proposals are generated using Region Proposal Network (RPN)\\n[57] trained with ground truth bounding boxes. Given N object proposals, phrase\\nproposals are constructed based on N(N \\xe2\\x88\\x921) object pairs that fully connect the\\nobject proposals with direct edges, where each direct edge represents a potential\\nphrase between an object pair.\\nEach phrase proposal is connected to the corresponding subject and object with\\ntwo directed edges. A phrase proposal and a region proposal are connected if their\\noverlapexceedsacertainfraction(e.g.,0.7)ofthephraseproposal.Therearenodirect\\nconnections between objects and regions since they can be indirectly connected via\\nphrases.\\nFeature Re\\xef\\xac\\x81nement. After obtaining the graph structure of different levels of\\nnodes, they perform feature re\\xef\\xac\\x81nement by iterative message passing. The message\\npassing procedure is divided into three parallel stages, including object re\\xef\\xac\\x81nement,\\nphrase re\\xef\\xac\\x81nement, and region re\\xef\\xac\\x81nement.\\nIn object feature re\\xef\\xac\\x81nement, the object proposal feature is updated with gated\\nfeatures from adjacent phrases. Given an object i, the aggregated feature from regions\\nthat are linked to object i via subject-predicate edges \\xcb\\x86xp\\xe2\\x86\\x92s\\ni\\ncan be de\\xef\\xac\\x81ned as follows:\\n\\xcb\\x86xp\\xe2\\x86\\x92s\\ni\\n=\\n1\\n\\xe2\\x88\\xa5Ei,p\\xe2\\x88\\xa5\\n\\x03\\n(i, j)\\xe2\\x88\\x88Es,p\\nf\\xe2\\x9f\\xa8o,p\\xe2\\x9f\\xa9(x(o)\\ni , x(p)\\nj )x(p)\\nj ,\\n(9.25)\\n9.4 Visual Relationship Detection\\n305\\nwhere Es,p is the set of subject predicate connections, and Ei,p denotes the number of\\nphrases connected with the object i as the subject predicate pairs. f\\xe2\\x9f\\xa8o,p\\xe2\\x9f\\xa9is a learnable\\ngate function that controls the weights of information from different sources:\\nf\\xe2\\x9f\\xa8o,p\\xe2\\x9f\\xa9(x(o)\\ni , x(p)\\nj ) =\\nK\\n\\x03\\nk=1\\nSigmoid(\\xcf\\x89(k)\\n\\xe2\\x9f\\xa8o,p\\xe2\\x9f\\xa9\\xc2\\xb7 [x(o)\\ni ; x(p)\\nj ]),\\n(9.26)\\nwhere \\xcf\\x89(k)\\n\\xe2\\x9f\\xa8o,p\\xe2\\x9f\\xa9is a gate template used to calculate the importance of the information\\nfrom a subject-predicate edge and K is the number of templates. The aggregated\\nfeature from object-predicate edges \\xcb\\x86xp\\xe2\\x86\\x92o\\ni\\ncan be similarly computed.\\nAfter obtaining information \\xcb\\x86xp\\xe2\\x86\\x92s\\ni\\nand \\xcb\\x86xp\\xe2\\x86\\x92o\\ni\\nfrom adjacent phrases, the object\\nre\\xef\\xac\\x81nement at time step t can be de\\xef\\xac\\x81ned as follows:\\nx(o)\\ni,t+1 = x(o)\\ni,t + f (p\\xe2\\x86\\x92s)(\\xcb\\x86xp\\xe2\\x86\\x92s\\ni\\n) + f (p\\xe2\\x86\\x92o)(\\xcb\\x86xp\\xe2\\x86\\x92o\\ni\\n),\\n(9.27)\\nwhere f (\\xc2\\xb7) = WReLU(\\xc2\\xb7), W is a learnable parameter and not shared between\\nf (p\\xe2\\x86\\x92s)(\\xc2\\xb7) and f (p\\xe2\\x86\\x92o)(\\xc2\\xb7).\\nThe re\\xef\\xac\\x81nement scheme of phrases and regions is similar to objects. The only\\ndifference is the information sources: Phrase proposals receive information from\\nadjacent objects and regions, and region proposals receive information from phrases.\\nAfter feature re\\xef\\xac\\x81nement via iterative message passing, the feature of different\\nlevels of nodes can be used for corresponding tasks. Region features can be used\\nas the initial state of a language model to generate region captions. Phrase features\\ncan be used to predict visual relation between objects, which composes of the scene\\ngraph of the image.\\nIn comparison with scene graph generation methods that model the dependencies\\nbetween relation instances by attention mechanism or message passing, [47] decom-\\nposes the scene graph task into a mixture of two phases: extracting primary relations\\nfrom input, and completing the scene graph with reasoning. The authors propose a\\nHybrid Scene Graph generator (HRE) that combines these two phases in a uni\\xef\\xac\\x81ed\\nframework and generates scene graphs from scratch.\\nSpeci\\xef\\xac\\x81cally, HRE \\xef\\xac\\x81rst encodes the object pair into representations and then\\nemploys a neural relation extractor resolving primary relations from inputs and a dif-\\nferentiable inductive logic programming model that iteratively completes the scene\\ngraph. As shown in Fig.9.9, HRE contains two units, a pair selector and a relation\\npredictor, and runs in an iterative way.\\nAt each time step, the pair selector takes a look at all object pairs P\\xe2\\x88\\x92that have not\\nbeen associated with a relation and chooses the next pair of entities whose relation\\nis to be determined. The relation predictor utilizes the information contained in all\\npairs P+ whose relations have been determined, and the contextual information of\\nthe pair to make the prediction on the relation. The prediction result is then added to\\nP+ and bene\\xef\\xac\\x81ts future predictions.\\nTo encode object pair into representations, HRE extends the union box encoder\\nproposed by [41] by adding the object features (what are the objects) and their\\n306\\n9\\nCross-Modal Representation\\nFig. 9.9 Framework of HRE that detects primary relations from inputs and iteratively completes\\nthe scene graph via inductive logic programming\\nFig. 9.10 Object pair encoder of HRE\\nlocations (where are the objects) into the object pair representation, as shown in\\nFig.9.10.\\nRelation Predictor. The relation predictor is composed of two modules: a neural\\nmodule predicting the relations between entities based on the given context (i.e., a\\nvisual image) and a differentiable inductive logic module performing reasoning on\\nP+. Both modules predict the relation score between a pair of objects individually.\\nThe relation scores from the two modules are \\xef\\xac\\x81nally integrated by multiplication.\\nPair Selector. The selector works as the predictor\\xe2\\x80\\x99s collaborator with the goal\\nto \\xef\\xac\\x81gure out the next relation which should be determined. Ideally, the choice p\\xe2\\x88\\x97\\nmade by the selector should satisfy the condition that all relations that will affect\\nthe predictor\\xe2\\x80\\x99s prediction on p\\xe2\\x88\\x97should be sent to the predictor ahead of p\\xe2\\x88\\x97. HRE\\nimplements the pair selector as a greedy selector which always chooses the entity\\npair from P\\xe2\\x88\\x92to be added to P+ as the entity pair of which the relation predictor is\\nmost con\\xef\\xac\\x81dent in its prediction.\\n9.4 Visual Relationship Detection\\n307\\nIt is worth noting that the task of scene graph generation resembles document-\\nlevel relation extraction in many aspects. Both tasks seek to extract structured graphs\\nconsisting of entities and relations. Also, they need to model the complex dependen-\\ncies between entities and relations in a rich context. We believe both tasks are worthy\\nto explore for future research.\\n9.5\\nVisual Question Answering\\nVisual Question Answering (VQA) aims to answer natural language questions about\\nan image, and can be seen as a single turn of dialogue about a picture. In this section,\\nwe will introduce widely used VQA datasets and several typical VQA models.\\n9.5.1\\nVQA and VQA Datasets\\nVQA was \\xef\\xac\\x81rst proposed in [46]. They \\xef\\xac\\x81rst propose a single-world approach by\\nmodeling the probability of an answer a given question q and a world w by\\nP(a|q, w) =\\n\\x03\\nz\\nP(a|z, w)P(z|q),\\n(9.28)\\nwhere z is a latent variable associated with the question and the world w is a represen-\\ntation of the image. They further extend the single-world approach to a multi-world\\napproach by marginalizing over different segments s of the given image. The prob-\\nability of an answer a given question q and a world w is given by\\nP(a|q, s) =\\n\\x03\\nw\\n\\x03\\nz\\nP(a|w, z)P(w|s)P(z|q).\\n(9.29)\\nThey also release the \\xef\\xac\\x81rst dataset of VQA named as DAQUAR in their paper.\\nBesides DAQUAR, researchers also release a lot of VQA datasets with vari-\\nous characteristics. The most widely used dataset was released in [4], where the\\nauthors provided cases and experimental evidence to demonstrate that to answer\\nthese questions, a human or an algorithm should use features of the image and exter-\\nnal knowledge. Figure9.11 shows examples of VQA dataset released in [4]. It is also\\ndemonstrated that this problem cannot be solved by converting images to captions\\nand answering questions according to captions. Experiment results show that the\\nperformance of vanilla methods is still far from human.\\nIn fact, there are also other existing datasets for Visual QA such as Visual7W\\n[85], Visual Madlibs [80], COCO-QA [56], and FM-IQA [19].\\n308\\n9\\nCross-Modal Representation\\nQuestion: Why are the men jumping?\\nAnswer: to catch frisbee\\nQuestion: Is the water still?\\nAnswer: no\\nQuestion: What is the kid doing?\\nAnswer: skateboarding\\nQuestion: What is hanging on the wall \\nabove the headboard?\\nAnswer: pictures\\nFig. 9.11 Examples of VQA dataset\\n9.5.2\\nVQA Models\\nBesides, [4, 46] further investigate approaches to solve speci\\xef\\xac\\x81c types of questions in\\nVQA. Moreover, [83] proposes an approach to solve \\xe2\\x80\\x9cYES/NO\\xe2\\x80\\x9d questions. Note that\\nthe model is an ensemble model of two similar models: Q-model and Tuple-model,\\nthe difference between which will be described later. The overall approach can be\\ndivided into two steps: (1) Language Parsing and (2) Visual Veri\\xef\\xac\\x81cation. In the former\\nstep, they extract \\xe2\\x9f\\xa8P, R, S\\xe2\\x9f\\xa9tuples from questions \\xef\\xac\\x81rst by parsing it and assigning an\\nentity to each word. Then they summarize the parsed sentences through removing\\n\\xe2\\x80\\x9cstop words\\xe2\\x80\\x9d, auxiliary verbs, and all words before a nominal subject or passive\\nnominal subject, and further split the summary into PRS arguments according to the\\npart of speech of phrases. The difference between Q-Model and Tuple-model is that\\nthe Q-model is the one used in their previous work [4], embedding the question into\\na dense 256-dim vector by LSTM, while Tuple-model is to convert \\xe2\\x9f\\xa8P, R, S\\xe2\\x9f\\xa9tuples\\ninto 256-dim embeddings by MLP. As for the Visual Veri\\xef\\xac\\x81cation step, they use the\\nsame feature of images as in [39] which was encoded into the dense 256-dim vector\\n9.5 Visual Question Answering\\n309\\nby an inner-product layer followed by a tanh layer. These two vectors are passed\\nthrough an MLP to produce the \\xef\\xac\\x81nal output (\\xe2\\x80\\x9cYes\\xe2\\x80\\x9d or \\xe2\\x80\\x9cNo\\xe2\\x80\\x9d).\\nMoreover, [61] proposes a method to calculate attention \\xce\\xb1 j by the set of image\\nfeatures I = (I1, I2, . . . , IK) and the question embedding q by\\n\\xce\\xb1 j = (W1I j + b1)\\xe2\\x8a\\xa4(W2q + b2),\\n(9.30)\\nwhere W1,W2,b1,b2 are trainable parameters.\\nAttention based techniques are quite ef\\xef\\xac\\x81cient for \\xef\\xac\\x81ltering noises that are irrelevant\\nto the question. However, some questions are only related to some small regions,\\nwhich encourages researchers to use stacked attention to further \\xef\\xac\\x81ltering noises. We\\nrefer readers to Fig.1b in [79] for an example of stacked attention.\\nYang et al. [79] further extend the attention-based model used in [61], which\\nemploys LSTMs to predict the answer. They take the question as input and attend\\nto different regions in the image to obtain additional input. The key idea is to grad-\\nually \\xef\\xac\\x81lter out noises and pinpoint the regions that are highly relevant to the answer\\nby reasoning through multiple stacked attention layers progressively. The stacked\\nattention could be calculated by stacking:\\nhk\\nA = tanh(Wk\\n1I \\xe2\\x8a\\x95(Wk\\n2uk\\xe2\\x88\\x921 + bk\\nA)).\\n(9.31)\\nNote that we denote the addition of a matrix and a vector by \\xe2\\x8a\\x95. The addition\\nbetween a matrix and a vector is performed by adding each column of the matrix by\\nthe vector. u is a re\\xef\\xac\\x81ned query vector that combines information from the question\\nand image regions. u0 (i.e, u from the \\xef\\xac\\x81rst attention layer with k = 0) could be\\ninitialized as the feature vector of the question. hk\\nA is then used to compute pk\\nI, which\\ncorresponds to the attention probability of each image region,\\npk\\nI = Softmax(Wk\\n3hk\\nA + bk\\nP).\\n(9.32)\\nuk could be iterated by\\n\\xcb\\x9cI\\nk =\\n\\x03\\ni\\npk\\ni Ii,\\n(9.33)\\nuk = uk\\xe2\\x88\\x921 + \\xcb\\x9cI\\nk.\\n(9.34)\\nThat is, in every layer, the model progressively uses the combined question and\\nimage vector uk\\xe2\\x88\\x921 as the query vector for attending the image region to obtain the\\nnew query.\\nThe above models attend only on images, but questions should also be attended.\\n[44] calculates co-attention by\\nZ = tanh(Q\\xe2\\x8a\\xa4WI),\\n(9.35)\\n310\\n9\\nCross-Modal Representation\\nWhat color on the stop light is lit up\\nWhat\\ncolor\\nWhat\\nthe \\nstop\\nlight\\ncolor\\ncolor\\nQuestion:What color on the stop light is lit up?\\nco-attention\\nimage\\nAnswer:green\\nstop\\nthe stop light\\ncolor stop light lit\\nstop\\nlight\\nlight\\n...\\n...\\n...\\n...\\nFig. 9.12 The architecture of hierarchical co-attention model\\nRegions\\nProposals\\nMulti-label CNN\\nCap 1: a dog laying on the floor with a\\nbird next to it and a cat behind them,\\non the other side of a sliding glass door.\\nCap 2: a brown and black dog laying\\non a floor next to a bird.\\nCap 3: the dog, cat, and bird are all on\\nthe floor in the room.\\n\\xe2\\x80\\xa6..\\nCaption\\nInternal Representation\\n\\xe2\\x80\\xa6\\nAverage\\nPooling\\nTop 5\\nAttributes\\nSPARQL\\nDBpedia\\nThe dog is a furry, carnivorous\\nmember of the canidae family,\\nmammal class. The cat is a small,\\nusually furry, domesticated, and\\ncarnivorous mammal. Birds, Aves\\nclass, are a group of endothermic\\nvertebrates,\\ncharacterised\\nby\\nfeathers, a beak with no teeth.\\nPlants, also called green plants ,\\nare multicellular eukaryotes of\\nthe\\xe2\\x80\\xa6.\\nDoc2Vec\\nExternal Knowledge\\n...\\n)\\n)\\n)\\n...\\n)\\nHow\\nmany\\n?\\n...\\nThere\\nThere\\nare\\nare\\ntwo\\nmammals\\nEND\\n...\\n. . .\\n1\\n\\xe2\\x88\\x99\\nminimizing cost function\\nGeneration\\nh0\\nh1\\nhn\\nhn+1\\nhn+l-1\\nLSTM\\nCap 1\\nCap 2\\nCap 5\\nFig. 9.13 The architecture of VQA incorporating external knowledge bases\\nwhere Zi j represents the af\\xef\\xac\\x81nity of the ith word and jth region. Figure9.12 shows\\nthe hierarchical co-attention model.\\nAnother intuitive approach is to use external knowledge from knowledge bases,\\nwhich will help us better explain the implicit information hiding behind the image.\\nSuch an approach was proposed in [75], which \\xef\\xac\\x81rst encodes the image into cap-\\ntions and vectors representing different attributes of the image to retrieve documents\\nabout a different part of the images from knowledge bases. Documents are encoded\\nthrough doc2vec [36]. The representation of captions, attributes, and documents are\\ntransformed and concatenated to form the initial vector of an LSTM, which is trained\\nin Seq2seq fashion. Details of the model are shown in Fig.9.13.\\nNeural Module Network is a framework for constructing deep networks with a\\ndynamic computational structure, which was \\xef\\xac\\x81rst proposed in [3]. In such a frame-\\nwork, every input is associated with a layout that provides a template for assembling\\nan instance-speci\\xef\\xac\\x81c network from a collection of shallow network fragments called\\n9.5 Visual Question Answering\\n311\\nWhere is\\nthe dog?\\nparser\\nLSTM\\ncouch\\nLayout\\ncount\\nwhere\\ncolor\\nstanding\\ng\\no\\nd\\nta\\nc\\nCNN\\n...\\n...\\nFig. 9.14 The architecture of the neural module network model\\nmodules. The proposed method processes the input question through two separate\\nways: (1) parsing and laying out several modules, and (2) encoding by an LSTM.\\nThe corresponding picture is processed by the modules laid out according to the\\nquestion, the types of which are prede\\xef\\xac\\x81ned, find, transform, combine,\\ndescribe, and measure. The authors de\\xef\\xac\\x81ned find to be a transformation from\\nImage to Attention map, transform to be a mapping from one Attention to\\nanother, combine to be a combination of two Attention, describe to be a\\ndescription relying on Image and Attention, and Measure to be a measure only\\nrelying on Attention. The model is shown in Fig.9.14.\\nA key drawback of [3] is that it relies on the parser to generate modules. [22]\\nproposes an end-to-end model to generate a sequence of Reverse-Polish expression\\nto describe the module network, as shown in Fig.9.15. And the overall architecture\\nis shown in Fig.9.16.\\nGraph Neural Networks (GNNs) have also been applied to VQA tasks. [68] tries\\nto build graphs about both the scene and the question. The authors described a deep\\nneural network to take advantage of such a structured representation. As shown in\\nFig.9.17, the GNN-based VQA model could capture the relationships between words\\nand objects.\\n9.6\\nSummary\\nIn this chapter, we \\xef\\xac\\x81rst introduce the concept of cross-modal representation learn-\\ning. Cross-modal learning is essential since many real-world tasks require the ability\\nto understand the information from different modalities, such as text and image.\\nNext, we introduce the concept of cross-modal representation learning, which aims\\nto exploit the links and enable better utilization of information from different modali-\\n312\\n9\\nCross-Modal Representation\\nFig. 9.15 The architecture of Reverse-Polish expression and corresponding module network model\\nHow many other things are of the\\nsame size as the green matte ball\\nQuestion encoder(RNN)\\nQuestion features\\nLayout predicion\\n(reverse Polish notation)\\nfind()\\nrelocate(_)\\nHow many other things are \\nof the same size as the\\ngreen matte ball?\\nHow many other things are\\nof the same size as the\\ngreen matte ball?\\nQuestion attentions\\nAnswer\\ncount\\nrelocate\\nrelocate\\ne the same size as th\\ne green matte ball?\\nImage encoder(CNN)\\nImage features\\n4\\nModule\\nnetwork\\nNetwork builder\\ncount(_)\\nLayout policy (RNN)\\nFig. 9.16 The architecture of end-to-end module network model\\nFig. 9.17 The architecture of GNN-based VQA models\\n9.6 Summary\\n313\\nties. And we overview existing cross-modal representation learning methods for sev-\\neral representative cross-modal tasks, including zero-shot recognition, cross-media\\nretrieval, image captioning, and visual question answering. These cross-modal learn-\\ning methods either try to fuse information from different modalities into uni\\xef\\xac\\x81ed\\nembeddings, or try to build embeddings for different modalities in a common seman-\\ntic space, allowing the model to compute cross-modal similarity. Cross-modal repre-\\nsentation learning is drawing more and more attention and can serve as a promising\\nconnection between different research areas.\\nFor further understanding of cross-modal representation learning, there are also\\nsome recommended surveys and books including:\\n\\xe2\\x80\\xa2 Skocaj et al., Cross-modal learning [64].\\n\\xe2\\x80\\xa2 Spence, Crossmodal correspondences: A tutorial review [66].\\n\\xe2\\x80\\xa2 Wang et al., A comprehensive survey on cross-modal retrieval [72].\\nIn the future, for better cross-modal representation learning, some directions are\\nrequiring further efforts:\\n(1) Fine-grained Cross-modal Grounding. Cross-modal grounding is a funda-\\nmental ability in solving cross-modal tasks, which aims to align semantic units in\\ndifferent modalities. For example, visual grounding aims to ground textual symbols\\n(e.g., words or phrases) into visual objects or regions. Many existing works [27, 74,\\n76] have been devoted to cross-modal grounding, which mainly focuses on coarse-\\ngrained semantic unit grounding (e.g., grounding of sentences and images). Better\\n\\xef\\xac\\x81ne-grained cross-modal grounding (e.g., grounding of words and objects) could\\npromote the development of a broad variety of cross-modal tasks.\\n(2) Cross-modal Reasoning. In addition to recognizing and grounding semantic\\nunits in different modalities, understanding and inferring the relationship between\\nsemantic units are also crucial to cross-modal tasks. Many existing works [37, 41,\\n82] have investigated detecting visual relation between objects. However, most visual\\nrelations in existing visual relation detection datasets do not require complex reason-\\ning. Some works [81] have made preliminary attempts on cross-modal commonsense\\nreasoning. Inferring the latent semantic relationships in cross-modal context is crit-\\nical for cross-modal understanding and modeling.\\n(3) Utilizing Unsupervised Cross-modal Data. Most current cross-modal learn-\\ning approaches rely on human-annotated datasets. The scale of such supervised\\ndatasets is usually limited, which also limits the capability of data-hungry neural\\nmodels. With the rapid development of the World Wide Web, cross-modal data on\\nthe Web have become larger and larger. Some existing works [42, 67] have lever-\\naged unsupervised cross-modal data for representation learning. They \\xef\\xac\\x81rst pretrained\\ncross-modal models on large-scale image-caption pairs, and then \\xef\\xac\\x81ne-tuned the mod-\\nels on those downstream tasks, which shows signi\\xef\\xac\\x81cant improvement in a broad\\nvariety of cross-modal tasks. It is thus promising to better leverage the vast amount\\nof unsupervised cross-modal data for representation learning.\\n314\\n9\\nCross-Modal Representation\\nReferences\\n1. Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. SPICE: semantic\\npropositional image caption evaluation. In Proceedings of ECCV, 2016.\\n2. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould,\\nand Lei Zhang. Bottom-up and top-down attention for image captioning and visual question\\nanswering. In Proceedings of CVPR, 2018.\\n3. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks.\\nIn Proceedings of CVPR, pages 39\\xe2\\x80\\x9348, 2016.\\n4. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence\\nZitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of ICCV, 2015.\\n5. Lei Jimmy Ba, Kevin Swersky, Sanja Fidler, and Ruslan Salakhutdinov. Predicting deep zero-\\nshot convolutional neural networks using textual descriptions. In Proceedings of ICCV, 2015.\\n6. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\\njointly learning to align and translate. In Proceedings of ICLR, 2015.\\n7. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana\\nYakhnenko. Translating embeddings for modeling multi-relational data. In Proceedings of\\nNeurIPS, 2013.\\n8. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\\nHolger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder\\xe2\\x80\\x93\\ndecoder for statistical machine translation. In Proceedings of EMNLP, 2014.\\n9. Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin. Towards diverse and natural image\\ndescriptions via a conditional GAN. In Proceedings of ICCV, 2017.\\n10. Bo Dai and Dahua Lin. Contrastive learning for image captioning. In Proceedings of NeurIPS,\\n2017.\\n11. Bo Dai, Deming Ye, and Dahua Lin. Rethinking the form of latent states in image captioning.\\nIn Proceedings of ECCV, 2018.\\n12. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\\nhierarchical image database. In Proceedings of CVPR, 2009.\\n13. Michael J. Denkowski and Alon Lavie. Meteor universal: Language speci\\xef\\xac\\x81c translation eval-\\nuation for any target language. In Proceedings of ACL, 2014.\\n14. Mohamed Elhoseiny, Babak Saleh, and Ahmed Elgammal. Write a classi\\xef\\xac\\x81er: Zero-shot learn-\\ning using purely textual descriptions. In Proceedings of ICCV, 2013.\\n15. Desmond Elliott and Arjen de Vries. Describing images using inferred visual dependency\\nrepresentations. In Proceedings of ACL, 2015.\\n16. DesmondElliottandFrankKeller.Imagedescriptionusingvisualdependencyrepresentations.\\nIn Proceedings of EMNLP, 2013.\\n17. Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian,\\nJulia Hockenmaier, and David Forsyth. Every picture tells a story: Generating sentences from\\nimages. In Proceedings of ECCV, 2010.\\n18. Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al.\\nDevise: A deep visual-semantic embedding model. In Proceedings of NeurIPS, 2013.\\n19. Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you\\ntalking to a machine? dataset and methods for multilingual image question. In Proceedings\\nof NeurIPS, 2015.\\n20. Jiuxiang Gu, Jianfei Cai, Gang Wang, and Tsuhan Chen. Stack-captioning: Coarse-to-\\xef\\xac\\x81ne\\nlearning for image captioning. In Proceedings of AAAI, 2018.\\n21. Micah Hodosh, Peter Young, and Julia Hockenmaier. Framing image description as a ranking\\ntask: Data, models and evaluation metrics. Journal of Machine Learning Research, 47:853\\xe2\\x80\\x93\\n899, 2013.\\n22. Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning\\nto reason: End-to-end module networks for visual question answering. In Proceedings of\\nICCV, 2017.\\nReferences\\n315\\n23. Xin Huang and Yuxin Peng. Deep cross-media knowledge transfer. In Proceedings of CVPR,\\n2018.\\n24. Xin Huang, Yuxin Peng, and Mingkuan Yuan. Cross-modal common representation learning\\nby hybrid transfer network. In Proceedings of IJCAI, 2017.\\n25. Zhaoyin Jia, Andrew Gallagher, Ashutosh Saxena, and Tsuhan Chen. 3d-based reasoning\\nwith blocks, support, and stability. In Proceedings of ICCV, 2013.\\n26. Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization\\nnetworks for dense captioning. In Proceedings of CVPR, 2016.\\n27. Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image\\ndescriptions. In Proceedings of CVPR, 2015.\\n28. Andrej Karpathy, Armand Joulin, and Fei Fei F Li. Deep fragment embeddings for bidirec-\\ntional image sentence mapping. In Proceedings of NeurIPS, 2014.\\n29. Yoon Kim. Convolutional neural networks for sentence classi\\xef\\xac\\x81cation. In Proceedings of\\nEMNLP, 2014.\\n30. Satwik Kottur, Ramakrishna Vedantam, Jos\\xc3\\xa9 MF Moura, and Devi Parikh. Visual word2vec\\n(vis-w2v): Learning visually grounded word embeddings using abstract scenes. In Proceed-\\nings of CVPR, 2016.\\n31. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz,\\nStephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and\\nLi Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image\\nannotations. International Journal of Computer Vision, 123(1):32\\xe2\\x80\\x9373, 2017.\\n32. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classi\\xef\\xac\\x81cation with deep\\nconvolutional neural networks. In Proceedings of NeurIPS, 2012.\\n33. Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and\\nTamara L Berg. Baby talk: Understanding and generating image descriptions. In Proceedings\\nof CVPR, 2011.\\n34. Polina Kuznetsova, Vicente Ordonez, Alexander C Berg, Tamara L Berg, and Yejin Choi.\\nCollective generation of natural image descriptions. In Proceedings of ACL, 2012.\\n35. Polina Kuznetsova, Vicente Ordonez, Tamara L Berg, and Yejin Choi. Treetalk: Composi-\\ntion and compression of trees for image descriptions. Transactions of the Association for\\nComputational Linguistics, 2(10):351\\xe2\\x80\\x93362, 2014.\\n36. Quoc V Le and Tomas Mikolov. Distributed representations of sentences and documents. In\\nProceedings of ICML, 2014.\\n37. Yikang Li, Wanli Ouyang, Bolei Zhou, Kun Wang, and Xiaogang Wang. Scene graph gener-\\nation from objects, phrases and region captions. In Proceedings of ICCV, 2017.\\n38. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summarization\\nBranches Out, 2004.\\n39. Xiao Lin and Devi Parikh. Don\\xe2\\x80\\x99t just listen, use your imagination: Leveraging visual common\\nsense for non-visual tasks. In Proceedings of CVPR, 2015.\\n40. Chenxi Liu, Junhua Mao, Fei Sha, and Alan L. Yuille. Attention correctness in neural image\\ncaptioning. In Proceedings of AAAI, 2017.\\n41. Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. Visual relationship detection\\nwith language priors. In Proceedings of ECCV, 2016.\\n42. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visi-\\nolinguistic representations for vision-and-language tasks. In Proceedings of NeurIPS, 2019.\\n43. Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. Knowing when to look: Adaptive\\nattention via a visual sentinel for image captioning. In Proceedings of CVPR, 2017.\\n44. Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-\\nattention for visual question answering. In Proceedings of NeurIPS, 2016.\\n45. Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In Proceedings of\\nCVPR, 2018.\\n46. Mateusz Malinowski and Mario Fritz. A multi-world approach to question answering about\\nreal-world scenes based on uncertain input. In Proceedings of NeurIPS, 2014.\\n316\\n9\\nCross-Modal Representation\\n47. Jiayuan Mao, Yuan Yao, Stefan Heinrich, Tobias Hinz, Cornelius Weber, Stefan Wermter,\\nZhiyuan Liu, and Maosong Sun. Bootstrapping knowledge graphs from images and text.\\nFrontiers in Neurorobotics, 13:93, 2019.\\n48. Harry McGurk and John MacDonald. Hearing lips and seeing voices. Nature, 1976.\\n49. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\\xef\\xac\\x81cient estimation of word\\nrepresentations in vector space. In Proceedings of ICLR, 2013.\\n50. Yasuhide Mori, Hironobu Takahashi, and Ryuichi Oka. Image-to-word transformation based\\non dividing and vector quantizing images with words. In Proceedings of WMISR, 1999.\\n51. Jonghwan Mun, Minsu Cho, and Bohyung Han. Text-guided attention model for image cap-\\ntioning. In Proceedings of AAAI, 2017.\\n52. Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng.\\nMultimodal deep learning. In Proceedings of ICML, 2011.\\n53. Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea\\nFrome, Greg S Corrado, and Jeffrey Dean. Zero-shot learning by convex combination of\\nsemantic embeddings. In Proceedings of ICLR, 2014.\\n54. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings of ACL, 2002.\\n55. Yuxin Peng, Xin Huang, and Yunzhen Zhao. An overview of cross-media retrieval: Concepts,\\nmethodologies, benchmarks and challenges. IEEE Transactions on Circuits and Systems for\\nVideo Technology, 2017.\\n56. Mengye Ren, Ryan Kiros, and Richard Zemel. Exploring models and data for image question\\nanswering. In Proceedings of NeurIPS, 2015.\\n57. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\\nobject detection with region proposal networks. In Proceedings of NeurIPS, 2015.\\n58. Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time\\nobject detection with region proposal networks. In Proceedings of NeurIPS, 2015.\\n59. Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. Self-\\ncritical sequence training for image captioning. In Proceedings of CVPR, 2017.\\n60. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\\nHuang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-\\nFei Li. Imagenet large scale visual recognition challenge. International Journal of Computer\\nVision, 115(3):211\\xe2\\x80\\x93252, 2015.\\n61. Kevin J Shih, Saurabh Singh, and Derek Hoiem. Where to look: Focus regions for visual\\nquestion answering. In Proceedings of CVPR, 2016.\\n62. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and\\nsupport inference from rgbd images. In Proceedings of ECCV, 2012.\\n63. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\\nimage recognition. arXiv:1409.1556, 2014.\\n64. Danijel Skocaj, Ales Leonardis, and Geert-Jan M. Kruijff. Cross-Modal Learning, pp. 861\\xe2\\x80\\x93\\n864. Boston, MA, 2012.\\n65. Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. Zero-shot learning\\nthrough cross-modal transfer. In Proceedings of NeurIPS, 2013.\\n66. Charles Spence. Crossmodal correspondences: A tutorial review. Attention, Perception, &\\nPsychophysics, 73(4):971\\xe2\\x80\\x93995, 2011.\\n67. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A\\njoint model for video and language representation learning. In Proceedings of ICCV, 2019.\\n68. Damien Teney, Lingqiao Liu, and Anton Van Den Hengel. Graph-structured representations\\nfor visual question answering. In Proceedings of CVPR, 2017.\\n69. Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based\\nimage description evaluation. In Proceedings of CVPR, 2015.\\n70. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural\\nimage caption generator. In Proceedings of CVPR, 2015.\\n71. Cheng Wang, Haojin Yang, Christian Bartz, and Christoph Meinel. Image captioning with\\ndeep bidirectional lstms. In Proceedings of ACMMM, 2016.\\nReferences\\n317\\n72. Kaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, and Liang Wang. A comprehensive survey on\\ncross-modal retrieval. arXiv:1607.06215, 2016.\\n73. Yufei Wang, Zhe Lin, Xiaohui Shen, Scott Cohen, and Garrison W. Cottrell. Skeleton key:\\nImage captioning by skeleton-attribute decomposition. In Proceedings of CVPR, 2017.\\n74. Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun, and Wei-Ying Ma.\\nUni\\xef\\xac\\x81ed visual-semantic embeddings: Bridging vision and language with structured meaning\\nrepresentations. In Proceedings of CVPR, 2019.\\n75. Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den Hengel. Ask me\\nanything: Free-form visual question answering based on knowledge from external sources.\\nIn Proceedings of CVPR, 2016.\\n76. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,\\nRich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with\\nvisual attention. In Proceedings of ICML, 2015.\\n77. Ran Xu, Jiasen Lu, Caiming Xiong, Zhi Yang, and Jason J Corso. Improving word represen-\\ntations via global visual context. In Proceedings of NeurIPS Workshop, 2014.\\n78. YezhouYang,ChingLikTeo,HalDaum\\xc3\\xa9III,andYiannisAloimonos.Corpus-guidedsentence\\ngeneration of natural images. In Proceedings of EMNLP, 2011.\\n79. Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention net-\\nworks for image question answering. In Proceedings of CVPR, 2016.\\n80. Licheng Yu, Eunbyung Park, Alexander C Berg, and Tamara L Berg. Visual madlibs: Fill in\\nthe blank description generation and question answering. In Proceedings of ICCV, 2015.\\n81. Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition:\\nVisual commonsense reasoning. In Proceedings of CVPR, 2019.\\n82. HanwangZhang,ZawlinKyaw,Shih-FuChang,andTat-SengChua.Visualtranslationembed-\\nding network for visual relation detection. In Proceedings of CVPR, 2017.\\n83. Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Yin and\\nyang: Balancing and answering binary visual questions. In Proceedings of CVPR, 2016.\\n84. Bo Zheng, Yibiao Zhao, Joey Yu, Katsushi Ikeuchi, and Song-Chun Zhu. Scene understanding\\nby reasoning stability and safety. International Journal of Computer Vision, 112(2):221\\xe2\\x80\\x93238,\\n2015.\\n85. Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question\\nanswering in images. In Proceedings of CVPR, 2016.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 10\\nResources\\nAbstract Deep learning has been shown as a powerful method for a variety of\\narti\\xef\\xac\\x81cial intelligence tasks, including some critical tasks in NLP. However, training a\\ndeep neural network is usually a very time-intensive process and requires lots of code\\nto build related models. To alleviate these issues, some deep learning frameworks\\nhave been developed and released, which incorporate some existing and necessary\\narithmetic operators for neural network constructions. And these frameworks exploit\\nhardware features such as multi-core CPUs and many-core GPUs to shorten the\\ntraining time. Each framework has its advantages and disadvantages. In this chapter,\\nwe aim to exhibit features and running performance of these frameworks so that\\nusers can select an appropriate framework for their usage.\\n10.1\\nOpen-Source Frameworks for Deep Learning\\nIn this section, we will introduce several typical open-source frameworks for deep\\nlearning including Caffe, Theano, TensorFlow, Torch, PyTorch, Keras, and MXNet.\\nIn fact, as the rapid development of the deep learning community, these open-source\\nframeworks are updating every day, and therefore the information in this section\\nmay not be up to date. In fact, this section mainly focuses on introducing the special\\nfeatures of these frameworks and lets the readers have a preliminary understanding\\nof them. To know the latest features of these deep learning frameworks, please refer\\nto their of\\xef\\xac\\x81cial sites.\\n10.1.1\\nCaffe\\nCaffe1 is a well-known framework and is widely used for computer vision tasks. It\\nwas created by Yangqing Jia and developed by Berkeley AI Research (BAIR). Caffe\\nuses a layer-wise approach to make building models become easy, and it is also\\n1http://caffe.berkeleyvision.org/.\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_10\\n319\\n320\\n10\\nResources\\nconvenient to \\xef\\xac\\x81ne-tune the existing neural networks without writing too much code\\nvia its simple interfaces. The underlying designs of Caffe are for the fast construction\\nof convolutional neural networks, which make it ef\\xef\\xac\\x81cient and effective.\\nOn the other hand, as normal pictures often have a \\xef\\xac\\x81xed size, the interfaces of\\nCaffe are \\xef\\xac\\x81xed and hard to be extended. It is thus dif\\xef\\xac\\x81cult to use Caffe for other\\ntasks with a variable input length, such as text, sound, or other time-series data.\\nRecurrent neural networks are also not well supported by Caffe. Although users can\\neasily build an existing network architecture with the layer-wise framework, it is not\\n\\xef\\xac\\x82exible when dealing with big and complex networks. If users want to design a new\\nlayer, the users need to use C/C++ and CUDA for the underlying coding of the new\\nlayer.\\n10.1.2\\nTheano\\nTheano2 is the typical framework developed to use symbolic tensor graphs for model\\nspeci\\xef\\xac\\x81cation. Any neural networks or other machine-learning models can be repre-\\nsented as symbolic tensor graphs. Forward, backward, and gradient updates can be\\ncalculated based on the \\xef\\xac\\x82ow between tensors. Hence, Theano provides more \\xef\\xac\\x82exibil-\\nity than Caffe using a layer-wise approach to build models. In Caffe, to de\\xef\\xac\\x81ne a new\\nlayer that is not already in the existing repository of layers is complicated, which\\nneeds to implement its forward, backward, and gradient update functions before. In\\nTheano, you only need to use basic operators to de\\xef\\xac\\x81ne the customized layer following\\nthe order of operations.\\nTheano is a platform and is easy to con\\xef\\xac\\x81gure as compared with other frameworks.\\nAnd some high-level frameworks are built on top of Theano such as Keras, which\\nfurther makes Theano easier to use. Theano supports cross-platform con\\xef\\xac\\x81guration\\nwell, which means it works on not only Linux but also Windows. Because of this,\\nmany researchers and engineers use Theano to build their models and then release\\nthese projects. Rich open resources based on Theano attract some more users.\\nThough Theano uses Python syntax to de\\xef\\xac\\x81ne symbolic tensor graphs, its graph\\nprocessor will compile the graphs into high-performance C++ or CUDA code for\\ncomputing. Owing to this, Theano can run very fast and make programmers code\\nmode simply. Only one de\\xef\\xac\\x81ciency is that the compilation process is slow and needs\\nsome time. If a neural network does not need to be trained for several days, it is\\nnot a good idea to select Theano. Compiling too often in Theano is maddening and\\nannoying. As a comparison, the later framework like TensorFlow uses the compiled\\npackage for the symbolic tensor operations, which seems a little more relaxing.\\nTheano has some other serious disadvantages. Theano cannot support many-core\\nGPUs very well, which makes it hard to train big neural networks. Besides the\\ncompilation process, importing Theano is also slow. When you run your code in\\nTheano, you will be stuck for a long time with a precon\\xef\\xac\\x81gured device. If you want to\\nimprove and contribute to Theano itself, this will also be maddening and annoying.\\n2http://www.deeplearning.net/software/theano/.\\n10.1 Open-Source Frameworks for Deep Learning\\n321\\nIn fact, Theano is no longer maintained, but it is still worth introducing as a landmark\\nwork in the history of deep learning frameworks, which inspires many subsequent\\nframeworks.\\n10.1.3\\nTensorFlow\\nTensorFlow3 is mainly developed and used by Google based on the experience on\\nTheano and DistBelief [1]. TensorFlow and Theano are in fact quite similar to some\\nextent. Both of them allow building a symbolic graph of the neural network archi-\\ntecture via the Python interface. Different from Theano, TensorFlow allows imple-\\nmenting new operations or machine-learning algorithms using C/C++ and Java. With\\nbuilding symbolic graphs, the auto-gradient can be easily used to train complicated\\nmodels. Hence, TensorFlow is more than a deep learning framework. Its \\xef\\xac\\x82exibil-\\nity enables it to solve various complex computing problems such as reinforcement\\nlearning.\\nIn TensorFlow, both code development and deployment is fast and convenient.\\nTrained models can be deployed quickly on a variety of devices, including servers\\nand mobile devices, without the need to implement a separate model setting code\\nor load Python/LuaJIT interpreter. Caffe also allows easy deployment of models.\\nHowever, Caffe has trouble running on devices without a GPU, which is a prevalent\\nsituation of smartphones. TensorFlow supports model decoding using ARM/NEON\\ninstructions and does not need too many operations to choose training devices.\\nTensorBoard of TensorFlow provides a platform for visualization of the model\\narchitectures, which is beautiful and also useful. By visualizing the symbolic graph, it\\nis not dif\\xef\\xac\\x81cult to \\xef\\xac\\x81nd bugs in the source code. To debug models on other deep learning\\nframeworks is relatively bothering. TensorBoard can also log and generate real-time\\nvisualization of variables during training, which is a pleasant way to monitor the\\ntraining process.\\nThough customizing operations in TensorFlow is convenient, it usually changes a\\nlot of function interfaces in every new release which is challenging for developers to\\nkeep their code compatible with different TensorFlow versions. And mastering Ten-\\nsorFlow is also not easy. As TensorFlow 2.0 has been released recently, TensorFlow\\nmay gradually handle these issues in the predictable future.\\n10.1.4\\nTorch\\nTorch4 is a computational framework mainly developed and used by Facebook and\\nTwitter. Torch provides an API written in Lua to support the implementation of some\\n3https://www.tensor\\xef\\xac\\x82ow.org/.\\n4http://torch.ch/.\\n322\\n10\\nResources\\nmachine-learning algorithms, especially convolutional neural networks. A temporal\\nconvolutional layer implemented by Torch can have a variable input length, which\\nis extremely useful for NLP tasks and not designed in Theano and TensorFlow.\\nTorch also contains the 3D convolutional layer, which can be easily used in video\\nrecognition tasks. Besides its various \\xef\\xac\\x82exible convolutional layers, Torch is light and\\nspeedy. The above reasons attract lots of researchers in universities and companies\\nto customize their own deep learning platforms.\\nHowever, the negative aspects of Torch are also apparent. Though Torch is pow-\\nerful, it is not designed to be widely accessible to the Python-based academic com-\\nmunity. And there are not any other interfaces but Lua. Lua is a multi-paradigm\\nscripting language, which was developed in Brazil in the early 1990s and is not a\\npopular mainstream programming language. Hence, it needs some time to learn Lua\\nbefore you use Torch to construct models. Different from convolutional neural net-\\nworks, there is no of\\xef\\xac\\x81cial support for recurrent neural networks. There are some open\\nresources about recurrent neural networks implemented by Torch, but they are not yet\\nintegrated to the main repository. And it is dif\\xef\\xac\\x81cult to distinguish the effectiveness\\nof these implementations.\\nSimilar to Caffe, Torch is not a framework based on symbolic tensor graphs, it\\nalso uses the layer-wise approach. This means that your models in Torch are a graph\\nof layers and not a graph of mathematical functions. The mechanism is convenient\\nto build a network whose layers are stable and hierarchical. If you want to design a\\nnew connection layer or change an existing neural model, you need lots of code to\\nimplement new layers with full forward, backward, and gradient update functions.\\nHowever, those frameworks based on symbolic tensor graphs, such as Theano and\\nTensorFlow, give more \\xef\\xac\\x82exibility to do this. In fact, these issues are handled as\\nPyTorch has been released, which we will introduce then.\\n10.1.5\\nPyTorch\\nPyTorch5 is a Python package built over Torch, developed by Facebook and other\\ncompanies. However, it is not just an interface, and PyTorch has amounts of improve-\\nments over Torch. The most important one is that PyTorch can use a symbolic graph\\nto de\\xef\\xac\\x81ne neural networks, and then use automatic differentiation following the graph\\nto automate the computation of backward passes in neural networks. Meanwhile,\\nPyTorch maintains some characteristics of the layer-wise approach in Torch, which\\nmeans coding with PyTorch is easy. Moreover, PyTorch has minimal framework over-\\nhead and custom memory allocators for the GPU, which means PyTorch is faster and\\nmemory-ef\\xef\\xac\\x81cient than Torch.\\n5http://pytorch.org/.\\n10.1 Open-Source Frameworks for Deep Learning\\n323\\nCompared with other deep learning frameworks, PyTorch has two main advan-\\ntages. First, most frameworks like TensorFlow are based on static computational\\ngraphs (de\\xef\\xac\\x81ne-and-run), while PyTorch uses dynamic computational graphs (de\\xef\\xac\\x81ne-\\nby-run). What it means is that with dynamic computational graphs, you can change\\nthe network architecture based on the data \\xef\\xac\\x82owing through the network. There is a\\nway to do something similar in TensorFlow, but your static computational graphs\\nmust contain all possible branches in advance, which will limit the performance.\\nSecond, PyTorch is built to be deeply integrated into Python and has a seamless\\nconnection with other popular Python packages, such as Numpy, Spicy, and Cython.\\nThus, it is easy to extend your model when needed.\\nAfter Facebook released it, PyTorch has drawn considerable attention from the\\ndeep learning community, and many past Torch users switch to this new package. For\\nnow, PyTorch already has a thriving community which contributes to its increasing\\npopularity among researchers. It is no exaggeration to say that PyTorch is one of the\\nmost popular frameworks at present.\\n10.1.6\\nKeras\\nKeras6 is a top-design deep learning framework that is based on Theano and Tensor-\\nFlow. Interestingly, Keras sits atop Theano and TensorFlow, however, its interfaces\\nare similar to Torch. To use Keras needs Python code, and there are lots of detailed\\ndocuments and examples for a quick start. There is also a very active community\\nof developers, and they make Keras fastly updated. Hence, it is a very fast-growing\\nframework.\\nBecause Theano and TensorFlow are the backends of Keras, disadvantages of\\nKeras are most similar to Theano and TensorFlow. With TensorFlow as the backend,\\nit will run even slower than the pure TensorFlow code. Because it is a high-level\\nframework, to customize a new neural layer is not easy, though you can easily use\\nexisting layers under Keras. The package is too advanced, and it hides too many\\ntraining parameters. You cannot touch and change all details of your own models\\nunless you use Theano, TensorFlow, or PyTorch.\\n10.1.7\\nMXNet\\nMXNet7 is an effective and ef\\xef\\xac\\x81cient open-source machine-learning framework,\\nmainly pushed by Amazon. It supports APIs with multiple languages, including\\nC++, Python, R, Scala, Julia, Perl, MATLAB, and JavaScript, some of which can be\\nadopted for Amazon Web Services. Some interfaces of MXNet are also reserved for\\n6https://keras.io/.\\n7http://mxnet.io/.\\n324\\n10\\nResources\\nfuture mobile devices, just like TensorFlow. MXNet is built on a dynamic dependency\\nscheduler that automatically parallelizes both symbolic and imperative operations on\\nthe \\xef\\xac\\x82y. A graph optimization layer on top of that makes symbolic execution fast and\\nmemory ef\\xef\\xac\\x81cient. The MXNet library is portable and lightweight, and it scales to\\nmultiple GPUs and multiple machines. The main problem of MXNet is the lack of\\ndetailed and well-organized documentation. The user groups are also smaller than\\nother frameworks, especially as compared with TensorFlow and PyTorch. It is more\\nchallenging to grasp MXNet for newbies. The MXNet is developing fastly, and these\\nproblems may be solved in the future.\\n10.2\\nOpen Resources for Word Representation\\n10.2.1\\nWord2Vec\\nWord2vec8 is a widely used toolkit for word representation learning, which provides\\nan effective and ef\\xef\\xac\\x81cient implementation of the continuous bag-of-words and Skip-\\ngram architectures. The word representations learned by Word2vec can be used in\\nmany natural language processing \\xef\\xac\\x81elds. Empirically, To use pretrained word vectors\\nas the model inputs can be a good way to enhance model performances.\\nWord2vec takes free text corpus as input and constructs the vocabulary list from\\nthe training data. Then it uses simple predictive models based on neural networks\\nto learn the language model, which encode the co-occurrence information between\\nwords into the resulting word representations.\\nTheresultingrepresentationsshowcaseinterestinglinearsubstructuresoftheword\\nvector space. The Euclidean distance (or cosine similarity) between two-word vectors\\nprovides an effective method for measuring the linguistic or semantic similarity of\\nthe corresponding words. Sometimes, the nearest neighbors, according to this metric,\\nreveal rare but relevant words that lie outside an average human\\xe2\\x80\\x99s vocabulary.\\nWords frequently appearing together in the text will have representations with\\nclose distance within the embedding space. Word2vec also provides a tool to \\xef\\xac\\x81nd the\\nclosest words for a user-speci\\xef\\xac\\x81ed word via the learned representations and distances\\nbetween representation embeddings.\\n10.2.2\\nGloVe\\nGloVe9 is a widely used toolkit, which supports an unsupervised learning method for\\nword representation learning. Similar to Word2vec, GloVe also trains on text corpus\\n8https://code.google.com/archive/p/word2vec/.\\n9https://nlp.stanford.edu/projects/glove/.\\n10.2 Open Resources for Word Representation\\n325\\nand captures the aggregated global word-word co-occurrence information for word\\nembeddings. However, GloVe uses count-based models instead of predictive models,\\nwhich are different from Word2vec.\\nThe GloVe model \\xef\\xac\\x81rst builds a global word-word co-occurrence matrix, which\\ncan show how frequently words co-occur with one another in a given text. Then\\nword representations are trained on the nonzero entries of the matrix. To construct\\nthis matrix requires the entire corpus traversal for the statistics collection. For large\\ncorpora, this pass can be computationally expensive, but it is a one-time up-front\\ncost. Subsequent training iterations are much faster because the number of nonzero\\nmatrix entries is typically much smaller than the total number of words in the corpus.\\n10.3\\nOpen Resources for Knowledge Graph Representation\\n10.3.1\\nOpenKE\\nOpenKE10 [2] is an open-source toolkit for Knowledge Embedding (KE), which pro-\\nvides a uni\\xef\\xac\\x81ed framework and various fundamental KE models. OpenKE prioritizes\\noperational ef\\xef\\xac\\x81ciency to support quick model validation and large-scale knowledge\\nrepresentation learning. Meanwhile, OpenKE maintains suf\\xef\\xac\\x81cient modularity and\\nextensibility to incorporate new models easily. Besides the toolkit, the embeddings\\nof some existing large-scale knowledge graphs pretrained by OpenKE are also avail-\\nable. The toolkit, documentation, and pretrained embeddings are all released on\\nhttp://openke.thunlp.org/.\\nAs compared to other implementations, OpenKE has \\xef\\xac\\x81ve advantages. First,\\nOpenKE has implemented nine classical knowledge embedding algorithms, includ-\\ning RESCAL, TransE, TransH, TransR, TransD, ComplEx, DistMult, HolE, and\\nAnalogy, which are veri\\xef\\xac\\x81ed effective and stable. Second, OpenKE shows high per-\\nformance due to memory optimization, multi-threading acceleration, and GPU learn-\\ning. OpenKE supports multiple computing devices and provides interfaces to control\\nCPU/GPU modes. Third, system encapsulation makes OpenKE easy to train and test\\nKE models. Users just need to set hyperparameters via interfaces of the platform to\\nconstruct KE models. Fourth, it is easy to construct new KE models. All speci\\xef\\xac\\x81c\\nmodels are implemented by inheriting the base class by designing their own scoring\\nfunctions and loss functions. Fifth, besides the toolkit, OpenKE also provides the\\nembeddings of some existing large-scale knowledge graphs pretrained by OpenKE,\\nwhich can be directly applied for many applications, including information retrieval,\\npersonalized recommendation, and question answering.\\n10https://github.com/thunlp/OpenKE.\\n326\\n10\\nResources\\n10.3.2\\nScikit-Kge\\nScikit-kge11 is an open-source Python library for knowledge representation learn-\\ning. The library supports different building blocks to train and develop models for\\nknowledge graph embeddings. The primary purpose of Scikit-kge is to compute the\\nembeddings of knowledge graphs for the method HolE; meanwhile, it also provides\\nsome other methods. Besides HolE, RESCAL, TransE, TransR, and ER-MLP can\\nalso be trained in Scikit-kge. The library contains some parameter update methods,\\nnot only the basic SGD but also AdaGrad. It also implements different negative\\nsampling strategies to select negative samples.\\n10.4\\nOpen Resources for Network Representation\\n10.4.1\\nOpenNE\\nOpenNE12 is an open-source standard NE/NRL (Network Representation Learning)\\ntraining and testing framework. It uni\\xef\\xac\\x81es the input and output interfaces of different\\nNE models and provides scalable options for each model. Moreover, typical NE\\nmodels under this framework are based on TensorFlow, which enables these models\\nto be trained with GPUs. The implemented or modi\\xef\\xac\\x81ed models include DeepWalk,\\nLINE, node2vec, GraRep, TADW, GCN, HOPE, GF, SDNE, and LE. The framework\\nalso provides classi\\xef\\xac\\x81cation and embedding visualization modules for evaluating the\\nresult of NRL.\\n10.4.2\\nGEM\\nGEM (Graph Embedding Methods)13 is a Python package that offers a general frame-\\nwork for graph embedding methods. It implements many state-of-the-art embedding\\ntechniques including Locally Linear Embedding, Laplacian Eigenmaps, Graph Fac-\\ntorization, High-Order Proximity preserved Embedding (HOPE), Structural Deep\\nNetwork Embedding (SDNE), and node2vec. Furthermore, the framework imple-\\nments several functions to evaluate the quality of the obtained embeddings including\\ngraph reconstruction, link prediction, visualization, and node classi\\xef\\xac\\x81cation. For faster\\nexecution, C++ backend is integrated using Boost for supported methods.\\n11https://github.com/mnick/scikit-kge.\\n12https://github.com/thunlp/OpenNE.\\n13https://github.com/palash1992/GEM.\\n10.4 Open Resources for Network Representation\\n327\\n10.4.3\\nGraphVite\\nGraphVite14 is a general and high-performance graph embedding system for various\\napplications including node embedding, knowledge graph embedding, and graph\\nhigh-dimensional data visualization.\\nGraphVite provides a complete pipeline for users to implement and evaluate graph\\nembedding models. For reproducibility, the system integrates several commonly\\nused models and benchmarks, and you can also develop your own models with the\\n\\xef\\xac\\x82exible interface. Additionally, for semantic tasks, GraphVite releases a bunch of\\npretrained knowledge graph embedding models to enhance language understanding.\\nThere are two core advantages of GraphVite over other toolkits: fast and large-scale\\ntraining. GraphVite accelerates graph embedding with multiple CPUs and GPUs.\\nIt takes around one minute to learn node embeddings for graphs with one million\\nnodes. Moreover, GraphVite is designed to be scalable. Even with limited memory,\\nGraphVite can process node embedding task on billion-scale graphs.\\n10.4.4\\nCogDL\\nCogDL15 is another graph representation learning toolkit that allows researchers\\nand developers to easily train and evaluate baseline or custom models for node\\nclassi\\xef\\xac\\x81cation, link prediction, and other tasks on graphs. It provides implementations\\nof many popular models, including non-GNN models and GNN-based ones.\\nCogDL bene\\xef\\xac\\x81ts from several unique techniques. First, utilizing sparse matrix\\noperation, CogDL is capable of performing fast network embedding on large-scale\\nnetworks. Second, CogDL has the ability to deal with different types of graph struc-\\ntures attributed, multiplex, and heterogeneous networks. Third, CogDL supports\\nparallel training. With different seeds and different models, CogDL performs train-\\ning on multiple GPUs and reports the result table automatically. Finally, CogDL is\\nextendable. New datasets, models, and tasks can be added without dif\\xef\\xac\\x81culty.\\n10.5\\nOpen Resources for Relation Extraction\\n10.5.1\\nOpenNRE\\nOpenNRE16 [3] is an open-source framework for neural relation extraction, which\\naims to easily build relation extraction (RE) models.\\n14https://graphvite.io/.\\n15http://keg.cs.tsinghua.edu.cn/cogdl/index.html.\\n16https://github.com/thunlp/OpenNRE.\\n328\\n10\\nResources\\nCompared with other implementations, OpenNRE has four advantages. First,\\nOpenNRE has implemented various state-of-the-art RE models, including atten-\\ntion mechanism, adversarial learning, and reinforcement learning. Second, Open-\\nNRE enjoys great system encapsulation. It divides the pipeline of relation extraction\\ninto four parts, namely, embedding, encoder, selector (for distant supervision), and\\nclassi\\xef\\xac\\x81er. For each part, it has implemented several methods. System encapsulation\\nmakes it easy to train and test models by changing hyperparameters or appoint model\\narchitectures by using Python arguments. Third, OpenNRE is extendable. Users can\\nconstruct new RE models by choosing speci\\xef\\xac\\x81c blocks provided in four parts as men-\\ntioned above and combining them freely, with only a few lines of codes. Fourth, the\\nframework has implemented multi-GPU learning, which is ef\\xef\\xac\\x81cient.\\nReferences\\n1. Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc\\xe2\\x80\\x99aurelio\\nRanzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. In\\nAdvances in neural information processing systems, pages 1223\\xe2\\x80\\x931231, 2012.\\n2. Xu Han, Shulin Cao, Xin Lv, Yankai Lin, Zhiyuan Liu, Maosong Sun, and Juanzi Li. OpenKE:\\nAn open toolkit for knowledge embedding. In Proceedings of EMNLP: System Demonstrations,\\npages 139\\xe2\\x80\\x93144, 2018.\\n3. Xu Han, Tianyu Gao, Yuan Yao, Deming Ye, Zhiyuan Liu, and Maosong Sun. OpenNRE: An\\nopen and extensible toolkit for neural relation extraction. In Proceedings of EMNLP: System\\nDemonstrations, pages 169\\xe2\\x80\\x93174, 2019.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 11\\nOutlook\\nAbstract The aforementioned representation learning models and methods have\\nshowntheireffectivenessinvariousNLPscenariosandtasks.Withtherapidgrowthof\\ndatascalesandthedevelopmentofcomputationdevices,therearealsonewchallenges\\nand opportunities for next-stage researches of deep learning techniques. In the last\\nchapter, we will look into the future directions of representation learning techniques\\nfor NLP. To be more speci\\xef\\xac\\x81c, we will consider the following directions including\\nusing more unsupervised data, utilizing few labeled data, employing deeper neural\\narchitectures, improving model interpretability and fusing the advantages of other\\nareas.\\n11.1\\nIntroduction\\nWe have used ten chapters to introduce the advances of representation learning\\nfor NLP, covering both multi-grained language entries including words, phrases,\\nsentences and documents, and closely related objects including world knowledge,\\nsememe knowledge, networks, and cross-modal data. Those mentioned models and\\nmethods of representation learning for NLP have shown their effectiveness in various\\nNLP scenarios and tasks.\\nAs shown by the unsatisfactory performance of most NLP systems in open\\ndomains, and recent great advances of pre-trained language models, representation\\nlearning for NLP is far from perfect. With the rapid growth of data scales and the\\ndevelopment of computation devices, we are facing new challenges and opportunities\\nfor next-stage researches of representation learning and deep learning techniques.\\nIn this last chapter, we will look into the future research and exploration directions\\nof representation learning techniques for NLP. Since we have summarized the future\\nwork of each individual part in the summary section of each previous chapter, here\\nwe focus on discussing the general and important issues that should be addressed by\\nrepresentation learning for NLP.\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_11\\n329\\n330\\n11\\nOutlook\\nFor general representation learning for NLP, we conclude the following direc-\\ntions, including using more unsupervised data, utilizing a few labeled data, employ-\\ning deeper neural architectures, improving model interpretability, and fusing the\\nadvances from other areas.\\n11.2\\nUsing More Unsupervised Data\\nThe rapid development of Internet technology and the popularization of information\\ndigitization have brought massive text data for NLP researches and applications.\\nFor example, the whole corpus of Wikipedia already contains more than 50 million\\narticles (including 6 million articles in English)1 and is growing rapidly every day\\ncontributed by collaborative work all over the world. The amount of user-generated\\ncontent onmanysocial platforms suchas Twitter, Weibo, andFacebookalsoincreases\\nquicklybybillionsofusers.Itisworthconsideringthesemassivetextdataforlearning\\nbetter NLP models. However, due to the expensive cost of expert annotations, it is\\nimpossible to label such massive amounts of data for speci\\xef\\xac\\x81c NLP tasks.\\nHence, an essential direction of NLP is how to take better advantages of unla-\\nbeled data for ef\\xef\\xac\\x81cient unsupervised representation learning. Though without labeled\\nannotations, unsupervised data can help initialize the randomized neural network\\nparameters and thus improve the performances of those downstream NLP tasks.\\nThis line of work usually employs a pipeline strategy: \\xef\\xac\\x81rst, pretrain the model\\nparameters and then \\xef\\xac\\x81ne-tune these parameters in speci\\xef\\xac\\x81c downstream NLP tasks.\\nRecurrent language model [7], word embeddings [6], and pre-trained language mod-\\nels (PLM) such as BERT [3], all utilize unsupervised plain text to pretrain neural\\nparameters and then bene\\xef\\xac\\x81t downstream supervised tasks via \\xef\\xac\\x81ne-tuning.\\nCurrent state-of-the-art PLM models still can only learn from limited plain text\\ndue to limited learning ef\\xef\\xac\\x81ciency and computation power. Moreover, there are various\\ntypes of large-scale data online with abundant informative signals and labels, such as\\nHTMLtags, anchor text, keywords, document meta-information, andother structured\\nand semi-structured data. How to take full advantage of the large-scale Web text data\\nhas not been extensively studied. In the future, with better computation devices (e.g.,\\nGPUs) and data resources, we are expected to develop more advanced methods to\\nutilize more unsupervised data.\\n11.3\\nUtilizing Fewer Labeled Data\\nAs NLP technologies become more powerful, people can explore more complicated\\nand \\xef\\xac\\x81ne-grained problems. Taking text classi\\xef\\xac\\x81cation as an example, early work tar-\\ngeted on \\xef\\xac\\x82at classi\\xef\\xac\\x81cation with limited categories, and now researchers are more\\n1http://en.wikipedia.org/wiki.\\n11.3 Utilizing Fewer Labeled Data\\n331\\ninterested in classi\\xef\\xac\\x81cation with hierarchical structure and a large number of classes.\\nHowever, when a problem gets more complicated, it requires more knowledge from\\nexperts to annotate training instances for \\xef\\xac\\x81ne-grained tasks and increases the cost of\\ndata labeling.\\nTherefore, we expect the models or systems can be developed ef\\xef\\xac\\x81ciently with\\n(very) few labeled data. When each class has only one or a few labeled instances, the\\nproblem becomes a one/few-shot learning problem. The few-shot learning problem is\\nderivedfromcomputervisionandhasalsobeenstudiedinNLPrecently.Forexample,\\nresearchers have explored few-shot relation extraction [5] where each relation has a\\nfew labeled instances, and low-resource machine translation [11] where the size of\\nthe parallel corpus is limited.\\nA promising approach to few-shot learning is to compare the semantic similarity\\nbetween the test instance and those labeled ones (i.e., the support set), and then make\\nthe prediction. The idea is similar to k-nearest neighbor classi\\xef\\xac\\x81cation (kNN) [10].\\nSince the key is to represent the semantic meanings of each instance for measuring\\ntheir semantic similarity, it has been veri\\xef\\xac\\x81ed that language models pretrained on\\nunsupervised data and \\xef\\xac\\x81ne-tuned on the target few-shot domain are very effective\\nfor few-shot learning.\\nAnother approach to few-shot learning is to transfer the models from some related\\ndomains into the target domain with the few-shot problem [2]. This is usually named\\nas transfer learning or domain adaptation. For these methods, representation learning\\ncan also help the transfer or adaptation process by learning joint representations of\\nboth domains.\\nIn the future, one may go beyond the abovementioned frameworks and design\\nmore appropriate methods according to the characteristics of NLP tasks and prob-\\nlems. The goal is to develop effective NLP methods with as less annotated data in the\\ntarget domain as possible, by better utilizing unsupervised data that are much cheaper\\nto get from the Web and existing supervised data from other domains. The explo-\\nration of the few-shot learning problem in NLP will help us develop data-ef\\xef\\xac\\x81cient\\nmethods for language learning.\\n11.4\\nEmploying Deeper Neural Architectures\\nAs the amount of available text data rapidly increases, the size of the training corpus\\nfor NLP tasks grows as well. With more training data, a natural way to boost model\\nperformances is to employ deeper neural architectures for modeling. Intuitively,\\ndeeper neural models that have more sophisticated architecture and parameters can\\nbetter \\xef\\xac\\x81t the increasing data. Another motivation for using deeper architectures for\\nmodeling comes from the development of computation devices (e.g., GPUs). Cur-\\nrent state-of-the-art methods are usually a compromise between ef\\xef\\xac\\x81ciency and effec-\\ntiveness. As the computation devices operate faster, the time/space complexities of\\ncomplicated models become acceptable, which motivate researchers to design more\\n332\\n11\\nOutlook\\ncomplex but effective models. To summarize, employing deeper neural architectures\\nwould be one of the de\\xef\\xac\\x81nite orientations for representation learning in NLP.\\nVery deep neural network architectures have been widely used in computer vision.\\nFor example, the well-known VGG [8] network which was proposed in the famous\\nImageNet contest has 16 layers of convolutional and fully connected layers. In NLP,\\nthe depths of neural architectures were relatively shallow until the Transformer [9]\\nstructure was proposed. Speci\\xef\\xac\\x81cally, as compared with word embedding [6] which is\\nbased on shallow models, the state-of-the-art pre-trained language model BERT [3]\\ncan be regarded as a giant model that stacks 12 self-attention layers and each layer has\\n8 attention heads. BERT has demonstrated its effectiveness in a number of NLP tasks.\\nBesides the well-designed model architecture and training objectives, the success of\\nBERT also bene\\xef\\xac\\x81ts from TPUs which is one of the most powerful devices for parallel\\ncomputations. In contrast, it may take months or years for a single CPU to \\xef\\xac\\x81nish the\\ntraining process of BERT. When these computation devices go popular, we can expect\\nmore deep neural architectures to be developed for NLP as well.\\n11.5\\nImproving Model Interpretability\\nModel transparency and interpretability are hot topics in arti\\xef\\xac\\x81cial intelligence and\\nmachine learning. Human interpretable predictions are very important for decision-\\ncritical applications related to ethics, privacy, and safety. However, neural network\\nmodels or deep learning techniques are short of model transparency for human inter-\\npretable predictions and thus are often treated as black boxes.\\nMost NLP techniques based on neural networks and distributed representation are\\nalso hard to be interpreted except for the attention mechanism where the attention\\nweights can be interpreted as the importance of corresponding inputs. For the sake of\\nemploying representation learning techniques for decision-critical applications, there\\nis a need to improve model interpretability and transparency of current representation\\nlearning and neural network models.\\nA recent survey [1] classi\\xef\\xac\\x81es interpretable machine learning methods into two\\nmaincategories:interpretablemodelsandpost-hocexplainabilitytechniques.Models\\nthat are understandable by themselves are called interpretable models. For example,\\nlinear models, decision trees, and rule-based systems are such transparent models.\\nHowever, in most cases, we have to probe into the model by a second one for expla-\\nnations, namely post-hoc explainability techniques. In NLP, there have been some\\nresearches to visualize neural models such as neural machine translation [4] for\\ninterpretable explanations. However, the understanding of most neural-based mod-\\nels remains unsolved. We are looking forward to more studies on improving model\\ninterpretability to facilitate the extensive use of representation learning methods for\\nNLP.\\n11.6 Fusing the Advances from Other Areas\\n333\\n11.6\\nFusing the Advances from Other Areas\\nDuring the development of deep learning techniques, mutual learning between dif-\\nferent research areas has never stopped.\\nFor example, Word2vec aims to learn word embeddings from large-scale text cor-\\npus published in 2013 and can be regarded as a milestone of representation learning\\nfor NLP. In 2014, the idea of Word2vec was adopted for learning node embeddings in\\na network/graph by treating random walks over the network as sentences, named as\\nDeepWalk; the analogical reasoning phenomenon learned by Word2vec, i.e., king \\xe2\\x88\\x92\\nman = queen \\xe2\\x88\\x92woman also inspired the representation learning of world knowledge,\\nnamed as TransE. Meanwhile, graph convolutional networks were \\xef\\xac\\x81rst proposed for\\nsemi-supervised graph learning in 2016, and have been widely applied on many NLP\\ntasks such as relation extraction and text classi\\xef\\xac\\x81cation recently. Another example is\\nthe Transformer model which was proposed for neural machine translation at \\xef\\xac\\x81rst\\nand then transferred to computer vision, data mining, and many other areas.\\nThe fusion also appears between two quite distant disciplines. We should recall\\nagain that, the idea of distributed representation proposed in the 1980s is inspired by\\ntheneuralcomputationschemeofhumansandotheranimals.Ittakesabout40yearsto\\nsee the development of distributed representation and deep learning come to fruition.\\nIn fact, many ideas such as convolution in CNN and the attention mechanism are\\ninspired by the computation scheme of human cognition.\\nTherefore, an intriguing direction of representation learning for NLP is to fuse\\nthe advances from other areas, including not only those closely related areas in AI\\nsuch as machine learning, computer vision, and data mining, but also those distant\\nareas to some extent such as linguistics, brain science, psychology, and sociology.\\nThis line of work requires researchers to have suf\\xef\\xac\\x81cient knowledge of other \\xef\\xac\\x81elds.\\nReferences\\n1. Alejandro Barredo Arrieta, Natalia D\\xc3\\xadaz-Rodr\\xc3\\xadguez, Javier Del Ser, Adrien Bennetot, Siham\\nTabik, Alberto Barbado, Salvador Garc\\xc3\\xada, Sergio Gil-L\\xc3\\xb3pez, Daniel Molina, Richard Ben-\\njamins, et al. Explainable arti\\xef\\xac\\x81cial intelligence (xai): Concepts, taxonomies, opportunities and\\nchallenges toward responsible ai. Information Fusion, 58:82\\xe2\\x80\\x93115, 2020.\\n2. Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A\\ncloser look at few-shot classi\\xef\\xac\\x81cation. In Proceedings of ICLR, 2019.\\n3. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019.\\n4. Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. Visualizing and understanding\\nneural machine translation. In Proceedings of ACL, 2017.\\n5. Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun.\\nFewRel: A large-scale supervised few-shot relation classi\\xef\\xac\\x81cation dataset with state-of-the-art\\nevaluation. In Proceedings of EMNLP, 2018.\\n6. T Mikolov and J Dean. Distributed representations of words and phrases and their composi-\\ntionality. Proceedings of NeurIPS, 2013.\\n7. Tomas Mikolov, Martin Kara\\xef\\xac\\x81\\xc3\\xa1t, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recur-\\nrent neural network based language model. In Proceedings of InterSpeech, 2010.\\n334\\n11\\nOutlook\\n8. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\\nimage recognition. arXiv preprint arXiv:1409.1556, 2014.\\n9. Ashish Vaswani, Noam Shazeer, Niki Parmar, Llion Jones, Jakob Uszkoreit, Aidan N Gomez,\\nand Lukasz Kaiser. Attention is all you need. In Proceedings of NeurIPS, 2017.\\n10. Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Laurens van der Maaten. Sim-\\npleshot: Revisiting nearest-neighbor classi\\xef\\xac\\x81cation for few-shot learning. arXiv preprint\\narXiv:1911.04623, 2019.\\n11. Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. Transfer learning for low-resource\\nneural machine translation. In Proceedings of EMNLP, 2016.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nCorrection to: Representation Learning\\nfor Natural Language Processing\\nCorrection to:\\nZ. Liu et al., Representation Learning for Natural Language\\nProcessing, https://doi.org/10.1007/978-981-15-5573-2\\nIn the original version of the book, the following belated correction has been incor-\\nporated in both Preface and Chapter 8. In the Preface, a new reference citation has\\nbeen added in Chapter 8.\\nThe original version of Chapter 8 was revised with a new paragraph replacing the\\n\\xef\\xac\\x81rst paragraph in section 8.3.\\nThe chapter and the book have been updated with the changes.\\nThe updated version of this chapter can be found at\\nhttps://doi.org/10.1007/978-981-15-5573-2,\\nhttps://doi.org/10.1007/978-981-15-5573-2_8\\n\\xc2\\xa9 The Author(s) 2023\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_12\\nC1\\n\\x0c'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathname = '/home/ngoni97/Documents/Python Programming/Natural Language Processing/Representation Learning for Natural Language Processing.pdf'\n",
    "get_document(pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66a68543-9fbe-45c2-ae54-3c39637ca2b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Text Analytics  \\nwith Python\\nA Practical Real-World Approach to  \\nGaining Actionable Insights from  \\nYour Data\\n\\xe2\\x80\\x94\\nDipanjan Sarkar\\nText Analytics  \\nwith Python\\nA Practical Real-World  \\nApproach to Gaining Actionable \\nInsights from your Data\\nDipanjan Sarkar\\nText Analytics with Python: A Practical Real-World Approach to Gaining Actionable \\nInsights from Your Data\\nDipanjan Sarkar \\t\\n\\t\\n\\t\\n\\t\\nBangalore, Karnataka\\t\\n\\t\\n\\t\\nIndia\\nISBN-13 (pbk): 978-1-4842-2387-1\\t\\n\\t\\nISBN-13 (electronic): 978-1-4842-2388-8\\nDOI 10.1007/978-1-4842-2388-8\\nLibrary of Congress Control Number: 2016960760\\nCopyright \\xc2\\xa9 2016 by Dipanjan Sarkar\\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole \\nor part of the material is concerned, specifically the rights of translation, reprinting, reuse of \\nillustrations, recitation, broadcasting, reproduction on microfilms or in any other physical \\nway, and transmission or information storage and retrieval, electronic adaptation, computer \\nsoftware, or by similar or dissimilar methodology now known or hereafter developed.\\nTrademarked names, logos, and images may appear in this book. Rather than use a trademark \\nsymbol with every occurrence of a trademarked name, logo, or image we use the names, logos, \\nand images only in an editorial fashion and to the benefit of the trademark owner, with no \\nintention of infringement of the trademark.\\nThe use in this publication of trade names, trademarks, service marks, and similar terms, even \\nif they are not identified as such, is not to be taken as an expression of opinion as to whether or \\nnot they are subject to proprietary rights.\\nWhile the advice and information in this book are believed to be true and accurate at the \\ndate of publication, neither the authors nor the editors nor the publisher can accept any legal \\nresponsibility for any errors or omissions that may be made. The publisher makes no warranty, \\nexpress or implied, with respect to the material contained herein.\\nManaging Director: Welmoed Spahr\\nLead Editor: Mr. Sarkar\\nTechnical Reviewer: Shanky Sharma\\nEditorial Board: Steve Anglin, Pramila Balan, Laura Berendson, Aaron Black,  \\nLouise Corrigan, Jonathan Gennick, Robert Hutchinson, Celestin Suresh John,  \\nNikhil Karkal, James Markham, Susan McDermott, Matthew Moodie, Natalie Pao, \\nGwenan Spearing\\nCoordinating Editor: Sanchita Mandal\\nCopy Editor: Corbin Collins\\nCompositor: SPi Global\\nIndexer: SPi Global\\nArtist: SPi Global\\nDistributed to the book trade worldwide by Springer Science+Business Media New York,  \\n233 Spring Street, 6th Floor, New York, NY 10013. Phone 1-800-SPRINGER, fax (201) 348-4505, \\ne-mail orders-ny@springer-sbm.com, or visit www.springeronline.com. Apress Media, LLC is a \\nCalifornia LLC and the sole member (owner) is Springer Science + Business Media Finance Inc \\n(SSBM Finance Inc). SSBM Finance Inc is a Delaware corporation. \\nFor information on translations, please e-mail rights@apress.com, or visit www.apress.com. \\nApress and friends of ED books may be purchased in bulk for academic, corporate,  \\nor promotional use. eBook versions and licenses are also available for most titles.  \\nFor more information, reference our Special Bulk Sales\\xe2\\x80\\x93eBook Licensing web page at  \\nwww.apress.com/bulk-sales.\\nAny source code or other supplementary materials referenced by the author in this text are \\navailable to readers at www.apress.com. For detailed information about how to locate your \\nbook\\xe2\\x80\\x99s source code, go to www.apress.com/source-code/. Readers can also access source code \\nat SpringerLink in the Supplementary Material section for each chapter.\\nPrinted on acid-free paper\\nThis book is dedicated to my parents, partner, well-wishers,  \\nand especially to all the developers, practitioners, and  \\norganizations who have created a wonderful and thriving  \\necosystem around analytics and data science.\\nv\\nContents at a Glance\\nAbout the Author\\x08............................................................................. xv\\nAbout the Technical Reviewer\\x08....................................................... xvii\\nAcknowledgments\\x08.......................................................................... xix\\nIntroduction\\x08.................................................................................... xxi\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 1: Natural Language Basics\\x08.............................................. 1\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 2: Python Refresher\\x08........................................................ 51\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 3: Processing and Understanding Text\\x08.......................... 107\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 4: Text Classification\\x08..................................................... 167\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 5: Text Summarization\\x08.................................................. 217\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 6: Text Similarity and Clustering\\x08................................... 265\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 7: Semantic and Sentiment Analysis\\x08............................ 319\\nIndex\\x08.............................................................................................. 377\\nvii\\nContents\\nAbout the Author\\x08............................................................................. xv\\nAbout the Technical Reviewer\\x08....................................................... xvii\\nAcknowledgments\\x08.......................................................................... xix\\nIntroduction\\x08.................................................................................... xxi\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 1: Natural Language Basics\\x08.............................................. 1\\nNatural Language\\x08.................................................................................... 2\\nWhat Is Natural Language?\\x08......................................................................................2\\nThe Philosophy of Language\\x08.....................................................................................2\\nLanguage Acquisition and Usage\\x08..............................................................................5\\nLinguistics\\x08............................................................................................... 8\\nLanguage Syntax and Structure\\x08............................................................ 10\\nWords\\x08.....................................................................................................................11\\nPhrases\\x08...................................................................................................................12\\nClauses\\x08...................................................................................................................14\\nGrammar\\x08.................................................................................................................15\\nWord Order Typology\\x08...............................................................................................23\\nLanguage Semantics\\x08............................................................................. 25\\nLexical Semantic Relations\\x08....................................................................................25\\nSemantic Networks and Models\\x08.............................................................................28\\nRepresentation of Semantics\\x08.................................................................................29\\n\\xe2\\x96\\xa0 Contents\\nviii\\nText Corpora\\x08.......................................................................................... 37\\nCorpora Annotation and Utilities\\x08.............................................................................38\\nPopular Corpora\\x08......................................................................................................39\\nAccessing Text Corpora\\x08..........................................................................................40\\nNatural Language Processing\\x08............................................................... 46\\nMachine Translation\\x08................................................................................................46\\nSpeech Recognition Systems\\x08.................................................................................47\\nQuestion Answering Systems\\x08.................................................................................47\\nContextual Recognition and Resolution\\x08..................................................................48\\nText Summarization\\x08................................................................................................48\\nText Categorization\\x08.................................................................................................49\\nText Analytics\\x08........................................................................................ 49\\nSummary\\x08............................................................................................... 50\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 2: Python Refresher\\x08........................................................ 51\\nGetting to Know Python\\x08......................................................................... 51\\nThe Zen of Python\\x08...................................................................................................54\\nApplications: When Should You Use Python?\\x08..........................................................55\\nDrawbacks: When Should You Not Use Python?\\x08.....................................................58\\nPython Implementations and Versions\\x08...................................................................59\\nInstallation and Setup\\x08........................................................................... 60\\nWhich Python Version?\\x08...........................................................................................60\\nWhich Operating System?\\x08......................................................................................61\\nIntegrated Development Environments\\x08..................................................................61\\nEnvironment Setup\\x08.................................................................................................62\\nVirtual Environments\\x08..............................................................................................64\\nPython Syntax and Structure\\x08................................................................. 66\\n\\xe2\\x96\\xa0 Contents\\nix\\nData Structures and Types\\x08.................................................................... 69\\nNumeric Types\\x08........................................................................................................70\\nStrings\\x08....................................................................................................................72\\nLists\\x08........................................................................................................................73\\nSets\\x08........................................................................................................................74\\nDictionaries\\x08.............................................................................................................75\\nTuples\\x08.....................................................................................................................76\\nFiles\\x08........................................................................................................................77\\nMiscellaneous\\x08.........................................................................................................78\\nControlling Code Flow\\x08........................................................................... 78\\nConditional Constructs\\x08...........................................................................................79\\nLooping Constructs\\x08.................................................................................................80\\nHandling Exceptions\\x08...............................................................................................82\\nFunctional Programming\\x08....................................................................... 84\\nFunctions\\x08................................................................................................................84\\nRecursive Functions\\x08...............................................................................................85\\nAnonymous Functions\\x08............................................................................................86\\nIterators\\x08..................................................................................................................87\\nComprehensions\\x08.....................................................................................................88\\nGenerators\\x08..............................................................................................................90\\nThe itertools and functools Modules\\x08......................................................................91\\nClasses\\x08.................................................................................................. 91\\nWorking with Text\\x08.................................................................................. 94\\nString Literals\\x08.........................................................................................................94\\nString Operations and Methods\\x08..............................................................................96\\nText Analytics Frameworks\\x08................................................................. 104\\nSummary\\x08............................................................................................. 106\\n\\xe2\\x96\\xa0 Contents\\nx\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 3: Processing and Understanding Text\\x08.......................... 107\\nText Tokenization\\x08................................................................................. 108\\nSentence Tokenization\\x08..........................................................................................108\\nWord Tokenization\\x08................................................................................................112\\nText Normalization\\x08............................................................................... 115\\nCleaning Text\\x08........................................................................................................115\\nTokenizing Text\\x08.....................................................................................................116\\nRemoving Special Characters\\x08...............................................................................116\\nExpanding Contractions\\x08........................................................................................118\\nCase Conversions\\x08.................................................................................................119\\nRemoving Stopwords\\x08............................................................................................120\\nCorrecting Words\\x08..................................................................................................121\\nStemming\\x08.............................................................................................................128\\nLemmatization\\x08......................................................................................................131\\nUnderstanding Text Syntax and Structure\\x08........................................... 132\\nInstalling Necessary Dependencies\\x08......................................................................133\\nImportant Machine Learning Concepts\\x08.................................................................134\\nParts of Speech (POS) Tagging\\x08.............................................................................135\\nShallow Parsing\\x08....................................................................................................143\\nDependency-based Parsing\\x08..................................................................................153\\nConstituency-based Parsing\\x08.................................................................................158\\nSummary\\x08............................................................................................. 165\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 4: Text Classification\\x08..................................................... 167\\nWhat Is Text Classification?\\x08................................................................. 168\\nAutomated Text Classification\\x08............................................................. 170\\nText Classification Blueprint\\x08................................................................ 172\\nText Normalization\\x08............................................................................... 174\\nFeature Extraction\\x08............................................................................... 177\\n\\xe2\\x96\\xa0 Contents\\nxi\\nBag of Words Model\\x08..............................................................................................179\\nTF-IDF Model\\x08........................................................................................................181\\nAdvanced Word Vectorization Models\\x08...................................................................187\\nClassification Algorithms\\x08..................................................................... 193\\nMultinomial Na\\xc3\\xafve Bayes\\x08......................................................................................195\\nSupport Vector Machines\\x08......................................................................................197\\nEvaluating Classification Models\\x08......................................................... 199\\nBuilding a Multi-Class Classification System\\x08...................................... 204\\nApplications and Uses\\x08......................................................................... 214\\nSummary\\x08............................................................................................. 215\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 5: Text Summarization\\x08.................................................. 217\\nText Summarization and Information Extraction\\x08................................. 218\\nImportant Concepts\\x08............................................................................. 220\\nDocuments\\x08...........................................................................................................220\\nText Normalization\\x08................................................................................................220\\nFeature Extraction\\x08................................................................................................221\\nFeature Matrix\\x08......................................................................................................221\\nSingular Value Decomposition\\x08..............................................................................221\\nText Normalization\\x08............................................................................... 223\\nFeature Extraction\\x08............................................................................... 224\\nKeyphrase Extraction\\x08.......................................................................... 225\\nCollocations\\x08..........................................................................................................226\\nWeighted Tag\\xe2\\x80\\x93Based Phrase Extraction\\x08...............................................................230\\nTopic Modeling\\x08.................................................................................... 234\\nLatent Semantic Indexing\\x08.....................................................................................235\\nLatent Dirichlet Allocation\\x08.....................................................................................241\\nNon-negative Matrix Factorization\\x08.......................................................................245\\nExtracting Topics from Product Reviews\\x08..............................................................246\\n\\xe2\\x96\\xa0 Contents\\nxii\\nAutomated Document Summarization\\x08................................................ 250\\nLatent Semantic Analysis\\x08.....................................................................................253\\nTextRank\\x08...............................................................................................................256\\nSummarizing a Product Description\\x08.....................................................................261\\nSummary\\x08............................................................................................. 263\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 6: Text Similarity and Clustering\\x08................................... 265\\nImportant Concepts\\x08............................................................................. 266\\nInformation Retrieval (IR)\\x08......................................................................................266\\nFeature Engineering\\x08.............................................................................................267\\nSimilarity Measures\\x08..............................................................................................267\\nUnsupervised Machine Learning Algorithms\\x08........................................................268\\nText Normalization\\x08............................................................................... 268\\nFeature Extraction\\x08............................................................................... 270\\nText Similarity\\x08...................................................................................... 271\\nAnalyzing Term Similarity\\x08.................................................................... 271\\nHamming Distance\\x08...............................................................................................274\\nManhattan Distance\\x08.............................................................................................275\\nEuclidean Distance\\x08...............................................................................................277\\nLevenshtein Edit Distance\\x08....................................................................................278\\nCosine Distance and Similarity\\x08.............................................................................283\\nAnalyzing Document Similarity\\x08........................................................... 285\\nCosine Similarity\\x08...................................................................................................287\\nHellinger-Bhattacharya Distance\\x08..........................................................................289\\nOkapi BM25 Ranking\\x08............................................................................................292\\nDocument Clustering\\x08........................................................................... 296\\n\\xe2\\x96\\xa0 Contents\\nxiii\\nClustering Greatest Movies of All Time\\x08................................................ 299\\nK-means Clustering\\x08..............................................................................................301\\nAffinity Propagation\\x08..............................................................................................308\\nWard\\xe2\\x80\\x99s Agglomerative Hierarchical Clustering\\x08.....................................................313\\nSummary\\x08............................................................................................. 317\\n\\xe2\\x96\\xa0\\n\\xe2\\x96\\xa0Chapter 7: Semantic and Sentiment Analysis\\x08............................ 319\\nSemantic Analysis\\x08............................................................................... 320\\nExploring WordNet\\x08............................................................................... 321\\nUnderstanding Synsets\\x08.........................................................................................321\\nAnalyzing Lexical Semantic Relations\\x08..................................................................323\\nWord Sense Disambiguation\\x08............................................................... 330\\nNamed Entity Recognition\\x08................................................................... 332\\nAnalyzing Semantic Representations\\x08.................................................. 336\\nPropositional Logic\\x08...............................................................................................336\\nFirst Order Logic\\x08...................................................................................................338\\nSentiment Analysis\\x08.............................................................................. 342\\nSentiment Analysis of IMDb Movie Reviews\\x08....................................... 343\\nSetting Up Dependencies \\x08....................................................................................343\\nPreparing Datasets\\x08...............................................................................................347\\nSupervised Machine Learning Technique\\x08.............................................................348\\nUnsupervised Lexicon-based Techniques\\x08.............................................................352\\nComparing Model Performances\\x08..........................................................................374\\nSummary\\x08............................................................................................. 376\\nIndex\\x08.............................................................................................. 377\\nxv\\nAbout the Author\\nDipanjan Sarkar is a data scientist at Intel, the world\\xe2\\x80\\x99s \\nlargest silicon company, which is on a mission to make \\nthe world more connected and productive. He primarily \\nworks on analytics, business intelligence, application \\ndevelopment, and building large-scale intelligent \\nsystems. He received his master\\xe2\\x80\\x99s degree in information \\ntechnology from the International Institute of Information \\nTechnology, Bangalore, with a focus on data science and \\nsoftware engineering. He is also an avid supporter of \\nself-learning, especially through massive open online \\ncourses, and holds a data science specialization from \\nJohns Hopkins University on Coursera.\\nSarkar has been an analytics practitioner for over four years, specializing in statistical, \\npredictive, and text analytics. He has also authored a couple of books on R and machine \\nlearning, reviews technical books, and acts as a course beta tester for Coursera. \\nDipanjan\\xe2\\x80\\x99s interests include learning about new technology, financial markets, disruptive \\nstartups, data science, and more recently, artificial intelligence and deep learning. In his \\nspare time he loves reading, gaming, and watching popular sitcoms and football.\\nxvii\\nAbout the Technical \\nReviewer\\nShanky Sharma Currently leading the AI team at Nextremer India, Shanky Sharma\\xe2\\x80\\x99s work \\nentails implementing various AI and machine learning\\xe2\\x80\\x93related projects and working on \\ndeep learning for speech recognition in Indic languages. He hopes to grow and scale new \\nhorizons in AI and machine learning technologies. Statistics intrigue him and he loves \\nplaying with numbers, designing algorithms, and giving solutions to people. He sees \\nhimself as a solution provider rather than a scripter or another IT nerd who codes. He \\nloves heavy metal and trekking and giving back to society, which, he believes, is the task \\nof every engineer. He also loves teaching and helping people. He is a firm believer that we \\nlearn more by helping others learn.\\nxix\\nAcknowledgments\\nThis book would definitely not be a reality without the help and support from some \\nexcellent people in my life. I would like to thank my parents, Digbijoy and Sampa, \\nmy partner Durba, and my family and well-wishers for their constant support and \\nencouragement, which really motivates me and helps me strive to achieve more.\\nThis book is based on various experiences and lessons learned over time. For that I \\nwould like to thank my managers, Nagendra Venkatesh and Sanjeev Reddy, for believing \\nin me and giving me an excellent opportunity to tackle challenging problems and also \\ngrow personally. For the wealth of knowledge I gained in text analytics in my early days, \\nI would like to acknowledge Dr. Mandar Mutalikdesai and Dr. Sanket Patil for not only \\nbeing good managers but excellent mentors.\\nA special mention goes out to my colleagues Roopak Prajapat and Sailaja \\nParthasarathy for collaborating with me on various problems in text analytics. Thanks to \\nTamoghna Ghosh for being a great mentor and friend who keeps teaching me something \\nnew every day, and to my team, Raghav Bali, Tushar Sharma, Nitin Panwar, Ishan \\nKhurana, Ganesh Ghongane, and Karishma Chug, for making tough problems look easier \\nand more fun.\\nA lot of the content in this book would not have been possible without Christine Doig \\nCardet, Brandon Rose, and all the awesome people behind Python, Continuum Analytics, \\nNLTK, gensim, pattern, spaCy, scikit-learn, and many more excellent open source \\nframeworks and libraries out there that make our lives easier. Also to my friend Jyotiska, \\nthank you for introducing me to Python and for learning and collaborating with me on \\nvarious occasions that have helped me become what I am today.\\nLast, but never least, a big thank you to the entire team at Apress, especially \\nto Celestin Suresh John, Sanchita Mandal, and Laura Berendson for giving me this \\nwonderful opportunity to share my experience and what I\\xe2\\x80\\x99ve learned with the community \\nand for guiding me and working tirelessly behind the scenes to make great things happen!\\nxxi\\nIntroduction\\nI have been into mathematics and statistics since high school, when numbers began to \\nreally interest me. Analytics, data science, and more recently text analytics came much \\nlater, perhaps around four or five years ago when the hype about Big Data and Analytics \\nwas getting bigger and crazier. Personally I think a lot of it is over-hyped, but a lot of it is \\nalso exciting and presents huge possibilities with regard to new jobs, new discoveries, and \\nsolving problems that were previously deemed impossible to solve.\\nNatural Language Processing (NLP) has always caught my eye because the human \\nbrain and our cognitive abilities are really fascinating. The ability to communicate \\ninformation, complex thoughts, and emotions with such little effort is staggering once \\nyou think about trying to replicate that ability in machines. Of course, we are advancing \\nby leaps and bounds with regard to cognitive computing and artificial intelligence (AI), \\nbut we are not there yet. Passing the Turing Test is perhaps not enough; can a machine \\ntruly replicate a human in all aspects?\\nThe ability to extract useful information and actionable insights from heaps of \\nunstructured and raw textual data is in great demand today with regard to applications in \\nNLP and text analytics. In my journey so far, I have struggled with various problems, faced \\nmany challenges, and learned various lessons over time. This book contains a major \\nchunk of the knowledge I\\xe2\\x80\\x99ve gained in the world of text analytics, where building a fancy \\nword cloud from a bunch of text documents is not enough anymore.\\nPerhaps the biggest problem with regard to learning text analytics is not a lack of \\ninformation but too much information, often called information overload. There are \\nso many resources, documentation, papers, books, and journals containing so much \\ntheoretical material, concepts, techniques, and algorithms that they often overwhelm \\nsomeone new to the field. What is the right technique to solve a problem? How does \\ntext summarization really work? Which are the best frameworks to solve multi-class text \\ncategorization? By combining mathematical and theoretical concepts with practical \\nimplementations of real-world use-cases using Python, this book tries to address this \\nproblem and help readers avoid the pressing issues I\\xe2\\x80\\x99ve faced in my journey so far.\\nThis book follows a comprehensive and structured approach. First it tackles the \\nbasics of natural language understanding and Python constructs in the initial chapters. \\nOnce you\\xe2\\x80\\x99re familiar with the basics, it addresses interesting problems in text analytics \\nin each of the remaining chapters, including text classification, clustering, similarity \\nanalysis, text summarization, and topic models. In this book we will also analyze text \\nstructure, semantics, sentiment, and opinions. For each topic, I cover the basic concepts \\nand use some real-world scenarios and data to implement techniques covering each \\nconcept. The idea of this book is to give you a flavor of the vast landscape of text analytics \\nand NLP and arm you with the necessary tools, techniques, and knowledge to tackle your \\nown problems and start solving them. I hope you find this book helpful and wish you the \\nvery best in your journey through the world of text analytics!\\n1\\n\\xc2\\xa9 Dipanjan Sarkar 2016 \\nD. Sarkar, Text Analytics with Python, DOI 10.1007/978-1-4842-2388-8_1\\n CHAPTER 1  \\n Natural Language Basics \\n We have ushered in the age of Big Data where organizations and businesses are having \\ndifficulty managing all the data generated by various systems, processes, and transactions. \\nHowever, the term  Big Data is misused a lot due to the nature of its popular but vague \\ndefinition of \\xe2\\x80\\x9cthe 3 V\\xe2\\x80\\x99s\\xe2\\x80\\x9d\\xe2\\x80\\x94volume, variety, and velocity of data. This is because sometimes \\nit is very difficult to exactly quantify what data is \\xe2\\x80\\x9cBig.\\xe2\\x80\\x9d Some might think a billion records \\nin a  database would be Big Data, but that number seems really minute compared to the \\n petabytes of data being generated by various  sensors or even social media. There is a large \\nvolume of unstructured textual data present across all organizations, irrespective of their \\ndomain. Just to take some examples, we have vast amounts of data in the form of tweets, \\nstatus updates, comments, hashtags, articles, blogs, wikis, and much more on social \\nmedia. Even retail and e-commerce stores generate a lot of textual data from new product \\ninformation and metadata with customer reviews and feedback. \\n The main challenges associated with  textual data are twofold. The first challenge \\ndeals with effective storage and management of this data. Usually textual data is \\nunstructured and does not adhere to any specific predefined data model or schema, \\nwhich is usually followed by relational databases. However, based on the data semantics, \\nyou can store it in either SQL-based database management  systems ( DBMS ) like  SQL \\nServer or even NoSQL-based systems like MongoDB. Organizations having enormous \\namounts of textual datasets often resort to file-based systems like Hadoop where they \\ndump all the data in the Hadoop Distributed File System (HDFS) and access it as needed, \\nwhich is one of the main principles of a  data lake . \\n The second challenge is with regard to analyzing this  data and trying to extract \\nmeaningful patterns and useful insights that would be beneficial to the organization. \\nEven though we have a large number of machine learning and data analysis techniques \\nat our disposal, most of them are tuned to work with numerical data, hence we have \\nto resort to areas like  natural language processing ( NLP ) and specialized techniques, \\ntransformations, and algorithms to analyze text data, or more specifically  natural \\nlanguage , which is quite different from programming languages that are easily \\nunderstood by machines. Remember that textual data, being highly unstructured, does \\nnot follow or adhere to structured or regular syntax and patterns\\xe2\\x80\\x94hence we cannot \\ndirectly use mathematical or statistical models to analyze it. \\nElectronic supplementary material The online version of this chapter \\n(doi:  10.1007/978-1-4842-2388-8_1  ) contains supplementary material, which is available \\nto authorized users.\\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n2\\n Before we dive into specific  techniques and algorithms to analyze textual data, we will be \\ngoing over some of the main concepts and theoretical principles associated with the nature \\nof text data in this chapter. The primary intent here is to get you familiarized with concepts \\nand domains associated with  natural language understanding ,  processing , and  text analytics . \\nWe will be using the Python programming language in this book primarily for accessing and \\nanalyzing text data. The examples in this chapter will be pretty straightforward and fairly easy \\nto follow. However, you can quickly skim over Chapter   2  in case you want to brush up on \\nPython before going through this chapter. All the examples are available with this book and \\nalso in my GithHub repository at   https://github.com/dipanjanS/text-analytics-with-\\npython  which includes programs, code snippets and datasets. This chapter covers concepts \\nrelevant to natural language, linguistics, text data formats, syntax, semantics, and grammars \\nbefore moving on to more advanced topics like  text corpora , NLP, and text analytics. \\n Natural Language \\n Textual data is unstructured data but it usually belongs to a specific language following \\nspecific syntax and semantics. Any piece of text data\\xe2\\x80\\x94a simple word, sentence, or \\ndocument\\xe2\\x80\\x94relates back to some natural language most of the time. In this section, we \\nwill be looking at the definition of natural language, the philosophy of language, language \\nacquisition, and the usage of language. \\n What Is Natural Language? \\n To understand text analytics and natural language  processing  , we need to understand \\nwhat makes a language \\xe2\\x80\\x9cnatural.\\xe2\\x80\\x9d In simple terms, a  natural  language is one developed \\nand evolved by humans through natural use and  communication  , rather than \\nconstructed and created artificially, like a computer programming language. \\n Human languages like English, Japanese, and Sanskrit are natural languages. Natural \\nlanguages can be communicated in different forms, including speech, writing, or even signs. \\nThere has been a lot of scholarship and effort applied toward understanding the origins, \\nnature, and philosophy of language. We will discuss that briefly in the following section.  \\n The Philosophy of Language \\n We now know what a natural language means. But think about the following questions. \\nWhat are the origins of a  language ? What makes the English language \\xe2\\x80\\x9cEnglish\\xe2\\x80\\x9d? How did \\nthe meaning of the word  fruit  come into existence? How do humans communicate among \\nthemselves with language? These are definitely some heavy philosophical questions. \\n The  philosophy  of language mainly deals with the following four problems and seeks \\nanswers to solve them:\\n\\xe2\\x80\\xa2 \\n The nature of meaning in a language \\n\\xe2\\x80\\xa2 \\n The use of language \\n\\xe2\\x80\\xa2 \\n Language cognition \\n\\xe2\\x80\\xa2 \\n The relationship between language and reality  \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n3\\n\\xe2\\x80\\xa2 \\n The nature of meaning in a language is concerned with the \\nsemantics of a language and the nature of meaning itself. Here, \\nphilosophers of language or linguistics try to find out what it \\nmeans to actually \\xe2\\x80\\x9cmean\\xe2\\x80\\x9d anything\\xe2\\x80\\x94that is, how the meaning of \\nany word or sentence originated and came into being and how \\ndifferent words in a language can be synonyms of each other and \\nform relations. Another thing of importance here is how structure \\nand syntax in the language pave the way for semantics, or to be \\nmore specific, how words, which have their own meanings, are \\nstructured together to form meaningful sentences.  Linguistics \\nis the scientific study of language, a special field that deals with \\nsome of these problems we will be looking at in more detail later \\non. Syntax, semantics, grammars, and parse trees are some ways \\nto solve these problems. The nature of meaning can be expressed \\nin linguistics between two human beings, notably a sender and \\na receiver, as what the sender tries to express or communicate \\nwhen they send a message to a receiver, and what the receiver \\nends up understanding or deducing from the context of the \\nreceived message. Also from a non-linguistic standpoint, things \\nlike body language, prior experiences, and psychological effects \\nare contributors to meaning of language, where each human \\nbeing perceives or infers meaning in their own way, taking into \\naccount some of these factors. \\n\\xe2\\x80\\xa2 \\n The use of  language    is more concerned with how language is used \\nas an entity in various scenarios and communication between \\nhuman beings. This includes analyzing speech and the usage of \\nlanguage when speaking, including the speaker\\xe2\\x80\\x99s intent, tone, \\ncontent and actions involved in expressing a message. This is often \\ntermed as a  speech act in linguistics. More advanced concepts such \\nas the origins of language creation and human cognitive activities \\nsuch as language acquisition which is responsible for learning and \\nusage of languages are also of prime interest.  \\n\\xe2\\x80\\xa2 \\n Language cognition specifically focuses on how the cognitive \\nfunctions of the human brain are responsible for understanding \\nand interpreting language. Considering the example of a typical \\nsender and receiver, there are many actions involved from \\nmessage communication to interpretation. Cognition tries to find \\nout how the mind works in combining and relating specific words \\ninto sentences and then into a meaningful message and what is \\nthe relation of language to the thought process of the sender and \\nreceiver when they use the language to communicate messages.  \\n\\xe2\\x80\\xa2 \\n The relationship between  language  and reality explores the \\nextent of truth of expressions originating from language. Usually, \\nphilosophers of language try to measure how factual these \\nexpressions are and how they relate to certain affairs in our world \\nwhich are true. This relationship can be expressed in several ways, \\nand we will explore some of them. \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n4\\n One of the most popular models is the  triangle of reference , which is used to explain \\nhow words convey meaning and ideas in the minds of the receiver and how that meaning \\nrelates back to a real world entity or fact. The triangle of reference was proposed by \\nCharles Ogden and Ivor Richards in their book,  The Meaning of Meaning , first published \\nin 1923, and is denoted in Figure\\xc2\\xa0 1-1 . \\n The  triangle of reference model is also known as the  meaning of meaning model, \\nand I have depicted the same in Figure1-1 with a real example of a  couch being perceived \\nby a person which is present in front of him. A  symbol is denoted as a linguistic symbol, \\nlike a word or an object that evokes thought in a person\\xe2\\x80\\x99s mind. In this case, the  symbol \\nis the couch, and this evokes thoughts like  what is a couch, a piece of furniture that can \\nbe used for sitting on or lying down and relaxing, something that gives us comfort . These \\nthoughts are known as a  reference and through this reference the person is able to relate it \\nto something that exists in the real world, termed a  referent. In this case the referent is the \\ncouch which the person perceives to be present in front of him. \\n The second way to find out relationships between language and reality is known as \\nthe  direction of fit , and we will talk about two main directions here. The  word-to-world \\ndirection of fit talks about instances where the usage of language can reflect reality. This \\nindicates using words to match or relate to something that is happening or has already \\nhappened in the real world. An example would be the sentence  The Eiffel Tower is really \\nbig, which accentuates a fact in reality. The other direction of fit, known as  world-to-word , \\ntalks about instances where the usage of language can change reality. An example here \\nwould be the sentence  I am going to take a swim , where the person  I  is changing reality \\nby going to take a swim by representing the same in the sentence being communicated. \\nFigure\\xc2\\xa0 1-2 shows the relationship between both the directions of fits. \\n Figure 1-1.  The triangle of reference model \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n5\\n It is quite clear from the preceding depiction that based on the referent that is \\nperceived from the real world, a person can form a representation in the form of a symbol \\nor word and consequently can communicate the same to another person, which forms a \\nrepresentation of the real world based on the received symbol, thus forming a cycle. \\n Language Acquisition  and Usage \\n By now, we have seen what natural languages mean and the concepts behind language, \\nits nature, meaning, and use. In this section, we will talk in further detail about how \\nlanguage is perceived, understood, and learned using cognitive abilities by humans, and \\nfinally we will end our discussion with the main forms of language usage, discussed in \\nbrief as  speech acts . It is important to not only understand what natural language denotes \\nbut also how humans interpret, learn, and use the same language so that we are able to \\nemulate some of these concepts programmatically in our algorithms and techniques \\nwhen we try to extract insights from textual data. \\n Language Acquisition  and Cognitive Learning \\n Language acquisition is defined as the process by which human beings utilize their \\ncognitive abilities, knowledge, and experience to understand language based on \\nhearing and perception and start using it in terms of words, phrases, and sentences to \\ncommunicate with other human beings. In simple terms, the ability of acquiring and \\nproducing languages is language acquisition. \\n Figure 1-2.  The direction of fit  representation \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n6\\n The history of language  acquisition dates back centuries. Philosophers and scholars \\nhave tried to reason and understand the origins of language acquisition and came up \\nwith several theories, such as language being a god-gifted ability that is passed down \\nfrom generation to generation. Plato indicated that a form of word-meaning mapping \\nwould have been responsible in language acquisition. Modern theories have been \\nproposed by various scholars and philosophers, and some of the popular ones, most \\nnotably B.S. Skinner, indicated that knowledge, learning, and use of language were \\nmore of a behavioral consequent. Human beings, or to be more specific, children, when \\nusing specific words or symbols of any language, experience language based on certain \\nstimuli which get reinforced in their memory thanks to consequent reactions to their \\nusage repeatedly. This theory is based on  operant or  instrumentation conditioning , \\nwhich is a type of conditional learning where the strength of a particular behavior or \\naction is modified based on its consequences such as reward or punishment, and these \\nconsequent stimuli help in reinforcing or controlling behavior and learning. An example \\nwould be that children would learn that a specific combination of sounds made up a word \\nfrom repeated usage of it by their parents or by being rewarded by appreciation when \\nthey speak it correctly or by being corrected when they make a mistake while speaking \\nthe same. This repeated conditioning would end up reinforcing the actual meaning and \\nunderstanding of the word in a child\\xe2\\x80\\x99s memory for the future. To sum it up, children try to \\nlearn and use language mostly behaviorally by imitating and hearing from adults. \\n However, this behavioral theory was challenged by renowned linguist Noam \\nChomsky, who proclaimed that it would be impossible for children to learn language just \\nby imitating everything from adults. This hypothesis does stand valid in the following \\nexamples. Although words like  go and  give are valid, children often end up using an \\ninvalid form of the word, like  goed or  gived instead of  went or  gave in the past tense. \\nIt is assured that their parents didn\\xe2\\x80\\x99t utter these words in front of them, so it would be \\nimpossible to pick these up based on the previous theory of Skinner. Consequently, \\nChomsky proposed that children must not only be imitating words they hear but also \\nextracting patterns, syntax, and rules from the same language constructs, which is \\nseparate from just utilizing generic cognitive abilities based on behavior. \\n Considering Chomsky\\xe2\\x80\\x99s view, cognitive  abilities along with language-specific \\nknowledge and abilities like syntax, semantics, concepts of parts of speech, and grammar \\ntogether form what he termed a  language acquisition device that enabled humans to \\nhave the ability of  language acquisition . Besides cognitive abilities, what is unique \\nand important in language learning is the syntax of the language itself, which can be \\nemphasized in his famous sentence  Colorless green ideas sleep furiously . If you observe \\nthe sentence and repeat it many times, it does not make sense.  Colorless  cannot be \\nassociated with green, and neither can ideas be associated with green, nor can they sleep \\nfuriously. However, the sentence has a grammatically correct syntax. This is precisely \\nwhat Chomsky tried to explain\\xe2\\x80\\x94that syntax and grammar depict information that is \\nindependent from the meaning and semantics of words. Hence, he proposed that the \\nlearning and identifying of language syntax is a separate human capability compared \\nto other cognitive abilities. This proposed hypothesis is also known as the  autonomy \\nof syntax . These theories are still widely debated among scholars and linguists, but it is \\nuseful to explore how the human mind tends to acquire and learn language. We will now \\nlook at the typical patterns in which language is generally  used . \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n7\\n Language  Usage \\n The previous section talked about speech acts and how the direction of fit model \\nis used for relating words and symbols to reality. In this section we will cover some \\nconcepts related to speech acts that highlight different ways in which language is used in \\ncommunication. \\n There are three main categories of speech acts:  locutionary ,  illocutionary , and \\n perlocutionary acts.  Locutionary acts are mainly concerned with the actual delivery \\nof the sentence when communicated from one human being to another by speaking \\nit.  Illocutionary acts focus further on the actual semantics and significance of the \\nsentence which was communicated.  Perlocutionary acts refer to the actual effect the \\ncommunication had on its receiver, which is more psychological or behavioral. \\n A simple example would be the phrase  Get me the book from the table  spoken by a \\nfather to his child. The phrase when spoken by the father forms the locutionary act. This \\nsignificance of this sentence is a directive, which directs the child to get the book from the \\ntable and forms an illocutionary act. The action the child takes after hearing this, that is, if \\nhe brings the book from the table to his father, forms the perlocutionary act. \\n The illocutionary act was a directive in this case. According to the philosopher John \\nSearle, there are a total of five different classes of illocutionary speech acts, as follows:\\n\\xe2\\x80\\xa2 \\n Assertives are speech acts that communicate how things are already \\nexistent in the world. They are spoken by the sender when he tries \\nto assert a proposition that could be true or false in the real world. \\nThese assertions could be statements or declarations. A simple \\nexample would be  The Earth revolves round the Sun . These messages \\nrepresent the word-to-world direction of fit discussed earlier.  \\n\\xe2\\x80\\xa2 \\n Directives are speech acts that the sender communicates to the \\nreceiver asking or directing them to do something. This represents \\na voluntary act which the receiver might do in the future after \\nreceiving a directive from the sender. Directives can either be \\ncomplied with or not complied with, since they are voluntary. These \\ndirectives could be simple requests or even orders or commands. \\nAn example directive would be  Get me the book from the table , \\ndiscussed earlier when we talked about types of speech acts.  \\n\\xe2\\x80\\xa2 \\n Commisives are speech acts that commit the sender or speaker \\nwho utters them to some future voluntary act or action. Acts like \\npromises, oaths, pledges, and vows represent commisives, and \\nthe direction of fit could be either way. An example commisive \\nwould be  I promise to be there tomorrow for the ceremony . \\n\\xe2\\x80\\xa2 \\n Expressives reveal a speaker or sender\\xe2\\x80\\x99s disposition and outlook \\ntoward a particular proposition communicated through the \\nmessage. These can be various forms of expression or emotion, \\nsuch as congratulatory, sarcastic, and so on. An example \\nexpressive would be  Congratulations on graduating top of the class .  \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n8\\n\\xe2\\x80\\xa2 \\n Declarations are powerful speech acts that have the capability \\nto change the reality based on the declared proposition in the \\nmessage communicated by the speaker\\\\sender. The usual direction \\nof fit is world-to-word, but it can go the other way also. An example \\ndeclaration would be  I hereby declare him to be guilty of all charges .  \\n These speech acts are the primary ways in which language is used and \\ncommunicated among human beings, and without even realizing it, you end up using \\nhundreds of them on any given day. We will now look at linguistics and some of the main \\nareas of research associated with it. \\n Linguistics \\n We have touched on what natural language means, how language is learned and used, \\nand the origins of language acquisition. These kinds of things are formally researched \\nand studied in linguistics by researchers and scholars called  linguists . Formally,  linguistics  \\nis  defined as the scientific study of language, including form and syntax of language, \\nmeaning, and semantics depicted by the usage of language and context of use. The origins \\nof linguistics can be dated back to the 4th century BCE, when Indian scholar and linguist \\nPanini formalized the Sanskrit language description. The  term  linguistics  was first defined \\nto indicate the scientific study of languages in 1847, approximately before which the term \\n philology was used to indicate the same. Although a detailed exploration of linguistics is \\nnot needed for text analytics, it is useful to know the different areas of linguistics because \\nsome of them are used extensively in natural language processing and text analytics \\nalgorithms. The main distinctive areas of study under linguistics are as follows:\\n\\xe2\\x80\\xa2 \\n Phonetics  : This is the study of the acoustic properties of sounds \\nproduced by the human vocal tract during speech. It includes \\nstudying the properties of sounds as well as how they are created \\nand by human beings. The smallest individual unit of human \\nspeech in a specific language is called a  phoneme. A more generic \\nterm across languages for this unit of speech is  phone . \\n\\xe2\\x80\\xa2 \\n Phonology   : This is the study of sound patterns as interpreted in \\nthe human mind and used for distinguishing between different \\nphonemes to find out which ones are significant. The structure, \\ncombination, and interpretations of phonemes are studied in \\ndetail, usually by taking into account a specific language at a \\ntime. The English language consists of around 45 phonemes. \\nPhonology usually extends beyond just studying phonemes and \\nincludes things like accents, tone, and syllable structures.  \\n\\xe2\\x80\\xa2 \\n Syntax : This is usually the study of sentences, phrases, words, and \\ntheir structures. It includes researching how words are combined \\ntogether grammatically to form phrases and sentences. Syntactic \\norder of words used in a phrase or a sentence matter because the \\norder can change the meaning entirely. \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n9\\n\\xe2\\x80\\xa2 \\n Semantics : This involves the study of meaning in language \\nand can be further subdivided into lexical and compositional \\nsemantics.\\n\\xe2\\x80\\xa2 \\n Lexical semantics : The study of the meanings of words and \\nsymbols using morphology and syntax. \\n\\xe2\\x80\\xa2 \\n Compositional semantics : Studying relationships among \\nwords and combination of words and understanding the \\nmeanings of phrases and sentences and how they are related.  \\n\\xe2\\x80\\xa2 \\n Morphology : A  morpheme is the smallest unit of language that \\nhas distinctive meaning. This includes things like words, prefixes, \\nsuffixes, and so on which have their own distinct meanings. \\nMorphology is the study of the structure and meaning of these \\ndistinctive units or morphemes in a language. Specific rules and \\nsyntaxes usually govern the way morphemes can combine together.  \\n\\xe2\\x80\\xa2 \\n Lexicon : This is the study of properties of words and phrases \\nused in a language and how they build the vocabulary of the \\nlanguage. These include what kinds of sounds are associated with \\nmeanings for words, the parts of speech words belong to, and \\ntheir morphological forms. \\n\\xe2\\x80\\xa2 \\n Pragmatics : This is the study of how both linguistic and non-\\nlinguistic factors like context and scenario might affect the \\nmeaning of an expression of a message or an utterance. This \\nincludes trying to infer whether there are any hidden or indirect \\nmeanings in the communication. \\n\\xe2\\x80\\xa2 \\n Discourse analysis : This analyzes language and exchange of \\ninformation in the form of sentences across conversations among \\nhuman beings. These conversations could be spoken, written, or \\neven signed. \\n\\xe2\\x80\\xa2 \\n Stylistics : This is the study of language with a focus on the style of \\nwriting, including the tone, accent, dialogue, grammar, and type \\nof voice. \\n\\xe2\\x80\\xa2 \\n Semiotics : This is the study of signs, symbols, and sign processes \\nand how they communicate meaning. Things like analogy, \\nmetaphors, and symbolism are covered in this area.    \\n Although these are the main areas of study and research, linguistics is an enormous field \\nwith a much bigger scope than what is mentioned here. However, things like language syntax \\nand semantics are some of the most important concepts that often form the foundations to \\nnatural language processing. The following section looks at them more closely.  \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n10\\n Language  Syntax and Structure \\n We already know what language, syntax, and structure indicate. Syntax and structure \\nusually go hand in hand, where a set of specific rules, conventions, and  principles usually \\ngovern the way words are combined into phrases, phrases get combines into clauses, and \\nclauses get combined into sentences. We will be talking specifically about the  English   \\nlanguage syntax and structure in this section because in this book we will be dealing \\nwith textual data that belongs to the  English  language. But a lot of these concepts can be \\nextended to other languages too. Knowledge about the structure and syntax of language is \\nhelpful in many areas like text processing, annotation, and parsing for further operations \\nsuch as text classification or summarization. \\n In English, words usually combine together to form other  constituent units . These \\nconstituents include words, phrases, clauses, and sentences. All these constituents \\nexist together in any message and are related to each other in a hierarchical structure. \\nMoreover, a sentence is a structured format of representing a collection of words provided \\nthey follow certain syntactic rules like grammar. Look at the bunch of words represented \\nin Figure\\xc2\\xa0 1-3 . \\n From the collection of words in Figure\\xc2\\xa0 1-3 , it is very difficult to ascertain what it \\nmight be trying to convey or mean. Indeed, languages are not just comprised of groups of \\nunstructured words. Sentences with proper syntax not only help us give proper structure \\nand relate words together but also help them convey meaning based on the order or \\nposition of the words. Considering our previous hierarchy of sentence \\xe2\\x86\\x92 clause \\xe2\\x86\\x92 phrase \\n\\xe2\\x86\\x92 word, we can construct the hierarchical sentence tree in Figure\\xc2\\xa0 1-4  using  shallow \\nparsing , a technique using for finding out the constituents in a sentence. \\n Figure 1-3.  A  collection of words without any relation or structure \\n Figure 1-4.  Structured  sentence following the hierarchical syntax \\n \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n11\\n From the  hierarchical tree in Figure\\xc2\\xa0 1-4 , we get the sentence  The brown fox is quick \\nand he is jumping over the lazy dog . We can see that the leaf nodes of the tree consist of \\nwords, which are the smallest unit here, and combinations of words form phrases, which \\nin turn form  clauses. Clauses are connected together through various filler terms or words \\nsuch as conjunctions and form the final sentence. In the next section, we will look at each \\nof these constituents in further detail and understand how to analyze them and find out \\nwhat the major syntactic categories are. \\n Words \\n Words  are the smallest units in a language that are independent and have a  meaning of \\ntheir own. Although  morphemes are the smallest distinctive units, morphemes are not \\nindependent like words, and a word can be comprised of several morphemes. It is useful \\nto annotate and tag words and analyze them into their parts of speech (POS) to see the \\nmajor syntactic categories. Here, we will cover the main categories and significance of the \\nvarious POS tags. Later in Chapter   3 we will examining them in further detail and looking \\nat methods of generating POS tags programmatically. \\n Usually, words can fall into one of the following major categories.\\n\\xe2\\x80\\xa2 \\n N(oun) : This usually denotes words that depict some object or \\nentity which may be living or nonliving. Some examples would be \\n fox ,  dog ,  book , and so on. The POS tag symbol for nouns is  N . \\n\\xe2\\x80\\xa2 \\n V(erb) :  Verbs   are words that are used to describe certain actions, \\nstates, or occurrences. There are a wide variety of further \\nsubcategories, such as auxiliary, reflexive, and transitive verbs (and \\nmany more). Some typical examples of verbs would be  running , \\n jumping ,  read , and  write . The POS tag symbol for verbs is  V .  \\n\\xe2\\x80\\xa2 \\n Adj(ective) : Adjectives are words used to describe or qualify other \\nwords, typically nouns and noun phrases. The phrase  beautiful \\nflower has the noun (N)  flower which is described or qualified \\nusing the adjective (ADJ)  beautiful . The POS tag symbol for \\nadjectives is  ADJ . \\n\\xe2\\x80\\xa2 \\n Adv(erb) :  Adverbs usually act as modifiers for other words \\nincluding nouns, adjectives, verbs, or other adverbs. The phrase \\n very beautiful flower has the adverb (ADV)  very , which modifies \\nthe adjective (ADJ)  beautiful , indicating the degree to which the \\nflower is beautiful. The POS tag symbol for adverbs is  ADV . \\n Besides these four major categories of  parts of speech  , there are other categories \\nthat occur frequently in the English language. These include pronouns, prepositions, \\ninterjections, conjunctions, determiners, and many others. Furthermore, each POS tag \\nlike the noun (N) can be further subdivided into categories like  singular nouns   (NN), \\n singular proper nouns (NNP), and  plural nouns (NNS). We will be looking at POS tags in \\nfurther detail in Chapter   3 when we process and parse textual data and implement POS \\ntaggers to annotate text. \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n12\\n Considering our previous example sentence ( The brown fox is quick and he is \\njumping over the lazy dog ) where we built the hierarchical syntax tree, if we were to \\nannotate it using basic POS tags, it would look like Figure\\xc2\\xa0 1-5 . \\n In Figure\\xc2\\xa0 1-5 you may notice a few unfamiliar tags. The tag DET stands for \\n determiner , which is used to depict articles like  a ,  an ,  the , and so on. The tag CONJ \\nindicates  conjunction , which is usually used to bind together clauses to form sentences. \\nThe  PRON tag  stands for  pronoun , which represents words that are used to represent or \\ntake the place of a noun. \\n The tags N, V, ADJ and ADV are typical open classes and represent words belonging \\nto an open vocabulary.  Open classes  are word classes that consist of an infinite set of words \\nand commonly accept the addition of new words to the vocabulary which are invented \\nby people. Words are usually added to open classes through processes like  morphological \\nderivation , invention based on usage, and creating  compound lexemes . Some popular \\nnouns added fairly recently include  Internet  and  multimedia. Closed classes  consist of a \\nclosed and finite set of words and do not accept new additions.  Pronouns are a closed class. \\n The following section looks at the next level of the hierarchy: phrases.  \\n Phrases \\n Words have their own lexical properties like parts of speech, which we saw earlier. Using \\nthese words, we can order them in ways that give meaning to the words such that each \\nword belongs to a corresponding phrasal  category and one of the words is the main or head \\nword. In the hierarchy tree, groups of words make up  phrases , which form the third level \\nin the syntax tree. By  principle  , phrases are assumed to have at least two or more words, \\nconsidering the pecking order of words \\xe2\\x86\\x90 phrases \\xe2\\x86\\x90 clauses \\xe2\\x86\\x90 sentences. However, a \\nphrase  can be a single word or a combination of words based on the syntax and position \\nof the phrase in a clause or sentence. For example, the sentence  Dessert was good  has only \\nthree words, and each of them rolls up to three phrases. The word  dessert is a noun as well \\nas a  noun phrase ,  is depicts a verb as well as a  verb phrase , and  good represents an adjective \\nas well as an  adjective phrase  describing the aforementioned dessert. \\n There are five major  categories of phrases:\\n\\xe2\\x80\\xa2 \\n Noun phrase (NP) : These are phrases where a noun acts as \\nthe head word. Noun phrases act as a subject or object to a \\nverb. Usually a noun phrases can be a set of words that can be \\nreplaced by a pronoun without rendering the sentence or clause \\nsyntactically incorrect. Some examples would be  dessert ,  the lazy \\ndog , and  the brown fox . \\n Figure 1-5.  Annotated words with their POS tags \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n13\\n\\xe2\\x80\\xa2 \\n Verb phrase (VP) : These phrases are lexical units that have a \\nverb acting as the head word. Usually there are two forms of verb \\nphrases. One form has the verb components as well as other \\nentities such as nouns, adjectives, or adverbs as parts of the \\nobject. The verb here is known as a  finite verb . It acts as a single \\nunit in the hierarchy tree and can function as the root in a clause. \\nThis form is prominent in  constituency grammars . The other form \\nis where the finite verb acts as the root of the entire clause and \\nis prominent in  dependency grammars . Another derivation of \\nthis includes verb phrases strictly consisting of verb components \\nincluding main, auxiliary, infinitive, and participles. The sentence \\n He has started the engine can be used to illustrate the two types of \\nverb phrases that can be formed. They would be  has started the \\nengine and  has started , based on the two forms just discussed. \\n\\xe2\\x80\\xa2 \\n Adjective phrase (ADJP) : These are phrases with an adjective as \\nthe head word. Their main role is to describe or qualify nouns \\nand pronouns in a sentence, and they will be either placed before \\nor after the noun or pronoun. The sentence  The cat is too quick \\nhas an adjective phrase,  too quick , qualifying  cat , which is a noun \\nphrase. \\n\\xe2\\x80\\xa2 \\n Adverb phrase (ADVP) : These phrases act like adverbs since \\nthe adverb acts as the head word in the phrase. Adverb phrases \\nare used as modifiers for nouns, verbs, or adverbs themselves \\nby providing further details that describe or qualify them. In \\nthe sentence  The train should be at the station pretty soon , the \\nadjective phrase  pretty soon describes when the train would be \\narriving. \\n\\xe2\\x80\\xa2 \\n Prepositional phrase (PP) : These phrases usually contain a \\npreposition as the head word and other lexical components like \\nnouns, pronouns, and so on. It acts like an adjective or adverb \\ndescribing other words or phrases. The phrase  going up the stairs \\ncontains a prepositional phrase  up , describing the direction of the \\nstairs. \\n These five major syntactic categories of phrases can be generated from words using \\nseveral rules, some of which have been discussed, like utilizing syntax and grammars \\nof different types. We will be exploring some of the popular grammars in a later section. \\n Shallow parsing  is a popular natural language processing technique to extract these \\nconstituents, including POS tags as well as phrases from a sentence. For our sentence  The \\nbrown fox is quick and he is jumping over the lazy dog , we have obtained seven phrases \\nfrom shallow parsing, as shown in Figure\\xc2\\xa0 1-6 . \\n Figure 1-6.  Annotated phrases with their tags \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n14\\n The phrase tags fall into the categories discussed earlier, although the word  and is a \\nconjunction and is usually used to combine clauses together. In the next section, we will \\nbe looking at clauses, their main categories, and some conventions and syntactic rules for \\nextracting clauses from sentences. \\n Clauses \\n By nature, clauses can act as  independent sentences , or several clauses can be combined \\ntogether to form a sentence. A  clause is a group of words with some  relation between \\nthem that usually contains a subject and a predicate. Sometimes the subject is not \\npresent, and the predicate usually has a verb phrase or a verb with an object. By default \\nyou can classify clauses into two distinct  categories : the  main clause and the  subordinate \\nclause . The main clause is also known as an  independent clause because it can form a \\nsentence by itself and act as both sentence and clause. The subordinate or  dependent \\nclause cannot exist just by itself and depends on the main clause for its meaning. They \\nare usually joined with other clauses using dependent words such as subordinating \\nconjunctions. \\n With regard to syntactic properties of language, clauses can be subdivided into \\nseveral categories based on syntax:\\n\\xe2\\x80\\xa2 \\n Declarative : These clauses usually occur quite frequently and \\ndenote statements that do not have any specific tone associated \\nwith them. These are just standard statements, which are declared \\nwith a neutral tone and which could be factual or non-factual. An \\nexample would be  Grass is green . \\n\\xe2\\x80\\xa2 \\n Imperative : These clauses are usually in the form of a request, \\ncommand, rule, or advice. The tone in this case would be a \\nperson issuing an order to one or more people to carry out an \\norder, request, or instruction. An example would be  Please do not \\ntalk in class . \\n\\xe2\\x80\\xa2 \\n Relative : The simplest interpretation of  relative clauses is that they \\nare subordinate clauses and hence dependent on another part \\nof the sentence that usually contains a word, phrase, or even a \\nclause. This element usually acts as the antecedent to one of the \\nwords from the relative clause and relates to it. A simple example \\nwould be  John just mentioned that he wanted a soda , having \\nthe antecedent proper noun  John , which was referred to in the \\nrelative clause  he wanted a soda . \\n\\xe2\\x80\\xa2 \\n Interrogative : These clauses usually are in the form of questions. \\nThe type of these questions can be either affirmative or negative. \\nSome examples would be  Did you get my mail? and  Didn\\xe2\\x80\\x99t you go \\nto school? \\n\\xe2\\x80\\xa2 \\n Exclamative : These clauses are used to express shock, surprise, \\nor even compliments. These expressions fall under  exclamations , \\nand these clauses often end with an exclamation mark. An \\nexample would be  What an amazing race! \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n15\\n Usually most clauses are expressed in one of the previously mentioned syntactic \\nforms, though this list of clause categories is not an exhaustive list and can be further \\ncategorized into several other forms. Considering our example sentence  The brown \\nfox is quick and he is jumping over the lazy dog , if you remember the syntax tree, the \\ncoordinating conjunction  and divides the sentence into two clauses:  The brown fox is \\nquick and  he is jumping over the lazy dog. Can you guess what categories they might fall \\ninto? (Hint: Look back at the definitions of declarative and relative clauses). \\n Grammar \\n Grammar helps in enabling both syntax and structure in  language . It primarily consists of a set \\nof rules used in determining how to position words, phrases, and clauses when constructing \\nsentences for any natural language. Grammar is not restricted to the written word\\xe2\\x80\\x94it also \\noperates verbally.  Rules of grammar can be specific to a region, language, or dialect or be \\nsomewhat universal like the Subject-Verb-Object (SVO) model. Origins of grammar have a \\nrich history, starting with Sanskrit in India. In the West, the study of grammar originated with \\nthe Greeks, and the earliest work was the  Art of Grammar , written by Dionysius Thrax. Latin \\ngrammar  models were developed from the Greek models, and gradually across several ages, \\ngrammars for various languages were created. It was only in the 18th century that grammar \\nwas considered as a serious candidate to be a field under linguistics. \\n Grammars have evolved over the  course of time  , leading to the birth of newer types \\nof grammars, and various older grammars slowly lost prominence. Hence grammar is \\nnot just a fixed set of rules but also its evolution based on the usage of language over the \\ncourse of time among humans. In English, there are several ways in which grammars can \\nbe  classified  . We will first talk about two broad classes, into which most of the popular \\ngrammatical frameworks can be grouped. Then we will further explore how these \\ngrammars represent language. \\n Grammar can be subdivided into two main classes\\xe2\\x80\\x94dependency grammars and \\nconstituency grammars\\xe2\\x80\\x94based on their representations for linguistic syntax and structure. \\n Dependency grammars \\n These grammars do not focus on constituents like words, phrases, and clauses but place \\nmore emphasis on words. These grammars are also known as  word-based  grammars. To \\nunderstand dependency grammars, we should first know what  dependency  means in this \\ncontext.  Dependencies in this context are labeled word-word relations or links that are \\nusually asymmetrical. A word has a relation or depends on another word based on the \\npositioning of the words in the sentence. Consequently, dependency grammars assume \\nthat further constituents of phrases and clauses are derived from this dependency \\nstructure between words. \\n The basic principle behind a dependency grammar is that in any sentence in the \\nlanguage, all the words except one word has some relationship or dependency on other \\nwords in the sentence. The word that has no dependency is called the  root of the sentence. \\nThe verb is taken as the root of the sentence in most cases. All the other words are directly \\nor indirectly linked to the root verb using  links , which are the dependencies. Although there \\nare no concepts of phrases or clauses, looking at the syntax and relations between words \\nand their dependents, one can determine the necessary constituents in the sentence. \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n16\\n Dependency  grammars always have a one-to-one relationship correspondence for \\neach word in the sentence. There are two aspects to this grammar representation. One \\nis the syntax or structure of the sentence, and the other is the semantics obtained from \\nthe relationships denoted between the words. The syntax or structure of the words and \\ntheir interconnections can be shown using a sentence syntax or parse tree similar to that \\ndepicted in an earlier section. Considering our sentence  The brown fox is quick and he is \\njumping over the lazy dog , if we wanted to draw the dependency syntax tree for this, we \\nwould have the structure denoted in Figure\\xc2\\xa0 1-7 . \\n Figure  1-7 shows that the dependencies form a tree\\xe2\\x80\\x94or to be more accurate, a \\n graph \\xe2\\x80\\x94over all the words in the sentence. The graph is connected where each word has \\nat least one directed edge going out or coming into it. The graph is also directed because \\neach edge between two words points to one specific direction. In essence, the dependency \\ntree is a  directed acyclic graph (DAG). Every node in the tree has at most one incoming \\nedge, except the root node. Because this is a directed graph, by nature dependency trees \\ndo not depict the order of the words in the sentence but emphasize more the relationship \\nbetween the words in the sentence. Our sentence is annotated with the relevant POS tags \\ndiscussed earlier, and the directed edges show the dependency. Now, if you remember, \\nwe just discussed earlier that there were two aspects to the representation of sentences \\nusing dependency grammar. Each directed edge represents a specific type of meaningful \\nrelationship (also known as  syntactic function ). We can annotate our sentence further \\nshowing the specific dependency relationship types between the  words . \\n The same is depicted in Figure\\xc2\\xa0 1-8 . An important point to remember here is that \\ndifferent variations of this graph might exist based on the parser you are using because \\nit depends on how the parser was initially trained, the kind of data which was used for \\ntraining it, and the kind of tag system it uses. \\n Figure 1-7.  Dependency  grammar based syntax tree with POS tags \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n17\\n These dependency relationships each have their own meaning and are a part of \\na list of universal dependency types. This is discussed in an original paper,   Universal \\nStanford Dependencies: A Cross-Linguistic Typology  by de Marneffe et\\xc2\\xa0al, 2014). You \\ncan check out the exhaustive list of dependency types and their meanings at   http://\\nuniversaldependencies.org/u/dep/index.html  . If we observe some of these \\ndependencies, it is not too hard to understand them. Let\\xe2\\x80\\x99s look in detail at some of the \\ntags used in the dependencies for the sentence in Figure\\xc2\\xa0 1-8 .\\n\\xe2\\x80\\xa2 \\n The dependency tag  det is pretty intuitive\\xe2\\x80\\x94it denotes the determiner \\nrelationship between a nominal head and the determiner. Usually \\nthe word with POS tag DET will also have the det dependency tag \\nrelation. Examples include ( fox \\xe2\\x86\\x92  the ) and ( dog \\xe2\\x86\\x92  the ).  \\n\\xe2\\x80\\xa2 \\n The dependency tag  amod stands for  adjectival modifier and \\nstands for any adjective that modifies the meaning of a noun. \\nExamples include ( fox \\xe2\\x86\\x92  brown ) and ( dog \\xe2\\x86\\x92  lazy ). \\n\\xe2\\x80\\xa2 \\n The dependency tag  nsubj stands for an entity that acts as a \\n subject or agent in a clause. Examples include ( is \\xe2\\x86\\x92  fox ) and \\n( jumping \\xe2\\x86\\x92  he ). \\n\\xe2\\x80\\xa2 \\n The  dependencies  cc and  conj are more to do with linkages \\nrelated to words connected by  coordinating conjunctions . \\nExamples include ( is \\xe2\\x86\\x92  and ) and ( is \\xe2\\x86\\x92  jumping ). \\n Figure 1-8.  Dependency grammar\\xe2\\x80\\x93based  syntax tree annotated with dependency \\nrelationship types \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n18\\n\\xe2\\x80\\xa2 \\n The dependency tag  aux indicates the  auxiliary or secondary verb \\nin the clause. Example: ( jumping \\xe2\\x86\\x92  is ). \\n\\xe2\\x80\\xa2 \\n The dependency tag  acomp stands for  adjective complement \\nand acts as the complement or object to a verb in the sentence. \\nExample: ( is \\xe2\\x86\\x92  quick ). \\n\\xe2\\x80\\xa2 \\n The dependency tag  prep denotes a  prepositional modifier, \\nwhich usually modifies the meaning of a noun, verb, adjective, or \\npreposition. Usually this representation is used for prepositions \\nhaving a noun or noun phrase complement. Example: (jumping \\n\\xe2\\x86\\x92 over). \\n\\xe2\\x80\\xa2 \\n The dependency tag  pobj is used to denote the  object of a \\npreposition . This is usually the head of a noun phrase following a \\npreposition in the sentence. Example: (over \\xe2\\x86\\x92 dog). \\n The preceding tags have been extensively used in our sample sentence for \\nannotating the various dependency relationships among the words. Now that you \\nunderstand dependency relationships better, consider that often when representing a \\ndependency grammar for sentences, instead of creating a tree with linear orders, you can \\nalso represent it with a normal graph because there is no concept of order of words in \\ndependency grammar. Figure\\xc2\\xa0 1-9 depicts the  same . \\n Figure\\xc2\\xa0 1-9  was created courtesy of spacy.io, which has some robust NLP modules \\nalso in a library that is open source. (When we cover constituency-based grammars next, \\nobserve that the number of nodes in dependency grammars is smaller compared to their \\nconstituency counterparts.) Currently there are various grammatical frameworks based \\non dependency grammar. Some popular ones include Algebraic Syntax and Operator \\nGrammar. \\n Figure 1-9.  Dependency  grammar annotated graph for our sample sentence \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n19\\n Constituency Grammars \\n Constituency grammars are a class of grammars built upon the principle that a sentence \\ncan be represented by several constituents derived from it. These grammars can be used \\nto  model or represent the internal structure of sentences in terms of a hierarchically \\nordered structure of their constituents. Each and every word usually belongs to a specific \\n lexical category in the case and forms the head word of different phrases. These phrases \\nare formed based on rules called  phrase structure rules . Hence, constituency grammars \\nare also called  phrase structure grammars . Phrase structure grammars were first \\nintroduced by Noam Chomsky in the 1950s. To understand constituency grammars we \\nmust know clearly what we mean by  constituents . To refresh your memory,  constituents \\nare words or groups of words that have specific meaning and can act together as a \\ndependent or independent unit. They can also be combined together further to form \\nhigher-order structures in a sentence, including phrases and clauses. \\n Phrase structure rules form the core of constituency grammars because they talk \\nabout syntax and rules that govern the hierarchy and ordering of the various constituents \\nin the sentences. These rules cater to two things primarily. First and foremost, they \\ndetermine what words are used to construct the phrases or constituents. Secondly, these \\nrules determine how we need to order these constituents together. If we want to analyze \\nphrase structure, we should we aware of typical schema patterns of the phrase structure \\nrules. The generic representation of a phrase structure rule is  S \\xe2\\x86\\x92 AB , which depicts that \\nthe structure  S consists of constituents  A and  B , and the ordering is  A followed by  B . \\n There are several  phrase structure rules , and we will explore them one by one to \\nunderstand how exactly we extract and order constituents in a sentence. The most \\nimportant rule describes how to divide a sentence or a clause. The phrase structure rule \\ndenotes a binary division for a sentence or a clause as  S \\xe2\\x86\\x92 NP VP where  S is the  sentence \\nor clause, and it is divided into the subject, denoted by the  noun phrase (NP) and the \\npredicate, denoted by the  verb phrase (VP). \\n We can apply more rules to break down each of the constituents further, but the top \\nlevel of the hierarchy usually starts with a NP and VP. The rule for representing a noun \\nphrase is  NP \\xe2\\x86\\x92 [DET][ADJ]N [PP] , where the square brackets denote that it is optional. \\nUsually a noun phrase consists of a noun (N) definitely as the head word and may \\noptionally contain determinants (DET) and adjectives (ADJ) describing the noun, and a \\nprepositional phrase (PP) at the right side in the syntax tree. Consequently, a noun phrase \\nmay contain another noun phrase as a constituent of it. Figure\\xc2\\xa0 1-10 shows a few examples \\nthat are governed by the aforementioned rules for noun phrases. \\n Figure 1-10.    Constituency syntax trees depicting structuring rules for  noun phrases \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n20\\n The  syntax trees in Figure\\xc2\\xa0 1-10 show us the various constituents a noun phrase \\ntypically contains. As mentioned, a noun phrase denoted by NP on the left side of the \\nproduction rule may also appear on the right side of the production rule, as depicted \\nin the preceding example. This is a property called  recursion , and we will talk about it \\ntoward the end of this  section . \\n We will now look at rules for representing verb phrases. The rule is of the form \\n VP \\xe2\\x86\\x92 V | MD [ VP ][ NP ][ PP ][ ADJP ][ ADVP ], where the head word is usually a verb (V) or a \\nmodal (MD). A  modal is itself an auxiliary verb, but we give it a different representation \\njust to distinguish it from a normal verb. This is followed by optionally another verb \\nphrase (VP) or noun phrase (NP), prepositional phrase (PP), adjective phrase (ADJP), or \\nadverbial phrase (ADVP). The verb phrase is always the second component when we split \\na sentence using the binary division rule, making the noun phrase the first component. \\nFigure\\xc2\\xa0 1-11  depicts a few examples for the different types of verb phrases that can be \\ntypically constructed and their representations as syntax trees. \\n As depicted earlier, the syntax trees in Figure\\xc2\\xa0 1-11 show the representations of the \\nvarious constituents in verb phrases. Using the property of recursion, a verb phrase may \\nalso contain another verb phrase inside it, as you can see in the second syntax tree. You \\ncan also see the hierarchy being maintained especially in the third and fourth syntax \\ntrees, where the NP and PP by itself are further constituents under the VP, and they can be \\nfurther broken down into smaller constituents. \\n Since we have seen a lot of prepositional phrases being used in examples, let\\xe2\\x80\\x99s look at \\nthe production rules for representing prepositional phrases. The basic rule has the form \\n PP \\xe2\\x86\\x92 PREP [ NP ], where PREP denotes a preposition, which acts as the head word, and it is \\noptionally followed by a noun phrase (NP). Figure\\xc2\\xa0 1-12 depicts some representations of \\nprepositional phrases and their corresponding syntax trees. \\n Figure 1-11.  Constituency syntax trees depicting structuring rules for  verb phrases \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n21\\n These two syntax trees  show some different representations for prepositional \\nphrases. \\n Recursion is an inherent property of language that allows constituents to be \\nembedded in other constituents, which are depicted by different phrasal categories that \\nappear on both sides of the production rules. Recursion lets us create long constituency-\\nbased syntax trees from sentences. A simple example is the representation of the sentence \\n The flying monkey in the circus on the trapeze by the river  depicted by the constituency \\nparse tree in Figure\\xc2\\xa0 1-13 . \\n Figure 1-12.    Constituency syntax trees depicting structuring rules for  prepositional phrases \\n Figure 1-13.    Constituency syntax tree depicting  recursive properties among constituents \\n \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n22\\n If you closely observe the syntax tree in Figure\\xc2\\xa0 1-13 , you will notice that it is only \\nconstituted of noun phrases and prepositional phrases. However, due to the inherent \\nrecursive property that a prepositional phrase itself can consist of a noun phrase, and the \\nnoun phrase can consist of a noun phrase as well as a prepositional phrase, we notice the \\nhierarchical structure with multiple NPs and PPs. If you go over the production rules for \\nnoun phrases and prepositional phrases, you will find the constituents shown in the tree \\nare in adherence with the rules. \\n Conjunctions are used to join clauses and phrases together and form an important \\npart of language syntax. Usually words, phrases, and even clauses can be combined \\ntogether using conjunctions. The production rule can be denoted as  S \\xe2\\x86\\x92 S conj S \\xdc\\x94 \\n S \\xe2\\x88\\x88{ S,NP,VP }, where two constituents can be joined together by a conjunction, denoted \\nby  conj  in the rule. A simple example for a sentence consisting of a noun phrase which, by \\nitself, is constructed out of two noun phrases and a conjunction, would be  The brown fox \\nand the lazy dog . This is depicted in Figure\\xc2\\xa0 1-14 by the constituency syntax tree showing \\nthe adherence to the production rule. \\n Figure\\xc2\\xa0 1-14 shows that the top level noun phrase is the sentence by itself and has \\ntwo noun phrases as its constituents, which are joined together by a conjunction, thus \\nsatisfying our aforementioned production rule. \\n What if we wanted to join two sentences or clauses together with a conjunction? \\nWe can do that by putting all these  rules and conventions together to generate the \\nconstituency-based syntax tree for our sample sentence  The brown fox is quick and he is \\njumping over the lazy dog . This would give us the syntactic representation of our sentence \\nas depicted in Figure\\xc2\\xa0 1-15 . \\n Figure 1-14.  Constituency syntax tree depicting noun phrases joined by a  conjunction \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n23\\n From Figure\\xc2\\xa0 1-15 , you can conclude that our sentence has two main clauses or \\nconstituents (discussed earlier), which are joined by a  coordinating conjunction ( and ). \\nMoreover, the constituency grammar\\xe2\\x80\\x93based production rules break down the top-level \\nconstituents into further constituents consisting of phrases and their words. Looking at \\nthis syntax tree, you can see that it does show the ordering of the words in the sentence \\nand is more of a hierarchical tree\\xe2\\x80\\x93based structure with un-directed edges. Hence, this \\nis very different compared to the dependency grammar\\xe2\\x80\\x93based syntax tree\\\\graph with \\nunordered words and directed edges. There are several popular grammar frameworks \\nbased on concepts derived from constituency grammar, including Phrase Structure \\nGrammar, Arc Pair Grammar, Lexical Functional Grammar, and even the famous Context-\\nFree Grammar, which is used extensively in describing formal language.   \\n Word Order Typology \\n Typology  in linguistics is a field that specifically deals with trying to classify languages \\nbased on their syntax, structure, and functionality. Languages can be classified in \\nseveral ways, and one of the most common models is to classify them according to their \\ndominant word orders, also known as  word order typology . The primary word orders of \\ninterest occur in clauses consisting of a subject, verb, and an object. Of course, not all \\nclauses use the subject, verb, and object, and often the subject and object are not used in \\ncertain languages. However, there exist several different classes of word orders that can \\nbe used to classify a wide variety of languages. A survey done by Russell Tomlin in 1986, \\nsummarized in Table\\xc2\\xa0  1-1  , shows some insights derived from his analysis.  \\n Figure 1-15.    Constituency syntax tree for our sample sentence       \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n24\\n In Table\\xc2\\xa0  1-1  , we can observe that there are six major classes of word orders, and \\nlanguages like English follow the Subject-Verb-Object word order class. A simple example \\nwould be the sentence  He ate cake , where  He is the subject,  ate  is the verb, and  cake \\nis the object. The majority of  languages from the table follow the Subject-Object-Verb \\nword order. In that case, the sentence  He cake ate would be correct if translated to those \\nlanguages. This is illustrated by the English-to-Hindi translation of the same sentence in \\nFigure\\xc2\\xa0 1-16 . \\n Even if you do not understand Hindi, you can understand by the English annotation \\nprovided by google that the word  cake (denoted by  kek in the text under the Hindi \\ntranslation) has moved from the right end to the middle of the sentence, and the verb \\n ate denoted by  khaaya has moved from the middle to the end of the sentence, thus \\nmaking the word order class become Subject-Object-Verb\\xe2\\x80\\x94the correct form for the Hindi \\nlanguage. This illustration gives us an indication of the importance of word order and \\nhow representation of messages can be grammatically different in various languages. \\n And that brings us to the end of our discussion of the  syntax and structure of \\nlanguages. Next we will be looking at some of the concepts around language semantics. \\n Table 1-1.  Word Order\\xe2\\x80\\x93Based Language Classification, Surveyed by Russell Tomlin, 1986 \\n Sl No. \\n Word Order \\n Language Frequency \\n Example Languages \\n 1 \\n Subject-Object-Verb \\n 180 (45%) \\n Sanskrit, Bengali, Gothic, \\nHindi, Latin \\n 2 \\n Subject-Verb-Object \\n 168 (42%) \\n English, French, Mandarin, \\nSpanish \\n 3 \\n Verb-Subject-Object \\n 37 (9%) \\n Hebrew, Irish, Filipino, \\nAramaic \\n 4 \\n Verb-Object-Subject \\n 12 (3%) \\n Baure, Malagasy, Aneityan \\n 5 \\n Object-Verb-Subject \\n 5 (1%) \\n Apalai, Hixkaryana, Arecua \\n 6 \\n Object-Subject-Verb \\n 1 (0%) \\n Warao \\n Figure 1-16.    English-to-Hindi translation changes the word order class for the sentence He \\nate cake (courtesy of Google Translate) \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n25\\n Language  Semantics \\n The simplest  definition of  semantics is the study of meaning. Linguistics has its own subfield \\nof  linguistic  semantics , which deals with the study of meaning in language, the relationships \\nbetween words, phrases, and symbols, and their indication, meaning, and representation \\nof the knowledge they signify. In simple words, semantics is more concerned with the \\nfacial expressions, signs, symbols, body language, and knowledge that are transferred \\nwhen passing messages from one entity to another. There are various representations for \\n syntax and rules for the same, including various forms of grammar we have covered in the \\nprevious sections. Representing semantics using formal rules or conventions has always \\nbeen a challenge in linguistics. However, there are different ways to represent meaning and \\nknowledge obtained from language. This section looks at relations between the lexical units \\nof a language\\xe2\\x80\\x94predominantly words and phrases\\xe2\\x80\\x94and explores several representations \\nand concepts around formalizing representation of knowledge and meaning. \\n Lexical  Semantic Relations \\n Lexical  semantics is usually concerned with identifying semantic relations between \\nlexical units in a language and how they are correlated to the syntax and structure of the \\nlanguage. Lexical units are usually represented by morphemes, the smallest meaningful \\nand syntactically correct unit of a language. Words are inherently a subset of these \\nmorphemes. Each lexical unit has its own syntax, form, and meaning. They also derive \\nmeaning from their surrounding lexical units in phrases, clauses, and sentences. A  lexicon \\nis a complete vocabulary of these lexical units. We will explore some concepts revolving \\naround lexical semantics in this section. \\n Lemmas and Wordforms \\n A  lemma is also known as the canonical or citation form for a set of words. The lemma \\nis usually the base form of a set of words, known as a  lexeme in this context. Lemma is \\nthe specific base form or head word that represents the lexeme.  Wordforms are inflected \\nforms of the lemma, which are part of the lexeme and can appear as one of the words \\nfrom the lexeme in text. A simple example would the lexeme {eating, ate, eats}, which \\ncontains the wordforms, and their lemma is the word  eat . \\n These  words have specific meaning based on their position among other words \\nin a sentence. This is also known as  sense of the word, or wordsense.  Wordsense  gives a \\nconcrete representation of the different aspects of a word\\xe2\\x80\\x99s meaning. Consider the word \\n fair in the following sentences:  They are going to the annual fair and  I hope the judgement \\nis fair to all . Even though the word  fair is the same in both the sentences, the meaning \\nchanges based on the surrounding words and context.  \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n26\\n Homonyms, Homographs, and Homophones \\n Homonyms  are defined as words that share the same spelling or pronunciation but have \\ndifferent meanings. An alternative definition restricts the constraint on same spelling. \\nThe relationship between these words is termed as  homonymy . Homonyms are often said \\nto be the superset of homographs and homophones. An example of homonyms for the \\nword  bat can be demonstrated in the following sentences:  The bat hangs upside down \\nfrom the tree and  That baseball bat is really sturdy. \\n Homographs  are words that have the same written form or spelling but have different \\nmeanings. Several alternate definitions say that the  pronunciation can also be different. \\nSome examples of homographs include, the word  lead as in  I am using a lead pencil and \\n Please lead the soldiers to the camp , and also the word  bass in  Turn up the bass for the song \\nand  I just caught a bass today while I was out fishing . Note that in both cases, the spelling \\nstays the same but the pronunciation changes based on the context in the sentences. \\n Homophones  are words that have the same pronunciation but different meanings, \\nand they can have the same or different spellings. Examples would be the words  pair \\n(meaning couple) and  pear (the fruit). They sound the same but have different meanings \\nand written forms. Often these words cause problems in NLP because it is very difficult to \\nfind out the actual context and meaning using machine intelligence. \\n Heteronyms and Heterographs \\n Heteronyms are words that have the same written form or spelling but different \\npronunciations and meanings. By nature, they are a subset of homographs. They are also \\noften called  heterophones , which means \\xe2\\x80\\x9cdifferent sound.\\xe2\\x80\\x9d Examples of heteronyms are \\nthe words  lead (metal, command) and  tear (rip off something, moisture from eyes). \\n Heterographs are words that have the same pronunciation but different meanings \\nand spellings. By nature they are a subset of homonyms. Their written representation \\nmight be different but they sound very similar or often exactly the same when spoken. \\nSome examples include the words  to ,  too , and  two , which sound similar but have \\ndifferent spellings and meanings. \\n Polysemes \\n Polysemes are words that have the same written form or spelling and different but very \\nrelatable meanings. While this is very similar to homonymy, the difference is subjective \\nand depends on the context, since these words are relatable to each other. A good \\nexample is the word  bank which can mean (1) a financial institution, (2) the bank of the \\nriver, (3) the building that belongs to the financial institution, or (4) a verb meaning  to rely \\nupon . These examples use the same word  bank and are homonyms. But only (1), (3), and \\n(4) are polysemes representing a common theme (the financial organization representing \\ntrust and security). \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n27\\n Capitonyms \\n Capitonyms are words that have the same written form or spelling but have different \\nmeanings when capitalized. They may or may not have different pronunciations. Some \\nexamples include the words  march ( March indicates the month, and  march depicts the \\naction of walking) and  may ( May indicates the month, and  may is a modal verb). \\n Synonyms and Antonyms \\n Synonyms are words that have different pronunciations and spellings but have the same \\nmeanings in some or all contexts. If two words or lexemes are synonyms, they can be \\nsubstituted for each other in various contexts, and it signifies them having the same \\npropositional meaning. Words that are synonyms are said to be  synonymous to each \\nother, and the state of being a synonym is called  synonymy . Perfect synonymy is, however, \\nalmost nonexistent. The reason is that synonymy is more of a relation between senses \\nand contextual meaning rather than just words. Consider the synonyms  big ,  huge , and \\n large . They are very relatable and make perfect sense in sentences like  That milkshake \\nis really ( big/large/huge ). However, for the sentence  Bruce is my big brother , it does not \\nmake sense if we substitute  big with either  huge or  large.  That\\xe2\\x80\\x99s because the word  big here \\nhas a context or sense depicting being grown up or older, and the other two synonyms \\nlack this sense. Synonyms can exist for all parts of speech, including nouns, adjective, \\nverbs, adverbs, and prepositions. \\n Antonyms are pairs of words that define a binary opposite relationship. These words \\nindicate specific sense and meaning that are completely opposite to each other. The state of \\nbeing an antonym is called  antonymy . There are three types of antonyms:  graded antonyms , \\n complementary antonyms , and  relational antonyms .  Graded antonyms , as the name \\nsuggests, are antonyms with a certain grade or level when measured on a continuous scale, \\nlike the pair ( fat ,  skinny ).  Complementary antonyms  are word pairs that are opposite in their \\nmeaning but cannot be measured on any grade or scale. An example of a complementary \\nantonym pair is ( divide ,  unite ).  Relational antonyms  are word pairs that have some \\nrelationship between them, and the antonymy is contextual, which is signified by this very \\nrelationship. An example of a relational antonym pair is ( doctor ,  patient ).  \\n Hyponyms and  Hypernyms \\n Hyponyms are words that are usually a subclass of another word. In this case, the \\nhyponyms are generally words with very specific sense and context as compared to \\nthe word that is their superclass.  Hypernyms are the words that act as the superclass to \\nhyponyms and have a more generic sense compared to the hyponyms. An example would \\nbe the word  fruit , which is a hypernym, and the words  mango ,  orange , and  pear would be \\npossible hyponyms. The relationships depicted between these words are often termed \\n hyponymy and  hypernymy . \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n28\\n Semantic  Networks and Models \\n We have seen several ways to formalize relations between words and their senses or \\nmeanings. Considering lexical semantics, there are approaches to find out the sense \\nand meaning of each lexical unit, but what if we want to represent the meaning of some \\nconcept or theory that would involve relating these lexical units together and forming \\nconnections between them based on their meaning?  Semantic networks  aim to tackle this \\nproblem of representing knowledge and concepts using a network or a graph. \\n The basic unit of semantic  network is an  entity or a  concept . A concept could be a \\ntangible or abstract item like an idea. Sets of concepts have some relation to each other \\nand can be represented with directed or undirected edges. Each  edge denotes a specific \\ntype of relationship between two concepts. Let\\xe2\\x80\\x99s say we are talking about the concept  fish . \\nWe can have different concepts around fish based on their relationship to it. For instance, \\n fish \\xe2\\x80\\x9c is-a \\xe2\\x80\\x9d  animal and  fish \\xe2\\x80\\x9c is-a \\xe2\\x80\\x9d part of  marine life . These relationships are depicted \\nas  is-a relationships. Other similar relationships include  has-a ,  part-of ,  related-to , and \\nthere are many more, depending on the context and semantics. These concepts and \\nrelationships together form a semantic network. There are several semantic models on \\nthe Web that have vast knowledge bases spanning different concepts. Figure\\xc2\\xa0 1-17 shows \\na possible representation for concepts related to  fish . This model is provided courtesy of \\nNodebox (   www.nodebox.net/perception/  ), where you can search for various concepts \\nand see associated concepts to the same. \\n Figure 1-17.    Semantic  network   around the concept fish       \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n29\\n In the  network in Figure\\xc2\\xa0 1-17 , we can see some of the concepts discussed earlier \\naround  fish and also specific types of fish like eel, salmon, shark, and so on, which can \\nbe hyponyms to the concept  fish . These semantic networks are formally denoted and \\nrepresented by semantic data models using graph structures, where concepts or entities \\nare the nodes and the edges denote the relationships. The Semantic Web is as extension \\nof the World Wide Web using semantic metadata annotations and embeddings using \\ndata-modeling techniques like Resource Description Framework (RDF) and Web \\nOntology Language (OWL). In linguistics, we have a rich lexical corpus and database \\ncalled WordNet, which has an exhaustive list of different lexical entities grouped \\ntogether based on semantic similarity (for example, synonyms) into  synsets . Semantic \\nrelationships between these synsets and consequently various words can be explored in \\nWordNet, making it in essence a type of semantic network. We will talk about WordNet in \\nmore detail in a later section when we cover  text corpora . \\n Representation of Semantics \\n So far we have seen how to represent semantics based on lexical units and how they can be \\ninterconnected by leveraging semantic networks. However, if we consider the normal form \\nof communication via messages, whether written or spoken, if an entity sends a message \\nto another entity and that entity takes some specific actions based on the message, then \\nthe second entity is said to have understood the meaning conveyed by that message. A \\nquestion that might come to mind is how we formally represent the meaning or semantics \\nconveyed by a simple sentence. Although it may be extremely easy for us to understand \\nthe meaning conveyed, representing semantics formally is not as easy as it seems. \\n Consider the example  Get me the book from the table . This sentence by nature is a \\ndirective, and it directs the listener to do something. Understanding the meaning conveyed \\nby this sentence may involve pragmatics like  which specific book? and  which specific table?  \\nbesides the actual deed of getting the book from the table. Although the human mind \\nis intuitive, formally representing the meanings and relationships between the various \\nconstituents is a challenge\\xe2\\x80\\x94but we can do it using techniques such as  propositional \\nlogic (PL) and  first order logic (FOL). Using these representations, one can represent the \\nmeaning indicated by different sentences, draw inference from them, and even discover \\nwhether one sentence entails another one based on their semantics. Representation \\nof semantics is useful especially for carrying out our various NLP operations to make \\nmachines understand the semantics behind messages using proper representations, since \\nmachines lack the cognitive power we humans have been bestowed with. \\n Propositional Logic \\n Propositional logic ( PL) , also known as  sentential logic or  statement logic , is defined \\nas the discipline of logic that is concerned with the study of propositions, statements, \\nand sentences. This includes studying logical relationships and properties between \\npropositions and statements, combining multiple propositions to form more complex \\npropositions, and observing how the value of propositions change based on their \\ncomponents and logical operators. A  proposition or  statement is usually  declarative and \\nis capable of having a binary truth value that is either true or false. Usually a statement \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n30\\nis more language-specific and concrete, and a proposition is more inclined toward the \\nidea or the concepts conveyed by the statement. A simple example would be the two \\nstatements  The rocket was faster than the airship and  The airship was slower than the \\nrocket , which are distinct but convey the same meaning or proposition. However, the \\nterms statement and proposition are often used interchangeably in propositional logic. \\n The main focus in propositional logic is to study different propositions and \\nhow combining various propositions with logical operators change the semantics \\nof the overall proposition. These logical operators are used more like  connectors or \\ncoordinating conjunctions (if you remember them from earlier). Operators include terms \\nlike  and ,  or , and  not , which can change the meaning of a proposition by itself or when \\ncombined with several propositions. A simple example would be two propositions,  The \\nEarth is round and  The Earth revolves around the Sun . These can be combined with the \\nlogical operator  and to give us the proposition  The Earth is round and it revolves around \\nthe Sun , which gives us the indication that the two propositions on either side of the  and \\noperator must be true for the combined proposition to be true. \\n The good part about propositional logic is that each proposition has its own  truth \\nvalue , and it is not concerned with further subdividing a proposition into smaller \\nchunks and verifying its logical characteristics. Each proposition is considered as an \\nindivisible, whole unit with its own truth value. Logical operators may be applied on it \\nand several other propositions. Subdividing parts of propositions like clauses or phrases \\nare not considered here. To represent the various building blocks of propositional logic, \\nwe use several conventions and symbols. Uppercase letters like  P and  Q are used to \\ndenote individual statements or propositions. The different operators used and their \\ncorresponding symbols are listed in Table\\xc2\\xa0  1-2 , based on their order of precedence.  \\n You can see that there are a total of five operators, with the  not  operator having the \\nhighest precedence, and the  iff operator having the lowest. Logical constants are denoted \\nas either being True or False. Constants and symbols are known as   atomic units \\xe2\\x80\\x94all \\nother units, more specifically the sentences and statements, are  complex units . A  literal is \\nusually an atomic statement or its negation on applying the  not operator. \\n Let\\xe2\\x80\\x99s look at a simple example of two sentences  P and  Q  and apply various operators \\non them. Consider the following representations:\\n P : He is hungry \\n Q : He will eat a sandwich \\n Table 1-2.  Logical Operators with Their Symbols and  Precedence \\n Sl No.  Operator Symbol \\n Operator Meaning  Precedence \\n 1 \\n \\xc2\\xac \\n not \\n Highest \\n 2 \\n \\xe2\\x88\\xa7 \\n and \\n 3 \\n \\xe2\\x88\\xa8 \\n or \\n 4 \\n \\xe2\\x86\\x92 \\n if-then \\n 5 \\n \\xe2\\x86\\x94 \\n iff (if and only if) \\n Lowest \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n31\\n The expression  P \\xe2\\x88\\xa7  Q translates to  He is hungry and he will eat a sandwich . This \\nexpresses that the outcome of this operation is itself also a sentence or proposition. \\nThis is the  conjunction operation where  P and  Q are the  conjuncts . The outcome of this \\nsentence is  True only if both  P and  Q are  True . \\n The expression  P \\xe2\\x88\\xa8  Q translates to  He is hungry or he will eat a sandwich . This \\nexpresses that the outcome of this operation is also another proposition formed from the \\n disjunction operation where  P and  Q are the  disjuncts . The outcome of this sentence is \\n True if either  P or  Q or both of them are  True . \\n The expression  P \\xe2\\x86\\x92  Q translates to  If he is hungry, then he will eat a sandwich . This \\nis the  implication operation which determines that  P is the  premise or  antecedent and  Q is \\nthe  consequent . It is just like a rule stating that  Q will occur only if  P has already occurred \\nor is  True . \\n The expression  P \\xe2\\x86\\x94  Q translates to  He will eat a sandwich if and only if he is \\nhungry which is basically a combination of the expressions  If he is hungry then he will \\neat a sandwich ( P \\xe2\\x86\\x92  Q ) and  If he will eat a sandwich, he is hungry ( Q \\xe2\\x86\\x92  P ). This is the \\n biconditional or  equivalence operation that will evaluate to  True if and only if the two \\nimplication operations described evaluate to  True . \\n The expression  \\xc2\\xacP translates to  He is not hungry , which depicts the negation \\noperation and will evaluate to  True if and only if  P evaluates to  False . \\n This gives us an idea of the basic operations between propositions and more complex \\noperations, which can be carried out with multiple logical connectives and by adding more \\npropositions. A simple example: The statements  P :  We will play football ,  Q :  The stadium is \\nopen , and  R :  It will rain today can be combined and represented as  Q \\xe2\\x88\\xa7  \\xc2\\xacR \\xe2\\x86\\x92  P to depict \\nthe complex proposition  If the stadium is open and it does not rain today, then we will \\nplay football . The semantics of the truth value or outcome of the final proposition can be \\nevaluated based on the truth value of the individual propositions and the operators. The \\nvarious outcomes of the truth values for the different operators are depicted in Figure\\xc2\\xa0 1-18 . \\n Thus, using the table in Figure\\xc2\\xa0 1-18 , we can evaluate even more complex \\npropositions by breaking them down into simpler binary operations, evaluating the truth \\nvalue for them, and combining them step by step. \\n Besides these outcomes, other properties like  associativity ,  commutativity , and \\n distributivity aid in evaluating complex proposition outcomes. The act of checking the \\nvalidity of each operation and proposition and finally evaluating the outcome is also \\nknown as  inference . However, besides evaluating extensive truth tables all the time, we \\ncan also make use of several inference rules to arrive at the final outcome or conclusion. \\nThe main reason for doing so would be that the size of these truth tables with the various \\noperations starts increasing exponentially with the number of propositions increasing. \\n Figure 1-18.  Truth values for various logical connectors \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n32\\nMoreover, rules of inference are easier to understand and well tested, and at the heart \\nof them, the same truth value tables are actually applied\\xe2\\x80\\x94but we do not have to bother \\nourselves with the internals. Usually, a sequence of inference rules, when applied, leads \\nto a conclusion that is often termed as a  logical proof . The usual form of an inference rule \\nis  P \\xe5\\x8b\\x91  Q , which indicates that  Q can be derived by some inference operations from the set \\nof statements represented by  P . The turnstile symbol (\\xe5\\x8b\\x91) indicates that  Q is some logical \\nconsequence of  P . The most popular inference rules are as follows:\\n\\xe2\\x80\\xa2 \\n Modus Ponens : Perhaps the most popular inference rule, it\\xe2\\x80\\x99s also \\nknown as the  Implication Elimination rule. It can be represented \\nas  {P \\xe2\\x86\\x92  Q, P}    \\xe5\\x8b\\x91  Q, which indicates that if  P implies  Q and  P is \\nasserted to be  True , then it is inferred that  Q is  True . You can \\nalso represent this using the representation ( (P \\xe2\\x86\\x92  Q) \\xe2\\x88\\xa7  P)  \\xe2\\x86\\x92 \\n Q , which can be evaluated easily using truth tables. A simple \\nexample would be the statement  If it is sunny, we will play \\nfootball , represented by  P \\xe2\\x86\\x92  Q . Now if we say that  It is sunny , this \\nindicates that  P is  True , hence  Q automatically is inferred as  True , \\nindicating  We will play football . \\n\\xe2\\x80\\xa2 \\n Modus Tollens : This is quite similar to the previous rule and \\nis represented formally as  {P \\xe2\\x86\\x92  Q, \\xc2\\xacQ}    \\xe5\\x8b\\x91   \\xc2\\xacP , which indicates \\nthat if  P implies  Q and  Q is actually asserted to be  False , then it \\nis inferred that  P is  False . You can also represent this using the \\nrepresentation ( (P \\xe2\\x86\\x92  Q) \\xe2\\x88\\xa7  \\xc2\\xacQ)  \\xe2\\x86\\x92  \\xc2\\xacP , which can be evaluated \\neasily using truth tables. An example proposition would be  If \\nhe is a bachelor, he is not married , indicated by  P \\xe2\\x86\\x92  Q . Now if we \\npropose that  He is married , represented by  \\xc2\\xacQ , then we can infer \\n \\xc2\\xacP , which translates to  He is not a bachelor . \\n\\xe2\\x80\\xa2 \\n Disjunctive Syllogism : This is also known as  Disjunction \\nElimination and is formally represented as  {P \\xe2\\x88\\xa8  Q, \\xc2\\xacP}    \\xe5\\x8b\\x91   Q , \\nwhich indicates that if either  P or  Q is  True and  P is  False , then  Q \\nis  True . A simple example would be the statement  He is a miracle \\nworker or a fraud represented by  P \\xe2\\x88\\xa8  Q and the statement  He is \\nnot a miracle worker represented by  \\xc2\\xacP . We can then infer  He is a \\nfraud , depicted by  Q . \\n\\xe2\\x80\\xa2 \\n Hypothetical Syllogism : This is often known as the  Chain Rule of \\nDeduction and is formally represented as  {P \\xe2\\x86\\x92  Q, Q \\xe2\\x86\\x92  R}   \\xe5\\x8b\\x91   P \\xe2\\x86\\x92 \\n R , which tells us that if  P implies  Q , and  Q implies  R , we can infer \\nthat  P implies  R . A really interesting example to understand this \\nwould be the statement  If I am sick, I can\\xe2\\x80\\x99t go to work represented \\nby  P \\xe2\\x86\\x92  Q and  If I can\\xe2\\x80\\x99t go to work, the building construction will \\nnot be complete represented by  Q \\xe2\\x86\\x92  R . Then we can infer  If I am \\nsick, the building construction will not be complete , which can be \\nrepresented by  P \\xe2\\x86\\x92  R . \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n33\\n\\xe2\\x80\\xa2 \\n Constructive Dilemma : This inference rule is the disjunctive version \\nof Modus Ponens and can be formally represented as  {(P \\xe2\\x86\\x92  Q) \\xe2\\x88\\xa7 \\n (R \\xe2\\x86\\x92  S), P \\xe2\\x88\\xa8  R}    \\xe5\\x8b\\x91  Q \\xe2\\x88\\xa8  S , which indicates that if  P implies  Q , and  R  \\nimplies  S , and either  P or  R is  True , then it can be inferred that either \\n Q or  S is  True . Consider the following propositions:  If I work hard, I \\nwill be successful  represented by  P \\xe2\\x86\\x92  Q , and  If I win the lottery, I will \\nbe rich  represented by  R \\xe2\\x86\\x92  S . Now we propose that  I work hard or I \\nwin the lottery is  True , which is represented by  P \\xe2\\x88\\xa8  R . We can then \\ninfer that  I will be successful or I will be rich , represented by  Q \\xe2\\x88\\xa8  S . \\nThe complement of this rule is  Destructive Dilemma  the disjunctive \\nversion of Modus Tollens.    \\n This should give you a clear idea of how intuitive inference rules can be, and \\nusing them is much easier than going over multiple truth tables trying to find out the \\noutcome of complex propositions. The interpretation we derive from inference gives \\nus the semantics of the statement or proposition. A valid statement is one which would \\nbe  True under all interpretations irrespective of the logical operations or various \\nstatements inside it. This is often termed as a  tautology . The complement of a tautology \\nis a  contradiction  or an inconsistent statement which is  False under all interpretations. \\nNote that the preceding list is just an indicative list of the most popular inference rules \\nand is by no way an exhaustive list. Interested readers can read up more on inference and \\npropositional calculus to get an idea of several other rules and axioms which are used \\nbesides the ones covered here. \\n Next we will be looking at first order logic, which tries to solve some of the \\nshortcomings in propositional logic. \\n First Order Logic \\n First order logic (FOL), also known popularly as  predicate logic and  first order predicate \\ncalculus , is defined as a  collection of well-defined formal systems which is used \\nextensively in deduction, inference, and representation of knowledge. FOL allows \\nus to use  quantifiers and variables in sentences, which enable us to overcome some \\nof the limitations with propositional logic. If we are to consider the pros and cons \\nof propositional logic (PL), considering the points in its favor, PL is declarative and \\nallows us to easily represent facts using a well-formed syntax. PL also allows complex \\nrepresentations like conjunctive, disjunctive, and negated knowledge representations. \\nThis by nature makes PL compositional wherein a composite or complex proposition is \\nbuilt from the simple propositions that are its components along with logical connectives. \\nHowever, there are several areas where PL is lacking. It is definitely not easy to represent \\nfacts in PL because for each possible atomic fact, we will need a unique symbolic \\nrepresentation. Hence, due to this limitation, PL has very limited expressive power. \\nHence, the basic idea behind FOL is to not treat propositions as atomic entities. \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n34\\n FOL has a much richer syntax and necessary components for the same compared to \\nPL. The basic  components in FOL are as follows:\\n\\xe2\\x80\\xa2 \\n Objects : These are specific entities or terms with individual \\nunique identities like people, animals, and so on. \\n\\xe2\\x80\\xa2 \\n Relations : These are also known as predicates and usually hold \\namong objects or sets of objects and express some form of \\nrelationship or connection, like  is_man ,  is_brother ,  is_mortal . \\nRelations typically correspond to verbs. \\n\\xe2\\x80\\xa2 \\n Functions : These are a subset of relations where there is always \\nonly one output value or object for some given input. Examples \\nwould be  height ,  weight ,  age_of . \\n\\xe2\\x80\\xa2 \\n Properties : These are specific attributes of objects that help in \\ndistinguishing them from other objects, like round, huge, and so on.  \\n\\xe2\\x80\\xa2 \\n Connectives : These are the logical connectives that are similar to \\nthe ones in PL, which include not (\\xc2\\xac), and (\\xe2\\x88\\xa7), or (\\xe2\\x88\\xa8), implies \\n(\\xe2\\x86\\x92), and iff (if and only if \\xe2\\x86\\x94). \\n\\xe2\\x80\\xa2 \\n Quantifiers : These include two types of quantifiers:  universal \\n(\\xe2\\x88\\x80), which stands for \\xe2\\x80\\x9cfor all\\xe2\\x80\\x9d or \\xe2\\x80\\x9call,\\xe2\\x80\\x9d and  existential (\\xe2\\x88\\x83), which \\nstands for \\xe2\\x80\\x9cthere exists\\xe2\\x80\\x9d or \\xe2\\x80\\x9cexists.\\xe2\\x80\\x9d They are used for quantifying \\nentities in a logical or mathematical expression.  \\n\\xe2\\x80\\xa2 \\n Constant symbols : These are used to represent concrete entities or \\nobjects in the world. Examples would be  John ,  King ,  Red , and  7 . \\n\\xe2\\x80\\xa2 \\n Variable symbols : These are used to represent variables like  x ,  y , \\nand  z . \\n\\xe2\\x80\\xa2 \\n Function symbols : These are used to map functions to outcomes. \\nExamples would be,  age_of(John) = 25 or  color_of(Tree) = \\nGreen. \\n\\xe2\\x80\\xa2 \\n Predicate symbols : These map specific entities and a relation or \\nfunction between them to a truth value based on the outcome. \\nExamples would be  color(sky, blue) = True. \\n These are the main  components that go into logical representations and syntax for \\nFOL. Usually, objects are represented by various  terms , which could be either a  function , \\n variable , or  constant based on the different components depicted previously. These terms \\ndo not need to be defined and do not return values. Various propositions are usually \\nconstructed using predicates and terms with the help of predicate symbols. An  n-ary \\npredicate is constructed from a function over n-terms which have either a  True or  False \\noutcome. An  atomic sentence can be represented by an n-ary predicate, and the outcome \\nis  True or  False depending on the semantics of the sentence\\xe2\\x80\\x94that is, if the objects \\nrepresented by the terms have the correct relation among themselves as specified by \\nthe predicate. A  complex sentence or statement is formed using several atomic sentences \\nand logical connectives. A  quantified sentence adds the quantifiers mentioned earlier to \\nsentences. \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n35\\n  Quantifiers are one advantage FOL has over PL, since they enable us to represent \\nstatements about entire sets of objects without needing to represent and enumerate each \\nobject by a different name. The  universal quantifier (\\xe2\\x88\\x80) asserts that a specific relation or \\npredicate is  True  for all values associated with a specific variable. The representation \\xe2\\x88\\x80 x \\nF(x)  indicates that  F holds  for all values of  x in the domain associated with  x . An example \\nwould be \\xe2\\x88\\x80 x cat(x) \\xe2\\x86\\x92  animal(x) , which indicates that all cats are animals. \\n Universal quantifiers are usually used with the  implies (\\xe2\\x86\\x92) connective to form rules \\nand statements. An important thing to remember is that universal quantifiers are almost \\nnever used in statements to indicate some relation for every entity in the world using the \\nconjunction (\\xe2\\x88\\xa7) connective. An example would be the representation \\xe2\\x88\\x80 x dog(x) \\xe2\\x88\\xa7 \\n eats_meat(x) , which actually means that every entity in the world is a dog and they eat \\nmeat, which sounds kind of absurd! The  existential quantifier (\\xe2\\x88\\x83) asserts that a specific \\nrelation or predicate holds  True for at least some value associated with a specific variable. \\nThe representation, \\xe2\\x88\\x83 x F(x) indicates that  F holds  for some  value of  x in the domain \\nassociated with  x . An example would be \\xe2\\x88\\x83 x student(x) \\xe2\\x88\\xa7  pass_exam(x) , which \\nindicates that there is at least one student who has passed the exam. This quantifier \\ngives FOL a lot of power since we can make statements about objects or entities without \\nspecifically naming them. Existential quantifiers are usually used with the conjunction \\n(\\xe2\\x88\\xa7) connective to form rules and statements. You should remember that existential \\nquantifiers are almost never used with the implies (\\xe2\\x86\\x92) connective in statements because \\nthe semantics indicated by it are usually wrong. An example would be \\xe2\\x88\\x83 x student(x) \\n\\xe2\\x86\\x92  knowledgeable(x) , which tells us if you are a student you are knowledgeable\\xe2\\x80\\x94but \\nthe real problem happens if you ask what about those who are not students, are they not \\nknowledgeable? \\n Considering the scope for nesting of quantifiers, ordering of multiple quantifiers \\nmay or may not matter depending on the type of quantifiers used. For multiple universal \\nquantifiers, switching the order does not change the meaning of the statement. This can be \\ndepicted by  ( \\xe2\\x88\\x80 x)( \\xe2\\x88\\x80 y) brother(x,y) \\xe2\\x86\\x94  ( \\xe2\\x88\\x80 y)( \\xe2\\x88\\x80 x) brother(x,y) , where  x and  y are \\nused as variable symbols to indicate two people are brothers to each other irrespective of \\nthe order. Similarly, you can also switch the order of existential quantifiers like  ( \\xe2\\x88\\x83 x)( \\xe2\\x88\\x83 y) \\nF(x,y) \\xe2\\x86\\x94  ( \\xe2\\x88\\x83 y)( \\xe2\\x88\\x83 x) F(x,y) . Switching the order for mixed quantifiers in a sentence \\ndoes matter and changes the interpretation of that sentence. This can be explained more \\nclearly in the following examples, which are very popular in FOL:\\n\\xe2\\x80\\xa2 \\n ( \\xe2\\x88\\x80 x)( \\xe2\\x88\\x83 y) loves(x, y) means that everyone in the world loves \\nat least someone. \\n\\xe2\\x80\\xa2 \\n ( \\xe2\\x88\\x83 y)( \\xe2\\x88\\x80 x) loves(x, y) means that someone is the world is \\nloved by everyone. \\n\\xe2\\x80\\xa2 \\n ( \\xe2\\x88\\x80 y)( \\xe2\\x88\\x83 x) loves(x, y) means that everyone in the world has \\nat least someone who loves them. \\n\\xe2\\x80\\xa2 \\n ( \\xe2\\x88\\x83 x)( \\xe2\\x88\\x80 y) loves(x, y) means that there is at least someone in \\nthe world who loves everyone. \\n From the preceding examples, you can see how the statements almost look the same \\nbut the ordering of  quantifiers change the meanings significantly. There are also several \\nother properties showing the relationship between the quantifiers. Some of the popular \\nquantifier identities and properties are as follows:\\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n36\\n\\xe2\\x80\\xa2 \\n ( \\xe2\\x88\\x80 x) \\xc2\\xac F(x) \\xe2\\x86\\x94 \\xc2\\xac ( \\xe2\\x88\\x83 x) F(x) \\n\\xe2\\x80\\xa2 \\n \\xc2\\xac ( \\xe2\\x88\\x80 x) F(x) \\xe2\\x86\\x94  ( \\xe2\\x88\\x83 x) \\xc2\\xac F(x) \\n\\xe2\\x80\\xa2 \\n ( \\xe2\\x88\\x80 x) F(x) \\xe2\\x86\\x94 \\xc2\\xac  ( \\xe2\\x88\\x83 x) \\xc2\\xac F(x) \\n\\xe2\\x80\\xa2 \\n ( \\xe2\\x88\\x83 x) F(x) \\xe2\\x86\\x94 \\xc2\\xac ( \\xe2\\x88\\x80 x) \\xc2\\xac F(x) \\n\\xe2\\x80\\xa2 \\n ( \\xe2\\x88\\x80 x) (P(x) \\xe2\\x88\\xa7  Q(x)) \\xe2\\x86\\x94 \\xe2\\x88\\x80 x P(x) \\xe2\\x88\\xa7 \\xe2\\x88\\x80 x Q(x) \\n\\xe2\\x80\\xa2 \\n ( \\xe2\\x88\\x83 x) (P(x) \\xe2\\x88\\xa8  Q(x)) \\xe2\\x86\\x94 \\xe2\\x88\\x83 x P(x) \\xe2\\x88\\xa8 \\xe2\\x88\\x83 x Q(x) \\n There are a couple of other important concepts for transformation rules in predicate \\nlogic. These include  instantiation and  generalization. Universal instantiation , also known \\nas  universal elimination , is a rule of inference involving the universal quantifier. It tells us \\nthat if  ( \\xe2\\x88\\x80 x) F(x) is  True , then  F(C) is  True where  C is any constant term that is present \\nin the domain of  x . The variable symbol here can be replaced by any ground term. An \\nexample depicting this would be  ( \\xe2\\x88\\x80 x) drinks(John, x) \\xe2\\x86\\x92  drinks(John, Water) . \\n Universal generalization , also known as  universal introduction , is the inference \\nrule that tells us that if  F(A) \\xe2\\x88\\xa7  F(B) \\xe2\\x88\\xa7  F(C) \\xe2\\x88\\xa7 \\xe2\\x80\\xa6 so on hold  True , then we can infer that \\n ( \\xe2\\x88\\x80 x) F(x) holds  True .  Existential instantiation , also known as  existential elimination , \\nis an inference rule involving the existential quantifier. It tells us that if the given \\nrepresentation  ( \\xe2\\x88\\x83 x) F(x) exists, we can infer  F(C) for a new constant or variable symbol \\n C . This is assuming that the constant or variable term  C introduced in this rule should be \\na brand new constant that has not occurred previously in this proof or in our complete \\nexisting knowledge base. This process is also known as  skolemization , and the constant \\n C is known as the  skolem constant. Existential generalization , also known as  existential \\nintroduction , is the inference rule that tells us that assuming  F(C) to be  True where  C \\nis a constant term, we can then infer  ( \\xe2\\x88\\x83 x) F(x) from it. This can be depicted by the \\nrepresentation  eats_fish(Cat) \\xe2\\x86\\x92  ( \\xe2\\x88\\x83 x) eats_fish(x) , which can be translated as  Cats \\neat fish, therefore there exists something or someone at least who eats fish . \\n We will now look at some examples of how FOL is used to represent natural language \\nstatements and vice versa. The examples in Table\\xc2\\xa0 1-3 depict some of the typical usage of \\nFOL for representing natural language statements.  \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n37\\n This gives us a good idea about the various components of FOL and the utility and \\nadvantages it gives us over PL. But FOL has its own limitation also. By nature, it allows us \\nto quantify over variables and objects but not properties or relations.  Higher order logic \\n(HOL)  allows us to quantify over relations, predicates, and functions. More specifically, \\nsecond order logic enables us to quantify over predicates and functions and third order \\nlogic enables us to quantify over predicates of predicates. While they are more expressive, \\nit is extremely difficult to determine the validity of all sentences in  HOL  .    \\n Text Corpora \\n Text corpora is the plural form of  text corpus . Text corpora are large and structured \\n collection  of texts or textual data, usually consisting of bodies of written or spoken text, \\noften stored in electronic form. This includes converting old historic text corpora from \\nphysical to electronic form so that it can be analyzed and processed with ease. The \\nprimary purpose of text corpora is to leverage them for linguistic as well as statistical \\nanalysis and to use them as data for building NLP tools.   Monolingual  corpora consist \\nof textual data in only one language, and  multilingual  corpora  consist of textual data in \\nmultiple languages. \\n Table 1-3.  Representation of  Natural Language Statements Using First Order Logic \\n Sl No.  FOL Representation \\n Natural Language Statement \\n 1 \\n \\xc2\\xac eats(John, fish) \\n John does not eat fish \\n 2 \\n is_hot(pie) \\xe2\\x88\\xa7  is_delicious(pie) \\n The pie is hot and delicious \\n 3 \\n is_hot(pie) \\xe2\\x88\\xa8  is_delicious(pie) \\n The pie is either hot or delicious \\n 4 \\n study(John, exam) \\xe2\\x86\\x92  pass(John, exam)  If John studies for the exam, he will \\npass the exam \\n 5 \\n \\xe2\\x88\\x80 x student(x) \\xe2\\x86\\x92  pass(x, exam) \\n All students passed the exam \\n 6 \\n \\xe2\\x88\\x83 x student(x) \\xe2\\x88\\xa7  fail(x, exam) \\n There is at least one student who \\nfailed the exam \\n 7 \\n ( \\xe2\\x88\\x83 x student(x) \\xe2\\x88\\xa7  fail(x, exam) \\xe2\\x88\\xa7 \\n ( \\xe2\\x88\\x80 y fail(y, exam) \\xe2\\x86\\x92  x=y)) \\n There was exactly one student who \\nfailed the exam \\n 8 \\n \\xe2\\x88\\x80 x (spider(x) \\xe2\\x88\\xa7  black_widow(x)) \\xe2\\x86\\x92 \\n poisonous(x) \\n All black widow spiders are \\npoisonous \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n38\\n To understand the significance of text corpora, it helps to understand the  origins   \\nof corpora and the reason behind it. It all started with the emergence of linguistics and \\npeople collecting data related to language to study its properties and structure. During \\nthe 1950s, statistical and quantitative methods were used to analyze collected data. \\nBut this endeavor soon reached a dead end due to the lack of large amounts of textual \\ndata over which statistical methods could be effectively applied. Besides that, cognitive \\nlearning and behavioral sciences gained a lot of focus. This empowered eminent linguist \\nNoam Chomsky to build and formulate a sophisticated rule-based language model that \\nformed the basis for building, annotating, and analyzing large scale text corpora. \\n Corpora  Annotation and Utilities \\n Text corpora are annotated with rich metadata, which is extremely useful for getting \\nvaluable insights when utilizing the corpora for NLP and text analytics. Popular \\nannotations for text corpora include tagging  parts of speech (POS)   tags, word stems, \\nlemmas, and many more. Here are some of the most used methods and techniques for \\nannotating text corpora:\\n\\xe2\\x80\\xa2 \\n POS tagging : This is mainly used to annotate each word with a \\nPOS tag indicating the part of speech associated with it.  \\n\\xe2\\x80\\xa2 \\n Word stems : A  stem for a word is a part of the word to which \\nvarious affixes can be attached. \\n\\xe2\\x80\\xa2 \\n Word lemmas : A  lemma is the canonical or base form for a set of \\nwords and is also known as the  head word . \\n\\xe2\\x80\\xa2 \\n Dependency grammar : This includes finding out the various \\nrelationships among the components in sentences and \\nannotating the dependencies. \\n\\xe2\\x80\\xa2 \\n Constituency grammar : This is used to add syntactic annotation \\nto sentences based on their constituents including phrases and \\nclauses. \\n\\xe2\\x80\\xa2 \\n Semantic types and roles : The various constituents of sentences \\nincluding words and phrases are annotated with specific \\nsemantic types and roles, often obtained from an ontology, which \\nindicates what they do. These include things like place, person, \\ntime, organization, agent, recipient, theme, and so forth.    \\n Advanced forms of annotations include adding syntactic and semantic structure \\nfor text. These are dependency and constituency grammar\\xe2\\x80\\x93based parse trees. These \\nspecialized corpora, also known as  treebanks , are extensively used in building POS \\ntaggers, syntax, and semantic parsers. Corpora are also used extensively by linguists for \\ncreating new dictionaries and grammars. Properties like  concordance ,  collocations , and \\n frequency counts  enable them to find out lexical information, patterns, morphosyntactic \\ninformation, and language learning. Besides linguistics, corpora are widely used in \\ndeveloping NLP tools like text taggers, speech recognition, machine translation, spelling \\nand grammar checkers, text-to-speech and speech-to-text synthesizers, information \\nretrieval, entity recognition, and knowledge  extraction . \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n39\\n Popular Corpora \\n Several popular  resources for text corpora have been built and have evolved over time. \\nThis section lists some of the most famous and popular corpora to whet your appetite. \\nYou can research and find out more details about the text corpora that catch your eye. \\nHere are some popular text corpora built over time:\\n\\xe2\\x80\\xa2 \\n Key Word in Context :  KWIC was a methodology invented in the \\n1860s but used extensively around the 1950s by linguists to index \\ndocuments and create corpora of concordances. \\n\\xe2\\x80\\xa2 \\n Brown Corpus : This was the first million-word corpus for the \\nEnglish language, published by Kucera and Francis in 1961, also \\nknown as \\xe2\\x80\\x9cA Standard Corpus of Present-Day American English.\\xe2\\x80\\x9d \\nThis corpus consists of text from a wide variety of sources and \\ncategories. \\n\\xe2\\x80\\xa2 \\n LOB Corpus : The Lancaster-Oslo-Bergen (LOB) corpus was \\ncompiled in the 1970s as a result of collaboration between the \\nUniversity of Lancaster, the University of Oslo, and the Norwegian \\nComputing Centre for the Humanities, Bergen. The main \\nmotivation of this project was to provide a British counterpart \\nto the Brown corpus. This corpus is also a million-word corpus \\nconsisting of text from a wide variety of sources and  categories . \\n\\xe2\\x80\\xa2 \\n Collins Corpus : The Collins Birmingham University International \\nLanguage Database (COBUILD), set up in 1980 at the University \\nof Birmingham and funded by the Collins publishers, built a large \\nelectronic corpus of contemporary text in the English language \\nthat also paved the way for future corpora like the Bank of English \\nand the Collins COBUILD English Language Dictionary.  \\n\\xe2\\x80\\xa2 \\n CHILDES :  The Child Language Data Exchange System (CHILDES) \\nis a corpus that was created by Brian and Catherine in 1984 that \\nserves as a repository for language acquisition data, including \\ntranscripts, audio and video in 26 languages from over 130 \\ndifferent corpora. This has been merged with a larger corpus \\nTalkbank recently. It is used extensively for analyzing the \\nlanguage and speech of young children.  \\n\\xe2\\x80\\xa2 \\n WordNet : This corpus is a semantic-oriented lexical database for \\nthe English language. It was created at Princeton University in 1985 \\nunder the supervision of George Armitage. The corpus consists \\nof words and synonym sets (synsets). Besides these, it consists of \\nword definitions, relationships, and examples of using words and \\nsynsets. Overall, it is a combination of a dictionary and a thesaurus.  \\n\\xe2\\x80\\xa2 \\n Penn Treebank : This corpus consists of tagged and parsed English \\nsentences including annotations like POS tags and grammar-based \\nparse trees typically found in treebanks. It can be also defined \\nas a bank of linguistic trees and was created in the University of \\nPennsylvania, hence the name Penn Treebank.      \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n40\\n\\xe2\\x80\\xa2 \\n BNC :  The British National Corpus (BNC) is one of the largest \\nEnglish corpora, consisting of over 100 million words of both \\nwritten and spoken text samples from a wide variety of sources. \\nThis corpus is a representative sample of written and spoken \\nBritish English of the late 20th century. \\n\\xe2\\x80\\xa2 \\n ANC  :  The American National Corpus (ANC)   is a large text corpus \\nin American English that consists of over 22 million words of both \\nspoken and written text samples since the 1990s. It includes data \\nfrom a wide variety of sources, including emerging sources like \\nemail, tweets, and web information not present in the BNC. \\n\\xe2\\x80\\xa2 \\n COCA :  The Corpus of Contemporary American English (COCA) \\nis the largest text corpus in American English and consists of over \\n450 million words, including spoken transcripts and written text \\nfrom various categories and sources. \\n\\xe2\\x80\\xa2 \\n Google N-gram Corpus : The  Google N-gram Corpus   consists \\nof over a trillion words from various sources including books, \\nweb pages, and so on. The corpus consists of n-gram files up to \\n5-grams for each language. \\n\\xe2\\x80\\xa2 \\n Reuters  Corpus : This corpus is a collection of Reuters news articles \\nand stories released in 2000 specifically for carrying out research \\nin NLP and machine learning. \\n\\xe2\\x80\\xa2 \\n Web, chat, email, tweets : These are entirely new forms of text \\ncorpora that have sprung up into prominence with the rise of \\nsocial media. They are obtainable on the Web from various \\nsources including Twitter, Facebook, chat rooms, and so on. \\n This gives us an idea of some of the most popular text corpora and also how they \\nhave evolved over time. The next section talks about how to access some of these text \\ncorpora with the help of Python and the Natural Language Toolkit (nltk) platform. \\n Accessing Text Corpora \\n We already have an idea about what constitutes a text corpus and have looked at a list \\nof several popular text corpora that exist today. In this section, we will be leveraging \\nPython and the  Natural Language Toolkit  NLTK to interface and access some of these text \\ncorpora. The next chapter talks more about Python and NLTK, so don\\xe2\\x80\\x99t worry if some \\nof the syntax or code seems overwhelming right now. The main intent of this section is \\nto give an idea of how you can access and utilize text corpora easily for your NLP and \\nanalytics needs. \\n I will be using the  ipython shell (  https://ipython.org  ) for running Python code \\nwhich provides a powerful interactive shell for running code as well as viewing charts \\nand plots. We will also be using the NLTK library. You can find out more details about \\nthis project at    www.nltk.org  , which is all about NLTK being a complete platform and \\nframework for accessing text resources, including corpora and libraries for various NLP \\nand machine learning capabilities. \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n41\\n To start with, make sure you have Python installed. You can install Python separately \\nor download the popular Anaconda Python distribution from Continuum Analytics from \\n   www.continuum.io/downloads  . That version comes with a complete suite of analytics \\npackages, including NLTK. If you want to know more about Python and what distribution \\nwould be best suited for you, Chapter   2 covers these topics in more detail. \\n Assuming you have Python installed now, if you installed the Anaconda distribution, \\nyou will already have NLTK installed. Note that we will be using Python 2.7 in this book, \\nbut you are welcome to use the latest version of Python\\xe2\\x80\\x94barring a few syntax changes, \\nmost of the code should be reproducible in the latest edition of Python. If you did not \\ninstall the Anaconda distribution but have Python installed, you can open your terminal \\nor command prompt and run the following command to install NLTK. \\n $ pip install nltk \\n  This will install the NLTK library, and you will be ready to use it. However, the default \\ninstallation of NLTK does not include all the components required in this book. To install \\nall the components and resources of NLTK, you can start your Python shell and type the \\nfollowing commands\\xe2\\x80\\x94you will see the various dependencies for  nltk being downloaded; \\na part of the output is shown in the following code snippet: \\n In [1]: import nltk \\n In [2]: nltk.download(\\'all\\') \\n [nltk_data] Downloading collection u\\'all\\' \\n [nltk_data]    |  \\n [nltk_data]    | Downloading package abc to \\n [nltk_data]    |     C:\\\\Users\\\\DIP.DIPSLAPTOP\\\\AppData\\\\Roaming\\\\nltk_data \\n [nltk_data]    |     ... \\n [nltk_data]    |   Package abc is already up-to-date! \\n [nltk_data]    | Downloading package alpino to \\n [nltk_data]    |     C:\\\\Users\\\\DIP.DIPSLAPTOP\\\\AppData\\\\Roaming\\\\nltk_data \\n [nltk_data]    |     ... \\n The preceding command will download all the necessary resources required by \\n NLTK  . If you don\\xe2\\x80\\x99t want to download everything, you can also select the necessary \\ncomponents from a graphical user interface (GUI) using the command  nltk.download() . \\nOnce the necessary dependencies are downloaded, you are now ready to start accessing \\ntext corpora! \\n Accessing the Brown Corpus \\n We have already talked a bit about the  Brown Corpus , developed in 1961 at Brown \\nUniversity. This corpus consists of texts from 500 sources and has been grouped into \\nvarious categories. The following code snippet loads the Brown Corpus into the system \\nmemory and shows the various available categories: \\n In [8]: # load the Brown Corpus \\n In [9]: from nltk.corpus import brown \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n42\\n In [10]: print \\'Total Categories:\\', len(brown.categories()) \\n Total Categories: 15 \\n In [11]: print brown.categories() \\n [u\\'adventure\\', u\\'belles_lettres\\', u\\'editorial\\', u\\'fiction\\', u\\'government\\', \\nu\\'hobbies\\', u\\'humor\\', u\\'learned\\', u\\'lore\\', u\\'mystery\\', u\\'news\\', u\\'religion\\', \\nu\\'reviews\\', u\\'romance\\', u\\'science_fiction\\'] \\n The preceding output tells us that there are a total of 15 categories in the corpus, like \\n news ,  mystery ,  lore , and so on. The following code snippet digs a little deeper into the \\n mystery category of the Brown Corpus: \\n In [19]: # tokenized sentences \\n In [20]: brown.sents(categories=\\'mystery\\') \\n Out[20]: [[u\\'There\\', u\\'were\\', u\\'thirty-eight\\', u\\'patients\\', u\\'on\\', u\\'the\\', \\nu\\'bus\\', u\\'the\\', u\\'morning\\', u\\'I\\', u\\'left\\', u\\'for\\', u\\'Hanover\\', u\\',\\', \\nu\\'most\\', u\\'of\\', u\\'them\\', u\\'disturbed\\', u\\'and\\', u\\'hallucinating\\', u\\'.\\'], \\n[u\\'An\\', u\\'interne\\', u\\',\\', u\\'a\\', u\\'nurse\\', u\\'and\\', u\\'two\\', u\\'attendants\\', \\nu\\'were\\', u\\'in\\', u\\'charge\\', u\\'of\\', u\\'us\\', u\\'.\\'], ...] \\n In [21]: # POS tagged sentences \\n In [22]: brown.tagged_sents(categories=\\'mystery\\') \\n Out[22]: [[(u\\'There\\', u\\'EX\\'), (u\\'were\\', u\\'BED\\'), (u\\'thirty-eight\\', u\\'CD\\'), \\n(u\\'patients\\', u\\'NNS\\'), (u\\'on\\', u\\'IN\\'), (u\\'the\\', u\\'AT\\'), (u\\'bus\\', u\\'NN\\'), \\n(u\\'the\\', u\\'AT\\'), (u\\'morning\\', u\\'NN\\'), (u\\'I\\', u\\'PPSS\\'), (u\\'left\\', u\\'VBD\\'), \\n(u\\'for\\', u\\'IN\\'), (u\\'Hanover\\', u\\'NP\\'), (u\\',\\', u\\',\\'), (u\\'most\\', u\\'AP\\'), \\n(u\\'of\\', u\\'IN\\'), (u\\'them\\', u\\'PPO\\'), (u\\'disturbed\\', u\\'VBN\\'), (u\\'and\\', u\\'CC\\'), \\n(u\\'hallucinating\\', u\\'VBG\\'), (u\\'.\\', u\\'.\\')], [(u\\'An\\', u\\'AT\\'), (u\\'interne\\', \\nu\\'NN\\'), (u\\',\\', u\\',\\'), (u\\'a\\', u\\'AT\\'), (u\\'nurse\\', u\\'NN\\'), (u\\'and\\', u\\'CC\\'), \\n(u\\'two\\', u\\'CD\\'), (u\\'attendants\\', u\\'NNS\\'), (u\\'were\\', u\\'BED\\'), (u\\'in\\', u\\'IN\\'), \\n(u\\'charge\\', u\\'NN\\'), (u\\'of\\', u\\'IN\\'), (u\\'us\\', u\\'PPO\\'), (u\\'.\\', u\\'.\\')], ...] \\n In [28]: # get sentences in natural form \\n In [29]: sentences = brown.sents(categories=\\'mystery\\') \\n In [30]: sentences = [\\' \\'.join(sentence_token) for sentence_token in \\nsentences] \\n In [31]: print sentences[0:5] # printing first 5 sentences \\n [u\\'There were thirty-eight patients on the bus the morning I left for \\nHanover , most of them disturbed and hallucinating .\\', u\\'An interne , a \\nnurse and two attendants were in charge of us .\\', u\"I felt lonely and \\ndepressed as I stared out the bus window at Chicago\\'s grim , dirty West Side \\n.\", u\\'It seemed incredible , as I listened to the monotonous drone of voices \\nand smelled the fetid odors coming from the patients , that technically I \\nwas a ward of the state of Illinois , going to a hospital for the mentally \\nill .\\', u\\'I suddenly thought of Mary Jane Brennan , the way her pretty eyes \\ncould flash with anger , her quiet competence , the gentleness and sweetness \\nthat lay just beneath the surface of her defenses .\\'] \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n43\\n From the preceding  snippet , we can see the written contents of the mystery genre \\nand how the sentences are available in tokenized as well as annotated formats. Suppose \\nwe want to see the top nouns in the  mystery genre? We can use the next code snippet \\nfor obtaining them. Remember that nouns have either an NN or NP in their POS tag to \\nindicate the various forms. Chapter   3 covers POS tags in further detail: \\n In [81]: # get tagged words \\n In [82]: tagged_words = brown.tagged_words(categories=\\'mystery\\') \\n In [83]: # get nouns from tagged words \\n In [84]: nouns = [(word, tag) for word, tag in tagged_words if any(noun_tag \\nin tag for noun_tag in [\\'NP\\', \\'NN\\'])] \\n In [85]: print nouns[0:10] # prints the first 10 nouns \\n [(u\\'patients\\', u\\'NNS\\'), (u\\'bus\\', u\\'NN\\'), (u\\'morning\\', u\\'NN\\'), (u\\'Hanover\\', \\nu\\'NP\\'), (u\\'interne\\', u\\'NN\\'), (u\\'nurse\\', u\\'NN\\'), (u\\'attendants\\', u\\'NNS\\'), \\n(u\\'charge\\', u\\'NN\\'), (u\\'bus\\', u\\'NN\\'), (u\\'window\\', u\\'NN\\')] \\n In [85]: # build frequency distribution for nouns \\n In [86]: nouns_freq = nltk.FreqDist([word for word, tag in nouns]) \\n In [87]: # print top 10 occuring nouns \\n In [88]: print nouns_freq.most_common(10) \\n [(u\\'man\\', 106), (u\\'time\\', 82), (u\\'door\\', 80), (u\\'car\\', 69), (u\\'room\\', 65), \\n(u\\'Mr.\\', 63), (u\\'way\\', 61), (u\\'office\\', 50), (u\\'eyes\\', 48), (u\\'hand\\', 46)] \\n That snippet prints the top ten  nouns that occur the most and includes terms like \\n man ,  time ,  room , and so on. We have used some advanced constructs and techniques like \\nlist comprehensions, iterables, and tuples. The next chapter covers them in further detail, \\nincluding how they work and their main functionality. For now, all you need to know is \\nwe filter out the nouns from all other words based on their POS tags and then compute \\ntheir frequency to get the top occurring nouns in the corpus.  \\n Accessing the Reuters Corpus \\n The  Reuters Corpus consists of 10,788 Reuters news documents from around 90 different \\ncategories and has been grouped into train and test sets. In machine learning terminology, \\n train sets are usually used to train a model, and  test sets are used to test the performance of \\nthat model. The following code snippet shows how to access the data for the Reuters Corpus: \\n In [94]: # load the Reuters Corpus \\n In [95]: from nltk.corpus import reuters \\n In [96]: print \\'Total Categories:\\', len(reuters.categories()) \\n Total Categories: 90 \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n44\\n In [97]: print reuters.categories() \\n [u\\'acq\\', u\\'alum\\', u\\'barley\\', u\\'bop\\', u\\'carcass\\', u\\'castor-oil\\', u\\'cocoa\\', \\nu\\'coconut\\', u\\'coconut-oil\\', u\\'coffee\\', u\\'copper\\', u\\'copra-cake\\', u\\'corn\\', \\nu\\'cotton\\', u\\'cotton-oil\\', u\\'cpi\\', u\\'cpu\\', u\\'crude\\', u\\'dfl\\', u\\'dlr\\', u\\'dmk\\', \\nu\\'earn\\', u\\'fuel\\', u\\'gas\\', ...] \\n In [104]: # get sentences in housing and income categories \\n In [105]: sentences = reuters.sents(categories=[\\'housing\\', \\'income\\']) \\n In [106]: sentences = [\\' \\'.join(sentence_tokens) for sentence_tokens in \\nsentences] \\n In [107]: print sentences[0:5]  # prints the first 5 sentences \\n [u\\xe2\\x80\\x9dYUGOSLAV ECONOMY WORSENED IN 1986 , BANK DATA SHOWS National Bank \\neconomic data for 1986 shows that Yugoslavia \\' s trade deficit grew , the \\ninflation rate rose , wages were sharply higher , the money supply expanded \\nand the value of the dinar fell .\\xe2\\x80\\x9d, u\\'The trade deficit for 1986 was 2 . \\n012 billion dlrs , 25 . 7 pct higher than in 1985 .\\', u\\'The trend continued \\nin the first three months of this year as exports dropped by 17 . 8 pct , \\nin hard currency terms , to 2 . 124 billion dlrs .\\', u\\'Yugoslavia this year \\nstarted quoting trade figures in dinars based on current exchange rates , \\ninstead of dollars based on a fixed exchange rate of 264 . 53 dinars per \\ndollar .\\', u\\xe2\\x80\\x9dYugoslavia \\' s balance of payments surplus with the convertible \\ncurrency area fell to 245 mln dlrs in 1986 from 344 mln in 1985 .\\xe2\\x80\\x9d] \\n In [109]: # fileid based access \\n In [110]: print reuters.fileids(categories=[\\'housing\\', \\'income\\']) \\n [u\\'test/16118\\', u\\'test/18534\\', u\\'test/18540\\', u\\'test/18664\\', u\\'test/18665\\', \\nu\\'test/18672\\', u\\'test/18911\\', u\\'test/19875\\', u\\'test/20106\\', u\\'test/20116\\', \\nu\\'training/1035\\', u\\'training/1036\\', u\\'training/10602\\', ...] \\n In [111]: print reuters.sents(fileids=[u\\'test/16118\\', u\\'test/18534\\']) \\n [[u\\'YUGOSLAV\\', u\\'ECONOMY\\', u\\'WORSENED\\', u\\'IN\\', u\\'1986\\', u\\',\\', u\\'BANK\\', \\nu\\'DATA\\', u\\'SHOWS\\', u\\'National\\', u\\'Bank\\', u\\'economic\\', u\\'data\\', u\\'for\\', \\nu\\'1986\\', u\\'shows\\', u\\'that\\', u\\'Yugoslavia\\', u\\xe2\\x80\\x9d\\'\\xe2\\x80\\x9c, u\\'s\\', u\\'trade\\', u\\'deficit\\', \\nu\\'grew\\', u\\',\\', u\\'the\\', u\\'inflation\\', u\\'rate\\', u\\'rose\\', u\\',\\', u\\'wages\\', \\nu\\'were\\', u\\'sharply\\', u\\'higher\\', u\\',\\', u\\'the\\', u\\'money\\', u\\'supply\\', \\nu\\'expanded\\', u\\'and\\', u\\'the\\', u\\'value\\', u\\'of\\', u\\'the\\', u\\'dinar\\', u\\'fell\\', \\nu\\'.\\'], [u\\'The\\', u\\'trade\\', u\\'deficit\\', u\\'for\\', u\\'1986\\', u\\'was\\', u\\'2\\', u\\'.\\', \\nu\\'012\\', u\\'billion\\', u\\'dlrs\\', u\\',\\', u\\'25\\', u\\'.\\', u\\'7\\', u\\'pct\\', u\\'higher\\', \\nu\\'than\\', u\\'in\\', u\\'1985\\', u\\'.\\'], ...] \\n This gives us an idea of how to access corpora data using both categories as well as \\nfile  identifiers . \\n Accessing the  WordNet Corpus \\n The WordNet corpus is perhaps one of the most used corpora out there because it \\nconsists of a vast corpus of words and semantically linked synsets for each word. We \\nwill explore some of the basic features of the WordNet Corpus here, including synsets \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n45\\nand methods of accessing the corpus data. For more advanced analysis and coverage \\nof WordNet capabilities, see Chapter   7 , which covers synsets, lemmas, hyponyms, \\nhypernyms, and several other concepts covered in the semantics section earlier. The \\nfollowing code snippet should give you an idea about how to access the WordNet corpus \\ndata and synsets: \\n In [113]: # load the Wordnet Corpus \\n In [114]: from nltk.corpus import wordnet as wn \\n In [127]: word = \\'hike\\' # taking hike as our word of interest \\n In [128]: # get word synsets \\n In [129]: word_synsets = wn.synsets(word) \\n In [130]: print word_synsets \\n [Synset(\\'hike.n.01\\'), Synset(\\'rise.n.09\\'), Synset(\\'raise.n.01\\'), \\nSynset(\\'hike.v.01\\'), Synset(\\'hike.v.02\\')] \\n In [132]: # get details for each synonym in synset \\n     ...: for synset in word_synsets: \\n     ...:     print \\'Synset Name:\\', synset.name() \\n     ...:     print \\'POS Tag:\\', synset.pos() \\n     ...:     print \\'Definition:\\', synset.definition() \\n     ...:     print \\'Examples:\\', synset.examples() \\n     ...:     print \\n     ...:      \\n Synset Name: hike.n.01 \\n POS Tag: n \\n Definition: a long walk usually for exercise or pleasure \\n Examples: [u\\'she enjoys a hike in her spare time\\'] \\n Synset Name: rise.n.09 \\n POS Tag: n \\n Definition: an increase in cost \\n Examples: [u\\'they asked for a 10% rise in rates\\'] \\n Synset Name: raise.n.01 \\n POS Tag: n \\n Definition: the amount a salary is increased \\n Examples: [u\\'he got a 3% raise\\', u\\'he got a wage hike\\'] \\n Synset Name: hike.v.01 \\n POS Tag: v \\n Definition: increase \\n Examples: [u\\'The landlord hiked up the rents\\'] \\n Synset Name: hike.v.02 \\n POS Tag: v \\n Definition: walk a long way, as for pleasure or physical exercise \\n Examples: [u\\'We were hiking in Colorado\\', u\\'hike the Rockies\\'] \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n46\\n The preceding code snippet depicts an interesting example with the word  hike \\nand its synsets, which include synonyms that are nouns as well as verbs having distinct \\nmeanings. WordNet makes it easy to semantically link words together with their \\nsynonyms as well as easily retrieve meanings and examples for various words. The \\npreceding example tells us that  hike can mean a long walk as well as an increase in price \\nfor salary or rent. Feel free to experiment with different words and find out their synsets, \\ndefinitions, examples, and relationships. \\n Besides these popular corpora, there are a vast number of text corpora available that \\nyou can check and access with the  nltk.corpus module. Thus, you can see how easy it is \\nto access and use data from any text corpus with the help of Python and NLTK. \\n This brings us to the end of our discussion about text corpora. The following sections \\ncover some ground regarding NLP and text analytics. \\n Natural Language Processing \\n I\\xe2\\x80\\x99ve mentioned the term  natural language processing (NLP) several times in this chapter. \\nBy now, you may have formed some idea about what NLP means. NLP is  defined as \\na specialized field of computer science and engineering and artificial intelligence \\nwith roots in computational linguistics. It is primarily concerned with designing and \\nbuilding applications and systems that enable interaction between machines and \\nnatural languages evolved for use by humans. This also makes NLP related to the area of \\nHuman-Computer Interaction ( HCI ) .  NLP techniques enable computers to process and \\nunderstand natural human language and utilize it further to provide useful output. Next, \\nwe will be talking about some of the main applications of NLP. \\n Machine Translation \\n Machine translation  is perhaps one of the most coveted and sought-after applications \\nfor NLP. It is defined as the technique that helps in providing syntactic, grammatical, \\nand semantically correct translation between any two pair of languages. It was perhaps \\nthe first major area of research and development in NLP. On a simple level, machine \\ntranslation is the translation of natural language carried out by a machine. By default, the \\nbasic building blocks for the machine translation process involve simple substitution of \\nwords from one language to another, but in that case we ignore things like grammar and \\nphrasal structure consistency. Hence, more sophisticated techniques have evolved over a \\nperiod of time, including combining large resources of text corpora along with statistical \\nand linguistic techniques. One of the most popular machine translation systems is Google \\nTranslate. Figure\\xc2\\xa0 1-19 shows a successful machine translation operation executed by \\nGoogle  Translate for the sentence  What is the fare to the airport? from English to Italian. \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n47\\n Over time, machine translation systems are getting better providing translations in \\nreal time as you speak or write into the application. \\n Speech  Recognition Systems \\n This is perhaps the most difficult application for NLP. Perhaps the most difficult test of \\nintelligence in artificial intelligence systems is the Turing Test. This test is defined as a \\ntest of intelligence for a computer. A question is posed to a computer and a human, and \\nthe test is passed if it is impossible to say which of the answers given was given by the \\nhuman. Over time, a lot of progress has been made in this area by using techniques like \\nspeech synthesis, analysis, syntactic parsing, and contextual reasoning. But one chief \\nlimitation for speech recognition systems still remains: They are very domain specific and \\nwill not work if the user strays even a little bit from the expected scripted inputs needed \\nby the system. Speech-recognition systems are now found in many places, from desktop \\ncomputers to mobile phones to virtual assistance systems. \\n     Question Answering Systems \\n  Question Answering Systems (QAS)   are built upon the principle of Question Answering, \\nbased on using techniques from NLP and information retrieval (IR).  QAS is primarily \\nconcerned with building robust and scalable systems that provide answers to questions \\ngiven by users in natural language form. Imagine being in a foreign country, asking a \\nquestion to your personalized assistant in your phone in pure natural language, and \\ngetting a similar response from it. This is the ideal state toward which researchers and \\ntechnologists are working. Some success in this field has been achieved with personalized \\nassistants like Siri and Cortana, but their scope is still limited because they understand \\nonly a subset of key clauses and phrases in the entire human natural language. \\n Figure 1-19.  Machine translation performed by Google Translate \\n \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n48\\n To build a successful QAS, you need a huge knowledgebase consisting of data about \\nvarious domains. Efficient querying systems into this knowledgebase would be leveraged \\nby the QAS to provide answers to questions in natural language form. Creating and \\nmaintaining a queryable vast knowledgebase is extremely difficult\\xe2\\x80\\x94hence, you find the \\nrise of QAS in niche domains like food, healthcare, e-commerce, and so on. Chatbots are \\none emerging trend that makes extensive use of QAS. \\n      Contextual Recognition and Resolution   \\n This covers a wide area in understanding natural language and includes both syntactic \\nand semantic-based reasoning.  Word sense disambiguation  is a popular application, \\nwhere we want to find out the contextual sense of a word in a given sentence. Consider \\nthe word  book . It can mean  an object containing knowledge and information when used \\nas a noun, and it can also mean  to reserve a seat or a table when used as a verb. Detecting \\nthese differences in sentences based on context is the main premise of word sense \\ndisambiguation\\xe2\\x80\\x94a daunting task covered in Chapter   7 . \\n Coreference resolution is another problem in linguistics NLP is trying to address. By \\ndefinition, coreference is said to occur when two or more terms or expressions in a body \\nof text refer to the same entity. Then they are said to have the same  referent . Consider \\n John just told me that he is going to the exam hall . In this sentence, the pronoun  he  has the \\nreferent  John . Resolving such pronouns is a part of coreference resolution, and it becomes \\nchallenging once we have multiple referents in a body of text. For example,  John just talked \\nwith Jim. He told me we have a surprise test tomorrow . In this body of text, the pronoun  he  \\ncould refer to either  John or  Jim , thus making pinpointing the exact referent difficult.  \\n Text Summarization \\n The main aim of  text summarization is to take a corpus of text documents\\xe2\\x80\\x94which could \\nbe a collection of texts, paragraphs, or sentences\\xe2\\x80\\x94and reducing the content appropriately \\nto create a summary that retains the key points of the collection. Summarization can \\nbe carried out by looking at the various documents and trying to find out the keywords, \\nphrases, and sentences that have an important prominence in the whole collection. Two \\nmain types of techniques for text summarization include  extraction-based summarization \\nand  abstraction-based summarization . With the advent of huge amounts of text and \\nunstructured data, the need for text summarization in getting to valuable insights quickly \\nis in great demand. \\n Text-summarization systems usually perform two main types of operations. The first \\nis  generic summarization , which tries to provide a generic summary of the collection of \\ndocuments under analysis. The second type of operation is  query-based summarization , \\nwhich provides query-relevant text summaries where the corpus is filtered further based \\non specific queries, relevant keywords and phrases are extracted relevant to the query, \\nand the summary is constructed. Chapter   5 covers this in detail . \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n49\\n Text Categorization \\n The main aim of  text categorization is identifying to which category or class a specific \\ndocument should be placed based on the contents of the document. This is one of the \\nmost popular applications of NLP and machine learning because with the right data, it \\nis extremely simple to understand the principles behind its internals and implement a \\nworking text categorization system. Both supervised and unsupervised machine learning \\ntechniques can be used in solving this problem, and sometimes a combination of both is \\nused. This has helped build a lot of successful and practical applications, including spam \\nfilters and news article categorization. We will be building our own text categorization \\nsystem in Chapter   4 . \\n Text Analytics  \\n As mentioned before, with the advent of huge amounts of computing power, unstructured \\ndata, and success with machine learning and statistical analysis techniques, it wasn\\xe2\\x80\\x99t long \\nbefore text analytics started garnering a lot of attention. However, text analytics poses \\nsome challenges compared to regular analytical methods. Free-flowing text is highly \\nunstructured and rarely follows any specific pattern\\xe2\\x80\\x94like weather data or structured \\nattributes in relational databases. Hence, standard statistical methods aren\\xe2\\x80\\x99t helpful when \\napplied out of the box on unstructured text data. This section covers some of the main \\nconcepts in text analytics and also discusses the definition and scope of text analytics, \\nwhich will give you a broad idea of what you can expect in the upcoming chapters. \\n Text analytics , also known as  text mining , is the methodology and process followed \\nto derive quality and actionable information and insights from textual data. This involves \\nusing NLP, information retrieval, and machine learning techniques to parse unstructured \\ntext data into more structured forms and deriving patterns and insights from this data \\nthat would be helpful for the end user. Text analytics comprises a collection of machine \\nlearning, linguistic, and statistical techniques that are used to model and extract \\ninformation from text primarily for analysis needs, including business intelligence, \\nexploratory, descriptive, and predictive analysis. Here are some of the main techniques \\nand operations in text analytics:.\\n\\xe2\\x80\\xa2 \\n Text classification \\n\\xe2\\x80\\xa2 \\n Text clustering \\n\\xe2\\x80\\xa2 \\n Text summarization \\n\\xe2\\x80\\xa2 \\n Sentiment analysis \\n\\xe2\\x80\\xa2 \\n Entity extraction and recognition \\n\\xe2\\x80\\xa2 \\n Similarity analysis and relation modeling \\nCHAPTER 1 \\xe2\\x96\\xa0 NATURAL LANGUAGE BASICS\\n50\\n Doing text analytics is sometimes a more involved process than normal statistical \\nanalysis or machine learning. Before applying any learning technique or algorithm, you \\nhave to convert the unstructured text data into a format acceptable by those algorithms. \\nBy definition, a body of text under analysis is often a document, and by applying various \\ntechniques we usually convert this document to a vector of words, which is a numeric \\narray whose values are specific weights for each word that could either be its frequency, \\nits occurrence, or various other depictions\\xe2\\x80\\x94some of which we will explore in Chapter   3 . \\nOften the text needs to be cleaned and processed to remove noisy terms and data, called \\n text pre-processing . \\n Once we have the data in a machine-readable and understandable format, we can \\napply relevant algorithms based on the problem to be solved at hand. The applications of \\ntext analytics are manifold. Some of the most popular ones include the following:\\n\\xe2\\x80\\xa2 \\n Spam detection \\n\\xe2\\x80\\xa2 \\n News articles categorization \\n\\xe2\\x80\\xa2 \\n Social media analysis and monitoring \\n\\xe2\\x80\\xa2 \\n Bio-medical \\n\\xe2\\x80\\xa2 \\n Security intelligence \\n\\xe2\\x80\\xa2 \\n Marketing and CRM \\n\\xe2\\x80\\xa2 \\n Sentiment analysis \\n\\xe2\\x80\\xa2 \\n Ad placements \\n\\xe2\\x80\\xa2 \\n Chatbots \\n\\xe2\\x80\\xa2 \\n Virtual  assistants \\n Summary \\n Congratulations on sticking it out till the end of this long chapter! We have started on our \\njourney of text analytics with Python by taking a trip into the world of natural language \\nand the various concepts and domains surrounding it. You now have a good idea of what \\nnatural language means and its significance in our world. You have also seen concepts \\nregarding the philosophy of language and language acquisition and usage. The field of \\nlinguistics was also touched on, providing a flavor of the origins of language studies and \\nhow they have been evolving over time. We covered language syntax and semantics in \\ndetail, including the essential concepts with interesting examples to easily understand \\nthem. We also talked about resources for natural language, namely text corpora, and also \\nlooked at some practical examples with code regarding how to interface and access corpora \\nusing Python and NTLK. The chapter concluded with a discussion about the various \\nfacets of NLP and text analytics. In the next chapter, we will talk about using Python for text \\nanalytics. We will touch on setting up your Python development environment, the various \\nconstructs of Python, and how to use it for text processing. We will also look at some of the \\npopular libraries, frameworks, and platforms we will be using in this book.  \\n51\\n\\xc2\\xa9 Dipanjan Sarkar 2016 \\nD. Sarkar, Text Analytics with Python, DOI 10.1007/978-1-4842-2388-8_2\\n CHAPTER 2  \\n Python Refresher \\n In the previous chapter, we took a journey into the world of natural language and explored \\nseveral interesting concepts and areas associated with it. We now have a better understanding \\nof the entire scope surrounding  natural language processing (NLP)  , linguistics, and text \\nanalytics. If you refresh your memory, we had also got our first taste of running Python  code to \\naccess and use text corpora resources with the help of the NLTK platform. \\n In this chapter, we will cover a lot of ground with regard to the core components and \\nfunctionality of Python as well as some of the important libraries and frameworks associated \\nwith NLP and text analytics. This chapter is aimed to be a refresher for Python and for \\nproviding the initial building blocks essential to get started with text analytics. This book \\nassumes you have some knowledge of Python or any other  programming language . If you are \\na Python practitioner, you can skim through the chapter, since the content here starts with \\nsetting up your Python development environment and moves on to the basics of Python. \\n Our main focus in the chapter will be exploring how  text data is handled in Python, \\nincluding data types and functions associated with it. However, we will also be covering \\nseveral advanced concepts in Python, including list comprehensions, generators, and \\ndecorators, which make your life easier in developing and writing quality and reusable \\ncode. This chapter follows a more  hands-on approach than the previous chapter, and we \\nwill cover various concepts with practical examples. \\n Getting to Know Python \\n Before we can dive into the Python ecosystem and look at the various components \\nassociated with it, we must look back at the origins and philosophy behind Python \\nand see how it has evolved over time to be the choice of language powering many \\napplications, servers, and systems today. Python is a high-level open source general-\\npurpose  programming language widely used as a scripting and across different domains. \\nThe brainchild of Guido Van Rossum, Python was conceived in the late 1980s as a \\nsuccessor to the  ABC language , and both were developed at the  Centrum Wiskunde and \\nInformatica (CWI) , Netherlands. Python was originally designed to be a scripting and \\ninterpreted language, and to this day it is still one of the most popular scripting languages \\nout there. But with  object-oriented programming (OOP) and constructs, you can use \\nit just like any other object-oriented language, such as Java. The name  Python , coined \\nby Guido for the language, does not refer to the snake but the hit comedy show  Monty \\nPython\\xe2\\x80\\x99s Flying Circus , since he was a big fan. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n52\\n As mentioned, Python is a general-purpose programming language that supports \\nmultiple programming paradigms, including the following popular  programming \\nparadigms :\\n\\xe2\\x80\\xa2 \\n Object-oriented programming \\n\\xe2\\x80\\xa2 \\n Functional programming \\n\\xe2\\x80\\xa2 \\n Procedural programming \\n\\xe2\\x80\\xa2 \\n Aspect-oriented programming \\n A lot of OOP concepts are present in Python, including classes, objects, data, and \\nmethods.  Principles  like abstraction, encapsulation, inheritance, and polymorphism can \\nalso be implemented and exhibited using Python. There are several advanced features \\nin Python, including iterators, generators, list comprehensions, lambda expressions, and \\nseveral modules like  itertools and  functools , which provide the ability to write code \\nfollowing the functional programming paradigm. \\n Python was designed keeping in mind the fact that simple and beautiful code is \\nmore elegant and easy to use rather than doing premature optimization and writing \\nhard-to-interpret code. Python\\xe2\\x80\\x99s standard libraries are power-packed with a wide variety \\nof capabilities and features ranging from low-level hardware interfacing to handling \\nfiles and working with text data. Easy extensibility and integration was considered when \\ndeveloping Python such that it can be easily integrated with existing applications\\xe2\\x80\\x94rich \\n application programming interfaces (APIs) can even be created to provide interfaces to \\nother applications and tools. \\n Python offers a lot of  advantages and benefits . Here are some of the major benefits:\\n\\xe2\\x80\\xa2 \\n Friendly and easy to learn : The Python programming language is \\nextremely easy to understand and learn. Schools are starting to \\npick up Python as the language of choice to teach kids to code. \\nThe learning curve is not very steep, and you can do a lot of fun \\nthings in Python, from building games to automating things \\nlike reading and sending email. (In fact, there is a popular book \\nand website dedicated to \\xe2\\x80\\x9cautomating the boring stuff\\xe2\\x80\\x9d using \\nPython at   https://automatetheboringstuff.com .) Python also \\nhas a thriving and helpful developer community, which makes \\nsure there is a ton of helpful resources and documentation out \\nthere on the Internet. The community also organizes various \\nworkshops and conferences throughout the world. \\n\\xe2\\x80\\xa2 \\n High-level abstractions : Python is a  high-level language (HLL) , \\nwhere a lot of the heavy lifting needed by writing low level code is \\neliminated by high-level abstractions. Python has a sharp focus \\non code simplicity and extensibility, and you can perform various \\noperations, simple or complex, in fewer lines of code than other \\ntraditional compiled languages like C++ and C. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n53\\n\\xe2\\x80\\xa2 \\n Boosts productivity : Python boosts productivity by reducing time \\ntaken to develop, run, debug, deploy, and maintain large codebases \\ncompared to other languages like Java, C++, and C. Large programs \\nof more than a 100 lines can be reduced to 20 lines or less on average \\nby porting them to Python. High-level abstractions help developers \\nfocus on the problem to be solved at hand rather than worry about \\nlanguage-specific nuances. The hindrance of compiling and linking \\nis also bypassed with Python. Hence, Python is often the choice of \\nlanguage especially when rapid prototyping and development are \\nessential for solving an important problem in little time.  \\n\\xe2\\x80\\xa2 \\n Complete  robust ecosystem : One of the main advantages of Python \\nis that it is a multipurpose programming language that can be \\nused for just about anything! From web applications to intelligent \\nsystems, Python powers a wide variety of applications and systems. \\nWe will talk about some of them later in this chapter. Besides being \\na multipurpose language, the wide variety of frameworks, libraries, \\nand platforms that have been developed by using Python and to \\nbe used for Python form a complete robust ecosystem around \\nPython. These libraries make life easier by giving us a wide variety of \\ncapabilities and functionality to perform various tasks with minimal \\ncode. Some examples would be libraries for handling databases, text \\ndata, machine learning, signal processing, image processing, deep \\nlearning, artificial intelligence\\xe2\\x80\\x94and the list goes on.  \\n\\xe2\\x80\\xa2 \\n Open source : As open source, Python is actively developed and \\nupdated constantly with improvements, optimizations, and new \\nfeatures. Now the Python Software Foundation (PSF) owns all \\nPython-related intellectual property (IP) and administers all \\nlicense-related issues. Being open source has boosted the Python \\necosystem with almost all of its libraries also being open source, to \\nwhich anyone can share, contribute, and suggest improvements \\nand feedback. This helps foster healthy collaboration among \\ntechnologists, engineers, researchers, and developers.  \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n54\\n\\xe2\\x80\\xa2 \\n Easy to port, integrate, and deploy : Python is supported on \\nall major operating systems (OS), including Linux, Windows, \\nand macOS. Code written in one OS can easily be ported into \\nanother OS by simply copying the code files, and they will work \\nseamlessly. Python can also be easily integrated and extended \\nwith existing applications and can interface with various APIs \\nand devices using sockets, networks, and ports. Python can be \\nused to invoke code for other languages, and there are Python \\nbindings for invoking Python code from other languages. This \\nhelps in easy integration of Python code wherever necessary. \\nThe most important advantage, though, is that it is very easy to \\ndevelop Python code and deploy it no matter how complex your \\ncodebase might be. If you follow the right  continuous integration \\n(CI) processes and manage your Python codebase properly, \\ndeployment usually involves updating your latest code and \\nstarting the necessary processes in your production environment. \\nIt is extremely easy to get proper working code in minimal time, \\nwhich is often difficult to do with other languages.    \\n All these features coupled with rapid strides in the application of Python in various \\nwidespread domains over the years have made Python extremely popular. Such has been \\nthe case that if the proper Python principles of simplicity, elegance, and minimalism are \\nnot followed when writing code, the code is said to be not \\xe2\\x80\\x9cpythonic.\\xe2\\x80\\x9d There is a known \\nstyle and convention around writing good Python code, and lots of articles and books \\nteach how to write pythonic code. Active users and developers in the Python community \\ncall themselves Pythonistas, Pythoneers, and many more interesting names. The thriving \\nPython community makes the language all the more exciting since Python and its entire \\necosystem is always under active improvement and development. \\n The Zen of Python \\n You may be wondering what on earth the  Zen of Python could be, but when you become \\nsomewhat familiar with Python, this is one of the first things you get to know. The \\nbeauty of Python lies in its simplicity and elegance. The Zen of Python is a set of 20 \\nguiding principles, or  aphorisms , that have been influential in Python\\xe2\\x80\\x99s design. Long-\\ntime Pythoneer Tim Peters documented 19 of them in 1999, and they can be accessed \\nat   https://hg.python.org/peps/file/tip/pep-0020.txt as a part of the Python \\nEnhancement Proposals (PEP) number 20 (PEP 20). The best part is, if you already have \\nPython installed, you can access the Zen of Python at any  time   by running the following \\ncode in the Python or IPython shell: \\n In [5]: import this \\n The Zen of Python, by Tim Peters \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n55\\n Beautiful is better than ugly. \\n Explicit is better than implicit. \\n Simple is better than complex. \\n Complex is better than complicated. \\n Flat is better than nested. \\n Sparse is better than dense. \\n Readability counts. \\n Special cases aren\\'t special enough to break the rules. \\n Although practicality beats purity. \\n Errors should never pass silently. \\n Unless explicitly silenced. \\n In the face of ambiguity, refuse the temptation to guess. \\n There should be one-- and preferably only one --obvious way to do it. \\n Although that way may not be obvious at first unless you\\'re Dutch. \\n Now is better than never. \\n Although never is often better than *right* now. \\n If the implementation is hard to explain, it\\'s a bad idea. \\n If the implementation is easy to explain, it may be a good idea. \\n Namespaces are one honking great idea -- let\\'s do more of those! \\n  The above output showing the 19 principles that form the Zen of Python is included \\nin the Python language itself as an easter egg. The principles are written in simple English \\nand a lot of them are pretty self-explanatory, even if you have not written code before, \\nand many of them contain inside jokes! Python focuses on writing simple and clean \\ncode that is readable. It also intends to make sure you focus a lot on error handling and \\nimplementing code that is easy to interpret and understand. The one principle I would \\nmost like you to remember is  Simple is better than complex , which is applicable not only \\nfor Python but for a lot of things when you are out there in the world solving problems. \\nSometimes a simple approach beats a more complex one, as long as you know what you \\nare doing, because it helps you avoid overcomplicating things. \\n Applications: When Should You Use Python? \\n Python, being a general and multipurpose programming language, can be used to build \\napplications and systems for different domains and solve diverse real-world problems. \\nPython comes with a standard library that hosts a large number of useful libraries and \\nmodules that can be leveraged to solve various problems. Besides the standard library, \\nthousands of third-party libraries are readily available on the Internet, encouraging open \\nsource and active development. The official repository for hosting third-party libraries \\nand utilities for enhancing development in Python is the  Python Package Index (PyPI) . \\nAccess it at   https://pypi.python.org and check out the various packages. Currently \\nthere are over 80,000 packages you can install and start using. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n56\\n Although Python can be used for solving a lot of problems, here are some of the most \\npopular domains:\\n\\xe2\\x80\\xa2 \\n Scripting  : Python is known as a scripting language. It can be \\nused to perform many tasks, such as interfacing with networks \\nand hardware and handling and processing files and databases, \\nperforming OS operations, and receiving and sending email. \\nPython is also used extensively for server-side scripting and even \\nfor developing entire web servers for serving web pages. A lot \\nof Python scripts are used in an ad-hoc fashion for automating \\noperations like network socket communication, handling email, \\nparsing and extracting web pages, file sharing and transfer via \\nFTP, communicating via different protocols, and several more.  \\n\\xe2\\x80\\xa2 \\n Web development : There are a lot of robust and stable Python \\nframeworks out there that are used extensively for web \\ndevelopment, including Django, Flask, Web2Py, and Pyramid. \\nYou can use them for developing complete enterprise web \\napplications, and Python supports various architecture styles like \\nRESTful APIs and the MVC architecture. It also provides ORM \\nsupport to interact with databases and use OOP on top of that. \\nPython even has frameworks like Kivy, which support cross-\\nplatform development for developing apps on multiple platforms \\nlike iOS, Android, Windows, and OS X. Python is also used for \\ndeveloping  rich internet applications (RIA) with the Silverlight \\nframework support in IronPython, a Python version that is well \\nintegrated with the popular Microsoft .NET framework and pyjs, \\na RIA development framework supporting a Python-to-JavaScript \\ncompiler and an AJAX framework. \\n\\xe2\\x80\\xa2 \\n Graphical user interfaces (GUIs) : A lot of desktop-based \\napplications with GUIs can be easily built with Python. Libraries \\nand APIs like tkinter, PyQt, PyGTK, and wxPython allow \\ndevelopers to develop GUI-based apps with simple as well as \\ncomplex interfaces. Various frameworks enable developers to \\ndevelop GUI-based apps for different OSes and platforms. \\n\\xe2\\x80\\xa2 \\n Systems programming : Being a high-level language, Python has \\na lot of interfaces to low-level OS services and protocols, and the \\nabstractions on top of these services enable developers to write \\nrobust and portable system monitoring and administration tools. \\nWe can use Python to perform OS operations including creating, \\nhandling, searching, deleting, and managing files and directories. \\nThe  Python standard library (PSL) has OS and POSIX bindings that \\ncan be used for handling files, multi-threading, multi-processing, \\nenvironment variables, controlling sockets, pipes, and processes. \\nThis also enhances writing Python scripts for performing system-\\nlevel administration tasks with minimal effort and lines of code.  \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n57\\n\\xe2\\x80\\xa2 \\n Database programming   : Python is used a lot in connecting and \\naccessing data from different types of databases, be it SQL or \\nNoSQL. APIs and connectors exist for these databases like MySQL, \\nMSSQL, MongoDB, Oracle, PostgreSQL, and SQLite. In fact, SQLite, \\na lightweight relational database, now comes as a part of the Python \\nstandard distribution itself. Popular libraries like SQLAlchemy and \\nSQLObject provide interfaces to access various relational databases \\nand also have ORM components to help implement OOP-style \\nclasses and objects on top of relational tables.  \\n\\xe2\\x80\\xa2 \\n Scientific computing : Python really shows its flair for being \\nmultipurpose in areas like numeric and scientific computing. You \\ncan perform simple as well as complex mathematical operations \\nwith Python, including algebra and calculus. Libraries like \\n SciPy and NumPy help researchers, scientists, and developers \\nleverage highly optimized functions and interfaces for numeric \\nand scientific programming. These libraries are also used as the \\nbase for developing complex algorithms in various domains like \\nmachine learning. \\n\\xe2\\x80\\xa2 \\n Machine learning : Python is regarded as one of the most popular \\nlanguages today for machine learning. There is a wide suite of \\nlibraries and frameworks, like  scikit-learn ,  h2o ,  tensorflow , \\n theano , and even core libraries like  numpy and  scipy , for not only \\nimplementing machine learning algorithms but also using them \\nto solve real-world advanced analytics problems. \\n\\xe2\\x80\\xa2 \\n Text analytics : As mentioned, Python can handle text data \\nvery well, and this has led to several popular libraries like \\n nltk ,  gensim , and  pattern for NLP, information retrieval, and \\ntext analytics. You can also apply standard machine learning \\nalgorithms to solve problems related to text analytics. This \\necosystem of readily available packages in Python reduces time \\nand efforts taken for development. We will be exploring several of \\nthese libraries in this book. \\n Even though the preceding list may seem a bit overwhelming, this is just scratching \\nthe surface of what is possible with Python. It is widely used in several other domains \\nincluding  artificial intelligence (AI) , game development, robotics, Internet of Things \\n(IoT), computer vision, media processing, and network and system monitoring, just to \\nname a few. To read some of the widespread success stories achieved with Python in \\ndifferent diverse domains like arts, science, computer science, education, and others, \\nenthusiastic programmers and researchers can check out    www.python.org/about/\\nsuccess/   . To find out various popular applications developed using Python, see \\n   www.python.org/about/apps/  and   https://wiki.python.org/moin/Applications  , \\nwhere you will definitely find some applications you have used\\xe2\\x80\\x94some of them are \\nindispensable. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n58\\n Drawbacks: When Should You Not Use Python? \\n I have been blowing the trumpet for Python till now, but you may be wondering are \\nthere any drawbacks? Well, like any tool or language, Python has advantages and \\ndisadvantages. Yes, even Python has some disadvantages, and here we will highlight \\nsome of them so that you are aware of them when developing and writing code in Python:\\n\\xe2\\x80\\xa2 \\n Execution speed performance :  Performance is a pretty heavy term \\nand can mean several things, so I\\xe2\\x80\\x99ll pinpoint the exact area to \\ntalk about and that is execution speed. Because Python is not a \\nfully compiled language, it will always be slower than low-level \\nfully compiled programming languages like C and C++. There \\nare several ways you can optimize your code, including multi-\\nthreading and multi-processing. You can also use static typing \\nand C extensions for Python (known as Cython). You can consider \\nusing PyPy also, which is much faster than normal Python since \\nit uses a just-in-time (JIT)  compiler (see   http://pypy.org ), \\nbut often, if you write well-optimized code, you can develop \\napplications in Python just fine and do not need to depend on \\nother languages. Remember that often the problem is not with \\nthe tool but the code you write\\xe2\\x80\\x94something all developers and \\nengineers realize with time and experience. \\n\\xe2\\x80\\xa2 \\n Global Interpreter Lock (GIL)  : The GIL is a mutual exclusion lock \\nused in several programming language interpreters, like Python \\nand Ruby. Interpreters using GIL only allow one single thread \\nto effectively execute at a time even when run on a multi-core \\nprocessor and thus limit the effectively of parallelism achieved by \\nmulti-threading depending on whether the  processes are I\\\\O bound \\nor CPU bound and how many calls it makes outside the interpreter.  \\n\\xe2\\x80\\xa2 \\n Version incompatibility : If you have been following Python news, \\nyou know that once Python released the 3.x version from 2.7.x, \\nit was backward-incompatible in several aspects, and that has \\nindeed opened a huge can of worms. Several major libraries and \\npackages that had been built in Python 2.7.x started breaking \\nwhen users unknowingly updated their Python versions. Hence, a \\nlarge chunk of enterprises and the developer community still use \\n Python 2.7.x due to legacy code and because newer versions of \\nthose packages and libraries were never built. Code deprecation \\nand version changes are some of the most important factors in \\nsystems breaking down. \\n Many of these issues are not specific to Python but apply to other languages too, so \\nyou should not be discouraged from using Python just because of the preceding points\\xe2\\x80\\x94\\nbut you should definitely remember them when writing code and building systems.  \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n59\\n Python Implementations and  Versions \\n There are several different implementations of Python and different versions of Python \\nwhich are released periodically since it is under active development. This section discusses \\nboth implementations and versions and their significance, which should give you some \\nidea of which Python you might want to use for your development needs. Currently, there \\nare four major production-ready, robust, and stable implementations of Python:\\n\\xe2\\x80\\xa2 \\n CPython is the regular old Python, which we know as just Python. \\nIt is both a compiler and interpreter and comes with its own \\nset of standard packages and modules which were all written \\nin standard C. This version can be used directly in all popular \\nmodern platforms. Most of the python third-party packages and \\nlibraries are compatible with this version.  \\n\\xe2\\x80\\xa2 \\n PyPy is a faster alternative Python implementation that uses \\na JIT compiler to make the code run faster than the CPython \\nimplementation\\xe2\\x80\\x94sometimes delivering speedups in the range of \\n10x\\xe2\\x80\\x93100x. It is also more memory efficient, supporting greenlets \\nand stackless for high parallelism and concurrency.  \\n\\xe2\\x80\\xa2 \\n Jython is a Python implementation for the Java platform \\nsupporting  Java Virtual Machine (JVM) for any version of \\nJava ideally above version 7. Using Jython you can write code \\nleveraging all types of Java libraries, packages, and frameworks. \\nIt works best when you know more about the Java syntax and \\nthe OOP principles that are used extensively in Java, like classes, \\nobjects, and interfaces. \\n\\xe2\\x80\\xa2 \\n IronPython is the Python implementation for the popular \\nMicrosoft .NET framework, also termed as the  Common \\nLanguage Runtime (CLR) . You can use all of Microsoft\\xe2\\x80\\x99s CLR \\nlibraries and frameworks in IronPython, and even though you do \\nnot essentially have to write code in C#, it is useful to know more \\nabout syntax and constructs for C# to use IronPython effectively.    \\n To start with I would suggest you to use the default Python which is the CPython \\nimplementation, and experiment with the other versions only if you are really interested in \\ninterfacing with other languages like C# and Java and need to use them in your codebase. \\n There are two major versions: the 2.x series and the 3.x series, where x is a number. \\nPython 2.7 was the last major version in the 2.x series, released in 2010. From then on, \\nfuture releases have included bug fixes and performance improvements but no new \\nfeatures. The latest version is Python 2.7.12, released in June 2016. The 3.x series started \\nwith Python 3.0, which introduced many backward-incompatible changes compared \\nto Python 2.x. Each version 3 release not only has bug fixes and improvements but also \\nintroduces new features, such as the AsyncIO module released recently. As of this writing, \\nPython 3.5.2 is the latest version in the 3.x series, released in June 2016. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n60\\n There are many arguments over which version of Python should be used. We \\nwill discuss some of them later on, but the best way to go about thinking about it is to \\nconsider what problem is to be solved and the entire software ecosystem you will need \\nto use for that, starting from  libraries , dependencies, and architecture to implementation \\nand deployment\\xe2\\x80\\x94and also considering things like reusing existing legacy codebases. \\n Installation and Setup \\n Now that you have been acquainted with Python and know more about the language, \\nits capabilities, implementations, and versions, we will be talking about which version \\nof Python we will be using in the book and also discussing details on how to set up your \\ndevelopment environment and handle package management and virtual environments. \\nThis section will give you a good head start on getting things ready for following along \\nwith the various hands-on examples we will be covering in this book. \\n Which Python Version? \\n The two major Python versions, as mentioned, are the 2.x series and the 3.x series. They \\nare quite similar, although there have been several backward-incompatible changes in \\nthe 3.x version, which has led to a huge drift between people who use 2.x and people \\nwho use 3.x. Most legacy code and a large majority of Python packages on PyPI were \\ndeveloped in  Python 2.7.x , and many package owners do not have the time or will to port \\nall their codebases to  Python 3.x , since the effort required would not be minimal. Some of \\nthe changes in 3.x are as follows:\\n\\xe2\\x80\\xa2 \\n All text strings are Unicode by default. \\n\\xe2\\x80\\xa2 \\n print and  exec are now functions and no longer statements.  \\n\\xe2\\x80\\xa2 \\n range() returns a memory-efficient iterable and not a list. \\n\\xe2\\x80\\xa2 \\n The style for classes has changed. \\n\\xe2\\x80\\xa2 \\n Library and name changes are based on convention and style \\nviolations. \\n To know more about changes introduced since  Python 3.0 , check   https://docs.\\npython.org/3/whatsnew/3.0.html  , the official documentation listing the changes. That \\nlink should give you a pretty good idea of what changes can break your code if you are \\nporting it from Python 2 to Python 3. \\n As for the problem of selecting which version, there is no absolute answer for this. \\nIt purely depends on the problem you are trying to solve and the current code and \\ninfrastructure you have and how you will be maintaining this code in the future along \\nwith all its necessary dependencies. If you are starting a new project completely and \\nhave a fairly good idea that you do not need any external packages and libraries that are \\nsolely dependent on Python 2.x, you can go ahead with Python 3.x and start developing \\nyour system. But if you have a lot of dependencies on external packages that might break \\nwith Python 3.x or that are available for only Python 2.x, you have no choice but to stick \\nwith  Python 2.x . Besides that, often you have to deal with legacy code that\\xe2\\x80\\x99s been around \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n61\\na long time, especially in large companies and organizations that have huge codebases. \\nIn that case, porting the whole code to Python 3.x would be wasted effort\\xe2\\x80\\x94kind of re-\\ninventing the wheel, since you are not missing out on major functionality and capabilities \\nby using Python 2.x, and in fact you might even end up breaking the existing code and \\nfunctionality without even realizing it. In the end, this is a decision left to you, the reader, \\nwhich you must make carefully considering all scenarios. \\n We will be using Python 2.7.11 in this book just to be on the safe side, since it is a tried \\nand tested version of Python in all major enterprises. You are most welcome to follow \\nalong even in Python 3.x\\xe2\\x80\\x94the algorithms and techniques will be the same, although \\nyou may have to take into account changes, such as the fact that the  print statement is a \\nfunction in Python 3.x and so on.  \\n Which Operating System? \\n There are several popular OSes out there, and everybody has their own preference. The \\nbeauty of  Python  is that is can run seamlessly on any OS without much hassle. The three \\nmost popular OSes include the following:\\n\\xe2\\x80\\xa2 \\n Windows \\n\\xe2\\x80\\xa2 \\n Linux \\n\\xe2\\x80\\xa2 \\n OS X (now known as macOS) \\n You can choose any OS of your choice and use it for following along with the \\nexamples in this book. We will be using Windows as the primary OS in this book. \\nThis book is aimed at working professionals and practitioners, most of whom in their \\nenterprise environment usually use the enterprise version of Windows. Besides that, \\nseveral Python external packages are really easy to install on a UNIX-based OS like Linux \\nand macOS. However, sometimes there are major issues in installing them for Windows, \\nso I want to highlight such instances and make sure to address them such that executing \\nany of the code snippets and samples here becomes easy for you. But again, you are most \\nwelcome to use any OS of your choice when following the examples in this book.  \\n Integrated Development Environments \\n Integrated development environments (IDEs) are software products that enable \\ndevelopers to be highly productive by providing a complete suite of tools and capabilities \\nnecessary for writing, managing, and executing code. The usual components of an \\nIDE include source editor, debugger, compiler, interpreter, and refactoring and build \\ntools. They also have other capabilities such as code-completion, syntax highlighting, \\nerror highlighting and checks, objects, and variable explorers.  IDEs   can be used to \\nmanage entire codebases\\xe2\\x80\\x94much better than trying to write code in a simple text \\neditor, which takes more time. That said, experienced developers often use simple \\nplain text editors to write code, especially if they are working in server environments. \\nYou\\xe2\\x80\\x99ll find a list of IDEs used specially for Python at   https://wiki.python.org/moin/\\nIntegratedDevelopmentEnvironments  . \\n We will be using the Spyder IDE, which comes with the Anaconda Python \\ndistribution for writing and executing our code. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n62\\n Environment Setup \\n This section covers details regarding how to set up your Python  environment   with \\nminimal effort and the main components required. \\n First, head over to the official Python website and download Python 2.7.11 from \\n   www.python.org/downloads/  . Or download a complete Python distribution with over \\n700 packages, known as the Anaconda Python distribution, from Continuum Analytics, \\nwhich is built specially for data science and analytics, at    www.continuum.io/downloads  . \\nThis package provides a lot of advantages, especially for Windows users, where installing \\nsome of the packages like  numpy and  scipy can sometimes cause issues. You can get more \\ninformation about Anaconda and Continuum Analytics at   https://docs.continuum.io/\\nanaconda/index  . Anaconda comes with conda, an open source package and environment \\nmanagement system, and Spyder (Scientific Python Development Environment), an IDE \\nfor writing and executing your code. \\n For other OS options, check out the relevant instructions on the website. \\n Once you have Python downloaded, start the executable and follow the instructions \\non the screen, clicking the Next button at each stage. But before starting the installation, \\nremember to check the two options shown in Figure\\xc2\\xa0 2-1 . \\n Figure 2-1.  Installing the Anaconda Python distribution \\n \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n63\\n Once the  installation is complete, either start up Spyder by double-clicking the \\nrelevant icon or start the Python or IPython shell from the command prompt. Spyder \\nprovides a complete IDE to write and execute code in both the regular Python and \\nIPython shell. Figure\\xc2\\xa0 2-2 shows how to run IPython from the command prompt.  \\n Figure\\xc2\\xa0 2-2 depicts printing a regular sentence saying  Welcome to Python!  just to show \\nyou that Python is properly installed and working fine. The input and output execution \\nhistory are kept in variables called  In and  Out , indicated in the figure by the prompt \\nnumbers, such as  In[1] . IPython provides a lot of advantages including code completion, \\ninline executions and plots, and running code snippets interactively. We will be running \\nmost of our snippets in the IPython shell just like the examples seen in Chapter   1 . \\n Now that you have  Anaconda installed, you are ready to start running the code \\nsamples in this book. Before we move on to the next section, I want to cover package \\nmanagement briefly. You can use either the  pip or  conda commands to install, uninstall, \\nand upgrade packages. The shell command shown in Figure\\xc2\\xa0 2-3  depicts installing the \\n pandas library via  pip . Because we already have the library installed, you can use the \\n --upgrade flag as shown in the figure. \\n Figure 2-2.  Starting IPython from the command prompt \\n Figure 2-3.  Package management using  pip \\n \\n \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n64\\n The  conda package manager is better than  pip in several aspects because it provides \\na holistic view of which dependencies are going to be upgraded and the specific versions \\nand other details during installation. Also  pip often fails to install some packages in \\nWindows, but  conda  has no such issues during installation. Figure\\xc2\\xa0 2-4  depicts how to \\nmanage packages using  conda . \\n Now you have a much better idea of how to install external  packages and libraries in \\nPython. This will be useful later when we install some libraries that have been specifically \\nbuilt for text analytics. Your Python environment should now be set up and ready for \\nexecuting code. Before we dive into the basic and advanced concepts in Python, we will \\nconclude this section with a discussion about virtual environments.  \\n Virtual Environments \\n A  virtual environment , or  venv , is a complete isolated Python environment with its own \\nPython interpreter, libraries, modules, and scripts. This environment is a standalone \\nenvironment isolated from other virtual environments and the default system-level \\nPython environment. Virtual environments are extremely useful when you have multiple \\nprojects or codebases that have dependencies on different versions of the same packages \\nor libraries. For example, if my project TextApp1 depends on  nltk 2.0 and another \\nproject, TextApp2, depends on  nltk 3.0 , then it would be impossible to run both projects \\non the same system. Hence, the need for virtual environments that provide complete \\nisolated environments that can be activated and deactivated as needed. \\n Figure 2-4.  Package management using  conda \\n \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n65\\n To set up a virtual environment, you need to install the  virtualenv  package as follows: \\n E:\\\\Apress>pip install virtualenv \\n Collecting virtualenv \\n  Downloading virtualenv-15.0.2-py2.py3-none-any.whl (1.8MB) \\n    100% |################################| 1.8MB 290kB/s \\n Installing collected packages: virtualenv \\n Successfully installed virtualenv-15.0.2 \\n Once installed, you can create a virtual environment as follows, where we create a new \\nproject directory called  test_proj  and create the virtual environment inside the directory: \\n E:\\\\Apress>mkdir test_proj && chdir test_proj \\n E:\\\\Apress\\\\test_proj>virtualenv venv \\n New python executable in E:\\\\Apress\\\\test_proj\\\\venv\\\\Scripts\\\\python.exe \\n Installing setuptools, pip, wheel...done. \\n  Once you have installed the  virtual environment   successfully, you can activate it \\nusing the following command: \\n E:\\\\Apress\\\\test_proj>venv\\\\Scripts\\\\activate \\n (venv) E:\\\\Apress\\\\test_proj>python --version \\n Python 2.7.11 :: Continuum Analytics, Inc. \\n For other OS platforms, you may need to use the command  source venv/bin/\\nactivate  to activate the virtual environment. \\n Once the virtual environment is active, you can see the  (venv) notation as shown in \\nthe preceding code output, and any new packages you install will be placed in the  venv \\nfolder in complete isolation from the global system Python installation. This difference \\nis illustrated by depicting different versions for the  pandas package in the global system \\nPython and the virtual environment Python in the following code: \\n C:\\\\Users\\\\DIP.DIPSLAPTOP>echo \\'This is Global System Python\\' \\n \\'This is Global System Python\\' \\n C:\\\\Users\\\\DIP.DIPSLAPTOP>pip freeze | grep pandas \\n pandas==0.18.0 \\n (venv) E:\\\\Apress\\\\test_proj>echo \\'This is VirtualEnv Python\\' \\n \\'This is VirtualEnv Python\\' \\n (venv) E:\\\\Apress\\\\test_proj>pip install pandas \\n Collecting pandas \\n  Downloading pandas-0.18.1-cp27-cp27m-win_amd64.whl (6.2MB) \\n    100% |################################| 6.2MB 142kB/s \\n Installing collected packages: pandas \\n Successfully installed pandas-0.18.1 \\n (venv) E:\\\\Apress\\\\test_proj>pip freeze | grep pandas \\n pandas==0.18.1 \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n66\\n You can see from that code how the  pandas package has different versions in the \\nsame machine:  0.18.0 for global Python and  0.18.1 for the virtual environment Python. \\nHence, these isolated virtual environments can run seamlessly on the same system. \\n Once you have finished working in the virtual environment, you can deactivate it \\nagain as follows: \\n (venv) E:\\\\Apress\\\\test_proj>venv\\\\Scripts\\\\deactivate \\n E:\\\\Apress\\\\test_proj> \\n This will bring you back to the system\\xe2\\x80\\x99s default  Python interpreter with all its \\ninstalled libraries. This gives us a good idea about the utility and advantages of virtual \\nenvironments, and once you start working on several projects, you should definitely \\nconsider using it. To find out more about virtual environments, check out   http://docs.\\npython-guide.org/en/latest/dev/virtualenvs/  , the official documentation for the \\n virtualenv package. \\n This brings us to the end of our installation and setup activities, and now we will \\nbe looking into Python concepts, constructs, syntax, and semantics using hands-on \\nexamples. \\n Python  Syntax and Structure \\n There is a defined hierarchical syntax for Python code that you should remember when \\nwriting code. Any big Python application or system is built using several modules, which \\nare themselves comprised of Python statements. Each statement is like a command or \\ndirection to the system directing what operations it should perform, and these statements \\nare comprised of expressions and objects. Everything in Python is an object\\xe2\\x80\\x94including \\nfunctions, data structures, types, classes and so on. This hierarchy is visualized in \\nFigure\\xc2\\xa0 2-5 . \\n Figure 2-5.  Python program structure hierarchy \\n \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n67\\n The basic statements consist of objects, expressions which usually make use of objects \\nand process and perform operations on them. Objects can be anything from simple data \\ntypes and  structures      to complex objects, including functions and reserved words that have \\ntheir own specific roles. Python has around 37  keywords , or reserved words, which have \\ntheir own designated roles and functions. Table\\xc2\\xa0  2-1  list each keyword in detail, including \\nexamples that should be useful and handy when you are using them in your code.  \\n Table 2-1.  Python Reserved Words \\n Sl No.  Keyword \\n Description \\n Example \\n 1 \\n and \\n The logical AND operator \\n  (5==5 and 1==2) == False \\n 2 \\n as \\n Used as a synonym to some \\nobject/reference \\n with open(\\'file.txt\\') as f \\n 3 \\n assert \\n Asserts/checks if some \\nexpression is True \\n assert 1==2, \"Not Equal\" \\n 4 \\n async \\n Declares a function as \\n asynchronous (co-routine) \\n async def get_data(): \\n 5 \\n await  \\n Used to invoke a co-routine \\n return await get_data() \\n 6 \\n break  \\n Breaks out of an executing loop \\n  while True: \\n break \\n 7 \\n class \\n Create a class (OOP) \\n class ClassifyText(object): \\n 8 \\n continue  \\n Continue with the next iteration \\nof the loop \\n while True: \\n if a==1: continue \\n 9 \\n def \\n Defines a function \\n def add(a,b): \\n return a+b \\n 10 \\n del \\n Deletes references \\n del arr \\n 11 \\n elif \\n Else-if conditional \\n if num==1: print \\'1\\' \\n elif num==2: print \\'2\\' \\n 12 \\n else \\n Else conditional \\n if num==1: print \\'1\\' \\n else: print \\'not 1\\' \\n 13 \\n except \\n Catch  exceptions \\n except ValueError, e: print e \\n 14 \\n exec \\n Dynamic execution of code \\n exec \\'print \"Hello Python\"\\' \\n 15 \\n False \\n Boolean False \\n False == 0 \\n 16 \\n finally  \\n Finally execute statements after \\ntry-except \\n finally: print \\'end of \\nexception\\' \\n 17 \\n for \\n The for loop \\n for num in arr: print num \\n 18 \\n from \\n Import specific components \\nfrom modules \\n from nltk.corpus import \\nbrown \\n 19 \\n global  \\n Declare variables as global \\n global var \\n 20 \\n if \\n If conditional \\n if num==1: print \\'1\\' \\n(continued)\\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n68\\n Table\\xc2\\xa0  2-1 shows us all of Python\\xe2\\x80\\x99s keywords that are used in statements. However, \\nthere are a few  caveats to remember. The  async and  await  keywords are only available \\nin Python 3.5.x onwards. The  exec and  print keywords are statements only in Python \\n2.x\\xe2\\x80\\x94starting from Python 3.x they are functions. The keywords  False ,  True , and  nonlocal \\nwere introduced starting with Python 3.x in the keywords list. \\n Python statements usually direct the interpreter as to what they should do when \\nexecuting the statements. A bunch of statements usually forms a logical block of code. \\nVarious constructs including functions and loops and conditionals help in segregating \\nand executing blocks of code using logic and design based on user decisions. Python also \\nfocuses a lot on readability\\xe2\\x80\\x94hence, indentation is an important part of Python code. By \\ndefault, Python does not use punctuation like semicolons to indicate end of statements. It \\nalso uses tabs or whitespaces to indicate and delimit specific blocks of code instead of the \\ntraditional braces or keywords as used in languages like C, C++, Java, and so on. Python \\nTable 2-1. (continued)\\n Sl No.  Keyword \\n Description \\n Example \\n 21 \\n import  \\n Import an existing module \\n  import numpy \\n 22 \\n in \\n Check or  loop through some \\nexisting object \\n for num in arr \\\\ if x in y \\n 23 \\n is \\n Used to check for equality \\n  type(\\'a\\') is str \\n 24 \\n lambda \\n Create an anonymous function \\n lambda a: a**a \\n 25 \\n None \\n Represents no value or null \\n num = None \\n 26 \\n nonlocal \\n Modify variable values of an \\nouter but non global scope in \\nfunctions \\n nonlocal var \\n 27 \\n not \\n The logical NOT operator \\n not 1 == 2 \\n 28 \\n or  \\n The logical OR operator \\n  1 or 2 == 1 \\n 29 \\n pass \\n Used as a placeholder \\nindicating an empty block \\n if a == 1: pass \\n 30 \\n print \\n Prints a string or other objects \\n print \\'Hello World!\\' \\n 31 \\n raise \\n Raises an exception \\n raise Exception(\\'overflow\\') \\n 32 \\n return \\n Returns object(s) from a \\nfunction after exiting \\n return a, b \\n 33 \\n try \\n Tries a code block and goes to \\n except if exception occurs \\n try: read_file() \\n except Exception, e: print e \\n 34 \\n while \\n The while loop \\n while True: print value \\n 35 \\n with \\n With an  object in an expression \\nperform some operation \\n with open(\\'file.txt\\') as f: \\n data = f.read() \\n 36 \\n yield \\n Generator functionality, pause \\nand return to the caller \\n def generate_func(arr): \\n for num in arr: yield num+1 \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n69\\naccepts both spaces and tabs as indentation, with the usual norm being one tab or four \\nspaces to indicate each specific block of code. Unindented code will always throw syntax \\nerrors, so anyone writing Python code must be extra careful with code formatting and \\nindentation. \\n Python programs are usually structured around the hierarchy mentioned earlier. \\nEach module is usually a directory with a  __init__.py  file, which makes the directory \\na package, and it may have multiple modules, each of which is an individual  Python \\n( .py ) file . Each module usually has classes and objects like functions that are invoked by \\nother modules and code. All interconnected modules finally make up a complete Python \\nprogram, application, or system. Usually you start any project by writing necessary code \\nin  Python (.py) files and making it modular as it gets bigger by adding more components.  \\n Data Structures and Types \\n Python has several data types and many are used as data structures for handling data. All \\ndata types are derived from the default object data type in Python. This object data type \\nis an abstraction used by Python for managing and handling data. Code and data are all \\nstored and handled by objects and relations among objects. Each object has three things \\nor properties that distinguish it from other objects:\\n\\xe2\\x80\\xa2 \\n Identity : This is unique and never changes once the object is \\ncreated and is usually represented by the object\\xe2\\x80\\x99s memory \\naddress. \\n\\xe2\\x80\\xa2 \\n Type : This determines the type of object (usually the data type, \\nwhich is again a child of the base object type). \\n\\xe2\\x80\\xa2 \\n Value  : The actual value stored by the object. \\n Let\\xe2\\x80\\x99s say a variable is holding a string that is one of the data types. To see the three \\nproperties in action, you can use the functions depicted in the following code snippet: \\n In [46]: new_string = \"This is a String\"  # storing a string \\n In [47]: id(new_string)  # shows the object identifier (address) \\n Out[47]: 243047144L \\n In [48]: type(new_string)  # shows the object type \\n Out[48]: str \\n In [49]: new_string  # shows the object value \\n Out[49]: \\'This is a String\\' \\n Python has several data types, including several core data types and complex ones \\nincluding functions and classes. In this section we will talk about the core data types of \\nPython, including some that are used extensively as data structures to handle data. These \\ncore  data types are as follows:\\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n70\\n\\xe2\\x80\\xa2 \\n Numeric \\n\\xe2\\x80\\xa2 \\n Strings \\n\\xe2\\x80\\xa2 \\n Lists \\n\\xe2\\x80\\xa2 \\n Sets \\n\\xe2\\x80\\xa2 \\n Dictionaries \\n\\xe2\\x80\\xa2 \\n Tuples \\n\\xe2\\x80\\xa2 \\n Files \\n\\xe2\\x80\\xa2 \\n Miscellaneous \\n Although that\\xe2\\x80\\x99s not an exhaustive  list , more than 90 percent of your time will be \\nspent writing Python statements that make use of these objects. Let\\xe2\\x80\\x99s discuss each of \\nthem in more detail to understand their properties and behavior better. \\n Numeric Types \\n The numeric data type is perhaps the most common and basic data type in Python. All \\nkinds of applications end up processing and using numbers in some form or the other. \\nThere are mainly three numeric types: integers, floats, and complex numbers. Integers are \\nnumbers that do not have a fractional part or mantissa after the decimal point. Integers \\ncan be represented and operated upon as follows: \\n In [52]: # representing integers and operations on them \\n In [53]: num = 123 \\n In [54]: type(num) \\n Out[54]: int \\n In [55]: num + 1000  # addition \\n Out[55]: 1123 \\n In [56]: num * 2  # multiplication \\n Out[56]: 246 \\n In [59]: num /  2  # integer division \\n Out[59]: 61 \\n There are also various types of integers, depending on their radix or base. These \\ninclude decimal, binary, octal, and hexadecimal integers. Normal nonzero leading \\nsequences of numbers are decimal integers. Integers that start with a  0 , or often  0o to \\nprevent making mistakes, are octal integers. Numbers that start with  0x are hexadecimal, \\nand those starting with  0b are binary integers. You can also make use of the  bin() ,  hex() , \\nand  oct() functions for converting decimal integers to the respective base form. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n71\\n The following code snippet illustrates the various forms of integers: \\n In [94]: # decimal \\n In [95]: 1 + 1 \\n Out[95]: 2 \\n In [96]: # binary \\n In [97]: bin(2) \\n Out[97]: \\'0b10\\' \\n In [98]: 0b1 + 0b1 \\n Out[98]: 2 \\n In [99]: bin(0b1 + 0b1) \\n Out[99]: \\'0b10\\' \\n In [100]: # octal \\n In [101]: oct(8) \\n Out[101]: \\'010\\' \\n In [102]: oct(07 + 01) \\n Out[102]: \\'010\\' \\n In [103]: 0o10 \\n Out[103]: 8 \\n In [104]: # hexadecimal \\n In [105]: hex(16) \\n Out[105]: \\'0x10\\' \\n In [106]: 0x10 \\n Out[106]: 16 \\n In [116]: hex(0x16 + 0x5) \\n Out[116]: \\'0x1b\\' \\n Floating point numbers, or floats, are represented as a sequence of numbers that \\ninclude a decimal point and some numbers following it (the mantissa), an exponent part \\n( e or  E followed by a  +/- sign followed by digits), or sometimes both of them. Here are \\nsome examples of floating point  numbers : \\n In [126]: 1.5 + 2.6 \\n Out[126]: 4.1 \\n In [127]: 1e2 + 1.5e3 + 0.5 \\n Out[127]: 1600.5 \\n In [128]: 2.5e4 \\n Out[128]: 25000.0 \\n In [129]: 2.5e-2 \\n Out[129]: 0.025 \\n  The floating point numbers have a range and precision similar to the double data \\ntype in the C language. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n72\\n Complex numbers have two components, a real and an imaginary component \\nrepresented by floating point numbers. The imaginary literal consists of the number \\nfollowed by the letter  j , and this symbol  j at the end of the literal indicates the square root \\nof \\xe2\\x80\\x931. The following code snippet shows some  representations and operations of complex \\nnumbers: \\n In [132]: cnum = 5 + 7j \\n In [133]: type(cnum) \\n Out[133]: complex \\n In [134]: cnum.real \\n Out[134]: 5.0 \\n In [135]: cnum.imag \\n Out[135]: 7.0 \\n In [136]: cnum + (1 - 0.5j) \\n Out[136]: (6+6.5j) \\n Strings \\n Strings are sequences or collections of characters used to store and represent textual \\ndata\\xe2\\x80\\x94which will be our data type of choice in most examples in the book. Strings can \\nbe used to store both textual as well as bytes as information. Strings have a wide variety \\nof methods that can be used for handling and manipulating strings, which we will see in \\ndetail later in this chapter. An important point to remember is that strings are  immutable , \\nand any operations performed on strings always creates a new string object (remember \\nthe three properties of an object?) rather than just mutating and changing the value of the \\nexisting string object. \\n The following code snippet shows some string representations and some basic \\noperations on strings: \\n In [147]: s1 = \\'this is a string\\' \\n In [148]: s2 = \\'this is \"another\" string\\' \\n In [149]: s3 = \\'this is the \\\\\\'third\\\\\\' string\\' \\n In [150]: s4 = \"\"\"this is a \\n     ...: multiline \\n     ...: string\"\"\" \\n In [151]: print s1, s2, s3, s4 \\n this is a string this is \"another\" string this is the \\'third\\' \\nstring this is a \\n multiline \\n string \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n73\\n In [152]: print s3 + \\'\\\\n\\' + s4 \\n this is the \\'third\\'  string \\n this is a \\n multiline \\n string \\n In [153]: \\' \\'.join([s1, s2]) \\n Out[153]: \\'this is a string this is \"another\" string\\' \\n In [154]: s1[::-1]  # reverses the string \\n Out[154]: \\'gnirts a si siht\\' \\n Lists \\n Lists are collections of arbitrary heterogeneous or homogenous typed objects. Lists also \\nfollow a sequence based on the order in which the objects are present in the list, and \\neach object has its own index with which it can be accessed. Lists are similar to  arrays in \\nother languages, with the distinction that unlike arrays, which hold homogenous items \\nof the same type, lists can contain different types of objects. A simple example would be \\na list containing numbers, strings, and even sublists. If a list contains objects that are lists \\nthemselves, these are often called  nested lists. \\n The following code snippet shows some examples of lists: \\n In [161]: l1 = [\\'eggs\\', \\'flour\\', \\'butter\\'] \\n In [162]: l2 = list([1, \\'drink\\', 10, \\'sandwiches\\', 0.45e-2]) \\n In [163]: l3 = [1, 2, 3, [\\'a\\', \\'b\\', \\'c\\'], [\\'Hello\\', \\'Python\\']] \\n In [164]: print l1, l2, l3 \\n [\\'eggs\\', \\'flour\\', \\'butter\\'] [1, \\'drink\\', 10, \\'sandwiches\\', 0.0045] [1, 2, 3, \\n[\\'a\\', \\'b\\', \\'c\\'], [\\'Hello\\', \\'Python\\']] \\n You can also perform numerous operations on lists, including indexing, slicing, \\nappending, popping, and many more. Some typical operations on lists are depicted in the \\nfollowing code snippet: \\n In [167]: # indexing lists \\n In [168]: l1 \\n Out[168]: [\\'eggs\\', \\'flour\\', \\'butter\\'] \\n In [169]: l1[0] \\n Out[169]: \\'eggs\\' \\n In [170]: l1[1] \\n Out[170]: \\'flour\\' \\n In [171]: l1[0] +\\' \\'+ l1[1] \\n Out[171]: \\'eggs flour\\' \\n In [171]: # slicing lists \\n In [172]: l2[1:3] \\n Out[172]: [\\'drink\\', 10] \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n74\\n In [173]: numbers = range(10) \\n In [174]: numbers \\n Out[174]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \\n In [175]: numbers[2:5] \\n Out[175]: [2, 3, 4] \\n In [180]: numbers[:] \\n Out[180]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \\n In [181]: numbers[::2] \\n Out[181]: [0, 2, 4, 6, 8] \\n In [181]: # concatenating and mutating lists \\n In [182]: numbers * 2 \\n Out[182]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9] \\n In [183]: numbers + l2 \\n Out[183]:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1, \\'drink\\', 10, \\'sandwiches\\', \\n0.0045] \\n In [184]: # handling nested lists \\n In [184]: l3 \\n Out[184]: [1, 2, 3, [\\'a\\', \\'b\\', \\'c\\'], [\\'Hello\\', \\'Python\\']] \\n In [185]: l3[3] \\n Out[185]: [\\'a\\', \\'b\\', \\'c\\'] \\n In [186]: l3[4] \\n Out[186]: [\\'Hello\\', \\'Python\\'] \\n In [187]: l3.append(\\' \\'.join(l3[4]))  # append operation \\n In [188]: l3 \\n Out[188]: [1, 2, 3, [\\'a\\', \\'b\\', \\'c\\'], [\\'Hello\\', \\'Python\\'], \\'Hello Python\\'] \\n In [189]: l3.pop(3)  # pop  operation \\n Out[189]: [\\'a\\', \\'b\\', \\'c\\'] \\n In [190]: l3 \\n Out[190]: [1, 2, 3, [\\'Hello\\', \\'Python\\'], \\'Hello Python\\'] \\n Sets \\n Sets are unordered collections of unique and immutable objects. You can use the  set() \\nfunction or the curly braces  {...} to create a new set. Sets are typically used to remove \\nduplicates from a list, test memberships, and perform mathematical set operations, \\nincluding union, intersection, difference, and symmetric difference. \\n Some set representations and operations are shown in the following code snippet: \\n In [196]: l1 = [1,1,2,3,5,5,7,9,1] \\n In [197]: set(l1)  # makes the list as a set \\n Out[197]: {1, 2, 3, 5, 7, 9} \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n75\\n In [198]: s1 = set(l1) \\n # membership testing \\n In [199]: 1 in s1   \\n Out[199]: True \\n In [200]: 100 in s1 \\n Out[200]: False \\n # initialize a second set \\n In [201]: s2 = {5, 7, 11} \\n # testing various set operations \\n In [202]: s1 - s2  # set difference \\n Out[202]: {1, 2, 3, 9} \\n In [203]: s1 | s2  # set union \\n Out[203]: {1, 2, 3, 5, 7, 9, 11} \\n In [204]: s1 & s2  # set intersection  \\n Out[204]: {5, 7} \\n In [205]: s1 ^ s2  # elements which do not appear in both sets \\n Out[205]: {1, 2, 3, 9, 11} \\n Dictionaries \\n Dictionaries in Python are key-value mappings that are unordered and mutable. They \\nare often known as  hashmaps ,  associative arrays , and  associative memories . Dictionaries \\nare indexed using  keys , which can be any immutable object type, like numeric types or \\nstrings, or even tuples, which we will see later on. Remember that keys should always \\nbe some immutable data type. Dictionary values can be immutable or mutable objects, \\nincluding lists and dictionaries themselves which would lead to nested dictionaries. \\nDictionaries have a lot of similarity with JSON objects, if you have worked with them \\npreviously. Dictionaries are often called  dicts in Python, and the  dict() function is also \\nused to create new dictionaries. \\n The following code snippets show some representations and operations on \\ndictionaries: \\n In [207]: d1 = {\\'eggs\\': 2, \\'milk\\': 3, \\'spam\\': 10, \\'ham\\': 15} \\n In [208]: d1 \\n Out[208]: {\\'eggs\\': 2, \\'ham\\': 15, \\'milk\\': 3, \\'spam\\': 10} \\n # retrieving items based on key \\n In [209]: d1.get(\\'eggs\\') \\n Out[209]: 2 \\n In [210]: d1[\\'eggs\\'] \\n Out[210]: 2 \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n76\\n # get is better than direct indexing since it does not throw  errors \\n In [211]: d1.get(\\'orange\\')  \\n In [212]: d1[\\'orange\\'] \\n Traceback (most recent call last): \\n  File \"<ipython-input-212-ebecbf415243>\", line 1, in <module> \\n    d1[\\'orange\\'] \\n KeyError: \\'orange\\' \\n # setting items with a specific key \\n In [213]: d1[\\'orange\\'] = 25 \\n In [214]: d1 \\n Out[214]: {\\'eggs\\': 2, \\'ham\\': 15, \\'milk\\': 3, \\'orange\\': 25, \\'spam\\': 10} \\n # viewing keys and values \\n In [215]: d1.keys() \\n Out[215]: [\\'orange\\', \\'eggs\\', \\'ham\\', \\'milk\\', \\'spam\\'] \\n In [216]: d1.values() \\n Out[216]: [25, 2, 15, 3, 10] \\n # create a new dictionary using dict function \\n In [219]: d2 = dict({\\'orange\\': 5, \\'melon\\': 17, \\'milk\\': 10}) \\n In [220]: d2 \\n Out[220]: {\\'melon\\': 17, \\'milk\\': 10, \\'orange\\': 5} \\n # update dictionary d1 based on new key-values in d2 \\n In [221]: d1.update(d2) \\n In [222]: d1 \\n Out[222]:  {\\'eggs\\': 2, \\'ham\\': 15, \\'melon\\': 17, \\'milk\\': 10, \\'orange\\': 5, \\n\\'spam\\': 10} \\n # complex and nested  dictionary \\n In [223]:  d3 = {\\'k1\\': 5, \\'k2\\': [1,2,3,4,5], \\'k3\\': {\\'a\\': 1, \\'b\\': 2, \\'c\\': \\n[1,2,3]}} \\n In [225]: d3 \\n Out[225]:  {\\'k1\\': 5, \\'k2\\': [1, 2, 3, 4, 5], \\'k3\\': {\\'a\\': 1, \\'b\\': 2, \\'c\\': \\n[1, 2, 3]}} \\n In [226]: d3.get(\\'k3\\') \\n Out[226]: {\\'a\\': 1, \\'b\\': 2, \\'c\\': [1, 2, 3]} \\n In [227]: d3.get(\\'k3\\').get(\\'c\\') \\n Out[227]: [1, 2, 3] \\n Tuples \\n Tuples are also sequences like lists, but they are immutable. Typically, tuples are used \\nto represent fixed collections of objects or values. Tuples are created using a comma-\\nseparated sequence of values enclosed by parentheses, and optionally the  tuple() \\nfunction can also be used. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n77\\n The following code snippet shows some representations and operations on tuples: \\n # creating a tuple with a single element  \\n In [234]: single_tuple = (1,) \\n In [235]: single_tuple \\n Out[235]: (1,) \\n # original address of the tuple \\n In [239]: id(single_tuple) \\n Out[239]: 176216328L \\n # modifying contents of the tuple but its location changes (new tuple is \\ncreated) \\n In [240]: single_tuple = single_tuple + (2, 3, 4, 5) \\n In [241]: single_tuple \\n Out[241]: (1, 2, 3, 4, 5) \\n In [242]:  id(single_tuple) # different address indicating new tuple with \\nsame name \\n Out[242]: 201211312L \\n # tuples are immutable hence assignment is not supported like lists \\n In [243]: single_tuple[3] = 100 \\n Traceback (most recent call last): \\n  File \"<ipython-input-247-37d1946d4128>\", line 1, in <module> \\n    single_tuple[3] = 100 \\n TypeError: \\'tuple\\' object does not support item assignment \\n # accessing and unpacking tuples \\n In [243]: tup = ([\\'this\\', \\'is\\', \\'list\\', \\'1\\'], [\\'this\\', \\'is\\', \\'list\\', \\'2\\']) \\n In [244]: tup[0] \\n Out[244]: [\\'this\\', \\'is\\', \\'list\\', \\'1\\'] \\n In [245]: l1, l2 = tup \\n In [246]: print l1, l2 \\n [\\'this\\', \\'is\\', \\'list\\', \\'1\\'] [\\'this\\', \\'is\\', \\'list\\', \\'2\\'] \\n Files \\n Files are special types of objects in Python that are used mainly for interfacing with \\nexternal objects in the filesystem, including text, binary, audio, and video files, plus \\ndocuments, images, and many more. Some might disagree about it being a data type in \\nPython, but it actually is a special data type, and the name of the type, file, suits its role \\nperfectly for handling all types of external files. You usually use the  open()  function to \\nopen a file, and there are various modes like read and write that are specified using a \\nprocessing mode character in the function. \\n Some examples of file handling are show in the following code snippet: \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n78\\n In [253]: f = open(\\'text_file.txt\\', \\'w\\')   # open in write mode \\n In [254]: f.write(\"This is some text\\\\n\")  # write some text \\n In [255]: f.write(\"Hello world!\") \\n In [256]: f.close()  # closes the file \\n # lists files in current directory \\n In [260]: import os \\n In [262]: os.listdir(os.getcwd()) \\n Out[262]: [\\'text_file.txt\\']   \\n In [263]: f = open(\\'text_file.txt\\', \\'r\\')  # opens file in read mode \\n In [264]: data = f.readlines()  # reads in all lines from file \\n In [265]: print data  # prints the text data \\n [\\'This is some text\\\\n\\', \\'Hello world!\\'] \\n Miscellaneous \\n Besides the already mentioned data types and structures, there are several other Python \\ndata types:\\n\\xe2\\x80\\xa2 \\n The None type indicates no value/no data or null object. \\n\\xe2\\x80\\xa2 \\n Boolean types include True and False. \\n\\xe2\\x80\\xa2 \\n Decimal and Fraction types handle numbers in a better way.    \\n This completes the list for Python\\xe2\\x80\\x99s core data types and data structures that you will \\nbe using most of the time in your code and implementations. We will now discuss some \\nconstructs typically used for controlling the flow of code. \\n Controlling Code Flow \\n Flow of code is extremely important. A lot of it is based on business logic and rules. It also \\ndepends on the type of implementation decisions developers take when building systems \\nand applications. Python provides several control flow tools and utilities that can be used \\nto control the flow of your code. Here are the most popular ones:\\n\\xe2\\x80\\xa2 \\n if-elif-else conditionals \\n\\xe2\\x80\\xa2 \\n for loop \\n\\xe2\\x80\\xa2 \\n while loop \\n\\xe2\\x80\\xa2 \\n break, continue, and else in loops \\n\\xe2\\x80\\xa2 \\n try-except \\n These constructs will help you understand several concepts including conditional \\ncode flow, looping, and handling exceptions. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n79\\n Conditional Constructs \\n The concept of  conditional code flow involves executing different blocks of code \\nconditionally based on some user-defined logic implemented in the code itself. It is \\nextremely useful when you do not want to execute a block of statements sequentially \\none after the other but execute a part of them based on fulfilling or not fulfilling certain \\nconditions. The if-elif-else statements help in achieving this. The general syntax for it is as \\nfollows: \\n if <conditional check 1 is True>:    # the if conditional (mandatory) \\n    <code block 1>   # executed only when check 1 evaluates to True \\n        ...         \\n    <code block 1>            \\n elif <conditional check 2 is True>:  # the elif conditional (optional) \\n    <code block 2>   # executed only when check 1 is False and 2 is True \\n        ...        \\n    <code block 2>    \\n else:                                # the else conditional (optional) \\n    <code block 3>   # executed only when check 1 and 2 are False \\n        ...      \\n    <code block 3>    \\n  An important point to remember from the preceding syntax is that the corresponding \\ncode blocks only execute based on satisfying the necessary conditions. Also, only the  if \\nstatement is mandatory, and the  elif and  else statements do not need to be mentioned \\nunless there is a need based on conditional logic. \\n The following  examples depict conditional code flow: \\n In [270]: var = \\'spam\\' \\n In [271]: if var == \\'spam\\': \\n     ...:     print \\'Spam\\' \\n     ...:      \\n Spam \\n In [272]: var = \\'ham\\' \\n In [273]: if var == \\'spam\\': \\n     ...:     print \\'Spam\\' \\n     ...: elif var == \\'ham\\': \\n     ...:     print \\'Ham\\' \\n     ...:      \\n Ham \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n80\\n In [274]: var = \\'foo\\' \\n In [275]: if var == \\'spam\\': \\n     ...:     print \\'Spam\\' \\n     ...: elif var == \\'ham\\': \\n     ...:     print \\'Ham\\' \\n     ...: else:  \\n     ...:     print \\'Neither Spam or Ham\\' \\n     ...:      \\n Neither Spam or Ham \\n Looping Constructs \\n There are two main types of loops in Python:  for and  while loops. These  looping \\nconstructs are used to execute blocks of code repeatedly until some condition is satisfied \\nor the loop exits based on some other statements or conditionals. \\n The  for  statement is generally used to loop through items in sequence and usually \\nloops through one or many iterables sequentially, executing the same block of code in \\neach turn. The  while  statement is used more as a conditional general loop, which stops \\nthe loop once some condition is satisfied or runs the loop till some condition is satisfied. \\nInterestingly, there is an optional  else  statement at the end of the loops that is executed \\nonly if the loop exits normally without any break statements. The  break statement is often \\nused with a conditional to stop executing all statements in the loop immediately and exit the \\nclosest enclosing loop. The  continue statement stops executing all statements below it in \\nthe loop and brings back control to the beginning of the loop for the next iteration. The  pass  \\nstatement is just used as an empty placeholder\\xe2\\x80\\x94it does not do anything and is often used to \\nindicate an empty code  block . These statements constitute the core looping constructs. \\n The following snippets show the typical syntax normally used when constructing  for \\nand  while loops: \\n # the for loop \\n for item in iterable:  # loop through each item in the iterable \\n    <code block>    # Code block executed repeatedly \\n else:                  # Optional else  \\n    <code block>     # code block executes only if loop exits normally \\nwithout \\'break\\' \\n # the while loop \\n while <condition>:  # loop till condition is satisfied \\n    <code block>    # Code block executed repeatedly \\n else:                  # Optional else  \\n    <code block>     # code block executes only if loop exits normally \\nwithout \\'break\\' \\n The following examples show how loops work along with the other looping \\nconstructs including  pass ,  break , and  continue : \\n # illustrating for loops \\n In [280]: numbers = range(0,5) \\n In [281]: for number in numbers: \\n     ...:     print number \\n     ...:      \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n81\\n 0 \\n 1 \\n 2 \\n 3 \\n 4 \\n In [282]: sum = 0 \\n In [283]: for number in numbers: \\n     ...:     sum += number \\n     ...:    \\n In [284]: print sum \\n 10 \\n # role of the trailing else and break constructs \\n In [285]: for number in numbers: \\n     ...:     print number \\n     ...: else: \\n     ...:     print \\'loop exited normally\\' \\n     ...:      \\n 0 \\n 1 \\n 2 \\n 3 \\n 4 \\n loop exited normally \\n In [286]: for number in numbers: \\n     ...:     if number < 3: \\n     ...:         print number \\n     ...:     else: \\n     ...:         break \\n     ...: else: \\n     ...:     print \\'loop exited normally\\' \\n     ...:      \\n 0 \\n 1 \\n 2 \\n # illustrating while loops \\n In [290]: number = 5 \\n In [291]: while number > 0: \\n     ...:     print number \\n     ...:     number -= 1  # important! else loop will keep running \\n     ...:      \\n 5 \\n 4 \\n 3 \\n 2 \\n 1 \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n82\\n # role of continue  construct \\n In [295]: number = 10 \\n In [296]: while number > 0: \\n     ...:     if number % 2 != 0: \\n     ...:         number -=1 # decrement but do not print odd numbers \\n     ...:         continue  # go back to beginning of loop for next \\niteration \\n     ...:     print number  # print even numbers and decrement count \\n     ...:     number -= 1   \\n     ...:      \\n 10 \\n 8 \\n 6 \\n 4 \\n 2 \\n # role of the pass construct \\n In [297]: number = 10 \\n In [298]: while number > 0: \\n     ...:     if number % 2 != 0: \\n     ...:         pass # don\\'t print odds \\n     ...:     else: \\n     ...:         print number \\n     ...:     number -=  1 \\n     ...:      \\n 10 \\n 8 \\n 6 \\n 4 \\n 2 \\n Handling Exceptions \\n Exceptions are specific events that are either triggered when some unnatural error \\noccurs or manually. They are used extensively for error handling, event notifications, and \\ncontrolling code flow. Using constructs like  try-except-finally , you can make Python \\nraise exceptions when executing code whenever any error occurs at runtime. This would \\nalso enable you to catch these exceptions and handle them as needed or ignore them \\naltogether. In Python versions prior to 2.5.x, there were generally two versions of exception \\nhandling using the  try construct. One would be  try-finally , and the other would involve \\n try-except and optionally an  else clause at the end for catching exceptions. Now we have \\na construct that includes them all, the  try-except-else-finally construct, which can be \\nused for exception handling. The syntax is depicted as follows: \\n try:                            # The try statement \\n    <main code block>      # Checks for errors in this block  \\n except <ExceptionType1>:        # Catch different exceptions \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n83\\n    <exception handler 1> \\n except <ExceptionType2>:        \\n    <exception handler 2>          \\n ... \\n else:                           # Optional else statement \\n    <optional else block>  # Executes only if there were no exceptions \\n finally:                        # The finally statement \\n    <finally block>        # Always executes in the end \\n The flow of code in the preceding code snippet starts from the  try statement and \\nthe  main code block in it, which is executed first and checked for any exceptions. If \\nany exceptions occur, they are matched based on the exception types as depicted in \\nthe preceding snippet. Assuming  ExceptionType1 matches, the exception handler for \\n ExceptionType1 is executed, which is  exception handler 1 . In case no exceptions were \\nraised, only then the  optional else block is executed. The  finally block is always \\nexecuted irrespective of any exceptions being raised or not. \\n The following  examples depict the use of the  try-except-else-finally construct: \\n In [311]: shopping_list = [\\'eggs\\', \\'ham\\', \\'bacon\\'] \\n # trying to access a non-existent item in the list \\n In [312]: try: \\n     ...:     print shopping_list[3] \\n     ...: except IndexError as e: \\n     ...:     print \\'Exception: \\'+str(e)+\\' has occurred\\' \\n     ...: else: \\n     ...:     print \\'No exceptions occurred\\' \\n     ...: finally: \\n     ...:     print \\'I will always execute no matter what!\\' \\n     ...:      \\n Exception: list index out of range has occurred \\n I will always execute no matter what! \\n # smooth code execution without any errors \\n In [313]: try: \\n     ...:     print shopping_list[2] \\n     ...: except IndexError as e: \\n     ...:     print \\'Exception: \\'+str(e)+\\' has occurred\\' \\n     ...: else: \\n     ...:     print \\'No exceptions occurred\\' \\n     ...: finally: \\n     ...:     print \\'I will always execute no matter what!\\' \\n     ...:      \\n bacon \\n No exceptions occurred \\n I will always execute no matter what! \\n  This brings us to the end of our discussion on the core constructs for controlling flow \\nof code in Python. The next section covers some core concepts and constructs that are \\nparts of the functional programming paradigm in Python. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n84\\n Functional Programming \\n The functional programming paradigm is a style of programming with origins in lambda \\ncalculus. It treats any form of computation purely on the basis of executing and evaluating \\nfunctions. Python is not a pure functional programming language but does have several \\nconstructs that can be used for functional programming. In this section we will talk \\nabout several of these constructs, including functions and some advanced concepts like \\ngenerators, iterators, and comprehensions. We will also look at modules like  itertools \\nand  functools that contain implementation of functional tools based on concepts from \\nHaskell and Standard ML. \\n Functions \\n A  function can be defined as a block of code that is executed only on request by invoking \\nit. Functions consist of a function definition that has the function signature (function \\nname, parameters) and a group of statements inside the function that are executed when \\nthe function is called. The Python standard library provides a huge suite of functions to \\nchoose from to perform different types of operations. Besides this, users can define their \\nown functions using the  def keyword. \\n Functions usually return some value always, and even when they do not return a \\nvalue, by default they return the  None type. One important thing to remember is that often \\nyou may see methods and functions being used interchangeably, but the distinction \\nbetween functions and methods is that methods are functions that are defined within \\nclass statements. Functions are also objects, since each and every type and construct in \\nPython is derived from the base object type. This opens up a whole new dimension where \\nyou can even pass functions as parameters or arguments to other functions. Moreover, \\nfunctions can be bound to variables and even returned as results from other functions. \\nHence functions are often known as first-class objects in Python. \\n The following code snippet shows the basic structure of a function definition in \\nPython: \\n def function(params):  # params are the input parameters \\n    <code block>       # code block consists of a group of statements \\n    return value(s)    # optional return statement \\n The  params indicate the list of input parameters, which are not mandatory, and in \\nmany functions there are actually no input parameters. You can even pass functions \\nthemselves as parameters. Some logic executes in the code block, which may or may not \\nmodify the input parameters, and finally you may return some output values or not return \\nanything entirely. \\n The following code snippets demonstrate some basic examples of functions with \\nfixed arguments, variable arguments, and built-in functions: \\n # function with single argument \\n In [319]: def square(number): \\n     ...:     return number*number \\n     ...:  \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n85\\n In [320]: square(5) \\n Out[320]: 25 \\n # built-in function from the numpy library \\n In [321]: import numpy as np \\n In [322]: np.square(5) \\n Out[322]: 25 \\n # a more complex function with variable number of arguments \\n In [323]: def squares(*args): \\n     ...:     squared_args = [] \\n     ...:     for item in args:  \\n     ...:         squared_args.append(item*item) \\n     ...:     return squared_args \\n     ...:  \\n In [324]: squares(1,2,3,4,5) \\n Out[324]: [1, 4, 9, 16, 25] \\n  The preceding example shows how to introduce variable  number   of arguments in a \\nfunction dynamically. You can also introduce keyword arguments, where each argument \\nhas its own variable name and value, as illustrated in the following code snippet: \\n # assign specific keyword based arguments dynamically \\n In [325]: def person_details(**kwargs): \\n     ...:     for key, value in kwargs.items(): \\n     ...:         print key, \\'->\\', value \\n     ...:          \\n In [326]:  person_details(name=\\'James Bond\\', alias=\\'007\\', job=\\'Secret Service \\nAgent\\') \\n alias -> 007 \\n job -> Secret Service Agent \\n name -> James Bond \\n Recursive Functions \\n Recursive functions use the concept of  recursion , wherein the function calls itself inside \\nits code block. Care should be taken to make sure there is a stopping condition that \\nultimately terminates the recursive calls\\xe2\\x80\\x94otherwise the function will run into an endless \\nloop of execution where it goes on calling itself. Recursion makes use of the call stack at \\neach recursive call, hence they are often not very efficient compared to regular functions; \\nnevertheless, they are extremely powerful. \\n The following example depicts our  squares function using recursion: \\n # using recursion to square numbers \\n In [331]: def recursive_squares(numbers): \\n     ...:     if not numbers: \\n     ...:         return [] \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n86\\n     ...:     else: \\n     ...:          return [numbers[0]*numbers[0]] + recursive_\\nsquares(numbers[1:]) \\n     ...:      \\n In [332]: recursive_squares([1, 2, 3, 4, 5]) \\n Out[332]: [1, 4, 9, 16, 25] \\n Anonymous Functions \\n Anonymous functions  are functions that do not have any name and usually consist of \\na one-line expression that denotes a function using the lambda construct. The  lambda \\nkeyword is used to define inline function objects that can be used just like regular \\nfunctions, with a few differences. The general syntax for a  lambda function is shown in the \\nfollowing code snippet: \\n lambda arg, arg2,... arg_n : <inline expression using args> \\n This expression can actually be even assigned to variables and then executed as a \\nnormal function call similar to functions created with  def . However, lambda functions are \\nexpressions and never statements like the code block inside a  def , and so it is extremely \\ndifficult to put complex logic inside a  lambda function because it is always a single-lined \\ninline expression. However,  lambda functions are very powerful and are even used inside \\nlists, functions, and function arguments. Besides  lambda functions, Python also provides \\nfunctions like  map() ,  reduce() , and  filter() , which make extensive use of  lambda \\nfunctions and apply them to iterables usually to transform, reduce, or filter respectively. \\n The following code snippet depicts some examples of  lambda functions used with the \\nconstructs we just talked about: \\n # simple lambda function to square a number \\n In [340]: lambda_square = lambda n: n*n \\n In [341]: lambda_square(5) \\n Out[341]: 25 \\n # map function to square numbers using lambda \\n In [342]: map(lambda_square, [1, 2, 3, 4, 5]) \\n Out[342]: [1, 4, 9, 16, 25] \\n # lambda function to find even numbers used for filtering \\n In [343]: lambda_evens = lambda n: n%2 == 0 \\n In [344]: filter(lambda_evens, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) \\n Out[344]: [2, 4, 6, 8, 10] \\n # lambda function to add numbers used for adding numbers in reduce function \\n In [345]: lambda_sum = lambda x, y: x + y \\n In [346]: reduce(lambda_sum, [1, 2, 3, 4, 5]) \\n Out[346]: 15 \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n87\\n # lambda function to make a sentence from word tokens with reduce function \\n In [347]:  lambda_sentence_maker = lambda word1, word2: \\' \\'.join([word1, \\nword2]) \\n In [348]:  reduce(lambda_sentence_maker, [\\'I\\', \\'am\\', \\'making\\', \\'a\\', \\n\\'sentence\\', \\'from\\', \\'words!\\']) \\n Out[348]: \\'I am making a sentence from words!\\' \\n The preceding examples should give you a pretty good idea about how  lambda \\nfunctions work and how powerful they are. Using a one-line construct you can create \\nfree-flowing sentences from word tokens and calculate a sum of  numbers in a list! The \\npossibilities of using  lambda functions are endless, and you can use them to solve even \\nthe most complex of problems. \\n Iterators \\n Iterators are constructs used to iterate through iterables.  Iterables are objects that are \\nbasically sequences of other objects and data. A good example would be a  for  loop, which is \\nactually an iterable that iterates through a list or sequence. Iterators are objects or constructs \\nthat can be used to iterate through iterables using the  next() function, which returns the next \\nitem from the iterable at each call. Once it has iterated through the entire iterable, it returns \\na  StopIteration exception. We have seen how a for loop works in general, however behind \\nthe abstraction, the  for loop actually calls the  iter() function on the iterable to create an \\niterator object and then traverses through it using the  next() function. \\n The following example illustrates how iterators work: \\n # typical for loop \\n In [350]: numbers = range(6) \\n In [351]: for number in numbers: \\n     ...:     print number \\n 0 \\n 1 \\n 2 \\n 3 \\n 4 \\n 5 \\n # illustrating how iterators work behind the scenes \\n In [352]: iterator_obj = iter(numbers) \\n In [353]: while True: \\n     ...:     try: \\n     ...:         print iterator_obj.next() \\n     ...:     except StopIteration: \\n     ...:         print \\'Reached end of sequence\\' \\n     ...:         break \\n 0 \\n 1 \\n 2 \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n88\\n 3 \\n 4 \\n 5 \\n Reached end of sequence \\n # calling next now would throw the StopIteration exception as expected \\n In [354]: iterator_obj.next() \\n Traceback (most recent call last): \\n  File \"<ipython-input-354-491178c4f97a>\", line 1, in <module> \\n    iterator_obj.next() \\n StopIteration \\n Comprehensions \\n Comprehensions  are interesting constructs that are similar to  for  loops but more \\nefficient. They fall rightly into the functional programming paradigm following the set \\nbuilder notation. Originally, the idea for list comprehensions came from Haskell, and \\nafter a series of lengthy discussions comprehensions were finally added and have been \\none of the most used constructs ever since. There are various types of comprehensions \\nthat can be applied on existing data types, including list, set, and dict comprehensions. \\nThe following code snippet shows the syntax of comprehensions using the very common \\nlist comprehensions and  for loops, a core component in comprehensions: \\n # typical comprehension syntax \\n [ expression for item in iterable ] \\n # equivalent for loop statement \\n for item in iterable: \\n    expression \\n # complex and nested iterations \\n [ expression for item1 in iterable1 if condition1 \\n             for item2 in iterable2 if condition2 ... \\n             for itemN in iterableN if conditionN ] \\n # equivalent for loop statement \\n for item1 in iterable1: \\n    if condition1: \\n        for item2 in iterable2: \\n            if condition2: \\n                ... \\n                   for itemN in iterableN: \\n                       if conditionN: \\n                           expression \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n89\\n  This gives us an idea of how similar comprehensions are to looping constructs. The \\nbenefit we get is that they are more efficient and perform better than loops. Some caveats \\ninclude that you cannot use assignment statements in comprehensions because, if you \\nremember the syntax from earlier, they support only expressions and not statements. The \\nsame syntax is used by set and dictionary comprehensions too. \\n The following examples illustrate the use of different  comprehensions : \\n In [355]: numbers = range(6) \\n In [356]: numbers \\n Out[356]: [0, 1, 2, 3, 4, 5] \\n # simple list comprehension to compute squares \\n In [357]: [num*num for num in numbers] \\n Out[357]: [0, 1, 4, 9, 16, 25] \\n # list comprehension to check if number is divisible by 2 \\n In [358]: [num%2 for num in numbers] \\n Out[358]: [0, 1, 0, 1, 0, 1] \\n # set comprehension returns distinct values of the above operation \\n In [359]: set(num%2 for num in numbers) \\n Out[359]: {0, 1} \\n # dictionary comprehension where key:value is number: square(number) \\n In [361]: {num: num*num for num in numbers} \\n Out[361]: {0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25} \\n # a more complex comprehension showcasing above operations in a single \\ncomprehension \\n In [362]: [{\\'number\\': num,  \\n            \\'square\\': num*num,  \\n            \\'type\\': \\'even\\' if num%2 == 0 else \\'odd\\'} for num in numbers] \\n Out[362]:  \\n [{\\'number\\': 0, \\'square\\': 0, \\'type\\': \\'even\\'}, \\n {\\'number\\': 1, \\'square\\': 1, \\'type\\': \\'odd\\'}, \\n {\\'number\\': 2, \\'square\\': 4, \\'type\\': \\'even\\'}, \\n {\\'number\\': 3, \\'square\\': 9, \\'type\\': \\'odd\\'}, \\n {\\'number\\': 4, \\'square\\': 16, \\'type\\': \\'even\\'}, \\n {\\'number\\': 5, \\'square\\': 25, \\'type\\': \\'odd\\'}] \\n # nested list comprehension - flattening a list of lists \\n In [364]: list_of_lists = [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]] \\n In [365]: list_of_lists \\n Out[365]: [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]] \\n In [367]: [item for each_list in list_of_lists for item in each_list] \\n Out[367]: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n90\\n Generators \\n Generators  are powerful, memory-efficient constructs for creating and consuming \\niterators. They exist in two variants: functions and expressions. Generators work on a \\nconcept known as  lazy evaluation \\xe2\\x80\\x94hence, they are more memory efficient and perform \\nbetter in most cases because they do not require the entire object to be evaluated \\nand loaded in one go, as in list comprehensions. However, the caveat is that because \\ngenerators yield one item at a time in an ad hoc fashion, there is a chance that they may \\nperform worse in terms of execution time compared to list comprehensions, unless you \\nare dealing with large objects with many elements. \\n Generator functions are implemented as regular functions using the  def statement. \\nHowever, they use the concept of lazy evaluation and return one object at a time using \\nthe  yield  statement. Unlike regular functions that have a  return statement, which once \\nexecuted ends the execution of the code block inside the function, generators use the \\n yield statement, which suspends and resumes execution and the state after generating \\nand returning each value or object. To be more precise, generator functions yield values \\nat each step rather than returning them. This ensures that the current state including \\ninformation about the local code block scope it retained and enables the generator to \\nresume from where it left off. \\n The following snippet shows some examples for generator functions: \\n In [369]: numbers = [1, 2, 3, 4, 5] \\n In [370]: def generate_squares(numbers): \\n     ...:     for number in numbers: \\n     ...:         yield number*number \\n In [371]: gen_obj = generate_squares(numbers) \\n In [372]: gen_obj \\n Out[372]: <generator object generate_squares at 0x000000000F2FC2D0> \\n In [373]: for item in gen_obj: \\n     ...:     print item \\n     ...:      \\n 1 \\n 4 \\n 9 \\n 16 \\n 25 \\n The advantages of these generators are both memory efficiency and execution time, \\nespecially when iterables and objects are large in size and occupy substantial memory. \\nYou also do not need to load whole objects into the main memory for performing various \\noperations on them. They often work very well on streaming data where you cannot keep \\nall the data in memory at all times. The same applies for generator expressions, which are \\nvery similar to comprehensions except they are enclosed in parentheses. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n91\\n The following example illustrates: \\n In [381]: csv_string = \\'The,fox,jumps,over,the,dog\\' \\n # making a sentence using list comprehension \\n In [382]: list_cmp_obj = [item for item in csv_string.split(\\',\\')] \\n In [383]: list_cmp_obj \\n Out[383]: [\\'The\\', \\'fox\\', \\'jumps\\', \\'over\\', \\'the\\', \\'dog\\'] \\n In [384]: \\' \\'.join(list_cmp_obj) \\n Out[384]: \\'The fox jumps over the dog\\' \\n # making a sentence using generator expression \\n In [385]: gen_obj = (item for item in csv_string.split(\\',\\')) \\n In [386]: gen_obj \\n Out[386]: <generator object <genexpr> at 0x000000000F2FC3F0> \\n In [387]: \\' \\'.join(gen_obj) \\n Out[387]: \\'The fox jumps over the dog\\' \\n Both  generator functions and expressions create generator objects that use the same \\nconstruct as iterators and starts, stops, and resumes the function or loop at each stage, \\nand once it is completed it raises the  StopIteration exception. \\n The itertools and functools Modules \\n Various modules which are available in the Python standard library. Some of the popular \\nones include  collections ,  itertools , and  functools , which have various constructs and \\nfunctions that can be used to boost productivity and reduce time spent writing extra code \\nto solve problems. The  itertools module is a complete module dedicated to building and \\noperating on iterators. It has various functions that support different operations including \\nslicing, chaining, grouping, and splitting iterators. The most comprehensive source of \\ninformation for  itertools is available in the official Python documentation at   https://\\ndocs.python.org/2/library/itertools.html  . The documentation lists each function \\nand its role with examples. The  functools  module   provides  with functions, which enable \\nconcepts from functional programming, including wrappers and partials. These functions \\nusually act on other functions, which it takes as input parameters and often returns a \\nfunction as the result itself. The official documentation at   https://docs.python.org/2/\\nlibrary/functools.html  provides extensive information on each function.  \\n Classes \\n Python  classes are constructs that enable us to write code following the OOP paradigm. \\nConcepts like objects, encapsulation, methods, inheritance, and polymorphism are heavily \\nused in this paradigm. If you have worked on any OOP language before, like C++ or Java, \\nchances are you will find using Python classes relatively similar to using classes in other \\nlanguages. Discussing each concept would be beyond the scope of this book, but I will briefly \\ncover the basic concepts of classes and touch up on different types of objects and inheritance. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n92\\n Classes are basically a software model or abstraction of real-world entities that \\nare objects. This abstraction leads to classes being called as a user-defined type, and \\nonce you define a class, you can instantiate and create instances or objects of that class. \\nEach object has its own instance variables and methods that define the properties and \\nbehavior of that object. All classes inherit from the base object type, and you can create \\nyour own classes and inherit further classes from these user-defined classes. Classes are \\nalso ultimately objects on their own and can be bound to variables and other constructs. \\n The following snippet gives the basic syntax for a class: \\n class ClassName(BaseClass): \\n    class_variable  # shared by all instances\\\\objects \\n    def __init__(self, ...): # the constructor \\n        # instance variables unique to each instance\\\\object \\n        self.instance_variables = ... \\n    def __str__(self):  # string representation of the instance\\\\object \\n        return repr(self) \\n    def methods(self, ...):  # instance methods \\n        <code block> \\n The preceding snippet tells us that the class named  ClassName inherits from \\nits parent class  BaseClass . There can be more than one parent or base class in the \\nparameters separated by commas. The  __init__() method acts as a constructor \\nthat instantiates and creates an object of the class using the call  ClassName(...) , \\nwhich automatically invokes the  __init__() method\\xe2\\x80\\x94which may optionally take \\nparameters based on its definition. The  __str__() method is optional. It prints the string \\nrepresentation of the object. You can modify the default method with your own definition, \\nand it is often used to print the current state of the object variables. The  class_variable \\nindicates class variables that are defined in the block just enclosing the class definition, \\nand these class variables are shared by all objects or instances of the class. The  instance_\\nvariables are variables that are specific to each object or instance. The methods denote \\ninstance methods that define specific behavior of the objects. The  self parameter is \\nusually used as the first parameter in methods, which is more of a convention that refers \\nto the instance or object of  ClassName on which you call the method. \\n The following example depicts a simple  class and how it works: \\n # class definition \\n In [401]: class Animal(object): \\n     ...:     species = \\'Animal\\' \\n     ...:  \\n     ...:     def __init__(self, name): \\n     ...:         self.name = name \\n     ...:         self.attributes = [] \\n     ...:  \\n     ...:     def add_attributes(self, attributes): \\n     ...:         self.attributes.extend(attributes) \\\\ \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n93\\n     ...:             if type(attributes) == list \\\\ \\n     ...:             else self.attributes.append(attributes) \\n     ...:  \\n     ...:     def __str__(self): \\n     ...:          return self.name+\" is of type \"+self.species+\" and has \\nattributes:\"+str(self.attributes) \\n     ...:      \\n # instantiating the class \\n In [402]: a1 = Animal(\\'Rover\\') \\n # invoking instance method \\n In [403]: a1.add_attributes([\\'runs\\', \\'eats\\', \\'dog\\']) \\n # user defined string representation of the Animal class \\n In [404]: str(a1) \\n Out[404]:  \"Rover is of type Animal and has attributes:[\\'runs\\', \\'eats\\', \\n\\'dog\\']\" \\n  This gives us an idea of how classes work. But what if we want to target specific \\nanimals like  dogs and  foxes ? We can apply the concept of inheritance and use the  super() \\nmethod to access the constructor of the base  Animal  class in each definition. The \\nfollowing examples illustrate the concept of inheritance: \\n # deriving class Dog from base class Animal \\n In [413]: class Dog(Animal):     \\n     ...:     species = \\'Dog\\' \\n     ...:  \\n     ...:     def __init__(self, *args): \\n     ...:         super(Dog, self).__init__(*args)  \\n # deriving class Fox from base class Animal \\n In [414]: class Fox(Animal):     \\n     ...:     species = \\'Fox\\' \\n     ...:  \\n     ...:     def __init__(self, *args): \\n     ...:         super(Fox, self).__init__(*args) \\n # creating instance of class Dog \\n In [415]: d1 = Dog(\\'Rover\\') \\n In [416]: d1.add_attributes([\\'lazy\\', \\'beige\\', \\'sleeps\\', \\'eats\\']) \\n In [417]: str(d1) \\n Out[417]:  \"Rover is of type Dog and has attributes:[\\'lazy\\', \\'beige\\', \\n\\'sleeps\\', \\'eats\\']\"  \\n # creating instance of class  Fox \\n In [418]: f1 = Fox(\\'Silver\\') \\n In [419]: f1.add_attributes([\\'quick\\', \\'brown\\', \\'jumps\\', \\'runs\\']) \\n In [420]: str(f1) \\n Out[420]:  \"Silver is of type Fox and has attributes:[\\'quick\\', \\'brown\\', \\n\\'jumps\\', \\'runs\\']\" \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n94\\n Working with Text \\n We have seen most of the constructs, data types, structures, concepts, and programming \\nparadigms associated with Python in the previous sections. This section briefly covers \\nspecific data types tailored to handle text data and shows how these data types and their \\nassociated utilities will be useful for us in the future chapters. The main data types used to \\nhandle text data in Python are strings, which can be normal strings, bytes storing binary \\ninformation, or Unicode. By default, all strings are Unicode in  Python 3.x  , but they are not \\nso in Python 2.x, and this is something you should definitely keep in mind when dealing \\nwith text in different Python distributions. Strings are a sequence of characters in Python \\nsimilar to arrays and code with a set of attributes and methods that can be leveraged to \\nmanipulate and operate on text data easily, which makes Python the language of choice \\nfor text analytics in many scenarios. We will discuss various types of strings with several \\nexamples in the next section. \\n String  Literals \\n There are various types of strings, as mentioned earlier, and you saw a few examples in \\none of the previous sections regarding data types. The following BNF (Backus-Naur  Form ) \\ngives us the general lexical definitions for producing strings as seen in the official Python \\ndocs: \\n stringliteral   ::=  [stringprefix](shortstring | longstring) \\n stringprefix    ::=  \"r\" | \"u\" | \"ur\" | \"R\" | \"U\" | \"UR\" | \"Ur\" | \"uR\" \\n                     | \"b\" | \"B\" | \"br\" | \"Br\" | \"bR\" | \"BR\" \\n shortstring     ::=  \"\\'\" shortstringitem* \"\\'\" | \\'\"\\' shortstringitem* \\'\"\\' \\n longstring      ::=  \"\\'\\'\\'\" longstringitem* \"\\'\\'\\'\" | \\'\"\"\"\\' longstringitem* \\n\\'\"\"\"\\' \\n shortstringitem ::=  shortstringchar | escapeseq \\n longstringitem  ::=  longstringchar | escapeseq \\n shortstringchar ::=  <any source character except \"\\\\\" or newline or the \\nquote> \\n longstringchar  ::=  <any source character except \"\\\\\"> \\n escapeseq       ::=  \"\\\\\" <any ASCII character> \\n The preceding rules tell us that different types of string prefixes exist that can be used \\nwith different string types to produce string literals. In simple terms, the following types of \\nstring literals are used the most:\\n\\xe2\\x80\\xa2 \\n Short strings : These strings are usually enclosed with single ( \\' ) or \\ndouble quotes ( \" ) around the characters. Some examples would \\nbe,  \\'Hello\\' and  \"Hello\" . \\n\\xe2\\x80\\xa2 \\n Long strings : These strings are usually enclosed with three \\nsingle ( \\'\\'\\' ) or double quotes ( \"\"\" ) around the characters. Some \\nexamples would be,  \"\"\"Hello, I\\xe2\\x80\\x99m a long string\"\"\" or \\n \\'\\'\\'Hello I\\\\\\xe2\\x80\\x99m a long string \\'\\'\\' . Note the ( \\\\\\xe2\\x80\\x99 ), indicates an \\nescape sequence discussed in the next bullet. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n95\\n\\xe2\\x80\\xa2 \\n Escape sequences in strings : These strings often have escape \\nsequences embedded in them, where the rule for escape \\nsequences starts with a backslash ( \\\\ ) followed by any ASCII \\ncharacter. Hence, they perform backspace interpolation. Popular \\nescape sequences include ( \\\\n ), indicating a new line character, \\nand ( \\\\t ), indicating a tab. \\n\\xe2\\x80\\xa2 \\n Bytes : These are used to represent bytestrings, which create \\nobjects of the  bytes data type. These strings can be created as \\n bytes(\\'...\\') or using the  b\\'...\\' notation. Examples would be \\n bytes(\\'hello\\') and  b\\'hello\\' . \\n\\xe2\\x80\\xa2 \\n Raw strings : These strings were originally created specifically for \\nregular expressions (regex) and creating regex patterns. These \\nstrings can be created using the  r\\'...\\' notation and keep the \\nstring in its raw or native form. Hence, it does not perform any \\nbackspace interpolation and turns off the escape sequences. An \\nexample would be  r\\'Hello\\' . \\n\\xe2\\x80\\xa2 \\n Unicode : These  strings support Unicode characters in text and \\nare usually non-ASCII character sequences. These strings are \\ndenoted with the  u\\'...\\' notation. Besides the string notation, \\nthere are several specific ways to represent special  Unicode \\ncharacters in the string. The usual include the hex byte value \\nescape sequence of the format  \\'\\\\xVV\\' . Besides this, we also \\nhave Unicode escape sequences of the form  \\'\\\\uVVVV\\' and  \\'\\\\\\nuVVVVVVVV\\', where the first form uses 4 hex-digits for encoding \\na 16-bit character, and the second uses 8 hex digits for encoding \\na 32-bit character. Some examples would be  u \\'H\\\\xe8llo\\' and  u \\n\\'H\\\\u00e8llo\\' which represents the string  \\'H\\xc3\\xa8llo\\' . \\n The following code snippet depicts these different types of string literals and their \\noutput: \\n # simple string \\n In [422]: simple_string = \\'hello\\' + \" I\\'m a simple string\" \\n In [423]: print simple_string \\n hello I\\'m a simple string \\n # multi-line string, note the \\\\n (newline) escape character automatically \\ncreated \\n In [424]: multi_line_string = \"\"\"Hello I\\'m \\n     ...: a multi-line \\n     ...: string!\"\"\" \\n In [425]: multi_line_string \\n Out[425]: \"Hello I\\'m\\\\na multi-line\\\\nstring!\" \\n In [426]: print multi_line_string \\n Hello I\\'m \\n a multi-line \\n string! \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n96\\n # Normal string with escape sequences leading to a wrong file path! \\n In [427]: escaped_string = \"C:\\\\the_folder\\\\new_dir\\\\file.txt\" \\n In [428]:  print escaped_string  # will cause errors if we try to open a file \\nhere \\n C:      he_folder \\n ew_dirile.txt \\n # raw string keeping the backslashes in its normal form \\n In [429]: raw_string = r\\'C:\\\\the_folder\\\\new_dir\\\\file.txt\\' \\n In [430]: print raw_string \\n C:\\\\the_folder\\\\new_dir\\\\file. txt \\n # unicode string literals \\n In [431]: string_with_unicode = u\\'H\\\\u00e8llo!\\' \\n     ...: print string_with_unicode \\n H\\xc3\\xa8llo! \\n String  Operations and Methods \\n Strings are iterable sequences, which means a lot of operations can be performed on \\nthem, useful especially when processing and parsing textual data into easy-to-consume \\nformats. Several operations can be performed on strings. I have categorized them into the \\nfollowing segments:\\n\\xe2\\x80\\xa2 \\n Basic operations \\n\\xe2\\x80\\xa2 \\n Indexing and slicing \\n\\xe2\\x80\\xa2 \\n Methods \\n\\xe2\\x80\\xa2 \\n Formatting \\n\\xe2\\x80\\xa2 \\n Regular expressions \\n These would cover the most frequently used techniques for working with strings and \\nform the base of what we would need to get started in the next chapter (where we look at \\nunderstanding and processing textual data based on concepts we learned in the first two \\nchapters). \\n Basic Operations \\n You can perform several basic operations on strings, including concatenation and \\nchecking for substrings, characters, and lengths. The following code snippet illustrates \\nthese operations with some examples: \\n # Different ways of String concatenation \\n In [436]: \\'Hello\\' + \\' and welcome \\' + \\'to Python!\\' \\n Out[436]: \\'Hello and welcome to Python!\\' \\n In [437]: \\'Hello\\' \\' and welcome \\' \\'to Python!\\' \\n Out[437]: \\'Hello and welcome to Python!\\' \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n97\\n # concatenation of variables and literals \\n In [438]: s1 = \\'Python!\\' \\n In [439]: \\'Hello \\' + s1 \\n Out[439]: \\'Hello Python!\\' \\n # we cannot concatenate a variable and a literal using this method \\n In [440]: \\'Hello \\' s1 \\n  File \"<ipython-input-440-2f801ddf3480>\", line 1 \\n    \\'Hello \\' s1 \\n              ^ \\n SyntaxError: invalid syntax \\n # some more ways of concatenating strings \\n In [442]: s2 = \\'--Python--\\' \\n In [443]: s2 * 5 \\n Out[443]: \\'--Python----Python----Python----Python----Python--\\' \\n In [444]: s1 + s2 \\n Out[444]: \\'Python!--Python--\\' \\n In [445]: (s1 + s2)*3 \\n Out[445]: \\'Python!--Python--Python!--Python--Python!--Python--\\' \\n # concatenating several strings together in parentheses \\n In [446]: s3 = (\\'This \\' \\n     ...:       \\'is another way \\' \\n     ...:       \\'to concatenate \\' \\n     ...:       \\'several strings!\\') \\n In [447]: s3 \\n Out[447]: \\'This is another way to concatenate several strings!\\' \\n # checking for substrings in a string \\n In [448]: \\'way\\' in s3 \\n Out[448]: True \\n In [449]: \\'python\\' in s3 \\n Out[449]: False \\n # computing total length of the string \\n In [450]: len(s3) \\n Out[450]: 51 \\n Indexing and Slicing \\n As mentioned, strings are iterables\\xe2\\x80\\x94ordered sequences of characters. Hence they can \\nbe indexed, sliced, and iterated through similarly to other iterables such as lists. Each \\ncharacter has a specific position in the string, called its  index . Using indexes, we can \\naccess specific parts of the string. Accessing a single character using a specific position \\nor index in the string is called  indexing , and accessing a part of a string, for example, \\na substring using a start and end index, is called  slicing . Python supports two types of \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n98\\nindexes. One starts from 0 and increases by 1 each time per character till the end of the \\nstring. The other starts from \\xe2\\x80\\x931 at the end of the string and decreases by 1 each time for \\neach character till the beginning of the  string . Figure\\xc2\\xa0 2-6 shows the two types of indexes \\nfor the string  \\'PYTHON\\' . \\n To access any particular character in the string, you need to use the corresponding \\nindex, and slices can be extracted using the syntax  var[start:stop] , which extracts all \\ncharacters in the string  var from index  start till index  stop excluding the character at the \\n stop index. \\n The following examples shows how to index, slice, and iterate through strings: \\n # creating a string \\n In [460]: s = \\'PYTHON\\' \\n # depicting string indexes \\n In [461]: for index, character in enumerate(s): \\n     ...:     print \\'Character\\', character+\\':\\', \\'has index:\\', index \\n Character P: has index: 0 \\n Character Y: has index: 1 \\n Character T: has index: 2 \\n Character H: has index: 3 \\n Character O: has index: 4 \\n Character N: has index: 5 \\n # string indexing \\n In [462]: s[0], s[1], s[2], s[3], s[4], s[5] \\n Out[462]: (\\'P\\', \\'Y\\', \\'T\\', \\'H\\', \\'O\\', \\'N\\') \\n In [463]: s[-1], s[-2], s[-3], s[-4], s[-5], s[-6] \\n Out[463]: (\\'N\\', \\'O\\', \\'H\\', \\'T\\', \\'Y\\', \\'P\\') \\n # string slicing \\n In [464]: s[:]  \\n Out[464]: \\'PYTHON\\'   # prints whole string when no indexes are specified  \\n In [465]: s[1:4] \\n Out[465]: \\'YTH\\' \\n In [466]: s[:3] \\n Out[466]: \\'PYT\\' \\n In [467]: s[3:] \\n Out[467]: \\'HON\\' \\n Figure 2-6.  String indexing syntax \\n \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n99\\n In [468]: s[-3:] \\n Out[468]: \\'HON\\' \\n In [469]: s[:3] + s[3:] \\n Out[469]: \\'PYTHON\\' \\n In [470]: s[:3] + s[-3:] \\n Out[470]: \\'PYTHON\\' \\n # string slicing with offsets \\n In [472]: s[::1]  # no offset \\n Out[472]: \\'PYTHON\\' \\n In [473]: s[::2]  # print every 2nd character in string \\n Out[473]: \\'PTO\\' \\n # strings are immutable hence assignment throws error \\n In [476]: s[0] = \\'X\\' \\n Traceback (most recent call last): \\n  File \"<ipython-input-476-2cd5921aae94>\", line 1, in <module> \\n    s[0] = \\'X\\' \\n TypeError: \\'str\\' object does not support item assignment \\n # creates a new string \\n In [477]: \\'X\\' + s[1:] \\n Out[477]: \\'XYTHON\\' \\n Methods \\n Strings and Unicode put a huge arsenal of  built-in methods   at your disposal, which \\nyou can use for performing various transformations, manipulations, and operations on \\nstrings. Although discussing each method in detail would be beyond the current scope, \\nthe official Python documentation at   https://docs.python.org/2/library/stdtypes.\\nhtml#string-methods  provides all the information you need about each and every \\nmethod, along with syntax and definitions. Methods are extremely useful and increase \\nyour productivity because you do not have to spend extra time writing boilerplate code to \\nhandle and manipulate strings. \\n The following code snippets show some popular examples of string methods in \\naction: \\n # case conversions \\n In [484]: s = \\'python is great\\' \\n In [485]: s.capitalize() \\n Out[485]: \\'Python is great\\' \\n In [486]: s.upper() \\n Out[486]: \\'PYTHON IS GREAT\\' \\n # string replace \\n In [487]: s.replace(\\'python\\', \\'analytics\\') \\n Out[487]: \\'analytics is great\\' \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n100\\n # string splitting and joining \\n In [488]: s = \\'I,am,a,comma,separated,string\\' \\n In [489]: s.split(\\',\\') \\n Out[489]: [\\'I\\', \\'am\\', \\'a\\', \\'comma\\', \\'separated\\', \\'string\\'] \\n In [490]: \\' \\'.join(s.split(\\',\\')) \\n Out[490]: \\'I am a comma separated string\\' \\n # stripping whitespace characters \\n In [497]: s = \\'   I am surrounded by spaces    \\' \\n In [498]: s \\n Out[498]: \\'   I am surrounded by spaces    \\' \\n In [499]: s.strip() \\n Out[499]: \\'I am surrounded by spaces\\' \\n # coverting to title case \\n In [500]: s = \\'this is in lower case\\' \\n In [501]: s.title() \\n Out[501]: \\'This Is In Lower Case\\' \\n The preceding examples just scratch the surface of the numerous  manipulations \\nand operations  possible on strings. Feel free to try out other operations using different \\nmethods mentioned in the docs. We will use several of them in subsequent chapters. \\n Formatting \\n String  formatting  is used to substitute specific data objects and types in a string. This \\nis mostly used when displaying text to the user. There are mainly two different types of \\nformatting used for strings:\\n\\xe2\\x80\\xa2 \\n Formatting expressions : These expressions are typically of the \\nsyntax  \\'...%s...%s...\\' %(values) , where the  %s  denotes a \\nplaceholder for substituting a string from the list of strings depicted \\nin  values . This is quite similar to the C style  printf model and has \\nbeen there in Python since the beginning. You can substitute values \\nof other types with the respective alphabet following the  % symbol, \\nlike  %d  for integers and  %f for floating point numbers.  \\n\\xe2\\x80\\xa2 \\n Formatting methods : These  strings take the form of  \\'...{}...\\n{}...\\'.format(values) , which makes use of the braces  {} \\nfor placeholders to place strings from  values using the format \\nmethod. These have been present in Python since version 2.6.x. \\n The following code snippets depict both types of string formatting using several \\nexamples: \\n # simple string formatting expressions \\n In [506]: \\'Hello %s\\' %(\\'Python!\\') \\n Out[506]: \\'Hello Python!\\' \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n101\\n In [507]: \\'Hello %s\\' %(\\'World!\\') \\n Out[507]: \\'Hello World!\\' \\n # formatting expressions with different data types \\n In [508]:  \\'We have %d %s containing %.2f gallons of %s\\' %(2, \\'bottles\\', 2.5, \\n\\'milk\\') \\n Out[508]: \\'We have 2 bottles containing 2.50 gallons of milk\\' \\n In [509]:  \\'We have %d %s containing %.2f gallons of %s\\' %(5, \\'jugs\\', 10.867, \\n\\'juice\\') \\n Out[509]: \\'We have 5 jugs containing 10.87 gallons of juice\\' \\n # formatting using the format method \\n In [511]:  \\'Hello {} {}, it is a great {} to meet you\\'.format(\\'Mr.\\', \\'Jones\\', \\n\\'pleasure\\') \\n Out[511]: \\'Hello Mr. Jones, it is a great pleasure to meet you\\' \\n In [512]:  \\'Hello {} {}, it is a great {} to meet you\\'.format(\\'Sir\\', \\n\\'Arthur\\', \\'honor\\') \\n Out[512]: \\'Hello Sir Arthur, it is a great honor to meet you\\' \\n # alternative ways of using format \\n In [513]:  \\'I have a {food_item} and a {drink_item} with me\\'.format(drink_\\nitem=\\'soda\\', food_item=\\'sandwich\\') \\n Out[513]: \\'I have a sandwich and a soda with me\\' \\n In [514]:  \\'The {animal} has the following attributes: {attributes}\\'.\\nformat(animal=\\'dog\\', attributes=[\\'lazy\\', \\'loyal\\']) \\n Out[514]: \"The dog has the following attributes: [\\'lazy\\', \\'loyal\\']\" \\n  From the preceding  examples , you can see that there is no hard-and-fast rule for \\nformatting strings, so go ahead and experiment with different formats and use the one \\nbest suited for your task. \\n Regular Expressions (Regexes) \\n Regular expressions, also called  regexes , allow you to create string patterns and use them \\nfor searching and substituting specific pattern matches in textual data. Python offers a \\nrich module named  re for creating and using regular expressions. Entire books have been \\nwritten on this topic because it is easy to use but difficult to master. Discussing every \\naspect of regular expressions would not be possible in these pages, but I will cover the \\nmain areas with sufficient examples. \\n Regular expressions or regexes are specific patterns often denoted using the \\nraw string notation. These patterns match a specific set of strings based on the rules \\nexpressed by the patterns. These patterns then are usually compiled into bytecode that is \\nthen executed for matching strings using a matching engine. The  re module also provides \\nseveral flags that can change the way the pattern matches are executed. Some important \\nflags include the following:\\n\\xe2\\x80\\xa2 \\n re.I or  re.IGNORECASE is used to match patterns ignoring case \\nsensitivity. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n102\\n\\xe2\\x80\\xa2 \\n re.S or  re.DOTALL causes the period ( . ) character to match any \\ncharacter including new lines. \\n\\xe2\\x80\\xa2 \\n re.U or  re.UNICODE helps in matching Unicode-based characters \\nalso (deprecated in Python 3.x). \\n For pattern matching, various rules are used in regexes. Some popular ones include \\nthe following:\\n\\xe2\\x80\\xa2 \\n . for matching any single character \\n\\xe2\\x80\\xa2 \\n ^ for matching the start of the string \\n\\xe2\\x80\\xa2 \\n $ for matching the end of the string \\n\\xe2\\x80\\xa2 \\n * for matching zero or more cases of the previous mentioned \\nregex before the  * symbol in the pattern \\n\\xe2\\x80\\xa2 \\n ? for matching zero or one case of the previous mentioned regex \\nbefore the  ? symbol in the pattern \\n\\xe2\\x80\\xa2 \\n [...] for matching any one of the set of characters inside the \\nsquare brackets \\n\\xe2\\x80\\xa2 \\n [^...] for matching a character not present in the square \\nbrackets after the  ^ symbol \\n\\xe2\\x80\\xa2 \\n |  denotes the OR  operator for matching either the preceding or \\nthe next regex \\n\\xe2\\x80\\xa2 \\n + for matching one or more cases of the previous mentioned regex \\nbefore the  + symbol in the pattern \\n\\xe2\\x80\\xa2 \\n \\\\d for matching decimal digits which is also depicted as  [0-9] \\n\\xe2\\x80\\xa2 \\n \\\\D  for matching non-digits, also depicted as  [^0-9] \\n\\xe2\\x80\\xa2 \\n \\\\s  for matching white space characters \\n\\xe2\\x80\\xa2 \\n \\\\S  for matching non whitespace characters \\n\\xe2\\x80\\xa2 \\n \\\\w  for matching alphanumeric characters also depicted as \\n [a-zA-Z0-9_] \\n\\xe2\\x80\\xa2 \\n \\\\W  for matching non alphanumeric characters also depicted as \\n [^a-zA-Z0-9_] \\n Regular expressions can be compiled into pattern objects and then used with a \\nvariety of methods for pattern search and substitution in strings. The main methods \\noffered by the  re module for performing these operations are as follows:\\n\\xe2\\x80\\xa2 \\n re.compile() : This method compiles a specified regular \\nexpression pattern into a regular expression object that can be \\nused for matching and searching. Takes a pattern and optional \\nflags as input, discussed earlier. \\n\\xe2\\x80\\xa2 \\n re.match() : This method is used to match patterns at the \\nbeginning of strings. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n103\\n\\xe2\\x80\\xa2 \\n re.search() : This method is used to match patterns occurring at \\nany position in the string. \\n\\xe2\\x80\\xa2 \\n re.findall() : This method returns all non-overlapping matches \\nof the specified regex pattern in the string.  \\n\\xe2\\x80\\xa2 \\n re.finditer() : This method returns all matched instances in the \\nform of an iterator for a specific pattern in a string when scanned \\nfrom left to right. \\n\\xe2\\x80\\xa2 \\n re.sub() : This method is used to substitute a specified regex \\npattern in a string with a replacement string. It only substitutes \\nthe leftmost occurrence of the pattern in the string.    \\n The following  code snippets depict some of the methods just discussed and how \\nthey are typically used when dealing with strings and regular expressions: \\n # importing the re module \\n In [526]: import re \\n # dealing with unicode matching using regexes \\n In [527]: s = u\\'H\\\\u00e8llo\\' \\n In [528]: s \\n Out[528]: u\\'H\\\\xe8llo\\' \\n In [529]: print s \\n H\\xc3\\xa8llo \\n # does not return the special unicode character even if it is alphanumeric \\n In [530]: re.findall(r\\'\\\\w+\\', s) \\n Out[530]: [u\\'H\\', u\\'llo\\'] \\n # need to explicitly specify the unicode flag to detect it using regex \\n In [531]: re.findall(r\\'\\\\w+\\', s, re.UNICODE) \\n Out[531]: [u\\'H\\\\xe8llo\\'] \\n # setting up a pattern we want to use as a regex \\n # also creating two sample strings \\n In [534]: pattern = \\'python\\' \\n     ...: s1 = \\'Python is an excellent language\\' \\n     ...:  s2 = \\'I love the Python language. I also use Python to build \\napplications at work!\\' \\n # match only returns a match if regex match is found at the beginning of the \\nstring \\n In [535]: re.match(pattern, s1) \\n # pattern is in lower case hence ignore case flag helps \\n # in matching same pattern with different cases \\n In [536]: re.match(pattern, s1, flags=re.IGNORECASE) \\n Out[536]: <_sre.SRE_Match at 0xf378308> \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n104\\n # printing matched string and its indices in the original string \\n In [537]: m = re.match(pattern, s1, flags=re.IGNORECASE) \\n In [538]: print \\'Found match {} ranging from index {} - {} in the string \\n\"{}\"\\'.format(m.group(0), m.start(), m.end(), s1) \\n Found match Python ranging from index 0 - 6 in the string \"Python is an \\nexcellent language\" \\n # match does not work when pattern is not there in the \\nbeginning of string s2 \\n In [539]: re.match(pattern, s2, re.IGNORECASE) \\n # illustrating find and search methods using the re module \\n In [540]: re.search(pattern, s2, re.IGNORECASE) \\n Out[540]: <_sre.SRE_Match at 0xf378920> \\n In [541]: re.findall(pattern, s2, re.IGNORECASE) \\n Out[541]: [\\'Python\\', \\'Python\\'] \\n In [542]: match_objs = re.finditer(pattern, s2, re.IGNORECASE) \\n In [543]: print \"String:\", s2 \\n     ...: for m in match_objs: \\n     ...:      print \\'Found match \"{}\" ranging from index {} - {}\\'.format(m.\\ngroup(0), m.start(), m.end())     \\n String: I love the Python language. I also use Python to build applications \\nat work! \\n Found match \"Python\" ranging from index 11 - 17 \\n Found match \"Python\" ranging from index 39 - 45 \\n # illustrating pattern substitution using sub and subn methods \\n In [544]: re.sub(pattern, \\'Java\\', s2, flags=re.IGNORECASE) \\n Out[544]:  \\'I love the Java language. I also use Java to build applications \\nat work!\\' \\n In [545]: re.subn(pattern, \\'Java\\', s2, flags=re.IGNORECASE) \\n Out[545]:  (\\'I love the Java language. I also use Java to build applications \\nat work!\\', 2) \\n This concludes our  discussion on the various aspects of strings and how they can \\nbe utilized for working with text data. Strings form the basis for processing text, which is \\nan important component in text analytics. The next section briefly discusses some of the \\npopular text analytics frameworks. \\n Text Analytics Frameworks \\n Like I\\xe2\\x80\\x99ve mentioned before, the Python ecosystem is very diverse and supports a wide \\nvariety of libraries, frameworks, and modules in many domains. Because we will be \\nanalyzing textual data and performing various operations on it, you need to know \\nabout dedicated frameworks and libraries for text analytics that you can just install and \\nstart using\\xe2\\x80\\x94just like any other built-in module in the Python standard library. These \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n105\\nframeworks have been built over a long period of time and contain various methods, \\ncapabilities, and features for operating on text, getting insights, and making the data \\nready for further analysis, such as applying machine learning algorithms on pre-\\nprocessed textual data. \\n Leveraging these frameworks saves a lot of effort and time that would have been \\notherwise spent on writing boilerplate code to handle, process, and manipulate text \\ndata. Thus, the frameworks enable developers and researchers to focus more on solving \\nactual problems and the necessary logic and  algorithms needed for doing so. We have \\nalready seen some of the NLTK library in the first chapter. The following list of libraries \\nand frameworks are some of the most popular text analytics frameworks, and we will be \\nutilizing several of them throughout the course of the book:\\n\\xe2\\x80\\xa2 \\n NLTK : The Natural Language  Toolkit is a complete platform \\nthat contains more than 50 corpora and lexical resources. It also \\nprovides the necessary tools, interfaces, and methods to process \\nand analyze text data. \\n\\xe2\\x80\\xa2 \\n pattern : The  pattern project started out as a research project \\nat the Computational Linguistics & Psycholinguistics research \\ncenter at the University of Antwerp. It provides tools and \\ninterfaces for web mining, information retrieval, NLP, machine \\nlearning, and network analysis. The  pattern.en module contains \\nmost of the utilities for text analytics. \\n\\xe2\\x80\\xa2 \\n gensim : The  gensim library has a rich set of capabilities for \\nsemantic analysis, including topic modeling and similarity \\nanalysis. But the best part is that it contains a Python port of \\nGoogle\\xe2\\x80\\x99s very popular word2vec model (originally available as \\na C package), a neural network model implemented to learn \\ndistributed representations of words where similar words \\n(semantic) occur close to each other. \\n\\xe2\\x80\\xa2 \\n textblob : This is another library that provides several capabilities \\nincluding text processing, phrase extraction, classification, POS \\ntagging, text translation, and sentiment analysis. \\n\\xe2\\x80\\xa2 \\n spacy : This is one of the newer libraries, which claims to provide \\nindustrial-strength NLP capabilities by providing the best \\nimplementation of each technique and algorithm, making NLP \\ntasks efficient in terms of performance and implementation.    \\n Besides these, there are several other frameworks and libraries that are not dedicated \\ntowards text analytics but that are useful when you want to use machine learning \\ntechniques on textual data. These include the  scikit-learn ,  numpy , and  scipy stack. \\nBesides these, deep learning and tensor-based libraries like  theano ,  tensorflow , and \\n keras also come in handy if you want to build advanced deep learning models based \\non deep neural nets, convnets, and LSTM-based  models . You can install most of these \\nlibraries using the  pip install <library> command from the command prompt or \\nterminal. We will talk about any caveats if present in the upcoming chapters when we use \\nthese libraries. \\nCHAPTER 2 \\xe2\\x96\\xa0 PYTHON REFRESHER\\n106\\n Summary \\n This chapter provides a birds-eye yet detailed view of the entire Python ecosystem and \\nwhat the language offers in terms of capabilities. You read about the origins of the Python \\nlanguage and saw how it has evolved overtime. The language has benefits of being open \\nsource, which has resulted in an active developer community constantly striving to \\nimprove the language and add new features. By now, you also know when you should use \\nPython and the drawbacks associated with the language\\xe2\\x80\\x94which every developer should \\nkeep in mind while building systems and applications. This chapter also discussed how to \\nset up your own Python environment and deal with multiple virtual environments. \\n Starting from the very basics, we have taken a deep dive into the various structures \\nand constructs in the Python language, including data types and controlling code flow \\nusing loops and conditionals. We also explored concepts in various programming \\nparadigms including OOP and functional programming. Constructs like classes, \\nfunctions, lambdas, iterators, generators, and comprehensions are tools that will come in \\nhandy in a lot of scenarios when writing quality Python code. You also saw how to work \\nwith text data using the  string data type and its various syntaxes, methods, operations, \\nand formats. We also talked about the power of regular expressions and how useful they \\ncan be in pattern matching and substitutions. To conclude our discussion, we looked at \\nvarious popular text analytics frameworks, which are useful in solving problems and tasks \\ndealing with NLP and analyzing and extracting insights from text data. \\n This should all get you started with programming in Python. The next chapter builds \\non the foundations of this chapter as we start to understand, process, and parse text data \\nin usable formats. \\n107\\n\\xc2\\xa9 Dipanjan Sarkar 2016 \\nD. Sarkar, Text Analytics with Python, DOI 10.1007/978-1-4842-2388-8_3\\n CHAPTER 3  \\n Processing and \\nUnderstanding Text \\n So far, we have reviewed the main concepts and areas surrounding  natural language \\nprocessing (NLP) and text analytics. We also got a good grip on the Python programming \\nlanguage in the last chapter, especially on the different constructs and syntax and how to \\nwork with strings to manage textual data. To carry out different operations and analyze \\ntext, you will need to process and parse textual data into more easy-to-interpret formats. \\n All  machine learning (ML) algorithms  , be they supervised or unsupervised techniques, \\nusually work with input features that are numeric in nature. Although this is a separate topic \\nunder feature engineering, which we shall explore in detail, to get to that, you need to clean, \\nnormalize, and pre-process the initial textual data. Usually text corpora and other textual \\ndata in their native raw format are not well formatted and standardized, and of course, we \\nshould expect this\\xe2\\x80\\x94after all, text data is highly unstructured! Text processing, or to be more \\nspecific, pre-processing, involves using a variety of techniques to convert raw text into well-\\ndefined sequences of linguistic components that have standard structure and notation. \\n Often additional metadata is also present in the form of annotations to give more \\nmeaning to the text components like tags. The following list gives us an idea of some of \\nthe most popular  text pre-processing techniques that we will be exploring in this chapter:\\n\\xe2\\x80\\xa2 \\n Tokenization \\n\\xe2\\x80\\xa2 \\n Tagging \\n\\xe2\\x80\\xa2 \\n Chunking \\n\\xe2\\x80\\xa2 \\n Stemming \\n\\xe2\\x80\\xa2 \\n Lemmatization \\n Besides these  techniques , you also need to perform some basic operations much \\nof the time, such as dealing with misspelled text, removing stopwords, and handling \\nother irrelevant components based on the problem to be solved. An important thing to \\nremember always is that a robust text pre-processing system is always an essential part \\nof any application on NLP and text analytics. The primary reason for that is because all \\nthe textual components that are obtained after pre-processing\\xe2\\x80\\x94be they words, phrases, \\nsentences, or any other tokens\\xe2\\x80\\x94form the basic building blocks of input that are fed into \\nthe further stages of the application that perform more complex analyses, including \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n108\\nlearning patterns and extracting information. Hence, the popular saying \\xe2\\x80\\x9cgarbage in, \\ngarbage out\\xe2\\x80\\x9d is very relevant here because if we do not process the text properly, we will \\nend up getting unwanted and irrelevant results from our applications and systems. \\n Text processing also helps in cleaning and standardization of the text, which helps \\nin analytical systems, like increasing the accuracy of classifiers. We also get additional \\ninformation and metadata in the form of annotations, which are also very useful in giving \\nmore information about the text. We will touch upon normalizing text using various \\ntechniques including cleaning, removing unnecessary tokens, stems, and lemmas in this \\nchapter. \\n Another important aspect is to understand textual data after processing and \\nnormalizing it. This will involve revisiting some of the concepts of language syntax and \\nstructure from Chapter   1 , where we talked about sentences, phrases, parts of speech, \\nshallow parsing, and grammars. In this chapter we will look at ways to implement these \\nconcepts and use them on real data. We will follow a structured and definite path in this \\nchapter, starting from text processing and gradually exploring the various concepts and \\ntechniques associated with it, and move on to understanding text structure and syntax. \\nBecause this book is specifically aimed towards practitioners, various code snippets and \\npractical examples will also enable and equip you with the right tools and frameworks for \\nimplementing the concepts under discussion in solving practical problems. \\n Text Tokenization \\n Chapter   1  talked about textual structure, its components, and tokens. To be more specific, \\n tokens are independent and minimal textual components that have some definite syntax \\nand semantics. A paragraph of text or a text document has several components including \\nsentences that can be further broken down into clauses, phrases, and words. The most \\npopular tokenization techniques include sentence and word tokenization, which are \\nused to break down a text corpus into sentences, and each sentence into words. Thus, \\ntokenization can be defined as the process of breaking down or splitting textual data into \\nsmaller meaningful components called tokens. In the following section, we will look at \\nsome ways to tokenize text into sentences. \\n     Sentence Tokenization \\n Sentence tokenization  is the process of splitting a text corpus into sentences that act as \\nthe first level of tokens which the corpus is comprised of. This is also known as  sentence \\nsegmentation , because we try to segment the text into meaningful sentences. Any  text \\ncorpus  is a body of text where each paragraph comprises several sentences. \\n There are various ways of performing sentence tokenization. Basic techniques \\ninclude looking for specific  delimiters between sentences, such as a period (.) or a \\nnewline character (\\\\n), and sometimes even a semi-colon (;). We will use the NLTK \\nframework, which provides various interfaces for performing sentence tokenization. We \\nwill primarily focus on the following sentence tokenizers:\\n\\xe2\\x80\\xa2 \\n sent_tokenize \\n\\xe2\\x80\\xa2 \\n PunktSentenceTokenizer \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n109\\n\\xe2\\x80\\xa2 \\n RegexpTokenizer \\n\\xe2\\x80\\xa2 \\n Pre-trained sentence tokenization models \\n Before we can tokenize sentences, we need some text on which we can try out \\nthese operations. We will load some sample text and also a part of the  Gutenberg corpus \\navailable in NLTK itself. We load the necessary dependencies using the following snippet: \\n import nltk \\n from nltk.corpus import gutenberg \\n from pprint import pprint \\n alice = gutenberg.raw(fileids=\\'carroll-alice.txt\\') \\n sample_text = \\'We will discuss briefly about the basic syntax, structure and \\ndesign philosophies. There is a defined hierarchical syntax for Python code \\nwhich you should remember when writing code! Python is a really powerful \\nprogramming language!\\' \\n We can check the length of the  Alice in Wonderland corpus and also the first few lines \\nin it using the following snippet: \\n In [124]: # Total characters in Alice in Wonderland \\n     ...: print len(alice) \\n 144395 \\n In [125]: # First 100 characters in the corpus \\n     ...: print alice[0:100] \\n [Alice\\'s Adventures in Wonderland by Lewis Carroll 1865] \\n CHAPTER I. Down the Rabbit-Hole \\n Alice was \\n The  nltk.sent_tokenize function is the default sentence tokenization function that \\n nltk recommends. It uses an instance of the  PunktSentenceTokenizer  class internally. \\nHowever, this is not just a normal object or instance of that class\\xe2\\x80\\x94it has been pre-trained \\non several language models and works really well on many popular languages besides \\njust English. \\n The following snippet shows the basic usage of this function on our  text samples : \\n default_st = nltk.sent_tokenize \\n alice_sentences = default_st(text=alice) \\n sample_sentences = default_st(text=sample_text) \\n print \\'Total sentences in sample_text:\\', len(sample_sentences) \\n print \\'Sample text sentences :-\\' \\n pprint(sample_sentences) \\n print \\'\\\\nTotal sentences in alice:\\', len(alice_sentences) \\n print \\'First 5 sentences in alice:-\\' \\n pprint(alice_sentences[0:5]) \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n110\\n On running the preceding snippet, you get the following output depicting the total \\nnumber of sentences and what those sentences look like in the  text corpora : \\n Total sentences in sample_text: 3 \\n Sample text sentences :- \\n [\\'We will discuss briefly about the basic syntax, structure and design \\nphilosophies.\\', \\n \\'There is a defined hierarchical syntax for Python code which you should \\nremember when writing code!\\', \\n \\'Python is a really powerful programming language!\\'] \\n Total sentences in alice: 1625 \\n First 5 sentences in alice:- \\n [u\"[Alice\\'s Adventures in Wonderland by Lewis Carroll 1865]\\\\n\\\\nCHAPTER I.\", \\n u\"Down the Rabbit-Hole\\\\n\\\\nAlice was beginning to get very tired of sitting \\nby her sister on the\\\\nbank, and of having nothing to do: once or twice she \\nhad peeped into the\\\\nbook her sister was reading, but it had no pictures \\nor conversations in\\\\nit, \\'and what is the use of a book,\\' thought Alice \\n\\'without pictures or\\\\nconversation?\\'\", \\n u\\'So she was considering in her own mind (as well as she could, for the\\\\nhot \\nday made her feel very sleepy and stupid), whether the pleasure\\\\nof making \\na daisy-chain would be worth the trouble of getting up and\\\\npicking the \\ndaisies, when suddenly a White Rabbit with pink eyes ran\\\\nclose by her.\\', \\n u\"There was nothing so VERY remarkable in that; nor did Alice think it so\\\\\\nnVERY much out of the way to hear the Rabbit say to itself, \\'Oh dear!\", \\n u\\'Oh dear!\\'] \\n Now, as you can see, the tokenizer is quite intelligent and doesn\\xe2\\x80\\x99t just use periods to \\ndelimit sentences. It also considers other punctuation and the capitalization of  words  . \\n We can also tokenize text of other languages. If we are dealing with  German text , \\nwe can use  sent_tokenize , which is already trained, or load a pre-trained tokenization \\nmodel on German text into a  PunktSentenceTokenizer instance and perform the same \\noperation. The following snippet shows the same. We start with loading a German text \\ncorpus and inspecting it: \\n In [4]: from nltk.corpus import europarl_raw \\n   ...:  \\n   ...: german_text = europarl_raw.german.raw(fileids=\\'ep-00-01-17.de\\') \\n   ...: # Total characters in the corpus \\n   ...: print len(german_text) \\n   ...: # First 100 characters in the corpus \\n   ...: print german_text[0:100] \\n 157171 \\n Wiederaufnahme der Sitzungsperiode Ich erkl\\xc3\\xa4re die am Freitag , dem 17. \\nDezember unterbrochene Sit \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n111\\n  Next, we tokenize the text corpus into sentences using both the default  sent_\\ntokenize  tokenizer and also a  pre-trained German language   tokenizer by loading it from \\nthe  nltk resources: \\n In [5]: german_sentences_def = default_st(text=german_text, \\nlanguage=\\'german\\') \\n   ...:  \\n   ...:  # loading german text tokenizer into a PunktSentenceTokenizer \\ninstance   \\n   ...:  german_tokenizer = nltk.data.load(resource_url=\\'tokenizers/punkt/\\ngerman.pickle\\') \\n   ...: german_sentences = german_tokenizer.tokenize(german_text) \\n   ...:  \\n   ...: # verify the type of german_tokenizer \\n   ...: # should be PunktSentenceTokenizer \\n   ...: print type(german_tokenizer) \\n <class \\'nltk.tokenize.punkt.PunktSentenceTokenizer\\'> \\n Thus we see that indeed the  german_ tokenizer   is an instance of \\n PunktSentenceTokenizer , which is specialized in dealing with the German language. \\n Next we check whether the sentences obtained from the default tokenizer are the \\nsame as the sentences obtained by this  pre-trained tokenizer  , and ideally it should be \\n True . We also print some sample tokenized sentences from the output after that: \\n In [9]: print german_sentences_def == german_sentences \\n   ...: # print first 5 sentences of the corpus \\n   ...: for sent in german_sentences[0:5]: \\n   ...:     print sent \\n True \\n Wiederaufnahme der Sitzungsperiode Ich erkl\\xc3\\xa4re die am Freitag , dem 17. \\nDezember unterbrochene Sitzungsperiode des Europ\\xc3\\xa4ischen Parlaments f\\xc3\\xbcr \\nwiederaufgenommen , w\\xc3\\xbcnsche Ihnen nochmals alles Gute zum Jahreswechsel und \\nhoffe , da\\xc3\\x9f Sie sch\\xc3\\xb6ne Ferien hatten . \\n Wie Sie feststellen konnten , ist der gef\\xc3\\xbcrchtete \" Millenium-Bug \" nicht \\neingetreten . \\n Doch sind B\\xc3\\xbcrger einiger unserer Mitgliedstaaten Opfer von schrecklichen \\nNaturkatastrophen geworden . \\n Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser \\nSitzungsperiode in den n\\xc3\\xa4chsten Tagen . \\n Heute m\\xc3\\xb6chte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen \\nund Kollegen - , allen Opfern der St\\xc3\\xbcrme , insbesondere in den verschiedenen \\nL\\xc3\\xa4ndern der Europ\\xc3\\xa4ischen Union , in einer Schweigeminute zu gedenken . \\n  Thus we see that our assumption was indeed correct, and you can tokenize \\nsentences belonging to different languages in two different ways. Using the default \\n PunktSentenceTokenizer class is also pretty straightforward. The following snippet \\nshows how to use it: \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n112\\n In [11]: punkt_st = nltk.tokenize.PunktSentenceTokenizer() \\n    ...: sample_sentences = punkt_st.tokenize(sample_text) \\n    ...: pprint(sample_sentences) \\n [\\'We will discuss briefly about the basic syntax, structure and design \\nphilosophies.\\', \\n \\'There is a defined hierarchical syntax for Python code which you should \\nremember when writing code!\\', \\n \\'Python is a really powerful programming language!\\'] \\n You can see we get a similar output, which is expected from this tokenization. \\nThe last tokenizer we will cover in sentence tokenization is using an instance of the \\n RegexpTokenizer class to tokenize text into sentences where we will use specific regular \\nexpression-based patterns to segment sentences. Recall the regular expressions (regex) \\nfrom the previous chapter, in case you want to refresh your memory. The following \\n snippet  shows how to use a regex pattern to tokenize sentences: \\n In [29]: SENTENCE_TOKENS_PATTERN = r\\'(?<!\\\\w\\\\.\\\\w.)(?<![A-Z][a-z]\\\\.)\\n(?<![A-Z]\\\\.)(?<=\\\\.|\\\\?|\\\\!)\\\\s\\' \\n    ...: regex_st = nltk.tokenize.RegexpTokenizer( \\n    ...:             pattern=SENTENCE_TOKENS_PATTERN, \\n    ...:             gaps=True) \\n    ...: sample_sentences = regex_st.tokenize(sample_text) \\n    ...: pprint(sample_sentences) \\n [\\'We will discuss briefly about the basic syntax, structure and design \\nphilosophies.\\', \\n \\' There is a defined hierarchical syntax for Python code which you should \\nremember  when writing code!\\', \\n \\'Python is a really powerful programming language!\\'] \\n That output shows that we obtained the same sentences as we had obtained using \\nthe other tokenizers. This gives us an idea of tokenizing text into sentences using different \\n nltk interfaces . In the next section we will look at tokenizing these sentences into words \\nusing several techniques. \\n Word Tokenization \\n Word tokenization  is the process of splitting or segmenting sentences into their \\nconstituent words. A  sentence is a collection of words, and with tokenization we \\nessentially split a sentence into a list of words that can be used to reconstruct the \\nsentence. Word tokenization is very important in many processes, especially in cleaning \\nand normalizing text where operations like  stemming and  lemmatization work on \\neach individual word based on its respective stems and lemma. Similar to sentence \\ntokenization,  nltk  provides various useful interfaces for word tokenization, and we will \\ntouch up on the following main  interfaces :\\n\\xe2\\x80\\xa2 \\n word_tokenize \\n\\xe2\\x80\\xa2 \\n TreebankWordTokenizer \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n113\\n\\xe2\\x80\\xa2 \\n RegexpTokenizer \\n\\xe2\\x80\\xa2 \\n  Inherited tokenizers from  RegexpTokenizer \\n For the hands-on examples, we will use the sample sentence  The brown fox wasn\\xe2\\x80\\x99t \\nthat quick and he couldn\\xe2\\x80\\x99t win the race as our input to the various tokenizers. The   nltk.\\nword_tokenize function is the default and recommended word tokenizer as specified \\nby  nltk . This tokenizer is actually an instance or object of the  TreebankWordTokenizer \\nclass in its internal implementation and acts as a wrapper to that core class. The following \\nsnippet illustrates its usage: \\n In [114]: sentence = \"The brown fox wasn\\'t that quick and he couldn\\'t win \\nthe race\" \\n     ...:  \\n     ...: default_wt = nltk.word_tokenize \\n     ...: words = default_wt(sentence) \\n     ...: print words   \\n [\\'The\\', \\'brown\\', \\'fox\\', \\'was\\', \"n\\'t\", \\'that\\', \\'quick\\', \\'and\\', \\'he\\', \\'could\\', \\n\"n\\'t\", \\'win\\', \\'the\\', \\'race\\'] \\n The  TreebankWordTokenizer    is based on the Penn Treebank and uses various regular \\nexpressions to tokenize the text. Of course, one primary assumption here is that we \\nhave already performed sentence tokenization beforehand. The original tokenizer used \\nin the Penn Treebank is available as a  sed script, and you can check it out at    www.cis.\\nupenn.edu/~treebank/tokenizer.sed   to get an idea of the  patterns used to tokenize the \\nsentences into words. Some of the main features of this tokenizer include the following:\\n\\xe2\\x80\\xa2 \\n Splits and separates out periods that appear at the end of a \\nsentence \\n\\xe2\\x80\\xa2 \\n Splits and separates commas and single quotes when followed by \\nwhitespaces \\n\\xe2\\x80\\xa2 \\n Most punctuation characters are split and separated into \\nindependent tokens \\n\\xe2\\x80\\xa2 \\n Splits words with standard contractions\\xe2\\x80\\x94examples would be \\n don\\xe2\\x80\\x99t to  do and  n\\xe2\\x80\\x99t \\n The following  snippet shows the usage of the  TreebankWordTokenizer for word \\ntokenization: \\n In [117]: treebank_wt = nltk.TreebankWordTokenizer() \\n     ...: words = treebank_wt.tokenize(sentence) \\n     ...: print words \\n [\\'The\\', \\'brown\\', \\'fox\\', \\'was\\', \"n\\'t\", \\'that\\', \\'quick\\', \\'and\\', \\'he\\', \\'could\\', \\n\"n\\'t\", \\'win\\', \\'the\\', \\'race\\'] \\n From the preceding output, as expected, the output is similar to  word_tokenize() \\nbecause both use the same tokenizing mechanism. \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n114\\n We will now look at how to use regular expressions and the  RegexpTokenizer class to \\ntokenize sentences into words. Remember, there are two main parameters that are useful \\nin tokenization: the regex  pattern for building the tokenizer and the  gaps parameter, \\nwhich, if set to  True , is used to find the gaps between the tokens. Otherwise, it is used to \\nfind the tokens themselves. \\n The following code snippet shows some examples of using  regular expressions to \\nperform word tokenization: \\n # pattern to identify tokens themselves \\n In [127]: TOKEN_PATTERN = r\\'\\\\w+\\'         \\n     ...: regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, \\n     ...:                                 gaps=False) \\n     ...: words = regex_wt.tokenize(sentence) \\n     ...: print words \\n [\\'The\\', \\'brown\\', \\'fox\\', \\'wasn\\', \\'t\\', \\'that\\', \\'quick\\', \\'and\\', \\'he\\', \\'couldn\\', \\n\\'t\\', \\'win\\', \\'the\\', \\'race\\'] \\n # pattern to identify gaps in tokens \\n In [128]: GAP_PATTERN = r\\'\\\\s+\\'         \\n     ...: regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, \\n     ...:                                 gaps=True) \\n     ...: words = regex_wt.tokenize(sentence) \\n     ...: print words \\n [\\'The\\', \\'brown\\', \\'fox\\', \"wasn\\'t\", \\'that\\', \\'quick\\', \\'and\\', \\'he\\', \"couldn\\'t\", \\n\\'win\\', \\'the\\', \\'race\\'] \\n # get start and end indices of each token and then print them \\n In [131]: word_indices = list(regex_wt.span_tokenize(sentence)) \\n     ...: print word_indices \\n     ...: print [sentence[start:end] for start, end in word_indices] \\n [(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), \\n(39, 47), (48, 51), (52, 55), (56, 60)] \\n [\\'The\\', \\'brown\\', \\'fox\\', \"wasn\\'t\", \\'that\\', \\'quick\\', \\'and\\', \\'he\\', \"couldn\\'t\", \\n\\'win\\', \\'the\\', \\'race\\'] \\n Besides the base  RegexpTokenizer class, there are several derived classes that \\nperform different types of word tokenization. The  WordPunktTokenizer  uses the pattern \\n r\\'\\\\w+|[^\\\\w\\\\s]+\\' to tokenize sentences into independent alphabetic and non-alphabetic \\ntokens. The  WhitespaceTokenizer tokenizes sentences into words based on whitespaces \\nlike tabs, newlines, and  spaces . \\n The following snippet demonstrates: \\n In [132]: wordpunkt_wt = nltk.WordPunctTokenizer() \\n     ...: words = wordpunkt_wt.tokenize(sentence) \\n     ...: print words \\n [\\'The\\', \\'brown\\', \\'fox\\', \\'wasn\\', \"\\'\", \\'t\\', \\'that\\', \\'quick\\', \\'and\\', \\'he\\', \\n\\'couldn\\', \"\\'\", \\'t\\', \\'win\\', \\'the\\', \\'race\\'] \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n115\\n In [133]: whitespace_wt = nltk.WhitespaceTokenizer() \\n     ...: words = whitespace_wt.tokenize(sentence) \\n     ...: print words \\n [\\'The\\', \\'brown\\', \\'fox\\', \"wasn\\'t\", \\'that\\', \\'quick\\', \\'and\\', \\'he\\', \"couldn\\'t\", \\n\\'win\\', \\'the\\', \\'race\\'] \\n  This concludes our discussion on tokenization. Now that we know how to separate \\nout raw text into sentences and words, we will build upon that in the next section, where \\nwe will normalize these tokens to get clean and standardized textual data that will be \\neasier to understand, interpret, and use in NLP and ML. \\n Text Normalization \\n Text normalization is defined as a process that consists of a series of steps that \\nshould be followed to wrangle, clean, and standardize textual data into a form that \\ncould be consumed by other NLP and analytics systems and applications as input. \\nOften tokenization itself also is a part of text normalization. Besides tokenization, \\nvarious other techniques include cleaning text, case conversion, correcting spellings, \\nremoving stopwords and other unnecessary terms, stemming, and lemmatization. Text \\nnormalization is also often called  text cleansing or  wrangling . \\n In this section, we will discuss various techniques used in the process of text \\nnormalization. Before we can jump into implementing and exploring the various \\ntechniques, use the following code snippet to load the basic dependencies and also the \\ncorpus we will be using in this section: \\n import nltk \\n import re \\n import string \\n from pprint import pprint \\n corpus = [\"The brown fox wasn\\'t that quick and he couldn\\'t win the race\", \\n          \"Hey that\\'s a great deal! I just bought a phone for $199\", \\n           \"@@You\\'ll (learn) a **lot** in the book. Python is an amazing \\n language !@@\"] \\n Cleaning Text \\n Often the textual data we want to use or analyze contains a lot of extraneous and \\nunnecessary tokens and characters that should be removed before performing any \\nfurther operations like tokenization or other normalization techniques. This includes \\nextracting out meaningful text from data sources like HTML data, which consists of \\nunnecessary HTML tags, or even data from XML and JSON feeds. There are many ways \\nto parse and clean this data to remove unnecessary tags. You can use functions like \\n clean_html() from  nltk or even the  BeautifulSoup  library to parse HTML data. You can \\nalso use your own custom logic, including regexes, xpath, and the lxml library, to parse \\nthrough XML data. And getting data from JSON is substantially easier because it has \\ndefinite key-value annotations. \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n116\\n Tokenizing Text \\n Usually, we tokenize text before or after removing unnecessary characters and symbols \\nfrom the data. This choice depends on the problem you are trying to solve and the data \\nyou are dealing with. We have already looked at various tokenization techniques in the \\nprevious section. We will define a generic tokenization function here and run the same on \\nour corpus mentioned earlier. \\n The following code snippet defines the tokenization function: \\n def tokenize_text(text): \\n    sentences = nltk.sent_tokenize(text) \\n    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]  \\n    return word_tokens \\n This function basically takes in textual data, extracts sentences from it, and finally \\nsplits each sentence into further tokens, which could be words or special characters and \\npunctuation. The following snippet depicts the preceding function in action: \\n In [297]: token_list = [tokenize_text(text)  \\n     ...:               for text in corpus] \\n     ...: pprint(token_list) \\n [[[\\'The\\',  \\'brown\\', \\'fox\\', \\'was\\', \"n\\'t\", \\'that\\', \\'quick\\', \\'and\\', \\'he\\', \\n\\'could\\', \"n\\'t\", \\n   \\'win\\', \\'the\\', \\'race\\']], \\n [[\\'Hey\\', \\'that\\', \"\\'s\", \\'a\\', \\'great\\', \\'deal\\', \\'!\\'], \\n  [\\'I\\', \\'just\\', \\'bought\\', \\'a\\', \\'phone\\', \\'for\\', \\'$\\', \\'199\\']], \\n [[\\'@\\',  \\'@\\', \\'You\\', \"\\'ll\", \\'(\\', \\'learn\\', \\')\\', \\'a\\', \\'**lot**\\', \\'in\\', \\'the\\', \\n\\'book\\', \\'.\\'], \\n  [\\'Python\\', \\'is\\', \\'an\\', \\'amazing\\', \\'language\\', \\'!\\'], \\n  [\\'@\\', \\'@\\']]] \\n You can now see how each text in the corpus has been tokenized using our custom \\ndefined  function  . Play around with more text data and see if you can make it even better! \\n Removing Special  Characters \\n One important task in text normalization involves removing unnecessary and special \\ncharacters. These may be special symbols or even punctuation that occurs in sentences. \\nThis step is often performed before or after tokenization. The main reason for doing so is \\nbecause often punctuation or special characters do not have much significance when we \\nanalyze the text and utilize it for extracting features or information based on NLP and ML. \\nWe will implement both types of special characters removal, before and after tokenization. \\n The following snippet shows how to remove special characters  after tokenization: \\n def remove_characters_after_tokenization(tokens): \\n    pattern = re.compile(\\'[{}]\\'.format(re.escape(string.punctuation))) \\n    filtered_tokens = filter(None, [pattern.sub(\\'\\', token) for token in tokens]) \\n    return filtered_tokens \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n117\\n In [299]: filtered_list_1 =   [filter(None,[remove_characters_after_\\ntokenization(tokens)  \\n     ...:                                 for tokens in sentence_tokens])  \\n     ...:                     for sentence_tokens in token_list] \\n     ...: print filtered_list_1 \\n [[[\\'The\\', \\'brown\\', \\'fox\\', \\'was\\', \\'nt\\', \\'that\\', \\'quick\\', \\'and\\', \\'he\\', \\n\\'could\\', \\'nt\\', \\'win\\', \\'the\\', \\'race\\']], [[\\'Hey\\', \\'that\\', \\'s\\', \\'a\\', \\'great\\', \\n\\'deal\\'], [\\'I\\', \\'just\\', \\'bought\\', \\'a\\', \\'phone\\', \\'for\\', \\'199\\']], [[\\'You\\', \\n\\'ll\\', \\'learn\\', \\'a\\', \\'lot\\', \\'in\\', \\'the\\', \\'book\\'], [\\'Python\\', \\'is\\', \\'an\\', \\n\\'amazing\\', \\'language\\']]] \\n Essentially, what we do here is use the  string.punctuation attribute, which consists \\nof all possible special characters/symbols, and create a regex pattern from it. We use it \\nto match tokens that are symbols and characters and remove them. The  filter function \\nhelps us remove empty tokens obtained after removing the special character tokens using \\nthe regex  sub method. \\n The following code  snippet shows how to remove special characters  before \\ntokenization: \\n def remove_characters_before_tokenization(sentence, \\n                                          keep_apostrophes=False): \\n    sentence = sentence.strip() \\n    if keep_apostrophes: \\n         PATTERN = r\\'[?|$|&|*|%|@|(|)|~]\\' # add other characters here to \\nremove them \\n        filtered_sentence = re.sub(PATTERN, r\\'\\', sentence) \\n    else: \\n        PATTERN = r\\'[^a-zA-Z0-9 ]\\' # only extract alpha-numeric characters \\n        filtered_sentence = re.sub(PATTERN, r\\'\\', sentence) \\n    return filtered_sentence \\n In [304]: filtered_list_2 = [remove_characters_before_tokenization(sentence)  \\n     ...:                     for sentence in corpus]     \\n     ...: print filtered_list_2 \\n [\\'The brown fox wasnt that quick and he couldnt win the race\\', \\'Hey thats a \\ngreat deal I just bought a phone for 199\\', \\'Youll learn a lot in the book \\nPython is an amazing language\\'] \\n In [305]: cleaned_corpus = [remove_characters_before_tokenization(sentence,   \\n                           keep_apostrophes=True)  \\n     ...:                   for sentence in corpus] \\n     ...: print cleaned_corpus \\n [\"The brown fox wasn\\'t that quick and he couldn\\'t win the race\", \"Hey that\\'s \\na great deal! I just bought a phone for 199\", \"You\\'ll learn a lot in the \\nbook. Python is an amazing language!\"] \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n118\\n The preceding outputs show two different ways of removing special characters before \\ntokenization\\xe2\\x80\\x94removing all special characters versus retaining apostrophes and sentence \\nperiods\\xe2\\x80\\x94using regular expressions. By now, you must have realized how powerful regular \\nexpressions can be, as mentioned in Chapter   2 . Usually after removing these characters, \\nyou can take the clean text and tokenize it or apply other normalization operations on it. \\nSometimes we want to preserve the apostrophes in the sentences as a way to track them \\nand expand them if needed. We will explore that in the following section.  \\n Expanding  Contractions \\n Contractions  are shortened version of words or syllables. They exist in either written or \\nspoken forms. Shortened versions of existing words are created by removing specific \\nletters and sounds. In case of English contractions, they are often created by removing \\none of the vowels from the word. Examples would be  is not  to  isn\\xe2\\x80\\x99t and  will not  to  won\\xe2\\x80\\x99t , \\nwhere you can notice the apostrophe being used to denote the contraction and some \\nof the vowels and other letters being removed. Usually contractions are avoided when \\nused in formal writing, but informally, they are used quite extensively. Various forms of \\ncontractions exist that are tied down to the type of auxiliary verbs that give us normal \\ncontractions, negated contractions, and other special colloquial contractions, some of \\nwhich may not involve auxiliaries. \\n By nature, contractions do pose a problem for NLP and text analytics because, to \\nstart with, we have a special apostrophe character in the word. Plus we have two or more \\nwords represented by a contraction, and this opens a whole new can of worms when \\nwe try to tokenize this or even standardize the words. Hence, there should be some \\ndefinite process by which we can deal with contractions when processing text. Ideally, \\nyou can have a proper mapping for contractions and their corresponding expansions \\nand then use it to expand all the contractions in your text. I have created a vocabulary \\nfor contractions and their corresponding expanded forms that you can access in the \\nfile  contractions.py in a Python dictionary (available along with the code files for this \\nchapter). Part of the contractions dictionary is shown below in the following snippet: \\n CONTRACTION_MAP = { \\n \"isn\\'t\": \"is not\", \\n \"aren\\'t\": \"are not\", \\n \"can\\'t\": \"cannot\", \\n \"can\\'t\\'ve\": \"cannot have\", \\n . \\n . \\n . \\n \"you\\'ll\\'ve\": \"you will have\", \\n \"you\\'re\": \"you are\", \\n \"you\\'ve\": \"you have\" \\n } \\n Remember, though, that some of the contractions can have multiple forms. An \\nexample would be that contracting  you\\xe2\\x80\\x99ll can indicate either  you will or  you shall . To \\nsimplify, I have taken one of these expanded forms for each contraction. The next step, to \\nexpand contractions, uses the following code snippet: \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n119\\n from contractions import CONTRACTION_MAP \\n def expand_contractions(sentence, contraction_mapping): \\n     contractions_pattern = re.compile(\\'({})\\'.format(\\'|\\'.join(contraction_\\nmapping.keys())),  \\n                                      flags=re.IGNORECASE|re.DOTALL) \\n    def expand_match(contraction): \\n        match = contraction.group(0) \\n        first_char = match[0] \\n        expanded_contraction = contraction_mapping.get(match)\\\\ \\n                                if contraction_mapping.get(match)\\\\ \\n                                else contraction_mapping.get(match.lower())             \\n        expanded_contraction = first_char+expanded_contraction[1:] \\n        return expanded_contraction \\n    expanded_sentence = contractions_pattern.sub(expand_match, sentence) \\n    return expanded_sentence  \\n The preceding snippet uses the function  expanded_match  inside the main  expand_\\ncontractions function to find each contraction that matches the regex pattern we \\ncreate out of all the contractions in our  CONTRACTION_MAP dictionary. On matching any \\ncontraction, we substitute it with its corresponding expanded version and retain the \\ncorrect case of the word. \\n To see it in action, we use it on the  cleaned_corpus of  text we obtained in the \\nprevious section: \\n In [311]: expanded_corpus = [expand_contractions(sentence, CONTRACTION_MAP)  \\n     ...:                     for sentence in cleaned_corpus]     \\n     ...: print expanded_corpus \\n [\\'The brown fox was not that quick and he could not win the race\\', \\'Hey that \\nis a great deal! I just bought a phone for 199\\', \\'You will learn a lot in \\nthe book. Python is an amazing language!\\'] \\n You can see how each contraction has been correctly expanded in the output just like \\nwe expected it. Can you build a better contraction expander? It is definitely an interesting \\nproblem to solve. \\n Case Conversions \\n Often we want to modify the case of  words or sentences to make things easier, like \\nmatching specific words or tokens. Usually there are two types of  case conversion \\noperations that are used a lot. These are lowercase and uppercase conversions, where a \\nbody of text is converted completely to lowercase or uppercase. There are other forms \\nalso, such as sentence case or proper case. Lowercase is a form where all the letters of the \\ntext are small letters, and in uppercase they are all capitalized. \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n120\\n The following snippet illustrates this concept: \\n # lower case  \\n In [315]: print corpus[0].lower() \\n the brown fox wasn\\'t that quick and he couldn\\'t win the race \\n # upper case \\n In [316]: print corpus[0].upper() \\n THE BROWN FOX WASN\\'T THAT QUICK AND HE COULDN\\'T WIN THE RACE \\n Removing Stopwords \\n Stopwords , sometimes written  stop words , are words that have little or no significance. \\nThey are usually removed from text during processing so as to retain words having \\nmaximum significance and context. Stopwords are usually words that end up occurring \\nthe most if you aggregated any corpus of text based on singular tokens and checked their \\nfrequencies. Words like  a, the ,  me , and so on are stopwords. There is no universal or \\nexhaustive list of stopwords. Each domain or language may have its own set of stopwords. \\n The following code snippet shows a method to filter out and remove stopwords for \\nEnglish: \\n def remove_stopwords(tokens): \\n    stopword_list = nltk.corpus.stopwords.words(\\'english\\') \\n     filtered_tokens = [token for token in tokens if token not in stopword_\\nlist] \\n    return filtered_tokens \\n In the preceding function, we leverage the use of  nltk , which has a list of stopwords \\nfor English, and use it to filter out all tokens that correspond to stopwords. We use our \\n tokenize_text function to tokenize the  expanded_corpus we obtained in the previous \\nsection and then remove the necessary stopwords using the preceding function: \\n In [332]: expanded_corpus_tokens = [tokenize_text(text) \\n     ...:                           for text in expanded_corpus]     \\n     ...: filtered_list_3 =  [[remove_stopwords(tokens)  \\n     ...:                         for tokens in sentence_tokens]  \\n     ...:                          for sentence_tokens in expanded_corpus_\\ntokens] \\n     ...: print filtered_list_3 \\n [[[\\'The\\', \\'brown\\', \\'fox\\', \\'quick\\', \\'could\\', \\'win\\', \\'race\\']], [[\\'Hey\\', \\n\\'great\\', \\'deal\\', \\'!\\'], [\\'I\\', \\'bought\\', \\'phone\\', \\'199\\']], [[\\'You\\', \\'learn\\', \\n\\'lot\\', \\'book\\', \\'.\\'], [\\'Python\\', \\'amazing\\', \\'language\\', \\'!\\']]] \\n The preceding output shows a reduced number of tokens compared to what we had \\nearlier, and you can compare and check the tokens that were removed as stopwords. \\nTo see the list of all English stopwords in  nltk\\xe2\\x80\\x99s vocabulary, print the contents of \\n nltk.corpus.stopwords.words(\\xe2\\x80\\x98english\\xe2\\x80\\x99) . One important thing to remember is \\nthat  negations like  not and  no are removed in this case  (in the first sentence) , and it is \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n121\\noften essential to preserve the same so the actual context of the sentence is not lost in \\napplications like sentiment analysis, so you would need to make sure you do not remove \\nsuch words in those scenarios. \\n Correcting  Words \\n One of the main challenges faced in text normalization is the presence of incorrect words \\nin the text. The definition of incorrect here covers words that have spelling mistakes as \\nwell as words with several letters repeated that do not contribute much to its overall \\nsignificance. To illustrate some examples, the word  finally could be mistakenly written as \\n fianlly , or someone expressing intense emotion could write it as  finalllllyyyyyy . The main \\nobjective here would be to standardize different forms of these words to the correct form \\nso that we do not end up losing vital information from different tokens in the text. This \\nsection covers dealing with repeated characters as well as correcting spellings. \\n Correcting  Repeating Characters \\n I will cover a method here of using a combination of syntax and semantics to correct \\nincorrectly spelled words. We will first start with correcting the syntax of these words and \\nthen move on to semantics. \\n The first step in our algorithm would be to identify repeated characters in a word \\nusing a regex pattern and then use a substitution to remove the characters one by one. \\nConsider the word  finalllyyy from the earlier example. The pattern  r\\'(\\\\w*)(\\\\w)\\\\2(\\\\w*)\\' \\ncan be used to identify characters that occur twice among other characters in the \\nword, and in each step we will try to eliminate one of the repeated characters using a \\nsubstitution for the match by utilizing the regex match groups (groups 1, 2, and 3) using \\nthe pattern  r\\xe2\\x80\\x99\\\\1\\\\2\\\\3\\xe2\\x80\\x99 and then keep iterating through this  process   till no repeated \\ncharacters remain. \\n The following snippet illustrates this: \\n In [361]: old_word = \\'finalllyyy\\' \\n     ...: repeat_pattern = re.compile(r\\'(\\\\w*)(\\\\w)\\\\2(\\\\w*)\\') \\n     ...: match_substitution = r\\'\\\\1\\\\2\\\\3\\' \\n     ...: step = 1 \\n     ...:  \\n     ...: while True: \\n     ...:     # remove one repeated character \\n     ...:     new_word = repeat_pattern.sub(match_substitution, \\n     ...:                                   old_word) \\n     ...:     if new_word != old_word: \\n     ...:         print \\'Step: {} Word: {}\\'.format(step, new_word) \\n     ...:         step += 1 # update step \\n     ...:         # update old word to last substituted state \\n     ...:         old_word = new_word   \\n     ...:         continue \\n     ...:     else: \\n     ...:         print \"Final word:\", new_word \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n122\\n     ...:         break \\n     ...:      \\n Step: 1 Word: finalllyy \\n Step: 2 Word: finallly \\n Step: 3 Word: finally \\n Step: 4 Word: finaly \\n Final word: finaly \\n The preceding snippet shows how one repeated character is removed at each stage \\nuntil we end up with the word  finaly in the end. However, semantically this word is \\nincorrect\\xe2\\x80\\x94the correct word was  finally , which we obtained in step 3. We will now utilize \\nthe WordNet corpus to check for valid words at each stage and terminate the loop once \\nit is obtained. This introduces the semantic correction needed for our algorithm, as \\nillustrated in the following snippet: \\n In [363]: from nltk.corpus import wordnet \\n     ...: old_word = \\'finalllyyy\\' \\n     ...: repeat_pattern = re.compile(r\\'(\\\\w*)(\\\\w)\\\\2(\\\\w*)\\') \\n     ...: match_substitution = r\\'\\\\1\\\\2\\\\3\\' \\n     ...: step = 1 \\n     ...:  \\n     ...: while True: \\n     ...:     # check for semantically correct word \\n     ...:     if wordnet.synsets(old_word): \\n     ...:         print \"Final correct word:\", old_word \\n     ...:         break \\n     ...:     # remove one repeated character \\n     ...:     new_word = repeat_pattern.sub(match_substitution, \\n     ...:                                   old_word) \\n     ...:     if new_word != old_word: \\n     ...:         print \\'Step: {} Word: {}\\'.format(step, new_word) \\n     ...:         step += 1 # update step \\n     ...:         # update old word to last substituted state \\n     ...:         old_word = new_word   \\n     ...:         continue \\n     ...:     else: \\n     ...:         print \"Final word:\", new_word \\n     ...:         break \\n     ...:      \\n Step: 1 Word: finalllyy \\n Step: 2 Word: finallly \\n Step: 3 Word: finally \\n Final correct word: finally \\n Thus we see from the preceding snippet that the code correctly terminated after the \\nthird step, and we obtained the correct word, adhering to both syntax and semantics. \\n We can build a better  version of this code by writing the logic in a function, as shown \\nin the following code, to make it more generic to deal with incorrect tokens from a list of \\ntokens: \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n123\\n from nltk.corpus import wordnet \\n def remove_repeated_characters(tokens): \\n    repeat_pattern = re.compile(r\\'(\\\\w*)(\\\\w)\\\\2(\\\\w*)\\') \\n    match_substitution = r\\'\\\\1\\\\2\\\\3\\' \\n    def replace(old_word): \\n        if wordnet.synsets(old_word): \\n            return old_word \\n        new_word = repeat_pattern.sub(match_substitution, old_word) \\n        return replace(new_word) if new_word != old_word else new_word \\n    correct_tokens = [replace(word) for word in tokens] \\n    return correct_tokens \\n That snippet uses the inner function  replace()  to basically emulate the behavior of \\nour algorithm, illustrated earlier, and then call it repeatedly on each token in a sentence \\nin the outer function  remove_repeated_characters() . \\n We can see the preceding code in action in the following snippet, with an actual \\nexample sentence: \\n In [369]: sample_sentence = \\'My schooool is realllllyyy amaaazingggg\\' \\n     ...: sample_sentence_tokens = tokenize_text(sample_sentence)[0] \\n     ...: print sample_sentence_tokens \\n [\\'My\\', \\'schooool\\', \\'is\\', \\'realllllyyy\\', \\'amaaazingggg\\'] \\n In [370]: print remove_repeated_characters(sample_sentence_tokens)   \\n [\\'My\\', \\'school\\', \\'is\\', \\'really\\', \\'amazing\\'] \\n We can see from the above  output that our function performs as intended and \\nreplaces the repeating characters in each token, giving us correct tokens as desired.  \\n Correcting Spellings \\n Another problem we face is incorrect or wrong spellings that occur due to human error, \\nor even machine-based errors you may have seen thanks to features like auto-correcting \\ntext. There are various ways of dealing with incorrect spellings where the final objective \\nis to have tokens of text with the correct spelling. This section will talk about one of the \\nfamous algorithms developed by Peter Norvig, the  director of research at Google. You can \\nfind the complete detailed post explaining his algorithm and findings at   http://norvig.\\ncom/spell-correct.html  . \\n The main objective of this exercise is that, given a word, we need to find the most \\nlikely word that is the correct form of that word. The approach we would follow is to \\ngenerate a set of  candidate words that are near to our input word and select the most \\nlikely word from this set as the correct word. We use a corpus of correct English words \\nin this context to identify the correct word based on its frequency in the corpus from our \\nfinal set of candidates with the nearest distance to our input word. This distance, which \\nmeasures how near or far a word is from our input word, is also called  edit distance . The \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n124\\ninput corpus we use is a file containing several books from the Gutenberg corpus and also \\na list of most frequent words from Wiktionary and the British National Corpus. You can \\nfind the file under the name  big.txt in this chapter\\xe2\\x80\\x99s code resources or download it from \\n  http://norvig.com/big.txt and use it. \\n I\\xe2\\x80\\x99ll use the following code snippet to generate a map of frequently occurring words in \\nthe  English language and their counts: \\n import re, collections \\n def tokens(text):  \\n    \"\"\" \\n    Get all words from the corpus \\n    \"\"\" \\n    return re.findall(\\'[a-z]+\\', text.lower())  \\n WORDS = tokens(file(\\'big.txt\\').read()) \\n WORD_COUNTS = collections.Counter(WORDS) \\n # top 10 words in the corpus \\n In [407]: print WORD_COUNTS.most_common(10) \\n [(\\'the\\', 80030), (\\'of\\', 40025), (\\'and\\', 38313), (\\'to\\', 28766), (\\'in\\', \\n22050), (\\'a\\', 21155), (\\'that\\', 12512), (\\'he\\', 12401), (\\'was\\', 11410), \\n(\\'it\\', 10681)] \\n Once we have our vocabulary, we define three functions that compute sets of words \\nthat are zero, one, and two edits away from our input word. These edits can be made by \\nthe means of insertions, deletions, additions, and transpositions. The following  code   \\ndefines the functions for doing this: \\n def edits0(word):  \\n    \"\"\" \\n    Return all strings that are zero edits away  \\n    from the input word (i.e., the word itself). \\n    \"\"\" \\n    return {word} \\n def edits1(word): \\n    \"\"\" \\n    Return all strings that are one edit away  \\n    from the input word. \\n    \"\"\" \\n    alphabet = \\'abcdefghijklmnopqrstuvwxyz\\' \\n    def splits(word): \\n        \"\"\" \\n        Return a list of all possible (first, rest) pairs  \\n        that the input word is made of. \\n        \"\"\" \\n        return [(word[:i], word[i:])  \\n                for i in range(len(word)+1)] \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n125\\n    pairs      = splits(word) \\n    deletes    = [a+b[1:]           for (a, b) in pairs if b] \\n    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1] \\n    replaces   = [a+c+b[1:]          for (a, b) in pairs for c in alphabet \\nif b] \\n    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet] \\n    return set(deletes + transposes + replaces + inserts) \\n def edits2(word): \\n    \"\"\"Return all strings that are two edits away  \\n    from the input word. \\n    \"\"\" \\n    return {e2 for e1 in edits1(word) for e2 in edits1(e1)} \\n We also define a function called  known() that returns a subset of words from our \\ncandidate set of words obtained from the edit functions, based on whether they occur in \\nour vocabulary dictionary  WORD_COUNTS . This gives us a list of valid words from our set of \\ncandidate words: \\n def known(words): \\n    \"\"\" \\n    Return the subset of words that are actually  \\n    in our WORD_COUNTS dictionary. \\n    \"\"\" \\n    return {w for w in words if w in WORD_COUNTS} \\n  We can see these functions in action on our test input word in the following code \\nsnippet, which shows lists of possible candidate words based on edit distances from the \\ninput word: \\n # input word \\n In [409]: word = \\'fianlly\\' \\n # zero edit distance from input word \\n In [410]: edits0(word) \\n Out[410]: {\\'fianlly\\'} \\n # returns null set since it is not a valid word \\n In [411]: known(edits0(word)) \\n Out[411]: set() \\n # one edit distance from input word \\n In [412]: edits1(word) \\n Out[412]:  \\n {\\'afianlly\\', \\n \\'aianlly\\', \\n . \\n . \\n \\'yianlly\\', \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n126\\n \\'zfianlly\\', \\n \\'zianlly\\'} \\n # get correct words from above set \\n In [413]: known(edits1(word)) \\n Out[413]: {\\'finally\\'} \\n # two edit distances from input word \\n In [417]: edits2(word) \\n Out[417]: \\n {\\'fchnlly\\', \\n \\'fianjlys\\', \\n  . \\n  . \\n \\'fiapgnlly\\', \\n \\'finanlqly\\'} \\n # get correct words from above set \\n In [418]: known(edits2(word)) \\n Out[418]: {\\'faintly\\', \\'finally\\', \\'finely\\', \\'frankly\\'} \\n The preceding outputs depict a set of valid candidate words that could be potential \\n replacements  for the incorrect input word. We select our candidate words from the \\npreceding list by giving higher priority to words with the smallest edit distances from the \\ninput word. The following code snippet illustrates: \\n In [420]: candidates = (known(edits0(word)) or  \\n     ...:               known(edits1(word)) or  \\n     ...:               known(edits2(word)) or  \\n     ...:               [word]) \\n In [421]: candidates \\n Out[421]: {\\'finally\\'} \\n In case there is a tie in the preceding candidates, we resolve it by taking the highest \\noccurring word from our vocabulary dictionary  WORD_COUNTS  using the   max(candidates, \\nkey=WORD_COUNTS.get) function . Thus we now define our function to correct words using \\nthe logic discussed earlier: \\n def correct(word): \\n    \"\"\" \\n    Get the best correct spelling for the input word \\n    \"\"\" \\n    # Priority is for edit distance 0, then 1, then 2 \\n    # else defaults to the input word itself. \\n    candidates = (known(edits0(word)) or  \\n                  known(edits1(word)) or  \\n                  known(edits2(word)) or  \\n                  [word]) \\n    return max(candidates, key=WORD_COUNTS.get) \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n127\\n We can use the  preceding function on incorrect words directly to correct them, as \\nillustrated in the following snippet: \\n In [438]: correct(\\'fianlly\\') \\n Out[438]: \\'finally\\' \\n In [439]: correct(\\'FIANLLY\\') \\n Out[439]: \\'FIANLLY\\' \\n We see that this function is case sensitive and fails to correct words that are not \\nlowercase, hence we write the following functions to make this generic to the case of \\nwords and correct their spelling regardless. The logic here is to preserve the original case \\nof the word, convert it to lowercase, correct its spelling, and finally reconvert it back to its \\noriginal case using the  case_of function : \\n def correct_match(match): \\n    \"\"\" \\n    Spell-correct word in match,  \\n    and preserve proper upper/lower/title case. \\n    \"\"\" \\n    word = match.group() \\n    def case_of(text): \\n        \"\"\" \\n        Return the case-function appropriate  \\n        for text: upper, lower, title, or just str.: \\n            \"\"\" \\n        return (str.upper if text.isupper() else \\n                str.lower if text.islower() else \\n                str.title if text.istitle() else \\n                str) \\n    return case_of(word)(correct(word.lower())) \\n def correct_text_generic(text): \\n    \"\"\" \\n    Correct all the words within a text,  \\n    returning the corrected text. \\n    \"\"\" \\n    return re.sub(\\'[a-zA-Z]+\\', correct_match, text) \\n We can now use the  preceding function to correct words irrespective of their case, as \\nillustrated in the following snippet: \\n In [441]: correct_text_generic(\\'fianlly\\') \\n Out[441]: \\'finally\\' \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n128\\n In [442]: correct_text_generic(\\'FIANLLY\\') \\n Out[442]: \\'FINALLY\\' \\n Of course, this method is not always completely accurate, and there may be words \\nthat might not be corrected if they do not occur in our  vocabulary dictionary . Using more \\ndata would help in this case, as long as we cover different words having correct spellings \\nin our vocabulary. This same algorithm is available to be used out of the box in the \\n pattern library, as is done in the following snippet: \\n from pattern.en import suggest \\n # test on wrongly spelt words \\n In [184]: print suggest(\\'fianlly\\') \\n [(\\'finally\\', 1.0)] \\n In [185]: print suggest(\\'flaot\\') \\n [(\\'flat\\', 0.85), (\\'float\\', 0.15)] \\n Besides this, there are several robust libraries available in Python, including \\n PyEnchant , based on the  enchant library (  http://pythonhosted.org/pyenchant/  ), and \\n aspell-python , which is a Python wrapper around the popular GNU Aspell. Feel free to \\ncheck them out and use them for correcting word spellings!   \\n Stemming \\n Understanding the process of stemming requires understanding what word stems \\nrepresent. Chapter   1 talked about morphemes, the smallest independent unit in any \\nnatural language. Morphemes consist of units that are stems and  affixes. Affixes   are units \\nlike prefixes, suffixes, and so on, which are attached to a word stem to change its meaning \\nor create a new word altogether. Word stems are also often known as the  base form of a \\nword, and we can create new words by attaching affixes to them in a process known as \\n inflection . The reverse of this is obtaining the base form of a word from its inflected form, \\nand this is known as  stemming . \\n Consider the word  JUMP . You can add affixes to it and form new words like  JUMPS , \\n JUMPED , and  JUMPING . In this case, the base word JUMP is the word stem. If we were to \\ncarry out stemming on any of its three inflected forms, we would get back the base form. \\nThis is illustrated in Figure\\xc2\\xa0 3-1 . \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n129\\n The figure shows how the word stem is present in all its inflections since it forms \\nthe base on which each  inflection is built upon using affixes. Stemming helps us in \\nstandardizing words to their base stem irrespective of their inflections, which helps many \\napplications like classifying or clustering text, and even in information retrieval. Search \\nengines make use of such techniques extensively to give better and more accurate results \\nirrespective of the word form. \\n The  nltk package has several implementations for stemmers. These stemmers are \\nimplemented in the  stem  module, which inherits the   StemmerI  interface   in the  nltk.stem.\\napi module. You can even create your own stemmer using this class (technically it is an \\n interface ) as your base class. One of the most popular stemmers is the Porter stemmer, \\nwhich is based on the algorithm developed by its inventor, Dr. Martin Porter. Originally, the \\nalgorithm is said to have had a total of five different phases for reduction of inflections to \\ntheir stems, where each phase has its own set of rules. There also exists a Porter2 algorithm, \\nwhich was the original stemming algorithm with some improvements suggested by Dr. \\nPorter. You can see the Porter stemmer in action in the following  code snippet: \\n # Porter Stemmer \\n In [458]: from nltk.stem import PorterStemmer \\n     ...: ps = PorterStemmer() \\n In [459]: print ps.stem(\\'jumping\\'), ps.stem(\\'jumps\\'), ps.stem(\\'jumped\\') \\n jump jump jump \\n In [460]: print ps.stem(\\'lying\\') \\n lie \\n In [461]: print ps.stem(\\'strange\\') \\n strang \\n Figure 3-1.  Word stem and  inflections \\n \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n130\\n The  Lancaster stemmer is based on the Lancaster stemming algorithm, also often \\nknown as the Paice/Husk stemmer, invented by Chris D. Paice. This stemmer is an iterative \\nstemmer that has over 120 rules specifying specific removal or replacement for affixes to \\nobtain the word stems. The following  snippet   shows the Lancaster stemmer in action: \\n # Lancaster Stemmer \\n In [465]: from nltk.stem import LancasterStemmer \\n     ...: ls = LancasterStemmer() \\n In [466]: print ls.stem(\\'jumping\\'), ls.stem(\\'jumps\\'), ls.stem(\\'jumped\\') \\n jump jump jump \\n In [467]: print ls.stem(\\'lying\\') \\n lying \\n In [468]: print ls.stem(\\'strange\\') \\n strange \\n You can see that the behavior of this stemmer is different from the Porter stemmer. \\n There are several other stemmers, including  RegexpStemmer , where you can build \\nyour own stemmer based on  user-defined rules , and  SnowballStemmer , which supports \\nstemming in 13 different languages besides English. \\n The following code snippet shows some ways of using them for performing \\nstemming. The   RegexpStemmer uses regular expressions to identify the morphological \\naffixes in words, and any part of the string matching the same is removed: \\n # Regex based stemmer \\n In [471]: from nltk.stem import RegexpStemmer \\n     ...: rs = RegexpStemmer(\\'ing$|s$|ed$\\', min=4) \\n In [472]: print rs.stem(\\'jumping\\'), rs.stem(\\'jumps\\'), rs.stem(\\'jumped\\') \\n jump jump jump \\n In [473]: print rs.stem(\\'lying\\') \\n ly \\n In [474]: print rs.stem(\\'strange\\') \\n strange \\n You can see how the stemming results are different from the previous stemmers \\nand are based completely on our custom-defined rules based on regular expressions. \\nThe following snippet shows how to use the  SnowballStemmer to stem words in \\nother languages (you can find more details about the  Snowball Project at   http://\\nsnowballstem.org  ): \\n # Snowball Stemmer \\n In [486]: from nltk.stem import SnowballStemmer \\n     ...: ss = SnowballStemmer(\"german\") \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n131\\n In [487]: print \\'Supported Languages:\\', SnowballStemmer.languages \\n Supported Languages: (u\\'danish\\', u\\'dutch\\', u\\'english\\', u\\'finnish\\', \\nu\\'french\\', u\\'german\\', u\\'hungarian\\', u\\'italian\\', u\\'norwegian\\', u\\'porter\\', \\nu\\'portuguese\\', u\\'romanian\\', u\\'russian\\', u\\'spanish\\', u\\'swedish\\') \\n # stemming on German words \\n # autobahnen -> cars \\n # autobahn -> car \\n In [488]: ss.stem(\\'autobahnen\\') \\n Out[488]: u\\'autobahn\\' \\n # springen -> jumping \\n # spring -> jump \\n In [489]: ss.stem(\\'springen\\') \\n Out[489]: u\\'spring\\' \\n  The  Porter stemmer is used most frequently\\xe2\\x80\\x94but you should choose your stemmer \\nbased on your problem and after trial and error. If needed, you can even build your own \\nstemmer with your own defined rules. \\n Lemmatization \\n The process of  lemmatization is very similar to stemming\\xe2\\x80\\x94you remove word affixes to \\nget to a base form of the word. But in this case, this base form is also known as the  root \\nword , but not the  root stem . The difference is that the root stem may not always be a \\nlexicographically correct word; that is, it may not be present in the dictionary. The root \\nword, also known as the  lemma , will always be present in the dictionary. \\n The lemmatization process is considerably slower than stemming because an \\nadditional step is involved where the root form or lemma is formed by removing the affix \\nfrom the word if and only if the lemma is present in the dictionary. The  nltk package has \\na robust lemmatization module that uses WordNet and the word\\xe2\\x80\\x99s syntax and semantics, \\nlike part of speech and context, to get the root word or lemma. Remember parts of speech \\nfrom Chapter   1 ? There were mainly three entities\\xe2\\x80\\x94nouns, verbs, and adjectives\\xe2\\x80\\x94that \\noccur most frequently in natural language. \\n The following code snippet shows how to use lemmatization for words belonging to \\neach of those types: \\n In [514]: from nltk.stem import WordNetLemmatizer \\n     ...:  \\n     ...: wnl = WordNetLemmatizer() \\n # lemmatize nouns \\n In [515]: print wnl.lemmatize(\\'cars\\', \\'n\\') \\n     ...: print wnl.lemmatize(\\'men\\', \\'n\\') \\n car \\n men \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n132\\n # lemmatize verbs \\n In [516]: print wnl.lemmatize(\\'running\\', \\'v\\') \\n     ...: print wnl.lemmatize(\\'ate\\', \\'v\\') \\n run \\n eat \\n # lemmatize adjectives \\n In [517]: print wnl.lemmatize(\\'saddest\\', \\'a\\') \\n     ...: print wnl.lemmatize(\\'fancier\\', \\'a\\') \\n sad \\n fancy \\n The preceding snippet shows how each word is converted back to its base form using \\nlemmatization. This helps us in standardizing words. The preceding code leverages the \\n WordNetLemmatizer class , which internally uses the  morphy()  function belonging to the \\n WordNetCorpusReader  class. This function basically finds the base form or lemma for a \\ngiven word using the word and its part of speech by checking the  Wordnet corpus   and \\nuses a recursive technique for removing affixes from the word until a match is found in \\nWordNet. If no match is found, the input word itself is returned unchanged. \\n The part of  speech is extremely important here because if that is wrong, the \\nlemmatization will not be effective, as you can see in the following snippet: \\n # ineffective lemmatization \\n In [518]: print wnl.lemmatize(\\'ate\\', \\'n\\') \\n     ...: print wnl.lemmatize(\\'fancier\\', \\'v\\') \\n ate \\n fancier \\n This brings us to the end of our discussion on various techniques for processing and \\n normalizing text. By now, you have learned a great deal about how to process, normalize, \\nand standardize text. In the next section, we will look at ways of analyzing and understanding \\nvarious facets of textual data with regard to its syntactic properties and structure.  \\n Understanding Text Syntax and Structure \\n Chapter   1  talked about language syntax and structure in detail. If you don\\xe2\\x80\\x99t remember, \\nhead over to the \\xe2\\x80\\x9cLanguage Syntax and Structure\\xe2\\x80\\x9d section and skim through it quickly to \\nget an idea of the various ways of analyzing and understanding the syntax and structure \\nof textual data. In this section, we will look and implement some of the concepts and \\ntechniques that are used for understanding text syntax and structure. This is extremely \\nuseful in NLP and is usually done after text  processing and normalization  . We will focus \\non implementing the following techniques:\\n\\xe2\\x80\\xa2 \\n Parts of speech (POS) tagging \\n\\xe2\\x80\\xa2 \\n Shallow parsing \\n\\xe2\\x80\\xa2 \\n Dependency-based parsing \\n\\xe2\\x80\\xa2 \\n Constituency-based parsing \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n133\\n This book is aimed at practitioners and enforces and emphasizes the best \\napproaches to implementing and using techniques and algorithms in real-world \\nproblems. Therefore, the following sections look at the best possible ways of leveraging \\nexisting libraries like   nltk and  spacy  to implement and execute some of these techniques. \\nAlso, because many readers may be interested in the internals and implementing \\nsome of these techniques on your own, we will also look at ways to do that. Remember, \\nour primary focus is always to look at ways implementing the concepts in action with \\npractical examples\\xe2\\x80\\x94and not re-invent the wheel. Before going further, we will look at the \\nnecessary dependencies and installation details for the required libraries, because some \\nof them are not very straightforward. \\n Installing Necessary Dependencies \\n We will be leveraging several  libraries and dependencies:\\n\\xe2\\x80\\xa2 \\n The  nltk library, preferably version  3.1 or  3.2.1 \\n\\xe2\\x80\\xa2 \\n The  spacy library \\n\\xe2\\x80\\xa2 \\n The  pattern library \\n\\xe2\\x80\\xa2 \\n The Stanford parser \\n\\xe2\\x80\\xa2 \\n Graphviz and necessary libraries for the same \\n We touched on installing  nltk in Chapter   1 . You can install it directly by going to your \\nterminal or command prompt and typing  pip install nltk , which will download and \\ninstall it. Remember to install the library having a version preferably other than  3.2.0 , \\nbecause there are some issues with several functions in that distribution, like  pos_tag() . \\n After downloading and installing  nltk , remember to download the corpora also \\ndiscussed in Chapter   1 . For more on downloading and installing  nltk , see    www.nltk.\\norg/install.html   and    www.nltk.org/data.html  , which describe how to install the data \\ndependencies. You can do the same by starting the Python interpreter and using the \\nfollowing snippet: \\n import nltk \\n # download all dependencies and corpora \\n nltk.download(\\'all\\', halt_on_error=False) \\n # OR use a GUI based downloader and select dependencies \\n nltk.download() \\n To install  pattern , typing  pip install pattern  should pretty much download and \\ninstall the library and its necessary dependencies. The link    www.clips.ua.ac.be/pages/\\npattern-en   offers more information about  pattern . For  spacy , you need to first install \\nthe package and then separately install its dependencies, also called a  language model . \\nTo install  spacy , type  pip install spacy from the terminal. Once done, download \\nthe English language model using the command  python -m spacy.en.download from \\nthe terminal, which will download around 500 MB of data in the directory of the  spacy \\npackage itself. For more details, refer to   https://spacy.io/docs/#getting-started , \\nwhich tells you how to get started with using  spacy . We will use  spacy  for tagging and \\ndepicting dependency-based parsing. \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n134\\n The Stanford Parser is a Java-based implementation for a language parser developed \\nat Stanford, which helps in parsing sentences to understand their underlying structure. \\nWe will perform both dependency and constituency grammar\\xe2\\x80\\x93based parsing using the \\nStanford Parser and  nltk , which provides an excellent wrapper to leverage and use the \\nparser from Python itself without the need to write code in Java. You can refer to the \\nofficial installation guide at   https://github.com/nltk/nltk/wiki/Installing-Third-\\nParty-Software  , which describes how to download and install the Stanford Parser and \\nintegrate it with  nltk . Personally, I have faced several issues, especially in Windows-\\nbased systems, so I will provide one of the best-known methods for installation of the \\nStanford Parser and its necessary dependencies. \\n To start with, make sure you first download and install the Java Development Kit \\n(not just JRE, also known as Java Runtime  Environment ) by going to    www.oracle.com/\\ntechnetwork/java/javase/downloads/index.html?ssSourceSiteId=otnjp   . That is the \\nofficial website. Java SE  8u101 /  8u102 are the latest versions at the time of writing this \\nbook\\xe2\\x80\\x94I have used  8u102 . After installing, make sure to set the \\xe2\\x80\\x9cPath\\xe2\\x80\\x9d for Java by adding it \\nto the  Path  system environment variable. You can also create a  JAVA_HOME environment \\nvariable  pointing to the  java.exe file belonging to the JDK. In my experience, neither \\nworked for me when running the code from Python, and I had to explicitly use the \\nPython  os  library to set the environment variable, which I will show when we dive into \\nthe implementation details. Once Java is installed, download the official Stanford Parser \\nfrom   http://nlp.stanford.edu/software/stanford-parser-full-2015-04-20.zip  , \\nwhich seems to work quite well. You can try out a later version by going to   http://nlp.\\nstanford.edu/software/lex-parser.shtml#Download  and checking the Release History \\nsection. After downloading, unzip it to a known location in your filesystem. Once done, \\nyou are now ready to use the parser from  nltk , which we will be exploring soon. \\n Graphviz is not really a necessity, and we will only be using it to view the dependency \\nparse tree generated by the Stanford Parser. You can download Graphviz from its official \\nwebsite at    www.graphviz.org/Download_windows.php  and install it. Next, install  pygraphviz , \\nwhich you can get by downloading the wheel file from    www.lfd.uci.edu/~gohlke/\\npythonlibs/#pygraphviz   , based on your system architecture and python version. Then \\ninstall it using the command  pip install pygraphviz-1.3.1-cp27-none-win_amd64.\\nwhl for a 64-bit system running Python  2.7.x . Once installed,  pygraphviz should be ready \\nto work. Some have reported running into additional issues, though, and you may need to \\ninstall  pydot-ng and  graphviz in the same order using the following snippet in the terminal: \\n pip install pydot-ng \\n pip install graphviz \\n With this, we are done installing necessary dependencies and can start \\nimplementing and looking at practical examples to understand text. However, we are not \\nready just yet. We still need to go through a few basic concepts of ML before we dive into \\ncode and examples. \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n135\\n Important  Machine Learning Concepts \\n We will be implementing and training some of our own taggers in the following section \\nusing corpora and also leverage existing pre-built taggers. There are some important \\nconcepts related to analytics and ML that you must know in order to better understand \\nthe implementations:\\n\\xe2\\x80\\xa2 \\n Data preparation : Usually consists of pre-processing the data \\nbefore extracting features and training \\n\\xe2\\x80\\xa2 \\n Feature extraction : The process of extracting useful features from \\nraw data that are used to train machine learning models \\n\\xe2\\x80\\xa2 \\n Features : Various useful attributes of the data (examples could be \\nage, weight, and so on for personal data) \\n\\xe2\\x80\\xa2 \\n Training data : A set of data points used to train a model \\n\\xe2\\x80\\xa2 \\n Testing/validation data : A set of data points on which a pre-\\ntrained model is tested and evaluated to see how well it performs \\n\\xe2\\x80\\xa2 \\n Model : Built using a combination of data/features and a machine \\nlearning algorithm that could be supervised or unsupervised \\n\\xe2\\x80\\xa2 \\n Accuracy : How well the model predicts something (also has other \\ndetailed evaluation metrics like precision, recall, and F1-score)    \\n These terms should be enough to get you started. Going into further detail is beyond \\nthe scope of this book, but you will find a lot of resources on the web about ML, in case \\nyou are interested in exploring machine learning further. Later chapters cover both \\nsupervised and unsupervised learning with regard to textual data.  \\n Parts of Speech (POS) Tagging \\n Parts of speech (POS) are specific lexical categories to which words are assigned based on their \\nsyntactic context and role. Chapter   1  covered some ground on POS and mentioned the main \\nPOS being noun, verb, adjective, and adverb. The process of classifying and labeling POS tags \\nfor words called  parts of speech tagging  or  POS tagging . POS tags are used to annotate words \\nand depict their POS, which is really helpful when we need to use the same annotated text \\nlater in NLP-based applications because we can filter by specific parts of speech and utilize \\nthat information to perform specific analysis, such as narrowing down upon nouns and seeing \\nwhich ones are the most prominent, word sense disambiguation, and grammar analysis. \\n We will be using the Penn Treebank  notation for POS tagging. You can find more \\ninformation about various POS tags and their notation at    www.cis.uni-muenchen.\\nde/~schmid/tools/TreeTagger/data/Penn-Treebank-Tagset.pdf   , which contains \\ndetailed documentation explaining each tag with examples. The Penn Treebank project \\nis part of the University of Pennsylvania. Its web site at    www.cis.upenn.edu/~treebank/  \\noffers more information about the project. Remember there are various tags, such as \\nPOS tags for parts of speech assigned to words, chunk tags, which are usually assigned \\nto phrases, and some tags are secondary tags used to depict relations. Table\\xc2\\xa0 3-1  gives a \\ndetailed overview of different tags with examples, in case you do not want to go through \\nthe detailed documentation for Penn Treebank tags. You can use this as a reference \\nanytime to understand POS tags and parse trees in a better way.  \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n136\\n Table 3-1.  Parts of Speech Tags \\n Sl No. \\n TAG \\n DESCRIPTION \\n EXAMPLE(S) \\n 1 \\n CC \\n Coordinating \\nConjunction \\n and, or \\n 2 \\n CD \\n Cardinal Number \\n five, one, 2 \\n 3 \\n DT \\n Determiner \\n a, the \\n 4 \\n EX \\n Existential  there \\n there were two cars \\n 5 \\n FW \\n Foreign Word \\n d\\'hoevre, mais \\n 6 \\n IN \\n Preposition/\\nSubordinating \\nConjunction \\n of, in, on, that \\n 7 \\n JJ \\n Adjective \\n quick, lazy \\n 8 \\n JJR \\n Adjective, \\ncomparative \\n quicker, lazier \\n 9 \\n JJS \\n Adjective, \\nsuperlative \\n quickest, laziest \\n 10 \\n LS \\n List item marker \\n 2) \\n 11 \\n MD \\n Verb, modal \\n could, should \\n 12 \\n NN \\n Noun, singular or \\nmass \\n fox, dog \\n 13 \\n NNS \\n Noun, plural \\n foxes, dogs \\n 14 \\n NNP \\n Noun, proper \\nsingular \\n John, Alice \\n 15 \\n NNPS \\n Noun, proper plural  Vikings, Indians, \\nGermans \\n 16 \\n PDT \\n Predeterminer \\n both the cats \\n 17 \\n POS \\n Possessive ending \\n boss\\'s \\n 18 \\n PRP \\n Pronoun, personal \\n me, you \\n 19 \\n PRP$ \\n Pronoun, possessive  our, my, your \\n 20 \\n RB \\n Adverb \\n naturally, extremely, \\nhardly \\n 21 \\n RBR \\n Adverb, comparative  better \\n 22 \\n RBS \\n Adverb, superlative \\n best \\n 23 \\n RP \\n Adverb, particle \\n about, up \\n 24 \\n SYM \\n Symbol \\n %, $ \\n 25 \\n TO \\n Infinitival to \\n how to, what to do \\n 26 \\n UH \\n Interjection \\n oh, gosh, wow \\n(continued)\\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n137\\n The table shows the main POS tag set used in the Penn Treebank and is the most \\nwidely used POS tag set in various text analytics and NLP applications. In the following \\nsections, we will look at some recommended  POS taggers and also see how we can build \\nour own tagger. \\nTable 3-1. (continued)\\n Sl No. \\n TAG \\n DESCRIPTION \\n EXAMPLE(S) \\n 27 \\n VB \\n Verb, base form \\n run, give \\n 28 \\n VBD \\n Verb, past tense \\n ran, gave \\n 29 \\n VBG \\n Verb, gerund/\\npresent participle \\n running, giving \\n 30 \\n VBN \\n Verb, past participle \\n given \\n 31 \\n VBP \\n Verb, non-3rd \\nperson singular \\npresent \\n I think, I take \\n 32 \\n VBZ \\n Verb, 3rd person \\nsingular present \\n he thinks, he takes \\n 33 \\n WDT \\n Wh-determiner \\n which, whatever \\n 34 \\n WP \\n Wh-pronoun, \\npersonal \\n who, what \\n 35 \\n WP$ \\n Wh-pronoun, \\npossessive \\n whose \\n 36 \\n WRB \\n Wh-adverb \\n where, when \\n 37 \\n NP \\n Noun Phrase \\n the brown fox \\n 38 \\n PP \\n Prepositional Phrase  in between, over the \\ndog \\n 39 \\n VP \\n Verb Phrase \\n was jumping \\n 40 \\n ADJP \\n Adjective Phrase \\n warm and snug \\n 41 \\n ADVP \\n Adverb Phrase \\n also \\n 42 \\n SBAR \\n Subordinating \\nConjunction \\n whether or not \\n 43 \\n PRT \\n Particle \\n up \\n 44 \\n INTJ \\n Interjection \\n hello \\n 45 \\n PNP \\n Prepositional Noun \\nPhrase \\n over the dog, as of \\ntoday \\n 46 \\n -SBJ \\n Sentence Subject \\n the fox jumped over \\nthe dog \\n 47 \\n -OBJ \\n Sentence  Object \\n the fox jumped over \\nthe dog \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n138\\n Recommended POS Taggers \\n We will discuss some recommended ways for tagging sentences here. The first method \\nis using  nltk \\xe2\\x80\\x99s recommended  pos_tag() function, which is actually based on the Penn \\nTreebank. We will reuse our interesting sentence from Chapter   1  here. The following code \\nsnippet depicts how to get the POS tags of a sentence using   nltk : \\n sentence = \\'The brown fox is quick and he is jumping over the lazy dog\\' \\n import nltk \\n tokens = nltk.word_tokenize(sentence) \\n tagged_sent = nltk.pos_tag(tokens, tagset=\\'universal\\') \\n In [13]: print tagged_sent \\n [(\\'The\\', u\\'DET\\'), (\\'brown\\', u\\'ADJ\\'), (\\'fox\\', u\\'NOUN\\'), (\\'is\\', u\\'VERB\\'), \\n(\\'quick\\', u\\'ADJ\\'), (\\'and\\', u\\'CONJ\\'), (\\'he\\', u\\'PRON\\'), (\\'is\\', u\\'VERB\\'), \\n(\\'jumping\\', u\\'VERB\\'), (\\'over\\', u\\'ADP\\'), (\\'the\\', u\\'DET\\'), (\\'lazy\\', u\\'ADJ\\'), \\n(\\'dog\\', u\\'NOUN\\')] \\n The preceding output shows us the POS tag for each word in the sentence. You \\nwill find the tags quite similar to the ones shown in Table 3.1. Some of them were also \\nmentioned in Chapter   1 as general/universal tags. You can also use the   pattern module \\nto get POS tags of a sentence using the following code snippet: \\n from pattern.en import tag \\n tagged_sent = tag(sentence) \\n In [15]: print tagged_sent \\n [(u\\'The\\', u\\'DT\\'), (u\\'brown\\', u\\'JJ\\'), (u\\'fox\\', u\\'NN\\'), (u\\'is\\', u\\'VBZ\\'), \\n(u\\'quick\\', u\\'JJ\\'), (u\\'and\\', u\\'CC\\'), (u\\'he\\', u\\'PRP\\'), (u\\'is\\', u\\'VBZ\\'), \\n(u\\'jumping\\', u\\'VBG\\'), (u\\'over\\', u\\'IN\\'), (u\\'the\\', u\\'DT\\'), (u\\'lazy\\', u\\'JJ\\'), \\n(u\\'dog\\', u\\'NN\\')] \\n That output gives us tags that purely follow the Penn Treebank format, specifying the \\nform of adjective, noun, or verb in more detail. \\n Building Your Own POS Taggers \\n In this section, we will explore some techniques by which we can build our own POS \\ntaggers and will be leveraging some classes provided by  nltk  for doing so. For evaluating \\nthe performance of our taggers, we will be using some test data from the  treebank corpus \\nin  nltk . We will also be using some training data for training some of our taggers. To start, \\nwe will get the necessary data for training and evaluating the taggers by reading in the \\ntagged  treebank corpus: \\n from nltk.corpus import treebank \\n data = treebank.tagged_sents() \\n train_data = data[:3500] \\n test_data = data[3500:] \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n139\\n # get a look at what each data point looks like \\n In [17]: print train_data[0] \\n [(u\\'Pierre\\', u\\'NNP\\'), (u\\'Vinken\\', u\\'NNP\\'), (u\\',\\', u\\',\\'), (u\\'61\\', u\\'CD\\'), \\n(u\\'years\\', u\\'NNS\\'), (u\\'old\\', u\\'JJ\\'), (u\\',\\', u\\',\\'), (u\\'will\\', u\\'MD\\'), \\n(u\\'join\\', u\\'VB\\'), (u\\'the\\', u\\'DT\\'), (u\\'board\\', u\\'NN\\'), (u\\'as\\', u\\'IN\\'), (u\\'a\\', \\nu\\'DT\\'), (u\\'nonexecutive\\', u\\'JJ\\'), (u\\'director\\', u\\'NN\\'), (u\\'Nov.\\', u\\'NNP\\'), \\n(u\\'29\\', u\\'CD\\'), (u\\'.\\', u\\'.\\')] \\n # remember tokens is obtained after tokenizing our sentence \\n tokens = nltk.word_tokenize(sentence) \\n In [18]: print tokens \\n [\\'The\\', \\'brown\\', \\'fox\\', \\'is\\', \\'quick\\', \\'and\\', \\'he\\', \\'is\\', \\'jumping\\', \\'over\\', \\n\\'the\\', \\'lazy\\', \\'dog\\'] \\n We will use the test data to evaluate our taggers and see how they work on our \\nsample sentence by using its tokens as input. All the taggers we will be leveraging from \\n nltk  are part of the  nltk.tag package. Each tagger is a child class of the base  TaggerI \\nclass, and each tagger implements a  tag() function that takes a list of sentence tokens \\nas input and returns the same list of words with their POS tags as output. Besides \\ntagging, there is an  evaluate() function that is used to evaluate the performance of the \\ntagger. This is done by tagging each input test sentence and then comparing the result \\nwith the actual tags of the sentence. We will be using the very same function to test the \\nperformance of our taggers on  test_data . \\n We will first look at the  DefaultTagger , which inherits from the \\n SequentialBackoffTagger base class and assigns the same user input POS tag to each \\nword. This may seem really na\\xc3\\xafve, but it is an excellent way to form a baseline POS tagger \\nand improve upon it: \\n from nltk.tag import DefaultTagger \\n dt = DefaultTagger(\\'NN\\') \\n # accuracy on test data \\n In [24]: print dt.evaluate(test_data) \\n 0.145415819537 \\n # tagging our sample sentence \\n In [25]: print dt.tag(tokens) \\n [(\\'The\\', \\'NN\\'), (\\'brown\\', \\'NN\\'), (\\'fox\\', \\'NN\\'), (\\'is\\', \\'NN\\'), (\\'quick\\', \\n\\'NN\\'), (\\'and\\', \\'NN\\'), (\\'he\\', \\'NN\\'), (\\'is\\', \\'NN\\'), (\\'jumping\\', \\'NN\\'), \\n(\\'over\\', \\'NN\\'), (\\'the\\', \\'NN\\'), (\\'lazy\\', \\'NN\\'), (\\'dog\\', \\'NN\\')] \\n We can see from the preceding output we have obtained 14 percent accuracy in \\ncorrectly tagging words from the treebank test dataset\\xe2\\x80\\x94which is not that great, and the \\noutput tags on our sample sentence are all nouns, just as we expected because we fed the \\ntagger with the same tag. \\n We will now use regular expressions and the  RegexpTagger to see if we can build a \\nbetter performing  tagger : \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n140\\n from nltk.tag import RegexpTagger \\n # define regex tag patterns \\n patterns = [ \\n        (r\\'.*ing$\\', \\'VBG\\'),               # gerunds \\n        (r\\'.*ed$\\', \\'VBD\\'),                # simple past \\n        (r\\'.*es$\\', \\'VBZ\\'),                # 3rd singular present \\n        (r\\'.*ould$\\', \\'MD\\'),               # modals \\n        (r\\'.*\\\\\\'s$\\', \\'NN$\\'),               # possessive nouns \\n        (r\\'.*s$\\', \\'NNS\\'),                 # plural nouns \\n        (r\\'^-?[0-9]+(.[0-9]+)?$\\', \\'CD\\'),  # cardinal numbers \\n        (r\\'.*\\', \\'NN\\')                     # nouns (default) ... ] \\n rt = RegexpTagger(patterns) \\n # accuracy on test data \\n In [27]: print rt.evaluate(test_data) \\n 0.240391131765 \\n # tagging our sample sentence \\n In [28]: print rt.tag(tokens) \\n [(\\'The\\', \\'NN\\'), (\\'brown\\', \\'NN\\'), (\\'fox\\', \\'NN\\'), (\\'is\\', \\'NNS\\'), (\\'quick\\', \\n\\'NN\\'), (\\'and\\', \\'NN\\'), (\\'he\\', \\'NN\\'), (\\'is\\', \\'NNS\\'), (\\'jumping\\', \\'VBG\\'), \\n(\\'over\\', \\'NN\\'), (\\'the\\', \\'NN\\'), (\\'lazy\\', \\'NN\\'), (\\'dog\\', \\'NN\\')] \\n That output shows that the accuracy has now increased to 24 percent. But can we do \\nbetter? We will now train some n-gram taggers.  n-grams are contiguous sequences of  n \\nitems from a sequence of text or speech. These items could consist of words, phonemes, \\nletters, characters, or syllables.  Shingles are n-grams where the items only consist of \\nwords. We will use n-grams of size 1, 2, and 3, which are also known as  unigram ,  bigram , \\nand  trigram respectively. The  UnigramTagger ,  BigramTagger , and  TrigramTagger are \\nclasses that inherit from the base class  NGramTagger , which itself inherits from the \\n ContextTagger  class , which inherits from the  SequentialBackoffTagger  class . We will \\nuse  train_data  as training data to train the n-gram taggers based on sentence tokens and \\ntheir POS tags. Then we will evaluate the trained taggers on  test_data and see the result \\non tagging our sample sentence: \\n from nltk.tag import UnigramTagger \\n from nltk.tag import BigramTagger \\n from nltk.tag import TrigramTagger \\n ut = UnigramTagger(train_data) \\n bt = BigramTagger(train_data) \\n tt = TrigramTagger(train_data) \\n # testing performance of unigram tagger \\n In [31]: print ut.evaluate(test_data) \\n 0.861361215994 \\n In [32]: print ut.tag(tokens) \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n141\\n [(\\'The\\', u\\'DT\\'), (\\'brown\\', None), (\\'fox\\', None), (\\'is\\', u\\'VBZ\\'), (\\'quick\\', \\nu\\'JJ\\'), (\\'and\\', u\\'CC\\'), (\\'he\\', u\\'PRP\\'), (\\'is\\', u\\'VBZ\\'), (\\'jumping\\', u\\'VBG\\'), \\n(\\'over\\', u\\'IN\\'), (\\'the\\', u\\'DT\\'), (\\'lazy\\', None), (\\'dog\\', None)] \\n # testing performance of bigram tagger \\n In [33]: print bt.evaluate(test_data) \\n 0.134669377481 \\n In [34]: print bt.tag(tokens) \\n [(\\'The\\', u\\'DT\\'), (\\'brown\\', None), (\\'fox\\', None), (\\'is\\', None), (\\'quick\\', \\nNone), (\\'and\\', None), (\\'he\\', None), (\\'is\\', None), (\\'jumping\\', None), \\n(\\'over\\', None), (\\'the\\', None), (\\'lazy\\', None), (\\'dog\\', None)] \\n # testing performance of trigram tagger \\n In [35]: print tt.evaluate(test_data) \\n 0.0806467228192 \\n In [36]: print tt.tag(tokens) \\n [(\\'The\\', u\\'DT\\'), (\\'brown\\', None), (\\'fox\\', None), (\\'is\\', None), (\\'quick\\', \\nNone), (\\'and\\', None), (\\'he\\', None), (\\'is\\', None), (\\'jumping\\', None), \\n(\\'over\\', None), (\\'the\\', None), (\\'lazy\\', None), (\\'dog\\', None)] \\n  The preceding output clearly shows that we obtain 86 percent accuracy on the test \\nset using  UnigramTagger tagger alone, which is really good compared to our last tagger. \\nThe  None  tag indicates the tagger was unable to tag that word, the reason being that it was \\nunable to get a similar token in the training data. Accuracies of the  bigram   and  trigram \\nmodels are far less because it is not always the case that the same bigrams and trigrams it \\nhad observed in the training data will also be present in the same way in the testing data. \\n We will now look at an approach to combine all the taggers by creating a combined \\ntagger with a list of taggers and use a backoff tagger. Essentially we would create a chain of \\ntaggers, and each tagger would fall back on a backoff tagger if it cannot tag the  input tokens  : \\n def combined_tagger(train_data, taggers, backoff=None): \\n    for tagger in taggers: \\n        backoff = tagger(train_data, backoff=backoff) \\n    return backoff \\n ct = combined_tagger(train_data=train_data,  \\n                     taggers=[UnigramTagger, BigramTagger, TrigramTagger], \\n                     backoff=rt) \\n # evaluating the new combined tagger with backoff taggers \\n In [38]: print ct.evaluate(test_data)         \\n 0.910155871817 \\n In [39]: print ct.tag(tokens) \\n [(\\'The\\', u\\'DT\\'), (\\'brown\\', \\'NN\\'), (\\'fox\\', \\'NN\\'), (\\'is\\', u\\'VBZ\\'), (\\'quick\\', \\nu\\'JJ\\'), (\\'and\\', u\\'CC\\'), (\\'he\\', u\\'PRP\\'), (\\'is\\', u\\'VBZ\\'), (\\'jumping\\', \\'VBG\\'), \\n(\\'over\\', u\\'IN\\'), (\\'the\\', u\\'DT\\'), (\\'lazy\\', \\'NN\\'), (\\'dog\\', \\'NN\\')] \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n142\\n We now obtain an accuracy of 91 percent on the test data, which is excellent. Also we \\nsee that this new tagger is able to successfully tag all the tokens in our sample sentence \\n(even though a couple of them are not correct, like  brown should be an adjective). \\n For our final tagger, we will use a supervised classification algorithm to train our \\ntagger. The   ClassifierBasedPOSTagger class lets us train a tagger by using a supervised \\nlearning algorithm in the  classifier_builder parameter. This class is inherited from the \\n ClassifierBasedTagger and has a  feature_detector() function that forms the core of \\nthe training process. This function is used to generate various features from the training \\ndata, like word, previous word, tag, previous tag, case, and so on. In fact, you can even \\nbuild your own feature detector function and pass it to the  feature_detector parameter \\nwhen instantiating an object of the  ClassifierBasedPOSTagger class. The classifier we \\nwill be using is the  NaiveBayesClassifier , which uses the Bayes\\xe2\\x80\\x99 theorem to build a \\nprobabilistic classifier, assuming the features are independent. Read more about it at \\n  https://en.wikipedia.org/wiki/Naive_Bayes_classifier if you like (since going into \\nmore detail about the algorithm is out of our current scope). \\n The following code snippet shows a  classification-based approach   to building and \\nevaluating a POS tagger: \\n from nltk.classify import NaiveBayesClassifier \\n from nltk.tag.sequential import ClassifierBasedPOSTagger \\n nbt = ClassifierBasedPOSTagger(train=train_data, \\n                                classifier_builder=NaiveBayesClassifier.\\ntrain) \\n # evaluate tagger on test data and sample sentence \\n In [41]: print nbt.evaluate(test_data) \\n 0.930680607997 \\n In [42]: print nbt.tag(tokens)     \\n [(\\'The\\', u\\'DT\\'), (\\'brown\\', u\\'JJ\\'), (\\'fox\\', u\\'NN\\'), (\\'is\\', u\\'VBZ\\'), \\n(\\'quick\\', u\\'JJ\\'), (\\'and\\', u\\'CC\\'), (\\'he\\', u\\'PRP\\'), (\\'is\\', u\\'VBZ\\'), \\n(\\'jumping\\', u\\'VBG\\'), (\\'over\\', u\\'IN\\'), (\\'the\\', u\\'DT\\'), (\\'lazy\\', u\\'JJ\\'), \\n(\\'dog\\', u\\'VBG\\')] \\n Using the preceding  tagger , we get an accuracy of 93 percent on our test data\\xe2\\x80\\x94the \\nhighest out of all our taggers. Also if you observe the output tags for our sample sentence, \\nyou will see they are correct and make perfect sense. This gives us an idea of how \\npowerful and effective classifier-based POS taggers can be. Feel free to use a different \\nclassifier, like   MaxentClassifier , and compare the performance with this tagger. There \\nare also several other ways to build and use POS taggers using  nltk  and other packages. \\nEven though it is not really necessary, and this should be enough to cover your POS \\ntagging needs, you can go ahead and explore other methods to compare with these \\nmethods and satisfy your curiosity. \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n143\\n Shallow Parsing \\n Shallow parsing , also known as  light parsing or  chunking , is a technique of analyzing the \\nstructure of a sentence to break it down into its smallest constituents (which are tokens \\nsuch as words) and group them together into higher-level phrases. In shallow parsing, \\nthere is more focus on identifying these phrases or chunks rather than diving into further \\ndetails of the internal syntax and relations inside each chunk, like we see in grammar-\\nbased parse trees obtained from deep parsing. The main objective of shallow parsing is to \\nobtain semantically meaningful phrases and observe relations among them. \\n Refer to the \\xe2\\x80\\x9cLanguage Syntax and Structure\\xe2\\x80\\x9d section from Chapter   1 to refresh your \\nmemory regarding how words and phrases give structure to a sentence consisting of a \\nbunch of words. A shallow parsed tree is also depicted there for our sample sentence. \\nWe will look at various ways of performing shallow parsing by starting with some \\nrecommended out-of-the-box shallow parsers. We will also implement some of our own \\nshallow parsers using techniques like regular expressions, chunking, chinking, and tag-\\nbased training. \\n Recommended Shallow Parsers \\n We will be leveraging the  pattern package here to create a shallow parser to extract \\nmeaningful chunks out of sentences. The following  code snippet shows how to perform \\nshallow parsing on our sample sentence: \\n sentence = \\'The brown fox is quick and he is jumping over the lazy dog\\' \\n from pattern.en import parsetree \\n tree = parsetree(sentence) \\n # print the shallow parsed sentence tree \\n In [5]: print tree \\n   ...:  \\n [Sentence(\\'The/DT/B-NP/O brown/JJ/I-NP/O fox/NN/I-NP/O is/VBZ/B-VP/O quick/\\nJJ/B-ADJP/O and/CC/O/O he/PRP/B-NP/O is/VBZ/B-VP/O jumping/VBG/I-VP/O over/\\nIN/B-PP/B-PNP the/DT/B-NP/I-PNP lazy/JJ/I-NP/I-PNP dog/NN/I-NP/I-PNP\\')] \\n  The preceding output is the raw shallow-parsed  sentence tree for our sample \\nsentence. Many of the tags will be quite familiar if you compare them to the earlier  POS \\ntags table. You will notice some new notations with  I ,  O , and  B  prefixes, the popular IOB \\nnotation used in chunking, that represent Inside, Outside, and Beginning. The  B- prefix \\nbefore a tag indicates it is the beginning of a chunk, and  I- prefix indicates that it is inside \\na chunk. The  O tag indicates that the token does not belong to any chunk. The  B-  tag is \\nalways used when there are subsequent tags following it of the same type without the \\npresence of  O tags between them. \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n144\\n The following  snippet shows how to get chunks in an easier-to-understand format: \\n # print all chunks \\n In [6]: for sentence_tree in tree: \\n   ...:     print sentence_tree.chunks \\n [Chunk(\\'The brown fox/NP\\'), Chunk(\\'is/VP\\'), Chunk(\\'quick/ADJP\\'), Chunk(\\'he/\\nNP\\'), Chunk(\\'is jumping/VP\\'), Chunk(\\'over/PP\\'), Chunk(\\'the lazy dog/NP\\')] \\n # Depict each phrase and its internal constituents \\n In [9]: for sentence_tree in tree: \\n   ...:     for chunk in sentence_tree.chunks: \\n   ...:         print chunk.type, \\'->\\', [(word.string, word.type)  \\n   ...:                                  for word in chunk.words] \\n NP -> [(u\\'The\\', u\\'DT\\'), (u\\'brown\\', u\\'JJ\\'), (u\\'fox\\', u\\'NN\\')] \\n VP -> [(u\\'is\\', u\\'VBZ\\')] \\n ADJP -> [(u\\'quick\\', u\\'JJ\\')] \\n NP -> [(u\\'he\\', u\\'PRP\\')] \\n VP -> [(u\\'is\\', u\\'VBZ\\'), (u\\'jumping\\', u\\'VBG\\')] \\n PP -> [(u\\'over\\', u\\'IN\\')] \\n NP -> [(u\\'the\\', u\\'DT\\'), (u\\'lazy\\', u\\'JJ\\'), (u\\'dog\\', u\\'NN\\')] \\n The preceding outputs show an easier-to-understand result obtained from shallow \\nparsing of our sample sentence, where each phrase and its constituents are clearly \\n shown . \\n We can create some  generic functions to parse and visualize shallow parsed \\nsentence trees in a better way and also reuse them to parse any sentence in general. The \\nfollowing  code shows how: \\n from pattern.en import parsetree, Chunk \\n from nltk.tree import Tree \\n # create a shallow parsed sentence tree \\n def create_sentence_tree(sentence, lemmatize=False): \\n    sentence_tree = parsetree(sentence,  \\n                              relations=True,  \\n                               lemmata=lemmatize) # if you want to lemmatize \\nthe tokens \\n    return sentence_tree[0] \\n # get various constituents of the parse tree     \\n def get_sentence_tree_constituents(sentence_tree): \\n    return sentence_tree.constituents() \\n # process the shallow parsed tree into an easy to understand format     \\n def process_sentence_tree(sentence_tree): \\n    tree_constituents = get_sentence_tree_constituents(sentence_tree) \\n    processed_tree = [ \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n145\\n                        (item.type, \\n                         [ \\n                             (w.string, w.type) \\n                             for w in item.words \\n                         ] \\n                        ) \\n                        if type(item) == Chunk \\n                        else (\\'-\\', \\n                              [ \\n                                   (item.string, item.type) \\n                              ] \\n                             ) \\n                             for item in tree_constituents \\n                    ] \\n    return processed_tree \\n # print the sentence tree using nltk\\'s Tree syntax     \\n def print_sentence_tree(sentence_tree): \\n    processed_tree = process_sentence_tree(sentence_tree) \\n    processed_tree = [ \\n                        Tree( item[0], \\n                             [ \\n                                 Tree(x[1], [x[0]]) \\n                                 for x in item[1] \\n                             ] \\n                            ) \\n                            for item in processed_tree \\n                     ] \\n    tree = Tree(\\'S\\', processed_tree ) \\n    print tree \\n # visualize the sentence tree using nltk\\'s Tree syntax     \\n def visualize_sentence_tree(sentence_tree): \\n    processed_tree = process_sentence_tree(sentence_tree) \\n    processed_tree = [ \\n                        Tree( item[0], \\n                             [ \\n                                 Tree(x[1], [x[0]]) \\n                                 for x in item[1] \\n                             ] \\n                            ) \\n                            for item in processed_tree \\n                     ] \\n    tree = Tree(\\'S\\', processed_tree ) \\n    tree.draw() \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n146\\n We can see the preceding functions in action on our sample sentence in the \\nfollowing  code snippet when we execute them: \\n # raw shallow parsed tree \\n In [11]: t = create_sentence_tree(sentence) \\n    ...: print t \\n Sentence(\\'The/DT/B-NP/O/NP-SBJ-1 brown/JJ/I-NP/O/NP-SBJ-1 fox/NN/I-NP/O/NP-\\nSBJ-1 is/VBZ/B-VP/O/VP-1 quick/JJ/B-ADJP/O/O and/CC/O/O/O he/PRP/B-NP/O/NP-\\nSBJ-2 is/VBZ/B-VP/O/VP-2 jumping/VBG/I-VP/O/VP-2 over/IN/B-PP/B-PNP/O the/\\nDT/B-NP/I-PNP/O lazy/JJ/I-NP/I-PNP/O dog/NN/I-NP/I-PNP/O\\') \\n # processed shallow parsed tree \\n In [16]: pt = process_sentence_tree(t) \\n    ...: pt \\n Out[16]:  \\n [(u\\'NP\\', [(u\\'The\\', u\\'DT\\'), (u\\'brown\\', u\\'JJ\\'), (u\\'fox\\', u\\'NN\\')]), \\n (u\\'VP\\', [(u\\'is\\', u\\'VBZ\\')]), \\n (u\\'ADJP\\', [(u\\'quick\\', u\\'JJ\\')]), \\n (\\'-\\', [(u\\'and\\', u\\'CC\\')]), \\n (u\\'NP\\', [(u\\'he\\', u\\'PRP\\')]), \\n (u\\'VP\\', [(u\\'is\\', u\\'VBZ\\'), (u\\'jumping\\', u\\'VBG\\')]), \\n (u\\'PP\\', [(u\\'over\\', u\\'IN\\')]), \\n (u\\'NP\\', [(u\\'the\\', u\\'DT\\'), (u\\'lazy\\', u\\'JJ\\'), (u\\'dog\\', u\\'NN\\')])] \\n # print shallow parsed tree in an easy to understand format using nltk\\'s \\nTree syntax \\n In [17]: print_sentence_tree(t) \\n (S \\n  (NP (DT The) (JJ brown) (NN fox)) \\n  (VP (VBZ is)) \\n  (ADJP (JJ quick)) \\n  (- (CC and)) \\n  (NP (PRP he)) \\n  (VP (VBZ is) (VBG jumping)) \\n  (PP (IN over)) \\n  (NP (DT the) (JJ lazy) (NN dog))) \\n # visualize the shallow parsed tree \\n In [18]: visualize_sentence_tree(t) \\n Figure 3-2.  Visual representation of a shallow parsed tree for our sample sentence \\n \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n147\\n  The preceding outputs show some ways of creating, representing, and visualizing \\nshallow parse trees from sentences. The visual representation shown in Figure\\xc2\\xa0 3-2  is very \\nsimilar to the tree shown in Chapter   1 for the same sentence. The lowest level indicates \\nthe values of the actual tokens; the next level indicates the  POS tags for each token; \\nand the next higher level indicates the chunk phrasal tags. Go ahead and try out these \\nfunctions on some other sentences and compare their results. In the following sections \\nwe will implement some of our own shallow parsers. \\n Building Your Own Shallow Parsers \\n We will use several techniques like regular expressions and tagging-based learners to build \\nour own shallow parsers. As with POS tagging, we will use some training data to train our \\nparsers if needed and evaluate all our parsers on some test data and also on our sample \\nsentence. The  treebank corpus is available in  nltk  with  chunk annotations  . We will load it \\nfirst and prepare our training and testing datasets using the following  code snippet : \\n from nltk.corpus import treebank_chunk \\n data = treebank_chunk.chunked_sents() \\n train_data = data[:4000] \\n test_data = data[4000:] \\n # view what a sample data point looks like \\n In [21]: print train_data[7] \\n (S \\n  (NP A/DT Lorillard/NNP spokewoman/NN) \\n  said/VBD \\n  ,/, \\n  ``/`` \\n  (NP This/DT) \\n  is/VBZ \\n  (NP an/DT old/JJ story/NN) \\n  ./.) \\n  From the preceding output, you can see that our data points are sentences that are \\nalready annotated with phrase and  POS tags metadata that will be useful in training \\nshallow parsers. We will start with using regular expressions for shallow parsing using \\nconcepts of chunking and chinking. Using the process of   chunking , we can use and \\nspecify specific patterns to identify what we would want to chunk or segment in a \\nsentence, like phrases based on specific metadata like POS tags for each token.  Chinking \\nis the reverse of chunking, where we specify which specific tokens we do not want to be \\na part of any chunk and then form the necessary chunks excluding these tokens. Let us \\nconsider a simple sentence and use regular expressions by leveraging the  RegexpParser \\nclass to create shallow parsers to illustrate both chunking and chinking for  noun phrases : \\n simple_sentence = \\'the quick fox jumped over the lazy dog\\' \\n from nltk.chunk import RegexpParser \\n from pattern.en import tag \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n148\\n # get POS tagged sentence \\n tagged_simple_sent = tag(simple_sentence) \\n In [83]: print tagged_simple_sent \\n [(u\\'the\\', u\\'DT\\'), (u\\'quick\\', u\\'JJ\\'), (u\\'fox\\', u\\'NN\\'), (u\\'jumped\\', u\\'VBD\\'), \\n(u\\'over\\', u\\'IN\\'), (u\\'the\\', u\\'DT\\'), (u\\'lazy\\', u\\'JJ\\'), (u\\'dog\\', u\\'NN\\')] \\n # illustrate NP chunking based on explicit chunk patterns \\n chunk_grammar = \"\"\" \\n NP: {<DT>?<JJ>*<NN.*>} \\n \"\"\" \\n rc = RegexpParser(chunk_grammar) \\n c = rc.parse(tagged_simple_sent) \\n # view NP chunked sentence using chunking \\n In [86]: print c \\n (S \\n  (NP the/DT quick/JJ fox/NN) \\n  jumped/VBD \\n  over/IN \\n  (NP the/DT lazy/JJ dog/NN)) \\n # illustrate NP chunking based on explicit chink patterns \\n chink_grammar = \"\"\" \\n NP: {<.*>+} # chunk everything as NP \\n }<VBD|IN>+{ \\n \"\"\" \\n rc = RegexpParser(chink_grammar) \\n c = rc.parse(tagged_simple_sent) \\n # view NP chunked sentence using chinking \\n In [89]: print c \\n (S \\n  (NP the/DT quick/JJ fox/NN) \\n  jumped/VBD \\n  over/IN \\n  (NP the/DT lazy/JJ dog/NN)) \\n Thus we can see from the preceding outputs that we obtained similar results on a \\ntoy NP shallow parser using chunking as well as chinking. Remember that  chunks are \\nsequences of tokens that are included in a collective group (chunk), and  chinks  are tokens \\nor  sequences of tokens that are excluded from chunks. \\n We will now train a more generic regular expression-based shallow parser and \\ntest its performance on our test  treebank data . Internally, several steps are executed \\nto perform this parsing. The  Tree structures used to represent parsed sentences in \\n nltk get converted to  ChunkString objects. We create an object of  RegexpParser using \\ndefined chunking and chinking rules. Objects of classes  ChunkRule and  ChinkRule \\nhelp in creating the complete shallow-parsed tree with the necessary chunks based on \\nspecified patterns. The following code snippet represents a shallow parser using regular \\n expression-based patterns : \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n149\\n # create POS tagged tokens for sample sentence \\n tagged_sentence = tag(sentence) \\n In [90]: print tagged_sentence \\n [(u\\'The\\', u\\'DT\\'), (u\\'brown\\', u\\'JJ\\'), (u\\'fox\\', u\\'NN\\'), (u\\'is\\', u\\'VBZ\\'), \\n(u\\'quick\\', u\\'JJ\\'), (u\\'and\\', u\\'CC\\'), (u\\'he\\', u\\'PRP\\'), (u\\'is\\', u\\'VBZ\\'), \\n(u\\'jumping\\', u\\'VBG\\'), (u\\'over\\', u\\'IN\\'), (u\\'the\\', u\\'DT\\'), (u\\'lazy\\', u\\'JJ\\'), \\n(u\\'dog\\', u\\'NN\\')] \\n # create the shallow parser \\n grammar = \"\"\" \\n NP: {<DT>?<JJ>?<NN.*>}   \\n ADJP: {<JJ>} \\n ADVP: {<RB.*>} \\n PP: {<IN>}       \\n VP: {<MD>?<VB.*>+} \\n \"\"\" \\n rc = RegexpParser(grammar) \\n c = rc.parse(tagged_sentence) \\n # view shallow parsed sample sentence \\n In [99]: print c \\n (S \\n  (NP The/DT brown/JJ fox/NN) \\n  (VP is/VBZ) \\n  quick/JJ \\n  and/CC \\n  he/PRP \\n  (VP is/VBZ jumping/VBG) \\n  (PP over/IN) \\n  (NP the/DT lazy/JJ dog/NN)) \\n # evaluate parser performance on test data \\n In [100]: print rc.evaluate(test_data) \\n ChunkParse score: \\n    IOB Accuracy:  54.5% \\n    Precision:     25.0% \\n    Recall:        52.5% \\n    F-Measure:     33.9% \\n From the preceding output, we can see that the parse tree for our sample sentence \\nis very similar to the one we obtained from the out-of-the-box parser in the previous \\nsection. Also, the accuracy on the overall test data is 54.5 percent, which is quite decent \\nfor a start. For more details on what each performance metric signifies, refer to the \\n\\xe2\\x80\\x9c Evaluating Classification Models \\xe2\\x80\\x9d section in Chapter   4 . \\n Remember when I said annotated tagged metadata for text is useful in many ways? \\nWe will use the chunked and tagged  treebank training data   now to build a shallow \\nparser. We will leverage two chunking utility functions,  tree2conlltags , to get triples of \\nword, tag, and chunk tags for each token, and  conlltags2tree to generate a parse tree \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n150\\nfrom these token triples. We will be using these functions to train our parser later. First \\nlet us see how these two functions work. Remember, the chunk tags use the  IOB format \\nmentioned earlier: \\n from nltk.chunk.util import tree2conlltags, conlltags2tree \\n # look at a sample training tagged sentence \\n In [104]: train_sent = train_data[7] \\n     ...: print train_sent \\n (S \\n  (NP A/DT Lorillard/NNP spokewoman/NN) \\n  said/VBD \\n  ,/, \\n  ``/`` \\n  (NP This/DT) \\n  is/VBZ \\n  (NP an/DT old/JJ story/NN) \\n  ./.) \\n # get the (word, POS tag, Chunk tag) triples for each token \\n In [106]: wtc = tree2conlltags(train_sent) \\n     ...: wtc \\n Out[106]:  \\n [(u\\'A\\', u\\'DT\\', u\\'B-NP\\'), \\n (u\\'Lorillard\\', u\\'NNP\\', u\\'I-NP\\'), \\n (u\\'spokewoman\\', u\\'NN\\', u\\'I-NP\\'), \\n (u\\'said\\', u\\'VBD\\', u\\'O\\'), \\n (u\\',\\', u\\',\\', u\\'O\\'), \\n (u\\'``\\', u\\'``\\', u\\'O\\'), \\n (u\\'This\\', u\\'DT\\', u\\'B-NP\\'), \\n (u\\'is\\', u\\'VBZ\\', u\\'O\\'), \\n (u\\'an\\', u\\'DT\\', u\\'B-NP\\'), \\n (u\\'old\\', u\\'JJ\\', u\\'I-NP\\'), \\n (u\\'story\\', u\\'NN\\', u\\'I-NP\\'), \\n (u\\'.\\', u\\'.\\', u\\'O\\')] \\n # get shallow parsed tree back from the WTC triples \\n In [107]: tree = conlltags2tree(wtc) \\n     ...: print tree \\n (S \\n  (NP A/DT Lorillard/NNP spokewoman/NN) \\n  said/VBD \\n  ,/, \\n  ``/`` \\n  (NP This/DT) \\n  is/VBZ \\n  (NP an/DT old/JJ story/NN) \\n  ./.) \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n151\\n Now that we know how these functions work, we will define a function  conll_tag_\\nchunks()  to extract  POS and chunk tags from sentences with chunked annotations and \\nalso reuse our  combined_taggers() function from POS tagging to train multiple taggers \\nwith backoff taggers, as shown in the following code snippet: \\n def conll_tag_chunks(chunk_sents): \\n  tagged_sents = [tree2conlltags(tree) for tree in chunk_sents] \\n  return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents] \\n def combined_tagger(train_data, taggers, backoff=None): \\n    for tagger in taggers: \\n        backoff = tagger(train_data, backoff=backoff) \\n    return backoff \\n  We will now define a class  NGramTagChunker that will take in tagged sentences \\nas training input, get their (word, POS tag, Chunk tag) WTC triples, and train a \\n BigramTagger with a  UnigramTagger as the backoff tagger. We will also define a  parse() \\nfunction  to perform shallow parsing on new sentences: \\n from nltk.tag import UnigramTagger, BigramTagger \\n from nltk.chunk import ChunkParserI \\n class NGramTagChunker(ChunkParserI): \\n  def __init__(self, train_sentences,  \\n               tagger_classes=[UnigramTagger, BigramTagger]): \\n    train_sent_tags = conll_tag_chunks(train_sentences) \\n    self.chunk_tagger = combined_tagger(train_sent_tags, tagger_classes) \\n  def parse(self, tagged_sentence): \\n    if not tagged_sentence:  \\n        return None \\n    pos_tags = [tag for word, tag in tagged_sentence] \\n    chunk_pos_tags = self.chunk_tagger.tag(pos_tags) \\n    chunk_tags = [chunk_tag for (pos_tag, chunk_tag) in chunk_pos_tags] \\n    wpc_tags = [(word, pos_tag, chunk_tag) for ((word, pos_tag), chunk_tag) \\n                     in zip(tagged_sentence, chunk_tags)] \\n    return conlltags2tree(wpc_tags) \\n In the preceding class, the constructor  __init__() function is used to train the \\nshallow parser using n-gram tagging based on the WTC triples for each sentence. \\nInternally, it takes a list of training sentences as input, which is annotated with chunked \\nparse tree metadata. It uses the  conll_tag_chunks() function that we defined earlier to \\nget a list of WTC triples for each chunked parse tree. Finally, it trains a  Bigram tagger with \\na  Unigram  tagger as a backoff tagger using these triples and stores the training model in \\n self.chunk_tagger . Remember you can parse other n-gram-based taggers for training by \\nusing the   tagger_classes parameter . Once trained, the  parse() function can be used to \\nevaluate the tagger on test data and also shallow parse new sentences. Internally, it takes \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n152\\na POS tagged sentence as input, separates out the POS tags from the sentence, and uses \\nour trained  self.chunk_tagger to get the IOB chunk tags for the sentence. This is then \\ncombined with the original sentence tokens, and we use the  conlltags2tree() function \\nto get our final shallow parsed tree. \\n The following snippet shows our parser in action: \\n # train the shallow parser \\n ntc = NGramTagChunker(train_data) \\n # test parser performance on test data \\n In [114]: print ntc.evaluate(test_data) \\n ChunkParse score: \\n    IOB Accuracy:  99.6% \\n    Precision:     98.4% \\n    Recall:       100.0% \\n    F-Measure:     99.2% \\n # parse our sample sentence \\n In [115]: tree = ntc.parse(tagged_sentence) \\n     ...: print tree \\n (S \\n  (NP The/DT brown/JJ fox/NN) \\n  is/VBZ \\n  (NP quick/JJ) \\n  and/CC \\n  (NP he/PRP) \\n  is/VBZ \\n  jumping/VBG \\n  over/IN \\n  (NP the/DT lazy/JJ dog/NN)) \\n That output shows that our  parser performance on the  treebank test set data has an \\noverall accuracy of 99.6 percent\\xe2\\x80\\x94which is really excellent! \\n Let us train and evaluate our parser on the  conll2000 corpus , which contains \\nexcerpts from the  Wall Street Journal and is a much larger corpus. We will train our parser \\non the first 7,500 sentences and test its performance on the remaining 3,448 sentences. \\nThe following snippet shows this: \\n from nltk.corpus import conll2000 \\n wsj_data = conll2000.chunked_sents() \\n train_wsj_data = wsj_data[:7500] \\n test_wsj_data = wsj_data[7500:] \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n153\\n # look at a sample sentence in the corpus \\n In [125]: print train_wsj_data[10] \\n (S \\n  (NP He/PRP) \\n  (VP reckons/VBZ) \\n  (NP the/DT current/JJ account/NN deficit/NN) \\n  (VP will/MD narrow/VB) \\n  (PP to/TO) \\n  (NP only/RB #/# 1.8/CD billion/CD) \\n  (PP in/IN) \\n  (NP September/NNP) \\n  ./.) \\n # train the shallow parser \\n tc = NGramTagChunker(train_wsj_data) \\n # test performance on the test data \\n In [126]: print tc.evaluate(test_wsj_data) \\n ChunkParse score: \\n    IOB Accuracy:  66.8% \\n    Precision:     77.7% \\n    Recall:        45.4% \\n    F-Measure:     57.3% \\n  The preceding output shows that our parser achieved an overall accuracy of around \\n67 percent, because this corpus is much larger than the  treebank  corpus  . You can also \\nlook at implementing shallow parsers using other techniques, like supervised classifiers, \\nby leveraging the  ClassifierBasedTagger class. \\n Dependency-based Parsing \\n In dependency-based parsing, we try to use dependency-based grammars to analyze \\nand infer both structure and semantic dependencies and relationships between tokens \\nin a sentence. (Refer to the \\xe2\\x80\\x9cDependency Grammars\\xe2\\x80\\x9d subsection under \\xe2\\x80\\x9cGrammar\\xe2\\x80\\x9d \\nin the \\xe2\\x80\\x9c Language Syntax and Structure \\xe2\\x80\\x9d section from Chapter   1  if you need to refresh \\nyour memory.) Dependency-based grammars help us in annotating sentences with \\ndependency tags that are one-to-one mappings between tokens signifying dependencies \\nbetween them. A dependency grammar-based parse tree representation is a labelled \\nand directed tree or  graph , to be more precise. The nodes are always the lexical tokens, \\nand the labelled edges show dependency relationships between the heads and their \\ndependents. The labels on the edges indicate the grammatical role of the dependent. If \\nyou remember our sample sentence  The brown fox is quick and he is jumping over the \\nlazy dog , Figure  3-3 from Chapter   1 is one of the many ways of depicting the dependency \\nrelationships. \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n154\\n In this section we will look at some ways in which we can further understand the \\nsyntax and semantics between  textual tokens using dependency grammar-based parsing. \\n Recommended Dependency Parsers \\n We will be using a couple of libraries to generate dependency-based parse trees and test \\nthem on our sample sentence. To start with, we will use  spacy  to analyze our sample \\nsentence and generate each token and its dependencies. Figure\\xc2\\xa0 3-3 was generated using \\n spacy\\'s  output  and putting some beautiful CSS to make the dependencies look clear and \\neasy to understand. \\n The following  code snippet show how to get dependencies for each token in our \\nsample sentence: \\n sentence = \\'The brown fox is quick and he is jumping over the lazy dog\\' \\n # load dependencies \\n from spacy.en import English \\n parser = English() \\n parsed_sent = parser(unicode(sentence)) \\n # generate dependency parser output \\n In [131]: dependency_pattern = \\'{left}<---{word}[{w_type}]---\\n>{right}\\\\n--------\\' \\n     ...: for token in parsed_sent: \\n     ...:     print dependency_pattern.format(word=token.orth_,  \\n     ...:                                   w_type=token.dep_, \\n     ...:                                   left=[t.orth_  \\n     ...:                                             for t  \\n     ...:                                             in token.lefts], \\n Figure 3-3.  Dependency grammar annotated graph for our sample sentence \\n \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n155\\n     ...:                                   right=[t.orth_  \\n     ...:                                              for t  \\n     ...:                                              in token.rights]) \\n []<---The[det]--->[] \\n -------- \\n []<---brown[amod]--->[] \\n -------- \\n [u\\'The\\', u\\'brown\\']<---fox[nsubj]--->[] \\n -------- \\n [u\\'fox\\']<---is[ROOT]--->[u\\'quick\\', u\\'and\\', u\\'jumping\\'] \\n -------- \\n []<---quick[acomp]--->[] \\n -------- \\n []<---and[cc]--->[] \\n -------- \\n []<---he[nsubj]--->[] \\n -------- \\n []<---is[aux]--->[] \\n -------- \\n [u\\'he\\', u\\'is\\']<---jumping[conj]--->[u\\'over\\'] \\n -------- \\n []<---over[prep]--->[u\\'dog\\'] \\n -------- \\n []<---the[det]--->[] \\n -------- \\n []<---lazy[amod]--->[] \\n -------- \\n [u\\'the\\', u\\'lazy\\']<---dog[pobj]--->[] \\n -------- \\n  The preceding output gives us each token and its dependency type, the left arrow \\npoints to the dependencies on its left, and the right arrow points to the dependencies \\non its right. You will find a lot of similarity if you match each line of the output with the \\nprevious figure showing the dependency tree. You can quickly look back at Chapter   1 in \\ncase you have forgotten what each of the dependency tags indicates. \\n Next, we will be using  nltk and the Stanford Parser to generate the dependency tree \\nfor our sample sentence using the following code snippet: \\n # set java path \\n import os \\n java_path = r\\'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_102\\\\bin\\\\java.exe\\' \\n os.environ[\\'JAVAHOME\\'] = java_path \\n # perform dependency parsing \\n from nltk.parse.stanford import StanfordDependencyParser \\n sdp = StanfordDependencyParser(path_to_jar=\\'E:/stanford/stanford-parser-\\nfull-2015-04-20/stanford-parser.jar\\', \\n                               path_to_models_jar=\\'E:/stanford/stanford-\\nparser-full-2015-04-20/stanford-parser-3.5.2-models.jar\\')     \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n156\\n result = list(sdp.raw_parse(sentence))   \\n # generate annotated dependency parse tree \\n In [134]: result[0] \\n Out[134]: \\n # generate dependency triples \\n Out[136]:  \\n [((u\\'quick\\', u\\'JJ\\'), u\\'nsubj\\', (u\\'fox\\', u\\'NN\\')), \\n ((u\\'fox\\', u\\'NN\\'), u\\'det\\', (u\\'The\\', u\\'DT\\')), \\n ((u\\'fox\\', u\\'NN\\'), u\\'amod\\', (u\\'brown\\', u\\'JJ\\')), \\n ((u\\'quick\\', u\\'JJ\\'), u\\'cop\\', (u\\'is\\', u\\'VBZ\\')), \\n ((u\\'quick\\', u\\'JJ\\'), u\\'cc\\', (u\\'and\\', u\\'CC\\')), \\n ((u\\'quick\\', u\\'JJ\\'), u\\'conj\\', (u\\'jumping\\', u\\'VBG\\')), \\n ((u\\'jumping\\', u\\'VBG\\'), u\\'nsubj\\', (u\\'he\\', u\\'PRP\\')), \\n ((u\\'jumping\\', u\\'VBG\\'), u\\'aux\\', (u\\'is\\', u\\'VBZ\\')), \\n ((u\\'jumping\\', u\\'VBG\\'), u\\'nmod\\', (u\\'dog\\', u\\'NN\\')), \\n ((u\\'dog\\', u\\'NN\\'), u\\'case\\', (u\\'over\\', u\\'IN\\')), \\n ((u\\'dog\\', u\\'NN\\'), u\\'det\\', (u\\'the\\', u\\'DT\\')), \\n ((u\\'dog\\', u\\'NN\\'), u\\'amod\\', (u\\'lazy\\', u\\'JJ\\'))] \\n Figure 3-4.  Annotated dependency parse tree for our  sample sentence \\n \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n157\\n # print simple dependency parse tree \\n In [137]: dep_tree = [parse.tree() for parse in result][0] \\n     ...: print dep_tree \\n (quick (fox The brown) is and (jumping he is (dog over the lazy))) \\n # visualize simple dependency parse tree \\n In [140]: dep_tree.draw() \\n Out [140]: \\n The preceding  outputs shows how easily we can generate dependency parse trees \\nfor sentences and analyze and understand relationships and dependencies amongst \\nthe tokens. The Stanford Parser is quite stable and robust and integrates well with  nltk . \\nA side note would be that you will need  graphviz installed to generate the annotated \\ndependency tree shown in Figure\\xc2\\xa0 3-4 . \\n Building Your Own Dependency Parsers \\n It is not very easy to build your own dependency grammar\\xe2\\x80\\x93based parsers from scratch \\nbecause you need sufficient data, and just checking based on grammar production \\nrules would not always scale well. The following example snippet shows how to build \\nyour own dependency parser. To do this, we first leverage  nltk\\xe2\\x80\\x99s  DependencyGrammar \\nclass to generate production rules from a user input grammar. Once this is done, we use \\n ProjectiveDependencyParser , a projective, production  rule-based dependency parser to \\nperform the dependency based parsing: \\n import nltk \\n tokens = nltk.word_tokenize(sentence) \\n dependency_rules = \"\"\" \\n \\'fox\\' -> \\'The\\' | \\'brown\\' \\n \\'quick\\' -> \\'fox\\' | \\'is\\' | \\'and\\' | \\'jumping\\' \\n \\'jumping\\' -> \\'he\\' | \\'is\\' | \\'dog\\' \\n \\'dog\\' -> \\'over\\' | \\'the\\' | \\'lazy\\' \\n \"\"\" \\n dependency_grammar = nltk.grammar.DependencyGrammar.fromstring(dependency_\\nrules) \\n Figure 3-5.  Simple dependency parse tree for our sample sentence \\n \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n158\\n # print production rules \\n In [143]: print dependency_grammar \\n Dependency grammar with 12 productions \\n  \\'fox\\' -> \\'The\\' \\n  \\'fox\\' -> \\'brown\\' \\n  \\'quick\\' -> \\'fox\\' \\n  \\'quick\\' -> \\'is\\' \\n  \\'quick\\' -> \\'and\\' \\n  \\'quick\\' -> \\'jumping\\' \\n  \\'jumping\\' -> \\'he\\' \\n  \\'jumping\\' -> \\'is\\' \\n  \\'jumping\\' -> \\'dog\\' \\n  \\'dog\\' -> \\'over\\' \\n  \\'dog\\' -> \\'the\\' \\n  \\'dog\\' -> \\'lazy\\' \\n # build dependency parser \\n dp = nltk.ProjectiveDependencyParser(dependency_grammar) \\n # parse our sample sentence \\n res = [item for item in dp.parse(tokens)] \\n tree = res[0] \\n # print dependency parse tree \\n In [145]: print tree \\n (quick (fox The brown) is and (jumping he is (dog over the lazy))) \\n You can see that the preceding dependency parse tree is the same one as the one \\ngenerated by the Stanford Parser. In fact, you can use  tree.draw() to visualize the tree \\nand compare it with the previous tree.  Scaling these is always a challenge, and a lot of \\nwork is being done in large projects to generate these systems for rule-based dependency \\ngrammars. Some examples include the  Lexical Functional Grammar (LFG) Pargram \\nproject and the Lexicalized Tree Adjoining Grammar XTAG project. \\n Constituency-based Parsing \\n Constituent-based grammars are used to analyze and determine the constituents \\na sentence is usually composed of. Besides determining the constituents, another \\nimportant objective is to find out the internal structure of these constituents and see how \\nthey link to each other. There are usually several rules for different types of phrases based \\non the type of components they can contain, and we can use them to build parse trees. \\nRefer to the \\xe2\\x80\\x9cConstituency Grammars\\xe2\\x80\\x9d subsection under \\xe2\\x80\\x9cGrammar\\xe2\\x80\\x9d in the \\xe2\\x80\\x9cLanguage \\nSyntax and Structure\\xe2\\x80\\x9d section from Chapter   1 if you need to refresh your memory and \\nlook at some examples of sample parse trees. \\n In general, a constituency-based grammar helps specify how we can break a \\nsentence into various constituents. Once that is done, it further helps in breaking down \\nthose constituents into further subdivisions, and this process repeats till we reach the \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n159\\nlevel of individual tokens or words. These grammars have various production rules and \\nusually a context-free grammar (CFG) or phrase structured grammar is sufficient for this. \\n Once we have a set of grammar rules, a constituency parser can be built that will \\nprocess input sentences according to these rules and help in building a parse tree. The \\nparser is what brings the grammar to life and can be said to be a procedural interpretation \\nof the grammar. There are various types of parsing algorithms, including the following:\\n\\xe2\\x80\\xa2 \\n Recursive Descent parsing \\n\\xe2\\x80\\xa2 \\n Shift Reduce parsing \\n\\xe2\\x80\\xa2 \\n Chart parsing \\n\\xe2\\x80\\xa2 \\n Bottom-up parsing \\n\\xe2\\x80\\xa2 \\n Top-down parsing \\n\\xe2\\x80\\xa2 \\n PCFG parsing \\n Going through these in detail would be impossible given the constraints of this book. \\nHowever,  nltk provides some excellent information on them in its official book, available \\nat   http://www.nltk.org/book/ch08.html . I will describe some of these parsers briefly \\nand look at PCFG parsing in detail when we implement our own parser later.  Recursive \\nDescent parsing usually follows a top-down parsing approach and it reads in tokens \\nfrom the input sentence and tries to match them with the terminals from the grammar \\nproduction rules. It keeps looking ahead by one token and advances the input read \\npointer each time it gets a match. \\n Shift Reduce parsing follows a bottom-up parsing approach where it finds sequences \\nof tokens (words/phrases) that correspond to the righthand side of grammar productions \\nand then replaces it with the lefthand side for that rule. This process continues until the \\nwhole sentence is reduced to give us a parse tree. \\n Chart parsing uses dynamic  programming , which stores intermediate results and \\nreuses them when needed to get significant efficiency gains. In this case, chart parsers \\nstore partial solutions and look them up when needed to get to the complete solution. \\n Recommended Constituency Parsers \\n We will be using  nltk and the  StanfordParser here to generate parse trees. We will need \\nto set the Java path before we run our code to parse our sample sentence. We will print \\nand also visualize the parse tree, which will be quite similar to some of the parse trees \\nfrom Chapter   1 , based on constituency grammars. \\n The following code snippet illustrates: \\n # set java path \\n import os \\n java_path = r\\'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_102\\\\bin\\\\java.exe\\' \\n os.environ[\\'JAVAHOME\\'] = java_path \\n sentence = \\'The brown fox is quick and he is jumping over the lazy dog\\' \\n from nltk.parse.stanford import StanfordParser \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n160\\n # create parser object \\n scp = StanfordParser(path_to_jar=\\'E:/stanford/stanford-parser-\\nfull-2015-04-20/stanford-parser.jar\\', path_to_models_jar=\\'E:/stanford/\\nstanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar\\') \\n # get parse tree                    \\n result = list(scp.raw_parse(sentence)) \\n # print the constituency parse tree \\n In [150]: print result[0] \\n     ...:  \\n (ROOT \\n  (NP \\n    (S \\n      (S \\n        (NP (DT The) (JJ brown) (NN fox)) \\n        (VP (VBZ is) (ADJP (JJ quick)))) \\n      (CC and) \\n      (S \\n        (NP (PRP he)) \\n        (VP \\n          (VBZ is) \\n          (VP \\n            (VBG jumping) \\n            (PP (IN over) (NP (DT the) (JJ lazy) (NN dog))))))))) \\n # visualize constituency parse tree \\n In [151]: result[0].draw() \\n Out [151]: \\n The preceding output shows how to build constituency grammar\\xe2\\x80\\x93based parse trees \\nfor sentences. Notice the parse tree depicted in Figure\\xc2\\xa0 3-6 being significantly different \\nfrom dependency parse trees and matching the constituency parse trees illustrated in \\nChapter   1 . Note the nested and hierarchical constituents shown in the tree above which \\nare some of the typical characteristics of constituency parse trees.   \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n161\\n Building Your Own Constituency Parsers \\n There are various ways of building your own constituency parsers, including creating \\nyour own CFG production rules and then using a parser to use that grammar. To build \\nyour own CFG, you can use the  nltk.CFG.fromstring  function to feed in your own \\nproduction rules and then use parsers like  ChartParser or  RecursiveDescentParser , \\nboth of which belong to the  nltk package. Feel free to build some toy grammars and play \\naround with these parsers. \\n We will look at a way to build a constituency parser that scales well and is efficient. \\nThe problem with regular CFG parsers, like chart and Recursive Descent parsers, is that \\nthey can get easily overwhelmed by the sheer number of total possible parses when \\nparsing sentences and can become extremely slow. This is where weighted grammars \\nlike PCFG (Probabilistic Context Free Grammar) and probabilistic parsers like the Viterbi \\nparser prove to be more effective. A PCFG is a context-free grammar that associates a \\nprobability with each of its production rules. The probability of a parse tree generated \\nfrom a PCFG is simply the production of the individual probabilities of the productions \\nused to generate it. \\n Figure 3-6.  Constituency parse tree for our sample sentence \\n \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n162\\n We will use  nltk\\xe2\\x80\\x99s ViterbiParser here to train a parser on the  treebank corpus \\nthat provides annotated parse  trees for each sentence in the corpus. This parser is a \\nbottom-up PCFG parser that uses dynamic programming to find the most likely parse at \\neach step. We will start our process of building our own parser by loading the necessary \\ntraining data and dependencies: \\n import nltk \\n from nltk.grammar import Nonterminal \\n from nltk.corpus import treebank \\n # get training data \\n training_set = treebank.parsed_sents() \\n # view a sample training sentence \\n In [161]: print training_set[1] \\n (S \\n  (NP-SBJ (NNP Mr.) (NNP Vinken)) \\n  (VP \\n    (VBZ is) \\n    (NP-PRD \\n      (NP (NN chairman)) \\n      (PP \\n        (IN of) \\n        (NP \\n          (NP (NNP Elsevier) (NNP N.V.)) \\n          (, ,) \\n          (NP (DT the) (NNP Dutch) (VBG publishing) (NN group)))))) \\n  (. .)) \\n Now we will build the production rules for our grammar by extracting the \\nproductions from the tagged and annotated training sentences and adding them: \\n # extract the productions for all annotated training sentences \\n treebank_productions = list( \\n                        set(production  \\n                            for sent in training_set   \\n                            for production in sent.productions() \\n                        ) \\n                    ) \\n # view sample productions \\n In [166]: treebank_productions[0:10] \\n Out[166]:  \\n [VBZ -> \\'cites\\', \\n VBD -> \\'spurned\\', \\n PRN -> , ADVP-TMP ,, \\n NNP -> \\'ACCOUNT\\', \\n JJ -> \\'36-day\\', \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n163\\n NP-SBJ-2 -> NN, \\n JJ -> \\'unpublished\\', \\n NP-SBJ-1 -> NNP, \\n JJ -> \\'elusive\\', \\n NNS -> \\'Lids\\'] \\n # add productions for each word, POS tag \\n for word, tag in treebank.tagged_words(): \\n        t = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\") \\n        for production in t.productions(): \\n                treebank_productions.append(production) \\n # build the PCFG based grammar   \\n treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal(\\'S\\'),  \\n                                         treebank_productions) \\n Now that we have our necessary grammar with production rules, we will create \\nour parser using the following  snippet by training it on the grammar and then trying to \\nevaluate it on our sample sentence: \\n # build the parser \\n viterbi_parser = nltk.ViterbiParser(treebank_grammar) \\n # get sample sentence tokens \\n tokens = nltk.word_tokenize(sentence) \\n # get parse tree \\n In [170]: result = list(viterbi_parser.parse(tokens)) \\n Traceback (most recent call last): \\n  File \"<ipython-input-170-c2cdab3cd56c>\", line 1, in <module> \\n    result = list(viterbi_parser.parse(tokens)) \\n  File \"C:\\\\Anaconda2\\\\lib\\\\site-packages\\\\nltk\\\\parse\\\\viterbi.py\", line 112, in \\nparse \\n    self._grammar.check_coverage(tokens) \\n ValueError: Grammar does not cover some of the input words: u\"\\'brown\\', \\n\\'fox\\', \\'lazy\\', \\'dog\\'\". \\n  Unfortunately, we get an error when we try to parse our sample sentence tokens \\nwith our newly built parser. The reason is quite clear from the error: Some of the words \\nin our sample sentence are not covered by the  treebank -based grammar because they \\nare not present in our  treebank corpus. Now, because this constituency-based grammar \\nuses POS tags and phrase tags to build the tree based on the training data, we will add the \\ntoken and POS tags for our sample sentence in our grammar and rebuild the parser: \\n # get tokens and their POS tags \\n from pattern.en import tag as pos_tagger \\n tagged_sent = pos_tagger(sentence) \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n164\\n # check the tokens and their POS tags \\n In [172]: print tagged_sent \\n     ...:  \\n [(u\\'The\\', u\\'DT\\'), (u\\'brown\\', u\\'JJ\\'), (u\\'fox\\', u\\'NN\\'), (u\\'is\\', u\\'VBZ\\'), \\n(u\\'quick\\', u\\'JJ\\'), (u\\'and\\', u\\'CC\\'), (u\\'he\\', u\\'PRP\\'), (u\\'is\\', u\\'VBZ\\'), \\n(u\\'jumping\\', u\\'VBG\\'), (u\\'over\\', u\\'IN\\'), (u\\'the\\', u\\'DT\\'), (u\\'lazy\\', u\\'JJ\\'), \\n(u\\'dog\\', u\\'NN\\')] \\n # extend productions for sample sentence tokens \\n for word, tag in tagged_sent: \\n    t = nltk.Tree.fromstring(\"(\"+ tag + \" \" + word  +\")\") \\n    for production in t.productions(): \\n                treebank_productions.append(production) \\n # rebuild grammar \\n treebank_grammar = nltk.grammar.induce_pcfg(Nonterminal(\\'S\\'),  \\n                                         treebank_productions) \\n # rebuild parser \\n viterbi_parser = nltk.ViterbiParser(tbank_grammar) \\n # get parse tree for sample sentence \\n result = list(viterbi_parser.parse(tokens)) \\n # print the constituency parse tree \\n In [178]: print result[0] \\n (S \\n  (NP-SBJ-163 (DT The) (JJ brown) (NN fox)) \\n  (VP \\n    (VBZ is) \\n    (PRT (JJ quick)) \\n    (S \\n      (CC and) \\n      (NP-SBJ (PRP he)) \\n      (VP \\n        (VBZ is) \\n        (PP-1 \\n          (VBG jumping) \\n          (NP (IN over) (DT the) (JJ lazy) (NN dog))))))) (p=2.02604e-48) \\n # visualize the constituency parse tree \\n In [179]: result[0].draw() \\n Out [179]: \\nCHAPTER 3 \\xe2\\x96\\xa0 PROCESSING AND UNDERSTANDING TEXT\\n165\\n  We are now able to successfully generate the parse tree for our sample sentence. \\nYou can see the visual representation of the tree in Figure\\xc2\\xa0 3-7 . Remember that this is a \\nprobabilistic PCFG  parser , and you can see the overall probability of this tree mentioned \\nin the output earlier when we printed our parse tree. The notations of the tags followed \\nhere are all based on the Treebank annotations we discussed earlier. Thus this shows how \\nto build our own constituency-based parser. \\n Summary \\n Congratulations on reaching the end of this chapter. We have covered a major chunk \\nof concepts, techniques, and implementations with regard to text processing, syntactic \\nanalysis, and understanding. A lot of the concepts from Chapter   1 should seem more \\nrelevant and clearer now that we have actually implemented them on real examples. \\n The content covered in this chapter is two-fold. We looked at concepts related to \\ntext processing and normalization. You now know the importance of processing and \\nnormalizing text, and as we move on to future chapters, you will see why it becomes \\nmore and more important to have well-processed and standardized textual data. We \\nhave covered various concepts and implemented techniques for text tokenization and \\nnormalization. These include cleaning and correcting text entities like spelling and \\ncontractions. We also built our own spelling corrector and contraction expander in the \\nsame context. We found out a way to leverage WordNet and correct words with repeated \\ncharacters. Finally, we looked at various stemming and lemmatization concepts and \\ntechniques. The next part of our chapter was dedicated to analyzing and understanding \\ntext syntax and structure, where we revisited concepts from Chapter   1 including POS \\ntagging, shallow parsing, dependency parsing, and constituency parsing. \\n You now know how to use taggers and parsers on real-world textual data and ways \\nto implement your own taggers and parsers. We will be diving more into analyzing and \\nderiving insights from text in the future chapters using various ML techniques, including \\nclassification, clustering, and summarization. \\n Figure 3-7.  Constituency parse tree for our sample sentence based on Treebank \\nannotations \\n \\n167\\n\\xc2\\xa9 Dipanjan Sarkar 2016 \\nD. Sarkar, Text Analytics with Python, DOI 10.1007/978-1-4842-2388-8_4\\nCHAPTER 4\\nText Classification\\nLearning to process and understand text is one of the first steps on the journey to \\ngetting meaningful insights from textual data. Though it is important to understand \\nhow language is structured and specific text syntax patterns, that alone is not sufficient \\nto be of much use to businesses and organizations who want to derive useful patterns \\nand insights and get maximum use out of their vast volumes of text data. Knowledge of \\nlanguage processing coupled with concepts from analytics and machine learning (ML) \\nhelp in building systems that can leverage text data and help solve real-world practical \\nproblems which benefit businesses.\\nVarious aspects of ML include supervised learning, unsupervised learning, \\nreinforcement learning, and more recently deep learning. Each of these concepts involves \\nseveral techniques and algorithms that can be leveraged on text data and to build self-\\nlearning systems that do not need too much manual supervision. An ML model is a \\ncombination of data and algorithms\\xe2\\x80\\x94you got a taste of that in Chapter 3 was we built our \\nown parsers and taggers. The benefit of ML is that once a model is trained, we can directly \\nuse it on new and previously unseen data to start seeing useful insights and desired results.\\nOne of the most relevant and challenging problems is text classification or \\ncategorization, which involves trying to organize text documents into various categories \\nbased on inherent properties or attributes of each text document. This is used in \\nvarious domains, including email spam identification and news categorization. The \\nconcept may seem simple, and if you have a small number of documents, you can look \\nat each document and gain some idea about what it is trying to indicate. Based on \\nthis knowledge, you can group similar documents into categories or classes. It\\xe2\\x80\\x99s more \\nchallenging when the number of text documents to be classified increases to several \\nhundred thousands or millions. This is where techniques like feature extraction and \\nsupervised or unsupervised ML come in handy. Document classification is a generic \\nproblem not limited to text alone but also can be extended for other items like music, \\nimages, video, and other media.\\nTo formalize our problem more clearly, we will have a given set of classes or \\ncategories and several text documents. Remember that documents are basically sentences \\nor paragraphs of text. This forms a corpus. Our task would be to determine which class \\nor classes each document belongs to. This entire process involves several steps which \\nwe will be discussing in detail later in this chapter. Briefly, for a supervised classification \\nproblem, we need to have some labelled data that we could use for training a text \\nclassification model. This data would essentially be curated documents that are already \\nassigned to some specific class or category beforehand. Using this, we would essentially \\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n168\\nextract features and attributes from each document and make our model learn these \\nattributes corresponding to each particular document and its class/category by feeding \\nit to a supervised ML algorithm. Of course, the data would need to be pre-processed and \\nnormalized before building the model. Once done, we would follow the same process of \\nnormalization and feature extraction and then feed it to the model to predict the class or \\ncategory for new documents. However, for an unsupervised classification problem, we \\nwould essentially not have any pre-labelled training documents. We would use techniques \\nlike clustering and document similarity measures to cluster documents together based on \\ntheir inherent properties and assign labels to them.\\nIn this chapter, we will discuss the concept of text classification and how it can be \\nformulated as a supervised ML problem. We will also talk about the various forms of \\nclassification and what they indicate. A clear depiction for the essential steps necessary \\nto complete a text classification workflow will also be presented, and we will be covering \\nsome of the essential steps from the same workflow, which have not been discussed \\nbefore, including feature extraction, classifiers, model evaluation, and finally we will put \\nthem all together in building a text classification system on real-world data.\\nWhat Is Text Classification?\\nBefore we define text classification, we need to understand the scope of textual data and \\nwhat we really mean by classification. The textual data involved here can be anything \\nranging from a phrase, sentence, or a complete document with paragraphs of text, which \\ncan be obtained from corpora, blogs, or anywhere from the Web. Text classification is \\nalso often called document classification just to cover all forms of textual content under \\nthe word document. The word document could be defined as some form of concrete \\nrepresentation of thoughts or events that could be in the form of writing, recorded \\nspeech, drawings, or presentations. I use the term document here to represent textual \\ndata such as sentences or paragraphs belonging to the English language.\\nText classification is also often called text categorization, although I explicitly use \\nthe word classification here for two reasons. First, it depicts the same essence as text \\ncategorization, where we want to classify documents. The second reason is to also show \\nthat we would be using classification or a supervised ML approach here to classify or \\ncategorize the text. Text categorization can be done in many ways, as mentioned. We \\nwill be focusing explicitly on a supervised approach using classification. The process of \\nclassification is not restricted to text alone. It is used quite frequently in other domains \\nincluding science, healthcare, weather forecasting, and technology.\\nText or document classification is the process of assigning text documents into one \\nor more classes or categories, assuming that we have a predefined set of classes. \\nDocuments here are textual documents, and each document can contain a sentence or \\neven a paragraph of words. A text classification system would successfully be able to \\nclassify each document to its correct class(es) based on inherent properties of the \\ndocument. Mathematically, we can define it like this: given some description and \\nattributes d for a document D, where d\\nD\\n\\xc3\\x8e\\n, and we have a set of predefined classes or \\ncategories, C\\nc c\\nc\\ncn\\n=\\n\\xc2\\xbc\\n{\\n}\\n1\\n2\\n3\\n,\\n,\\n,\\n,\\n. The actual document D can have many inherent \\nproperties and attributes that lead it to being an entity in a high-dimensional space. Using \\na subset of that space with a limit set of descriptions and features depicted by d, we \\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n169\\nshould be able to successfully assign the original document D to its correct class Cx using \\na text classification system T. This can be represented by T D\\nCx\\n:\\n\\xc2\\xae\\n.\\nWe will talk more about the text classification system in detail later in the chapter. \\nFigure\\xc2\\xa04-1 shows a high-level conceptual representation of the text classification process.\\nIn Figure\\xc2\\xa04-1, we can see there are several documents representing products which \\ncan be assigned to various categories of food, mobile phones, and movies. Initially, \\nthese documents are all present together, just as a text corpus has various documents in \\nit. Once it goes through a text classification system, represented as a black box here, we \\ncan see that each document is assigned to one specific class or category we had defined \\npreviously. Here the documents are just represented by their names, but in real data, they \\ncan contain much more, including descriptions of each product, specific attributes such \\nas movie genre, product specifications, constituents, and many more properties that can \\nbe used as features in the text classification system to make document identification and \\nclassification easier.\\nThere are various types of text classification. This chapter focuses on two major \\ntypes, which are based on the type of content that makes up the documents:\\n\\xe2\\x80\\xa2 \\nContent-based classification\\n\\xe2\\x80\\xa2 \\nRequest-based classification\\nBoth types are more like different philosophies or ideals behind approaches to \\nclassifying text documents rather than specific technical algorithms or processes. Content-\\nbased classification is the type of text classification where priorities or weights are given \\nto specific subjects or topics in the text content that would help determine the class of the \\ndocument. A conceptual example would be that a book with more than 30 percent of its \\ncontent about food preparations can be classified under cooking/recipes. Request-based \\nclassification is influenced by user requests and is targeted towards specific user groups \\nand audiences. This type of classification is governed by specific policies and ideals.\\nFigure 4-1.\\xe2\\x80\\x82 Conceptual overview of text classification\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n170\\nAutomated Text Classification\\nWe now have an idea of the definition and scope of text classification. We have also \\nformally defined text classification both conceptually and mathematically, where we \\ntalked about a \\xe2\\x80\\x9ctext classification system\\xe2\\x80\\x9d being able to classify text documents to their \\nrespective categories or classes. Consider several humans doing the task of going through \\neach document and classifying it. They would then be a part of the text classification \\nsystem we are talking about. However, that would not scale very well once there were \\nmillions of text documents to be classified quickly. To make the process more efficient \\nand faster, we can consider automating the task of text classification, which brings us to \\nautomated text classification.\\nTo automate text classification, we can make use of several ML techniques and \\nconcepts. There are mainly two types of ML techniques that are relevant to solving this \\nproblem:\\n\\xe2\\x80\\xa2 \\nSupervised machine learning\\n\\xe2\\x80\\xa2 \\nUnsupervised machine learning\\nBesides these two techniques, there are also other families of learning algorithms, \\nsuch as reinforcement learning and semi-supervised learning. Let us look at both \\nsupervised and unsupervised learning algorithms in more detail, from both an ML \\nperspective how it can be leveraged in classifying text documents.\\nUnsupervised learning refers to specific ML techniques or algorithms that do not \\nrequire any pre-labelled training data samples to build a model. We usually have a \\ncollection of data points, which could be text or numeric, depending on the problem we \\nare trying to solve. We extract features from each of the data points using a process known \\nas feature extraction and then feed the feature set for each data point into our algorithm. \\nWe are trying to extract meaningful patterns from the data, such as trying to group \\ntogether similar data points using techniques like clustering or summarizing documents \\nbased on topic models. This is extremely useful in text document categorization and is \\nalso called document clustering, where we cluster documents into groups purely based \\non their features, similarity, and attributes, without training any model on previously \\nlabelled data. Later chapters further discuss unsupervised learning, covering topic \\nmodels, document summarization, similarity analysis, and clustering.\\nSupervised learning refers to specific ML techniques or algorithms that are trained \\non pre-labelled data samples known as training data. Features or attributes are extracted \\nfrom this data using feature extraction, and for each data point we will have its own \\nfeature set and corresponding class/label. The algorithm learns various patterns for each \\ntype of class from the training data. Once this process is complete, we have a trained \\nmodel. This model can then be used to predict the class for future test data samples once \\nwe feed their features to the model. Thus the machine has actually learned, based on \\nprevious training data samples, how to predict the class for new unseen data samples.\\nThere are two major types of supervised learning algorithms:\\n\\xe2\\x80\\xa2 \\nClassification: The process of supervised learning is referred to \\nas classification when the outcomes to be predicted are distinct \\ncategories, thus the outcome variable is a categorical variable in \\nthis case. Examples would be news categories or movie genres.\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n171\\n\\xe2\\x80\\xa2 \\nRegression: Supervised learning algorithms are known as \\nregression algorithms when the outcome we want to predict is a \\ncontinuous numeric variable. Examples would be house prices or \\npeople\\xe2\\x80\\x99s weights.\\nWe will be specifically focusing on classification for our problem (hence the name of \\nthe chapter\\xe2\\x80\\x94we are trying to classify or categorize text documents into distinct classes or \\ncategories. We will be following a supervised learning approach in our implementations \\nlater on.\\nNow we are ready to define the process of automated or ML-based text classification \\nmathematically. Say we have a training set of documents labelled with their corresponding \\nclass or category. This can be represented by TS, which is a set of paired documents and \\nlabels, TS\\nd c\\nd\\nc\\nd c\\nn\\nn\\n= (\\n) (\\n) \\xc2\\xbc (\\n)\\n{\\n}\\n1\\n1\\n2\\n2\\n,\\n,\\n,\\n,\\n,\\n,\\n where d1,\\xc2\\xa0d2,\\xc2\\xa0\\xe2\\x80\\xa6,\\xc2\\xa0dn is the list of text documents, \\nand their corresponding labels are c1,\\xc2\\xa0c2,\\xc2\\xa0\\xe2\\x80\\xa6,\\xc2\\xa0cn such that c\\nC\\nc\\nc\\ncn\\nx \\xc3\\x8e\\n=\\n\\xc2\\xbc\\n{\\n}\\n1\\n2\\n,\\n,\\n,\\n where cx \\ndenotes the class label for document x and C denotes the set of all possible distinct classes, \\nany of which can be the class or classes for each document. Assuming we have our training \\nset, we can define a supervised learning algorithm F such that when it is trained on our \\ntraining dataset TS, we build a classification model or classifier \\xce\\xb3 such that we can say \\nthat F TS\\n(\\n) =g . Thus the supervised learning algorithm F takes the input set of (document, \\nclass) pairs TS and gives us the trained classifier \\xce\\xb3, which is our model. This process is \\nknown as the training process.\\nThis model can then take a new, previously unseen document ND and predict its \\nclass cND such that c\\nC\\nND \\xc3\\x8e\\n. This process is known as the prediction process and can be \\nrepresented by g :TD\\ncND\\n\\xc2\\xae\\n. Thus we can see that there are two main processes in the \\nsupervised text classification process:\\n\\xe2\\x80\\xa2 \\nTraining\\n\\xe2\\x80\\xa2 \\nPrediction\\nAn important point to remember is that some manually labelled training data \\nis necessary for supervised text classification, so even though we are talking about \\nautomated text classification, to kick start the process we need some manual efforts. Of \\ncourse, the benefits of this are manifold because once we have a trained classifier, we can \\nkeep using it to predict and classify new documents with minimal efforts and manual \\nsupervision.\\nThere are various learning methods or algorithms that we will be discussing in a \\nfuture section. These learning algorithms are not specific to text data but are generic ML \\nalgorithms that can be applied toward various types of data after due pre-processing \\nand feature extraction. I will touch upon a couple of supervised ML algorithms and use \\nthem in solving a real-world text classification problem. These algorithms are usually \\ntrained on the training data set and often an optional validation set such that the model \\nthat is trained does not overfit to the training data, which basically means it would then \\nnot be able to generalize well and predict properly for new instances of text documents. \\nOften the model is tuned on several of its internal parameters based on the learning \\nalgorithm and by evaluating various performance metrics like accuracy on the validation \\nset or by using cross-validation where we split the training dataset itself into training and \\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n172\\nvalidation sets by random sampling. This comprises the training process, the outcome \\nof which yields a fully trained model that is ready to predict. In the prediction stage, \\nwe usually have new data points from the test dataset. We can use them to feed into \\nthe model after normalization and feature extraction and see how well the model is \\nperforming by evaluating its prediction performance.\\nThere are a few types of text classification based on the number of classes to predict \\nand the nature of predictions. These types of classification are based on the dataset, the \\nnumber of classes/categories pertaining to that dataset, and the number of classes that \\ncan be predicted on any data point:\\n\\xe2\\x80\\xa2 \\nBinary classification is when the total number of distinct classes \\nor categories is two in number and any prediction can contain \\neither one of those classes.\\n\\xe2\\x80\\xa2 \\nMulti-class classification, also known as multinomial \\nclassification, refers to a problem where the total number of \\nclasses is more than two, and each prediction gives one class \\nor category that can belong to any of those classes. This is an \\nextension of the binary classification problem where the total \\nnumber of classes is more than two.\\n\\xe2\\x80\\xa2 \\nMulti-label classification refers to problems where each prediction \\ncan yield more than one outcome/predicted class for any data \\npoint.\\nText Classification Blueprint\\nNow that we know the basic scope of automated text classification, this section will look \\nat a blueprint for a complete workflow of building an automated text classifier system. \\nThis will consist of a series of steps that must be followed in both the training and testing \\nphases mentioned in the earlier section. For building a text classification system, we \\nneed to make sure we have our source of data and retrieve that data so that we can start \\nfeeding it to our system. The following main steps outline a typical workflow for a text \\nclassification system, assuming we have our dataset\\xc2\\xa0already downloaded and ready to  \\nbe used:\\n\\t\\n1.\\t\\nPrepare train and test datasets\\n\\t\\n2.\\t\\nText normalization\\n\\t\\n3.\\t\\nFeature extraction\\n\\t\\n4.\\t\\nModel training\\n\\t\\n5.\\t\\nModel prediction and evaluation\\n\\t\\n6.\\t\\nModel deployment\\nThese steps are carried out in that order for building a text classifier. Figure\\xc2\\xa04-2 shows \\na detailed workflow for a text classification system with the main processes highlighted in \\ntraining and prediction.\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n173\\nNotice that there are two main boxes for Training and Prediction, which are the \\ntwo main processes involved in building a text classifier. In general, the dataset we have \\nis usually divided into two or three splits called the training, validation (optional), and \\ntesting datasets, respectively. You can see an overlap of the Text Normalization and Feature \\nExtraction modules in Figure\\xc2\\xa04-2 for both processes, indicating that no matter which \\ndocument we want to classify and predict its class, it must go through the same series \\nof transformations in both the training and prediction process. Each document is first \\npre-processed and normalized, and then specific features pertaining to the document are \\nextracted. These processes are always uniform in both the training and prediction processes \\nto make sure that our classification model performs consistently in its predictions.\\nIn the Training process, each document has its own corresponding class/category \\nthat was manually labeled or curated beforehand. These training text documents are \\nprocessed and normalized in the Text Normalization module, giving us clean and \\nstandardized training text documents. They are then passed to the Feature Extraction \\nmodule where different types of feature-extraction techniques are used to extract \\nmeaningful features from the processed text documents. We will cover some popular \\nfeature extraction techniques in a future section. These features are usually numeric \\narrays or vectors because standard ML algorithms work on numeric vectors. Once we \\nhave our features, we select a supervised ML algorithm and train our model.\\nTraining the model involves feeding the feature vectors for the documents and \\nthe corresponding labels such that the algorithm is able to learn various patterns \\ncorresponding to each class/category and can reuse this learned knowledge to predict \\nclasses for future new documents. Often an optional validation dataset is used to evaluate \\nthe performance of the classification algorithm to make sure it generalizes well with \\nthe data during training. A combination of these features and the ML algorithm yields a \\nClassification Model, which is the end stage of the Training process. Often this model is \\ntuned using various model parameters with a process called hyperparameter tuning to \\nbuild a better performing optimal model.\\nFigure 4-2.\\xe2\\x80\\x82 Blueprint for building an automated text classification system\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n174\\nThe Prediction process shown in the figure involves trying to either predict classes \\nfor new documents or evaluating how predictions are working on testing data. The test \\ndataset documents go through the same process of normalization and feature extraction, \\nand the test document features are passed to the trained Classification Model, which \\npredicts the possible class for each of the documents based on previously learned \\npatterns. If you have the true class labels for the documents that were manually labelled, \\nyou can evaluate the prediction performance of the model by comparing the true labels \\nand the predicted labels using various metrics like accuracy. This would give an idea of \\nhow well the model performs its predictions for new documents. \\nOnce we have a stable and working model, the last step is usually deploying the \\nmodel, which normally involves saving the model and its necessary dependencies \\nand deploying it as a service or as a running program that predicts categories for new \\ndocuments as a batch job, or based on serving user requests if accessed as a web service. \\nThere are various ways to deploy ML models, and this usually depends on how you want \\nto access it later on.\\nWe will now discuss some of the main modules from the preceding blueprint and \\nimplement these modules so that we can integrate them all together to build a real-world \\ntext classifier.\\nText Normalization\\nChapter 3 covered text processing and normalization in detail\\xe2\\x80\\x94refer it to see the various \\nmethods and techniques available. In this section, we will define a normalizer module to \\nnormalize text documents and will be using it later when we build our classifier. Although \\nvarious techniques are available, we will keep it fairly simple and straightforward here so \\nthat is it not too hard to follow our implementations step by step. We will implement and \\nuse the following normalization techniques in our module:\\n\\xe2\\x80\\xa2 \\nExpanding contractions\\n\\xe2\\x80\\xa2 \\nText standardization through lemmatization\\n\\xe2\\x80\\xa2 \\nRemoving special characters and symbols\\n\\xe2\\x80\\xa2 \\nRemoving stopwords\\nWe are not focusing too much on correcting spellings and other advanced \\ntechniques, but you can integrate the functions from the previous chapter \\nimplementation if you are interested. Our normalization module is implemented and \\navailable in normalization.py, available in the code files for this chapter. I will also be \\nexplaining each function here for your convenience. We will first start with loading the \\nnecessary dependencies. Remember that you will need our custom-defined contractions \\nmapping file from Chapter 3, named contractions.py, for expanding contractions.\\nThe following snippet shows the necessary imports and dependencies:\\nfrom contractions import CONTRACTION_MAP\\nimport re\\nimport nltk\\nimport string\\nfrom nltk.stem import WordNetLemmatizer\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n175\\nstopword_list = nltk.corpus.stopwords.words(\\'english\\')\\nwnl = WordNetLemmatizer()\\nWe load all the English stopwords, the contraction mappings in CONTRACTION_MAP, \\nand an instance of WordNetLemmatizer for carrying our lemmatization. We now define \\na function to tokenize text into tokens that will be used by our other normalization \\nfunctions. The following function tokenizes and removes any extraneous whitespace from \\nthe tokens:\\ndef tokenize_text(text):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tokens = nltk.word_tokenize(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tokens = [token.strip() for token in tokens]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return tokens\\nNow we define a function for expanding contractions. This function is similar to our \\nimplementation from Chapter 3\\xe2\\x80\\x94it takes in a body of text and returns the same with its \\ncontractions expanded if there is a match. The following snippet helps us achieve this:\\ndef expand_contractions(text, contraction_mapping):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07contractions_pattern = re.compile(\\'({})\\'.format(\\'|\\'.join(contraction_\\nmapping.keys())),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0flags=re.IGNORECASE|re.DOTALL)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0def expand_match(contraction):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0match = contraction.group(0)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0first_char = match[0]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0expanded_contraction = contraction_mapping.get(match)\\\\\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if contraction_mapping.get(match)\\\\\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else contraction_mapping.get(match.lower())\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0expanded_contraction = first_char+expanded_contraction[1:]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return expanded_contraction\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0expanded_text = contractions_pattern.sub(expand_match, text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0expanded_text = re.sub(\"\\'\", \"\", expanded_text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return expanded_text\\nNow that we have a function for expanding contractions, we implement a function \\nfor standardizing our text data by bringing word tokens to their base or root form using \\nlemmatization. The following functions will help us in achieving that:\\nfrom pattern.en import tag\\nfrom nltk.corpus import wordnet as wn\\n# Annotate text tokens with POS tags\\ndef pos_tag_text(text):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# convert Penn treebank tag to wordnet tag\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n176\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0def penn_to_wn_tags(pos_tag):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if pos_tag.startswith(\\'J\\'):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return wn.ADJ\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0elif pos_tag.startswith(\\'V\\'):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return wn.VERB\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0elif pos_tag.startswith(\\'N\\'):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return wn.NOUN\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0elif pos_tag.startswith(\\'R\\'):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return wn.ADV\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return None\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tagged_text = tag(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for word, pos_tag in\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tagged_text]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return tagged_lower_text\\n# lemmatize text based on POS tags\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\ndef lemmatize_text(text):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0pos_tagged_text = pos_tag_text(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else word\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for word, pos_tag in pos_tagged_text]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0lemmatized_text = \\' \\'.join(lemmatized_tokens)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return lemmatized_text\\nThe preceding snippet depicts two functions implemented for lemmatization. The \\nmain function is lemmatize_text, which takes in a body of text data and lemmatizes \\neach word of the text based on its POS tag if it is present and then returns the lemmatized \\ntext back to the user. For this, we need to annotate the text tokens with their POS tags. \\nWe use the tag function from pattern to annotate POS tags for each token and then \\nfurther convert the POS tags from the Penn treebank syntax to WordNet syntax, since \\nthe WordNetLemmatizer checks for POS tag annotations based on WordNet formats. We \\nconvert each word token to lowercase, annotate it with its correct, converted WordNet \\nPOS tag, and return these annotated tokens, which are finally fed into the lemmatize_\\ntext function.\\nThe following function helps us remove special symbols and characters:\\ndef remove_special_characters(text):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tokens = tokenize_text(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0pattern = re.compile(\\'[{}]\\'.format(re.escape(string.punctuation)))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07filtered_tokens = filter(None, [pattern.sub(\\'\\', token) for token in \\ntokens])\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0filtered_text = \\' \\'.join(filtered_tokens)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return filtered_text\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n177\\nWe remove special characters by tokenizing the text just so we can remove some of \\nthe tokens that are actually contractions, but we may have failed to remove them in our \\nfirst step, like \"s\" or \"re\". We will do this when we remove stopwords. However, you can \\nalso remove special characters without tokenizing the text. We remove all special symbols \\ndefined in string.punctuation from our text using regular expression matches. The \\nfollowing function helps us remove stopwords from our text data:\\ndef remove_stopwords(text):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tokens = tokenize_text(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07filtered_tokens = [token for token in tokens if token not in  \\nstopword_list]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0filtered_text = \\' \\'.join(filtered_tokens)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return filtered_text\\nNow that we have all our functions defined, we can build our text normalization \\npipeline by chaining all these functions one after another. The following function \\nimplements this, where it takes in a corpus of text documents and normalizes them and \\nreturns a normalized corpus of text documents:\\ndef normalize_corpus(corpus, tokenize=False):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0normalized_corpus = []\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for text in corpus:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = expand_contractions(text, CONTRACTION_MAP)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = lemmatize_text(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = remove_special_characters(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = remove_stopwords(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0normalized_corpus.append(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if tokenize:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = tokenize_text(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0normalized_corpus.append(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return normalized_corpus\\nThat brings us to the end of our discussion and implementation of necessary \\nfunctions for our text normalization module. We will now look at concepts and practical \\nimplementation for feature extraction.\\nFeature Extraction\\nThere are various feature-extraction techniques that can be applied on text data, but \\nbefore we jump into then, let us consider what we mean by features. Why do we need \\nthem, and how they are useful? In a dataset, there are typically many data points. Usually \\nthe rows of the dataset and the columns are various features or properties of the dataset, \\nwith specific values for each row or observation. In ML terminology, features are unique, \\nmeasurable attributes or properties for each observation or data point in a dataset. \\nFeatures are usually numeric in nature and can be absolute numeric values or categorical \\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n178\\nfeatures that can be encoded as binary features for each category in the list using a \\nprocess called one-hot encoding. The process of extracting and selecting features is both \\nart and science, and this process is called feature extraction or feature engineering.\\nUsually extracted features are fed into ML algorithms for learning patterns that can \\nbe applied on future new data points for getting insights. These algorithms usually expect \\nfeatures in the form of numeric vectors because each algorithm is at heart a mathematical \\noperation of optimization and minimizing loss and error when it tries to learn patterns \\nfrom data points and observations. So, with textual data there is the added challenge of \\nfiguring out how to transform textual data and extract numeric features from it.\\nNow we will look at some feature-extraction concepts and techniques specially \\naligned towards text data.\\nThe Vector Space Model is a concept and model that is very useful in case we are \\ndealing with textual data and is very popular in information retrieval and document \\nranking. The Vector Space Model, also known as the Term Vector Model, is defined as a \\nmathematical and algebraic model for transforming and representing text documents as \\nnumeric vectors of specific terms that form the vector dimensions. Mathematically this \\ncan be defines as follows. Say we have a document D in a document vector space VS. The \\nnumber of dimensions or columns for each document will be the total number of distinct \\nterms or words for all documents in the vector space. So, the vector space can be denoted\\nVS\\nW W\\nWn\\n=\\n\\xc2\\xbc\\n{\\n}\\n1\\n2\\n,\\n,\\n,\\nwhere there are n distinct words across all documents. Now we can represent document \\nD in this vector space as\\nD\\nw\\nw\\nw\\nD\\nD\\nDn\\n=\\n\\xc2\\xbc\\n{\\n}\\n1\\n2\\n,\\n,\\n,\\nwhere wDn denotes the weight for word n in document D. This weight is a numeric value \\nand can represent anything, ranging from the frequency of that word in the document, to \\nthe average frequency of occurrence, or even to the TF-IDF weight (discussed shortly).\\nWe will be talking about and implementing the following feature-extraction \\ntechniques:\\n\\xe2\\x80\\xa2 \\nBag of Words model\\n\\xe2\\x80\\xa2 \\nTF-IDF model\\n\\xe2\\x80\\xa2 \\nAdvanced word vectorization models\\nAn important thing to remember for feature extraction is that once we build a \\nfeature extractor using some transformations and mathematical operations, we need to \\nmake sure we reuse the same process when extracting features from new documents to \\nbe predicted, and not rebuild the whole algorithm again based on the new documents. \\nWe will be depicting this also with an example for each technique. Do note that for \\nimplementations based on practical examples in this section, we will be making use \\nof the nltk, gensim, and scikit-learn libraries, which you can install using pip as \\ndiscussed earlier (in case you do not have them installed already).\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n179\\nThe implementations are divided into two major modules. The file feature_\\nextractors.py contains the generic functions we will be using later on when building the \\nclassifier, and we have used the same functions in the feature_extraction_demo.py file to \\nshow how each technique works with some practical examples. You can access them from \\nthe code files, and as always I will be presenting the same code in this chapter for ease of \\nunderstanding. We will be using the following documents depicted in the CORPUS variable \\nto extract features from and building some of the vectorization models. To illustrate how \\nfeature extraction will work for a new document (as a part of test dataset), we will also use \\na separate document as shown in the variable new_doc in the following snippet:\\nCORPUS = [\\n\\'the sky is blue\\',\\n\\'sky is blue and sky is beautiful\\',\\n\\'the beautiful sky is so blue\\',\\n\\'i love blue cheese\\'\\n]\\nnew_doc = [\\'loving this blue sky today\\']\\nBag of Words Model\\nThe Bag of Words model is perhaps one of the simplest yet most powerful techniques \\nto extract features from text documents. The essence of this model is to convert text \\ndocuments into vectors such that each document is converted into a vector that \\nrepresents the frequency of all the distinct words that are present in the document \\nvector space for that specific document. Thus, considering our sample vector from the \\nprevious mathematical notation for D, the weight for each word is equal to its frequency \\nof occurrence in that document.\\nAn interesting thing is that we can even create the same model for individual word \\noccurrences as well as occurrences for n-grams, which would make it an n-gram Bag of \\nWords model such that frequency of distinct n-grams in each document would also be \\nconsidered.\\nThe following code snippet gives us a function that implements a Bag of Words\\xe2\\x80\\x93\\nbased feature-extraction model that also accepts an ngram_range parameter to take into \\naccount n-grams as features:\\nfrom sklearn.feature_extraction.text import CountVectorizer\\ndef bow_extractor(corpus, ngram_range=(1,1)):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0features = vectorizer.fit_transform(corpus)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return vectorizer, features\\nThe preceding function uses the CountVectorizer class. You can access its detailed \\nAPI (Application Programming Interface) documentation at http://scikit-learn.org/\\nstable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n180\\nhtml#sklearn.feature_extraction.text.CountVectorizer, which has a whole bunch \\nof various parameters for more fine-tuning based on the type of features you want to \\nextract. We use its default configuration, which is enough for most scenarios, with min_df \\nset to 1 indicating taking terms having a minimum frequency of 1 in the overall document \\nvector space. You can set ngram_range to various parameters like (1, 3) would build \\nfeature vectors consisting of all unigrams, bigrams, and trigrams. The following snippet \\nshows the function in action on our sample corpora of four training documents and one \\ntest document:\\n# build bow vectorizer and get features\\nIn [371]: bow_vectorizer, bow_features = bow_extractor(CORPUS)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: features = bow_features.todense()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print features\\n[[0 0 1 0 1 0 1 0 1]\\n\\xc2\\xa0[1 1 1 0 2 0 2 0 0]\\n\\xc2\\xa0[0 1 1 0 1 0 1 1 1]\\n\\xc2\\xa0[0 0 1 1 0 1 0 0 0]]\\n# extract features from new document using built vectorizer\\nIn [373]: new_doc_features = bow_vectorizer.transform(new_doc)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: new_doc_features = new_doc_features.todense()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print new_doc_features\\n[[0 0 1 0 0 0 1 0 0]]\\n# print the feature names\\nIn [374]: feature_names = bow_vectorizer.get_feature_names()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print feature_names\\n[u\\'and\\', u\\'beautiful\\', u\\'blue\\', u\\'cheese\\', u\\'is\\', u\\'love\\', u\\'sky\\', u\\'so\\', \\nu\\'the\\']\\nThat output shows how each text document has been converted to vectors. Each row \\nrepresents one document from our corpus, and we do the same for both our corpora. The \\nvectorizer is built using documents from CORPUS. We extract features from it and also use \\nthis built vectorizer to extract features from a completely new document. Each column in \\na vector represents the words depicted in feature_names, and the value is the frequency \\nof that word in the document represented by the vector. It may be hard to comprehend \\nthis at first glance, so I have prepared the following function, which I hope you can use to \\nunderstand the feature vectors better:\\nimport pandas as pd\\ndef display_features(features, feature_names):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0df = pd.DataFrame(data=features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0columns=feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print df\\nNow you can feed the feature names and vectors to this function and see the feature \\nmatrix in a much easier-to-understand structure, shown here: \\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n181\\nIn [379]: display_features(features, feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0and\\xc2\\xa0\\xc2\\xa0beautiful\\xc2\\xa0\\xc2\\xa0blue\\xc2\\xa0\\xc2\\xa0cheese\\xc2\\xa0\\xc2\\xa0is\\xc2\\xa0\\xc2\\xa0love\\xc2\\xa0\\xc2\\xa0sky\\xc2\\xa0\\xc2\\xa0so\\xc2\\xa0\\xc2\\xa0the\\n0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\n1\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\n2\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\n3\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\nIn [380]: display_features(new_doc_features, feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0and\\xc2\\xa0\\xc2\\xa0beautiful\\xc2\\xa0\\xc2\\xa0blue\\xc2\\xa0\\xc2\\xa0cheese\\xc2\\xa0\\xc2\\xa0is\\xc2\\xa0\\xc2\\xa0love\\xc2\\xa0\\xc2\\xa0sky\\xc2\\xa0\\xc2\\xa0so\\xc2\\xa0\\xc2\\xa0the\\n0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00\\nThat makes things much clearer, right? Consider the second document of CORPUS, \\nrepresented in the preceding in row 1 of the first table. You can see that \\'sky is blue \\nand sky is beautiful\\' has value 2 for the feature sky, 1 for beautiful, and so on. \\nValues of 0 are assigned for words not present in the document. Note that for the new \\ndocument new_doc, there is no feature for the words today, this, or loving in the \\nsentence. The reason for this is what I mentioned before\\xe2\\x80\\x94that the feature-extraction \\nprocess, model, and vocabulary are always based on the training data and will never \\nchange or get influenced on newer documents, which it will predict later as a part of \\ntesting or otherwise. You might have guessed that this is because a model is always \\ntrained on some training data and is never influenced by newer documents unless we \\nplan on rebuilding that model. Hence, the features in this model are always limited based \\non the document vector space of the training corpus.\\nYou have now started to get an idea of how to extract meaningful vector-based \\nfeatures from text data, which previously seemed impossible. Try out the preceding \\nfunctions by setting ngram_range to (1, 3) and see the outputs.\\nTF-IDF Model\\nThe Bag of Words model is good, but the vectors are completely based on absolute \\nfrequencies of word occurrences. This has some potential problems where words that \\nmay tend to occur a lot across all documents in the corpus will have higher frequencies \\nand will tend to overshadow other words that may not occur as frequently but may \\nbe more interesting and effective as features to identify specific categories for the \\ndocuments. This is where TF-IDF comes into the picture. TF-IDF stands for Term \\nFrequency-Inverse Document Frequency, a combination of two metrics: term frequency \\nand inverse document frequency. This technique was originally developed as a metric for \\nranking functions for showing search engine results based on user queries and has come \\nto be a part of information retrieval and text feature extraction now.\\nLet us formally define TF-IDF now and look at the mathematical representations for \\nit before diving into its implementation. Mathematically, TF-IDF is the product of two \\nmetrics and can be represented as tfidf\\ntf\\nidf\\n=\\n\\xc2\\xb4\\n, where term frequency (tf) and \\ninverse-document frequency (idf) represent the two metrics.\\nTerm frequency denoted by tf is what we had computed in the Bag of Words model. \\nTerm frequency in any document vector is denoted by the raw frequency value of that \\nterm in a particular document. Mathematically it can be represented as tf w D\\nfwD\\n,\\n(\\n) =\\n, \\nwhere fwD  denotes frequency for word w in document D, which becomes the term \\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n182\\nfrequency (tf). There are various other representations and computations for term \\nfrequency, such as converting frequency to a binary feature where 1 means the term has \\noccurred in the document and 0 means it has not. Sometimes you can also normalize the \\nabsolute raw frequency using logarithms or averaging the frequency. We will be using the \\nraw frequency in our computations.\\nInverse document frequency denoted by idf is the inverse of the document frequency \\nfor each term. It is computed by dividing the total number of documents in our corpus \\nby the document frequency for each term and then applying logarithmic scaling on the \\nresult. In our implementation we will be adding 1 to the document frequency for each \\nterm just to indicate that we also have one more document in our corpus that essentially \\nhas every term in the vocabulary. This is to prevent potential division-by-zero errors \\nand smoothen the inverse document frequencies. We also add 1 to the result of our idf \\ncomputation to avoid ignoring terms completely that might have zero idf. Mathematically \\nour implementation for idf can be represented by\\nidf t\\nC\\ndf t\\n( ) = +\\n+\\n( )\\n1\\n1\\nlog\\nwhere idf(t) represents the idf for the term t, C represents the count of the total number of \\ndocuments in our corpus, and df(t) represents the frequency of the number of documents \\nin which the term t is present.\\nThus the term frequency-inverse document frequency can be computed by \\nmultiplying the above two measures together. The final TF-IDF metric we will be using is \\na normalized version of the tfidf matrix we get from the product of tf and idf. We will \\nnormalize the tfidf matrix by dividing it with the L2 norm of the matrix, also known as the \\nEuclidean norm, which is the square root of the sum of the square of each term\\xe2\\x80\\x99s tfidf \\nweight. Mathematically we can represent the final tfidf feature vector as tfidf\\ntfidf\\ntfidf\\n=\\n, \\nwhere tfidf  represents the Euclidean L2 norm for the tfidf matrix.\\nThe following code snippet shows an implementation of getting the tfidf-based \\nfeature vectors, considering we have our Bag of Words feature vectors we obtained in the \\nprevious section:\\nfrom sklearn.feature_extraction.text import TfidfTransformer\\ndef tfidf_transformer(bow_matrix):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0transformer = TfidfTransformer(norm=\\'l2\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0smooth_idf=True,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0use_idf=True)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tfidf_matrix = transformer.fit_transform(bow_matrix)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return transformer, tfidf_matrix\\nYou can see that we have used the L2 norm option in the parameters and also made \\nsure we smoothen the idfs to give weightages also to terms that may have zero idf so that \\nwe do not ignore them. We can see this function in action in the following code snippet:\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n183\\nimport numpy as np\\nfrom feature_extractors import tfidf_transformer\\nfeature_names = bow_vectorizer.get_feature_names()\\n# build tfidf transformer and show train corpus tfidf features\\nIn [388]: tfidf_trans, tdidf_features = tfidf_transformer(bow_features)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: features = np.round(tdidf_features.todense(), 2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: display_features(features, feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0and\\xc2\\xa0\\xc2\\xa0beautiful\\xc2\\xa0\\xc2\\xa0blue\\xc2\\xa0\\xc2\\xa0cheese\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0is\\xc2\\xa0\\xc2\\xa0love\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sky\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0so\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0the\\n0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.40\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.49\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.49\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.60\\n1\\xc2\\xa0\\xc2\\xa00.44\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.35\\xc2\\xa0\\xc2\\xa00.23\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.56\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.56\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.00\\n2\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.43\\xc2\\xa0\\xc2\\xa00.29\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.35\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.35\\xc2\\xa0\\xc2\\xa00.55\\xc2\\xa0\\xc2\\xa00.43\\n3\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.35\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.66\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.66\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.00\\n# show tfidf features for new_doc using built tfidf transformer\\nIn [389]: nd_tfidf = tfidf_trans.transform(new_doc_features)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: nd_features = np.round(nd_tfidf.todense(), 2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: display_features(nd_features, feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0and\\xc2\\xa0\\xc2\\xa0beautiful\\xc2\\xa0\\xc2\\xa0blue\\xc2\\xa0\\xc2\\xa0cheese\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0is\\xc2\\xa0\\xc2\\xa0love\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sky\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0so\\xc2\\xa0\\xc2\\xa0the\\n0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.63\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.77\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.0\\nThus the preceding outputs show the tfidf feature vectors for all our sample \\ndocuments. We use the TfidfTransformer class, which helps us in computing the tfidfs \\nfor each document based on the equations described earlier.\\nNow we will show how the internals of this class work. You will also see how to \\nimplement the mathematical equations described earlier to compute the tfidf-based \\nfeature vectors. This section is dedicated to ML experts (and curious readers who are \\ninterested in how things work behind the scenes). We will start with loading necessary \\ndependencies and computing the term frequencies (TF) by reusing our Bag of Words-\\nbased features for our sample corpus, which can also act as the term frequencies for our \\ntraining CORPUS:\\nimport scipy.sparse as sp\\nfrom numpy.linalg import norm\\nfeature_names = bow_vectorizer.get_feature_names()\\n# compute term frequency\\ntf = bow_features.todense()\\ntf = np.array(tf, dtype=\\'float64\\')\\n# show term frequencies\\nIn [391]: display_features(tf, feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0and\\xc2\\xa0\\xc2\\xa0beautiful\\xc2\\xa0\\xc2\\xa0blue\\xc2\\xa0\\xc2\\xa0cheese\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0is\\xc2\\xa0\\xc2\\xa0love\\xc2\\xa0\\xc2\\xa0sky\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0so\\xc2\\xa0\\xc2\\xa0the\\n0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa01.0\\n1\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa02.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa02.0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.0\\n2\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa01.0\\n3\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.0\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n184\\nWe will now compute our document frequencies (DF) for each term based on the \\nnumber of documents in which it occurs. The following snippet shows how to obtain it \\nfrom our Bag of Words feature matrix:\\n# build the document frequency matrix\\ndf = np.diff(sp.csc_matrix(bow_features, copy=True).indptr)\\ndf = 1 + df # to smoothen idf later\\n# show document frequencies\\nIn [403]: display_features([df], feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0and\\xc2\\xa0\\xc2\\xa0beautiful\\xc2\\xa0\\xc2\\xa0blue\\xc2\\xa0\\xc2\\xa0cheese\\xc2\\xa0\\xc2\\xa0is\\xc2\\xa0\\xc2\\xa0love\\xc2\\xa0\\xc2\\xa0sky\\xc2\\xa0\\xc2\\xa0so\\xc2\\xa0\\xc2\\xa0the\\n0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa03\\nThis tells us the document frequency (DF) for each term and you can verify it with \\nthe documents in CORPUS. Remember that we have added 1 to each frequency value to \\nsmoothen the idf values later and prevent division-by-zero errors by assuming we have a \\ndocument (imaginary) that has all the terms once. Thus, if you check in the CORPUS, you \\nwill see that blue occurs 4(+1) times, sky occurs 3(+1) times, and so on, considering (+1) \\nfor our smoothening.\\nNow that we have the document frequencies, we will compute the inverse document \\nfrequency (idf) using our formula defined earlier. Remember to add 1 to the total count of \\ndocuments in the corpus to add the document that we had assumed earlier to contain all \\nthe terms at least once for smoothening the idfs:\\n# compute inverse document frequencies\\ntotal_docs = 1 + len(CORPUS)\\nidf = 1.0 + np.log(float(total_docs) / df)\\n# show inverse document frequencies\\nIn [406]: display_features([np.round(idf, 2)], feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0and\\xc2\\xa0\\xc2\\xa0beautiful\\xc2\\xa0\\xc2\\xa0blue\\xc2\\xa0\\xc2\\xa0cheese\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0is\\xc2\\xa0\\xc2\\xa0love\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sky\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0so\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0the\\n0\\xc2\\xa0\\xc2\\xa01.92\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.51\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.92\\xc2\\xa0\\xc2\\xa01.22\\xc2\\xa0\\xc2\\xa01.92\\xc2\\xa0\\xc2\\xa01.22\\xc2\\xa0\\xc2\\xa01.92\\xc2\\xa0\\xc2\\xa01.51\\n# compute idf diagonal matrix\\ntotal_features = bow_features.shape[1]\\nidf_diag = sp.spdiags(idf, diags=0, m=total_features, n=total_features)\\nidf = idf_diag.todense()\\n# print the idf diagonal matrix\\nIn [407]: print np.round(idf, 2)\\n[[ 1.92\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0]\\n\\xc2\\xa0[ 0.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.51\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0]\\n\\xc2\\xa0[ 0.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0]\\n\\xc2\\xa0[ 0.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.92\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0]\\n\\xc2\\xa0[ 0.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.22\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0]\\n\\xc2\\xa0[ 0.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.92\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0]\\n\\xc2\\xa0[ 0.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.22\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0]\\n\\xc2\\xa0[ 0.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.92\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0]\\n\\xc2\\xa0[ 0.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.51]]\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n185\\nYou can now see the idf matrix that we created based on our mathematical equation, \\nand we also convert it to a diagonal matrix, which will be helpful later on when we want \\nto compute the product with term frequency.\\nNow that we have our tfs and idfs, we can compute the tfidf feature matrix using \\nmatrix multiplication, as shown in the following snippet:\\n# compute tfidf feature matrix\\ntfidf = tf * idf\\n# show tfidf feature matrix\\nIn [410]: display_features(np.round(tfidf, 2), feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0and\\xc2\\xa0\\xc2\\xa0beautiful\\xc2\\xa0\\xc2\\xa0blue\\xc2\\xa0\\xc2\\xa0cheese\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0is\\xc2\\xa0\\xc2\\xa0love\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sky\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0so\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0the\\n0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa01.22\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa01.22\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa01.51\\n1\\xc2\\xa0\\xc2\\xa01.92\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.51\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa02.45\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa02.45\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.00\\n2\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.51\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa01.22\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa01.22\\xc2\\xa0\\xc2\\xa01.92\\xc2\\xa0\\xc2\\xa01.51\\n3\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.92\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa01.92\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.00\\nWe now have our tfidf feature matrix, but wait! It is not yet over. We have to divide it \\nwith the L2 norm, if you remember from our equations depicted earlier. The following \\nsnippet computes the tfidf norms for each document and then divides the tfidf weights \\nwith the norm to give us the final desired tfidf matrix:\\n# compute L2 norms\\nnorms = norm(tfidf, axis=1)\\n# print norms for each document\\nIn [412]: print np.round(norms, 2)\\n[ 2.5\\xc2\\xa0\\xc2\\xa0\\xc2\\xa04.35\\xc2\\xa0\\xc2\\xa03.5\\xc2\\xa0\\xc2\\xa0\\xc2\\xa02.89]\\n# compute normalized tfidf\\nnorm_tfidf = tfidf / norms[:, None]\\n# show final tfidf feature matrix\\nIn [415]: display_features(np.round(norm_tfidf, 2), feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0and\\xc2\\xa0\\xc2\\xa0beautiful\\xc2\\xa0\\xc2\\xa0blue\\xc2\\xa0\\xc2\\xa0cheese\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0is\\xc2\\xa0\\xc2\\xa0love\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sky\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0so\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0the\\n0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.40\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.49\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.49\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.60\\n1\\xc2\\xa0\\xc2\\xa00.44\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.35\\xc2\\xa0\\xc2\\xa00.23\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.56\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.56\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.00\\n2\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.43\\xc2\\xa0\\xc2\\xa00.29\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.35\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.35\\xc2\\xa0\\xc2\\xa00.55\\xc2\\xa0\\xc2\\xa00.43\\n3\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.35\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.66\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.66\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.00\\nCompare the preceding obtained tfidf feature matrix for the documents in CORPUS \\nto the feature matrix obtained using TfidfTransformer earlier. Note they are exactly the \\nsame, thus verifying that our mathematical implementation was correct\\xe2\\x80\\x94and in fact this \\nvery same implementation is adopted by scikit-learn\\xe2\\x80\\x99s TfidfTransformer behind the \\nscenes using some more optimizations. Now, suppose we want to compute the tfidf-\\nbased feature matrix for our new document new_doc. We can do it using the following \\nsnippet. We reuse the new_doc_features Bag of Words vector from before for the term \\nfrequencies:\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n186\\n# compute new doc term freqs from bow freqs\\nnd_tf = new_doc_features\\nnd_tf = np.array(nd_tf, dtype=\\'float64\\')\\n# compute tfidf using idf matrix from train corpus\\nnd_tfidf = nd_tf*idf\\nnd_norms = norm(nd_tfidf, axis=1)\\nnorm_nd_tfidf = nd_tfidf / nd_norms[:, None]\\n# show new_doc tfidf feature vector\\nIn [418]: display_features(np.round(norm_nd_tfidf, 2), feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0and\\xc2\\xa0\\xc2\\xa0beautiful\\xc2\\xa0\\xc2\\xa0blue\\xc2\\xa0\\xc2\\xa0cheese\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0is\\xc2\\xa0\\xc2\\xa0love\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sky\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0so\\xc2\\xa0\\xc2\\xa0the\\n0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.63\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.77\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.0\\nThe preceding output depicts the tfidf-based feature vector for new_doc, and you can \\nsee it is the same as the one obtained by TfidfTransformer.\\nNow that we know how the internals work, we are going to implement a generic \\nfunction that can directly compute the tfidf-based feature vectors for documents from the \\nraw documents themselves. The following snippet depicts the same:\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\ndef tfidf_extractor(corpus, ngram_range=(1,1)):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vectorizer = TfidfVectorizer(min_df=1,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0norm=\\'l2\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0smooth_idf=True,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0use_idf=True,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ngram_range=ngram_range)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0features = vectorizer.fit_transform(corpus)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return vectorizer, features\\nThe preceding function makes use of the TfidfVectorizer, which directly computes \\nthe tfidf vectors by taking the raw documents themselves as input and internally \\ncomputing the term frequencies as well as the inverse document frequencies, eliminating \\nthe need to use the CountVectorizer for computing the term frequencies based on the \\nBag of Words model. Support is also present for adding n-grams to the feature vectors. We \\ncan see the function in action in the following snippet:\\n# build tfidf vectorizer and get training corpus feature vectors\\nIn [425]: tfidf_vectorizer, tdidf_features = tfidf_extractor(CORPUS)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: \\x07display_features(np.round(tdidf_features.todense(), 2), feature_\\nnames)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0and\\xc2\\xa0\\xc2\\xa0beautiful\\xc2\\xa0\\xc2\\xa0blue\\xc2\\xa0\\xc2\\xa0cheese\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0is\\xc2\\xa0\\xc2\\xa0love\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sky\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0so\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0the\\n0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.40\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.49\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.49\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.60\\n1\\xc2\\xa0\\xc2\\xa00.44\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.35\\xc2\\xa0\\xc2\\xa00.23\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.56\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.56\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.00\\n2\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.43\\xc2\\xa0\\xc2\\xa00.29\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.35\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.35\\xc2\\xa0\\xc2\\xa00.55\\xc2\\xa0\\xc2\\xa00.43\\n3\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.35\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.66\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.66\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.00\\xc2\\xa0\\xc2\\xa00.00\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n187\\n# get tfidf feature vector for the new document\\nIn [426]: nd_tfidf = tfidf_vectorizer.transform(new_doc)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: display_features(np.round(nd_tfidf.todense(), 2), feature_names)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0and\\xc2\\xa0\\xc2\\xa0beautiful\\xc2\\xa0\\xc2\\xa0blue\\xc2\\xa0\\xc2\\xa0cheese\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0is\\xc2\\xa0\\xc2\\xa0love\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sky\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0so\\xc2\\xa0\\xc2\\xa0the\\n0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.63\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.77\\xc2\\xa0\\xc2\\xa00.0\\xc2\\xa0\\xc2\\xa00.0\\nYou can see from the preceding outputs that the tfidf feature vectors match to the \\nones we obtained previously. This brings us to the end of our discussion on feature \\nextraction using tfidf. Now we will look at some advanced word vectorization techniques.\\nAdvanced Word Vectorization Models\\nThere are various approaches to creating more advanced word vectorization models for \\nextracting features from text data. Here we will discuss a couple of them that use Google\\xe2\\x80\\x99s \\npopular word2vec algorithm. The word2vec model, released in 2013 by Google, is a neural \\nnetwork\\xe2\\x80\\x93based implementation that learns distributed vector representations of words \\nbased on continuous Bag of Words and skip-gram\\xe2\\x80\\x93based architectures. The word2vec \\nframework is much faster than other neural network\\xe2\\x80\\x93based implementations and does \\nnot require manual labels to create meaningful representations among words. You can \\nfind more details on Google\\xe2\\x80\\x99s word2vec project at https://code.google.com/archive/p/\\nword2vec/. You can even try out some of the implementations yourself if you are interested.\\nWe will be using the gensim library in our implementation, which is Python \\nimplementation for word2vec that provides several high-level interfaces for easily building \\nthese models. The basic idea is to provide a corpus of documents as input and get feature \\nvectors for them as output. Internally, it constructs a vocabulary based on the input text \\ndocuments and learns vector representations for words based on various techniques \\nmentioned earlier, and once this is complete, it builds a model that can be used to \\nextract word vectors for each word in a document. Using various techniques like average \\nweighting or tfidf weighting, we can compute the averaged vector representation of a \\ndocument using its word vectors. You can get more details about the interface for gensim\\xe2\\x80\\x98s \\nword2vec implementation at http://radimrehurek.com/gensim/models/word2vec.html.\\nWe will be mainly focusing on the following parameters when we build our model \\nfrom our sample training corpus:\\n\\xe2\\x80\\xa2 \\nsize: This parameter is used to set the size or dimension for the \\nword vectors and can range from tens to thousands. You can try \\nout various dimensions to see which gives the best result.\\n\\xe2\\x80\\xa2 \\nwindow: This parameter is used to set the context or window size. \\nwhich specifies the length of the window of words that should be \\nconsidered for the algorithm to take into account as context when \\ntraining.\\n\\xe2\\x80\\xa2 \\nmin_count: This parameter specifies the minimum word count \\nneeded across the corpus for the word to be considered in the \\nvocabulary. This helps in removing very specific words that may \\nnot have much significance because they occur very rarely in the \\ndocuments.\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n188\\n\\xe2\\x80\\xa2 \\nsample: This parameter is used to downsample effects of \\noccurrence of frequent words. Values between 0.01 and 0.0001 are \\nusually ideal.\\nOnce we build a model, we will define and implement two techniques of combining \\nword vectors together in text documents based on certain weighing schemes. We will \\nimplement two techniques mentioned as follows.\\n\\xe2\\x80\\xa2 \\nAveraged word vectors\\n\\xe2\\x80\\xa2 \\nTF-IDF weighted word vectors\\nLet us start the feature-extraction process by building our word2vec model on our \\nsample training corpus before going into further implementations. The following code \\nsnippet shows how:\\nimport gensim\\nimport nltk\\n# tokenize corpora\\nTOKENIZED_CORPUS = [nltk.word_tokenize(sentence)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for sentence in CORPUS]\\ntokenized_new_doc = [nltk.word_tokenize(sentence)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for sentence in new_doc]\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n# build the word2vec model on our training corpus\\nmodel = gensim.models.Word2Vec(TOKENIZED_CORPUS, size=10, window=10,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0min_count=2, sample=1e-3)\\nAs you can see, we have built the model using the parameters described earlier; you \\ncan play around with these and also look at other parameters from the documentation to \\nchange the architecture type, number of workers, and so on. Now that we have our model \\nready, we can start implementing our feature extraction techniques.\\nAveraged Word Vectors\\nThe preceding model creates a vector representation for each word in the vocabulary. We \\ncan access them by just typing in the following code:\\nIn [430]: print model[\\'sky\\']\\n[ 0.01608407 -0.04819566\\xc2\\xa0\\xc2\\xa00.04227461 -0.03011346\\xc2\\xa0\\xc2\\xa00.0254148\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.01728328\\n\\xc2\\xa0\\xc2\\xa00.0155535\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.00774884 -0.02752112\\xc2\\xa0\\xc2\\xa00.01646519]\\nIn [431]: print model[\\'blue\\']\\n[-0.0472235\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.01662185 -0.01221706 -0.04724348 -0.04384995\\xc2\\xa0\\xc2\\xa00.00193343\\n\\xc2\\xa0-0.03163504 -0.03423524\\xc2\\xa0\\xc2\\xa00.02661656\\xc2\\xa0\\xc2\\xa00.03033725]\\nEach word vector is of length 10 based on the size parameter specified earlier. But \\nwhen we deal with sentences and text documents, they are of unequal length, and we \\nmust carry out some form of combining and aggregation operations to make sure the \\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n189\\nnumber of dimensions of the final feature vectors are the same, regardless of the length of \\nthe text document, number of words, and so on. In this technique, we will use an average \\nweighted word vectorization scheme, where for each text document we will extract all \\nthe tokens of the text document, and for each token in the document we will capture the \\nsubsequent word vector if present in the vocabulary. We will sum up all the word vectors \\nand divide the result by the total number of words matched in the vocabulary to get a \\nfinal resulting averaged word vector representation for the text document. This can be \\nmathematically represented using the equation\\nAWV D\\nwv w\\nn\\nn\\n(\\n) =\\n( )\\n\\xc3\\xa5\\n1\\nwhere AVW(D) is the averaged word vector representation for document D, containing \\nwords w1,\\xc2\\xa0w2,\\xe2\\x80\\x89\\xe2\\x80\\xa6,\\xc2\\xa0wn, and wv(w) is the word vector representation for the word w.\\nThe following snippet shows the pseudocode for the algorithm just described:\\nmodel := the word2vec model we built\\nvocabulary := unique_words(model)\\ndocument := [words]\\nmatched_word_count := 0\\nvector := []\\nfor word in words:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if word in vocabulary:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vector := vector + model[word]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0matched_word_count :=\\xc2\\xa0\\xc2\\xa0matched_word_count + 1\\naveraged_word_vector := vector / matched_word_count\\nThat snippet shows the flow of operations in a better way that is easier to understand.\\nWe will now implement our algorithm in Python using the following code snippet:\\nimport numpy as np\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n# define function to average word vectors for a text document\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\ndef average_word_vectors(words, model, vocabulary, num_features):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_vector = np.zeros((num_features,),dtype=\"float64\")\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0nwords = 0.\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for word in words:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if word in vocabulary:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0nwords = nwords + 1.\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_vector = np.add(feature_vector, model[word])\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n190\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if nwords:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_vector = np.divide(feature_vector, nwords)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return feature_vector\\n# generalize above function for a corpus of documents\\xc2\\xa0\\xc2\\xa0\\ndef averaged_word_vectorizer(corpus, model, num_features):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vocabulary = set(model.index2word)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07features = [average_word_vectors(tokenized_sentence, model, vocabulary, \\nnum_features)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for tokenized_sentence in corpus]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return np.array(features)\\nThe average_word_vectors() function must seem familiar to you\\xe2\\x80\\x94it is the concrete \\nimplementation of our algorithm shown using our pseudocode earlier. We also create a \\ngeneric function averaged_word_vectorizer() to perform averaging of word vectors for \\na corpus of documents. The following snippet shows our function in action on our sample \\ncorpora:\\n# get averaged word vectors for our training CORPUS\\nIn [445]: avg_word_vec_features = averaged_word_vectorizer(corpus=TOKENIZED_\\nCORPUS,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0model=model,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_features=10)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print np.round(avg_word_vec_features, 3)\\n[[ 0.006 -0.01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.015 -0.014\\xc2\\xa0\\xc2\\xa00.004 -0.006 -0.024 -0.007 -0.001\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0]\\n\\xc2\\xa0[-0.008 -0.01\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.021 -0.019 -0.002 -0.002 -0.011\\xc2\\xa0\\xc2\\xa00.002\\xc2\\xa0\\xc2\\xa00.003 -0.001]\\n\\xc2\\xa0[-0.003 -0.007\\xc2\\xa0\\xc2\\xa00.008 -0.02\\xc2\\xa0\\xc2\\xa0-0.001 -0.004 -0.014 -0.015\\xc2\\xa0\\xc2\\xa00.002 -0.01 ]\\n\\xc2\\xa0[-0.047\\xc2\\xa0\\xc2\\xa00.017 -0.012 -0.047 -0.044\\xc2\\xa0\\xc2\\xa00.002 -0.032 -0.034\\xc2\\xa0\\xc2\\xa00.027\\xc2\\xa0\\xc2\\xa00.03 ]]\\n# get averaged word vectors for our test new_doc\\nIn [447]: nd_avg_word_vec_features = averaged_word_\\nvectorizer(corpus=tokenized_new_doc,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0model=model,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07num_\\nfeatures=10)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print np.round(nd_avg_word_vec_features, 3)\\n[[-0.016 -0.016\\xc2\\xa0\\xc2\\xa00.015 -0.039 -0.009\\xc2\\xa0\\xc2\\xa00.01\\xc2\\xa0\\xc2\\xa0-0.008 -0.013\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.023]]\\nFrom the preceding outputs, you can see that we have uniformly sized averaged \\nword vectors for each document in the corpus, and these feature vectors can be used later \\nfor classification by feeding it to the ML algorithms.\\nTF-IDF Weighted Averaged Word Vectors\\nOur previous vectorizer simply sums up all the word vectors pertaining to any document \\nbased on the words in the model vocabulary and calculates a simple average by dividing \\nwith the count of matched words. This section introduces a new and novel technique \\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n191\\nof weighing each matched word vector with the word TF-TDF score and summing up \\nall the word vectors for a document and dividing it by the sum of all the TF-IDF weights \\nof the matched words in the document. This would basically give us a TF-IDF weighted \\naveraged word vector for each document.\\nThis can be mathematically represented using the equation\\nTWA D\\nwv w\\ntfidf w\\nn\\nn\\n(\\n) =\\n( )\\xc2\\xb4\\n( )\\n\\xc3\\xa5\\n1\\nwhere TWA(D) is the TF-IDF weighted averaged word vector representation for document \\nD, containing wordsw1,\\xc2\\xa0w2,\\xe2\\x80\\x89\\xe2\\x80\\xa6,\\xc2\\xa0wn, where wv(w) is the word vector representation and \\ntfidf(w) is the TF-IDF weight for the wordw. The following snippet shows the pseudocode \\nfor this algorithm:\\nmodel := the word2vec model we built\\nvocabulary := unique_words(model)\\ndocument := [words]\\ntfidfs := [tfidf(word) for each word in words]\\nmatched_word_wts := 0\\nvector := []\\nfor word in words:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if word in vocabulary:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0word_vector := model[word]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0weighted_word_vector := tfidfs[word] x word_vector\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vector := vector + weighted_word_vector\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0matched_word_wts :=\\xc2\\xa0\\xc2\\xa0matched_word_wts + tfidfs[word]\\ntfidf_wtd_avgd_word_vector := vector / matched_word_wts\\nThat pseudocode gives structure to our algorithm and shows how to implement the \\nalgorithm from the mathematical formula we defined earlier.\\nThe following code snippet implements this algorithm in Python so we can use it for \\nfeature extraction:\\n# define function to compute tfidf weighted averaged word vector for a document\\ndef tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, \\nnum_features):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if tfidf_vocabulary.get(word)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else 0 for word in words]\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_\\ntfidfs)}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_vector = np.zeros((num_features,),dtype=\"float64\")\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n192\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vocabulary = set(model.index2word)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0wts = 0.\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for word in words:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if word in vocabulary:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0word_vector = model[word]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0weighted_word_vector = word_tfidf_map[word] * word_vector\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0wts = wts + word_tfidf_map[word]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_vector = np.add(feature_vector, weighted_word_vector)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if wts:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_vector = np.divide(feature_vector, wts)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return feature_vector\\n# generalize above function for a corpus of documents\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\ndef tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tfidf_vocabulary, model, num_features):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0docs_tfidfs = [(doc, doc_tfidf)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for doc, doc_tfidf\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in zip(corpus, tfidf_vectors)]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_\\nvocabulary,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0model, num_features)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for tokenized_sentence, tfidf in docs_tfidfs]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return np.array(features)\\nThe tfidf_wtd_avg_word_vectors() function helps us in getting the TF-IDF \\nweighted averaged word vector representation for a document. We also create a \\ncorresponding generic function tfidf_weighted_averaged_word_vectorizer() to \\nperform TF-IDF weighted averaging of word vectors for a corpus of documents. We \\ncan see our implemented function in action on our sample corpora using the following \\nsnippet:\\n# get tfidf weights and vocabulary from earlier results and compute result\\nIn [453]: corpus_tfidf = tdidf_features\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: vocab = tfidf_vectorizer.vocabulary_\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_\\nvectorizer(corpus=TOKENIZED_CORPUS, tfidf_vectors=corpus_tfidf,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07tfidf_vocabulary=vocab, model=model, \\nnum_features=10)\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n193\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print np.round(wt_tfidf_word_vec_features, 3)\\n[[ 0.011 -0.011\\xc2\\xa0\\xc2\\xa00.014 -0.011\\xc2\\xa0\\xc2\\xa00.007 -0.007 -0.024 -0.008 -0.004 -0.004]\\n\\xc2\\xa0[ 0.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0-0.014\\xc2\\xa0\\xc2\\xa00.028 -0.014\\xc2\\xa0\\xc2\\xa00.004 -0.003 -0.012\\xc2\\xa0\\xc2\\xa00.011 -0.001 -0.002]\\n\\xc2\\xa0[-0.001 -0.008\\xc2\\xa0\\xc2\\xa00.007 -0.019\\xc2\\xa0\\xc2\\xa00.001 -0.004 -0.012 -0.018\\xc2\\xa0\\xc2\\xa00.001 -0.014]\\n\\xc2\\xa0[-0.047\\xc2\\xa0\\xc2\\xa00.017 -0.012 -0.047 -0.044\\xc2\\xa0\\xc2\\xa00.002 -0.032 -0.034\\xc2\\xa0\\xc2\\xa00.027\\xc2\\xa0\\xc2\\xa00.03 ]]\\n# compute avgd word vector for test new_doc\\nIn [454]: nd_wt_tfidf_word_vec_features = tfidf_weighted_averaged_word_\\nvectorizer(corpus=tokenized_new_doc, tfidf_vectors=nd_tfidf, tfidf_\\nvocabulary=vocab, model=model, num_features=10)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print np.round(nd_wt_tfidf_word_vec_features, 3)\\xc2\\xa0\\xc2\\xa0\\n[[-0.012 -0.019\\xc2\\xa0\\xc2\\xa00.018 -0.038 -0.006\\xc2\\xa0\\xc2\\xa00.01\\xc2\\xa0\\xc2\\xa0-0.006 -0.011 -0.003\\xc2\\xa0\\xc2\\xa00.023]]\\nFrom the preceding results, you can see how we can converted each document \\ninto TF-IDF weighted averaged numeric vectors. We also used our TF-IDF weights \\nand vocabulary, obtained earlier when we implemented TF-IDF\\xe2\\x80\\x93based feature vector \\nextraction from documents.\\nNow you have a good grasp on how to extract features from text data that can be used \\nfor training a classifier.\\nClassification Algorithms\\nClassification algorithms are supervised ML algorithms that are used to classify, \\ncategorize, or label data points based on what it has observed in the past. Each \\nclassification algorithm, being a supervised learning algorithm, requires training data. \\nThis training data consists of a set of training observations where each observation is a \\npair consisting of an input data point, usually a feature vector like we observed earlier, \\nand a corresponding output outcome for that input observation. There are mainly three \\nprocesses classification algorithms go through:\\n\\xe2\\x80\\xa2 \\nTraining is the process where the supervised learning algorithm \\nanalyzes and tries to infer patterns out of training data such that \\nit can identify which patterns lead to a specific outcome. These \\noutcomes are often known as the class labels/class variables/\\nresponse variables. We usually carry out the process of feature \\nextraction or feature engineering to derive meaningful features \\nfrom the raw data before training. These feature sets are fed to \\nan algorithm of our choice, which then tries to identify and learn \\npatterns from them and their corresponding outcomes. The \\nresult is an inferred function known as a model or a classification \\nmodel. This model is expected to be generalized enough from \\nlearning patterns in the training set such that it can predict the \\nclasses or outcomes for new data points in the future.\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n194\\n\\xe2\\x80\\xa2 \\nEvaluation involves trying to test the prediction performance \\nof our model to see how well it has trained and learned on the \\ntraining dataset. For this we usually use a validation dataset and \\ntest the performance of our model by predicting on that dataset \\nand testing our predictions against the actual class labels, also \\ncalled as the ground truth. Often we also use cross-validation, \\nwhere the data is divided into folds and a chunk of it is used \\nfor training, with the remainder used to validate the trained \\nmodel. Note that we also tune the model based on the validation \\nresults to get to an optimal configuration that yields maximum \\naccuracy and minimum error. We also evaluate our model against \\na holdout or test dataset, but we never tune our model against \\nthat dataset because that would lead to it being biased or overfit \\nagainst very specific features from the dataset. The holdout or test \\ndataset is something of a representative sample of what new, real \\ndata samples might look like for which the model will generate \\npredictions and how it might perform on these new data samples. \\nLater we will look at various metrics that are typically used to \\nevaluate and measure model performance.\\n\\xe2\\x80\\xa2 \\nTuning, also known as hyperparameter tuning or optimization, \\nis where we focus on trying to optimize a model to maximize its \\nprediction power and reduce errors. Each model is at heart a \\nmathematical function with several parameters that determine \\nmodel complexity, learning capability, and so on. These are \\nknown as hyperparameters because they cannot be learned \\ndirectly from data and must be set prior to running and training \\nthe model. Hence, the process of choosing an optimal set of \\nmodel hyperparameters such that the performance of the model \\nyields good prediction accuracy is known as model tuning, and we \\ncan carry it out in various ways, including randomized search and \\ngrid search. We will not be covering this in our implementations \\nsince this is more inclined towards core machine learning and is \\nout of our current scope as the models we will be building work \\nwell with default hyperparameter configurations. But there are \\nplenty of resources on the Web if you are interested in model \\ntuning and optimization.\\nThere are various types of classification algorithms, but we will not be venturing \\ninto each one in detail. Our focus remains text classification, and I do not want to bore \\neveryone with excessive mathematical derivations for each algorithm. However, I will \\ntouch upon a couple of algorithms that are quite effective for text classification and \\ntry to explain them, keeping the mathematical formulae to the base essentials. These \\nalgorithms are the following:\\n\\xe2\\x80\\xa2 \\nMultinomial Na\\xc3\\xafve Bayes\\n\\xe2\\x80\\xa2 \\nSupport vector machines\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n195\\nThere are also several other algorithms besides these you can look up, including \\nlogistic regression, decision trees, and neural networks. And ensemble techniques use \\na collection or ensemble of models to learn and predict outcomes that include random \\nforests and gradient boosting, but they often don\\xe2\\x80\\x99t perform very well for text classification \\nbecause they are very prone to overfitting. I recommend you be careful if you plan on \\nexperimenting with them. Besides these, deep learning\\xe2\\x80\\x93based techniques have also \\nrecently become popular. They use multiple hidden layers and combine several neural \\nnetwork models to build a complex classification model.\\nWe will now briefly look at some of the concepts surrounding multinomial na\\xc3\\xafve \\nBayes and support vector machines before using them for our classification problem.\\nMultinomial Na\\xc3\\xafve Bayes\\nThis algorithm is a special case of the popular na\\xc3\\xafve Bayes algorithm, which is used \\nspecifically for prediction and classification tasks where we have more than two classes. \\nBefore looking at multinomial na\\xc3\\xafve Bayes, let us look at the definition and formulation of \\nthe na\\xc3\\xafve Bayes algorithm. The na\\xc3\\xafve Bayes algorithm is a supervised learning algorithm \\nthat puts into action the very popular Bayes\\xe2\\x80\\x99 theorem. However, there is a \\xe2\\x80\\x9cna\\xc3\\xafve\\xe2\\x80\\x9d \\nassumption here that each feature is independent of the others. Mathematically we can \\nformulate this as follows: Given a response class variable y and a set of n features in the \\nform of a feature vector {x1,\\xc2\\xa0x2,\\xe2\\x80\\x89\\xe2\\x80\\xa6,\\xc2\\xa0xn}, using Bayes\\xe2\\x80\\x99 theorem we can denote the probability \\nof the occurrence of y given the features as\\nP y x x\\nx\\nP y\\nP x\\nx\\nx\\ny\\nP x\\nx\\nx\\nn\\nn\\nn\\n|\\n,\\n,\\n,\\n,\\n,\\n,\\n|\\n,\\n,\\n,\\n1\\n2\\n1\\n2\\n1\\n2\\n\\xc2\\xbc\\n(\\n) =\\n( )\\xc2\\xb4\\n\\xc2\\xbc\\n(\\n)\\n\\xc2\\xbc\\n(\\n)\\nunder the assumption that P x\\ny x x\\nx\\nx\\nx\\nP x\\ny\\ni\\ni\\ni\\nn\\ni\\n| ,\\n,\\n,\\n,\\n,\\n,\\n,\\n|\\n1\\n2\\n1\\n1\\n\\xc2\\xbc\\n\\xc2\\xbc\\n(\\n) = (\\n)\\n-\\n+\\n, and for all i we \\ncan represent this as\\nP y x x\\nx\\nP y\\nP x\\ny\\nP x x\\nx\\nn\\ni\\nn\\ni\\nn\\n|\\n,\\n,\\n,\\n|\\n,\\n,\\n,\\n1\\n2\\n1\\n1\\n2\\n\\xc2\\xbc\\n(\\n) =\\n( )\\xc2\\xb4\\n(\\n)\\n\\xc2\\xbc\\n(\\n)\\n=\\xc3\\x95\\n \\nwhere\\xc2\\xa0\\xc2\\xa0i ranges from 1 to n. In simple terms, this can be written as\\nposterior\\nprior\\nlikelihood\\nevidence\\n=\\n\\xc2\\xb4\\n and now, since P(x1,\\xc2\\xa0x2,\\xe2\\x80\\x89\\xe2\\x80\\xa6,\\xc2\\xa0xn) is constant, the model can be \\nexpressed like this:\\nP y x x\\nx\\nP y\\nP x\\ny\\nn\\ni\\nn\\ni\\n|\\n,\\n,\\n,\\n|\\n1\\n2\\n1\\n\\xc2\\xbc\\n(\\n)\\xc2\\xb5 ( )\\xc2\\xb4\\n(\\n)\\n=\\xc3\\x95\\nThis means that under the previous assumptions of independence among the \\nfeatures where each feature is conditionally independent of every other feature, the \\nconditional distribution over the class variable which is to be predicted, y can be \\nrepresented using the following mathematical equation as \\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n196\\nP y x x\\nx\\nZ P y\\nP x\\ny\\nn\\ni\\nn\\ni\\n|\\n,\\n,\\n,\\n|\\n1\\n2\\n1\\n1\\n\\xc2\\xbc\\n(\\n) =\\n( )\\xc2\\xb4\\n(\\n)\\n=\\xc3\\x95\\nwhere the evidence measure, Z\\np x\\n= ( )  is a constant scaling factor dependent on the \\nfeature variables. From this equation, we can build the na\\xc3\\xafve Bayes classifier by \\ncombining it with a rule known as the MAP decision rule, which stands for maximum a \\nposteriori. Going into the statistical details would be impossible in the current scope, but \\nby using it, the classifier can be represented as a mathematical function that can assign a \\npredicted class label \\xcb\\x86y\\nCk\\n=\\n for some k using the following representation:\\n\\xcb\\x86\\n|\\n, ,\\n,\\ny\\nargmax P C\\nP x C\\nk\\nK\\nk\\ni\\nn\\ni\\nk\\n=\\n(\\n)\\xc2\\xb4\\n(\\n)\\n\\xc3\\x8e\\n\\xc2\\xbc\\n{\\n}\\n=\\xc3\\x95\\n1 2\\n1\\nThis classifier is often said to be simple, quite evident from its name and also \\nbecause of several assumptions we make about our data and features that might not \\nbe so in the real world. Nevertheless, this algorithm still works remarkably well in \\nmany use cases related to classification, including multi-class document classification, \\nspam filtering, and so on. They can train really fast compared to other classifiers and \\nalso work well even when we do not have sufficient training data. Models often do not \\nperform well when they have a lot of features, and this phenomenon is known as the \\ncurse of dimensionality. Na\\xc3\\xafve Bayes takes care of this problem by decoupling the class \\nvariable\\xe2\\x80\\x93related conditional feature distributions, thus leading to each distribution being \\nindependently estimated as a single dimension distribution.\\nMultinomial na\\xc3\\xafve Bayes is an extension of the preceding algorithm for predicting \\nand classifying data points, where the number of distinct classes or outcomes is more \\nthan two. In this case the feature vectors are usually assumed to be word counts from the \\nBag of Words model, but TF-IDF\\xe2\\x80\\x93based weights will also work. One limitation is that \\nnegative weight-based features can\\xe2\\x80\\x98t be fed into this algorithm. This distribution can be \\nrepresented as p\\np\\np\\np\\ny\\ny\\ny\\nyn\\n=\\n\\xc2\\xbc\\n{\\n}\\n1\\n2\\n,\\n,\\n,\\n for each class label y, and the total number of \\nfeatures is n, which could be represented as the total vocabulary of distinct words or \\nterms in text analytics. From the preceding equation, p\\nP x\\ny\\nyi\\ni\\n= (\\n)\\n|\\n represents the \\nprobability of feature i in any observation sample that has an outcome or classy. The \\nparameter py can be estimated with a smoothened version of maximum likelihood \\nestimation (with relative frequency of occurrences), and represented as\\n\\xcb\\x86p\\nF\\nF\\nn\\nyi\\nyi\\ny\\n=\\n+\\n+\\na\\na\\nwhere F\\nx\\nyi\\nx TD\\ni\\n=\\n\\xc3\\x8e\\xc3\\xa5\\n is the frequency of occurrence for the feature i in a sample for class \\nlabel y in our training dataset TD, and F\\nF\\ny\\ni\\nTD\\nyi\\n=\\n=\\xc3\\xa5\\n1\\n is the total frequency of all features for \\nthe class label y. There is some amount of smoothening one with the help of priors a \\xc2\\xb3 0 , \\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n197\\nwhich accounts for the features that are not present in the learning data points and helps \\nin getting rid of zero-probability\\xe2\\x80\\x93related issues. Some specific settings for this parameter \\nare used quite often. The value of a =1 is known as Laplace smoothing, and a <1  is \\nknown as Lidstone smoothing. The scikit-learn library provides an excellent \\nimplementation for multinomial na\\xc3\\xafve Bayes in the class MultinomialNB, which we will \\nbe leveraging when we build our text classifier later on.\\nSupport Vector Machines\\nIn machine learning, support vector machines (SVM) are supervised learning algorithms \\nused for classification, regression, novelty, and anomaly or outlier detection. Considering \\na binary classification problem, if we have training data such that each data point or \\nobservation belongs to a specific class, the SVM algorithm can be trained based on this \\ndata such that it can assign future data points into one of the two classes. This algorithm \\nrepresents the training data samples as points in space such that points belonging to \\neither class can be separated by a wide gap between them, called a hyperplane, and \\nthe new data points to be predicted are assigned classes based on which side of this \\nhyperplane they fall into. This process is for a typical linear classification process. \\nHowever, SVM can also perform non-linear classification by an interesting approach \\nknown as a kernel trick, where kernel functions are used to operate on high-dimensional \\nfeature spaces that are non-linear separable. Usually, inner products between data points \\nin the feature space help achieve this. \\nThe SVM algorithm takes in a set of training data points and constructs a hyperplane \\nof a collection of hyperplanes for a high dimensional feature space. The larger the \\nmargins of the hyperplane, the better the separation, so this leads to lower generalization \\nerrors of the classifier. Let us represent this formally and mathematically. Consider a \\ntraining dataset of n data points \\xef\\x81\\xb2\\n\\xef\\x81\\xb2\\nx\\ny\\nx\\ny\\nn\\nn\\n1\\n1\\n,\\n,\\n,\\n,\\n(\\n) \\xc2\\xbc (\\n)  such that the class variable \\nyi \\xc3\\x8e -\\n{\\n}\\n1 1,\\n where each value indicates the class corresponding to the point \\xef\\x81\\xb2xi . Each data \\npoint \\xef\\x81\\xb2xi  is a feature vector. The objective of the SVM algorithm is to find the max-margin \\nhyperplane that separates the set of data points having class label of yi =1  from the set of \\ndata points having class label yi = -1  such that the distance between the hyperplane and \\nsample data points from either class nearest to it is maximized. These sample data points \\nare known as the support vectors. Figure\\xc2\\xa04-3, courtesy of Wikipedia, shows what the \\nvector space with the hyperplane looks like.\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n198\\nYou can clearly see the hyperplane and the support vectors in the figure. The \\nhyperplane can be defined as the set of points \\xef\\x81\\xb2x  which satisfy w\\nx\\nb\\n\\x1f\\x1e\\x1f \\x1f\\x1e\\x1f\\n\\xc3\\x97\\n+\\n= 0  where \\xef\\x81\\xb2w  is \\nthe normal vector to the hyperplane, as shown in Figure\\xc2\\xa04-3, and b\\nw\\x1f\\n\\x1e \\x1f\\n\\x1e\\x1e  gives us the offset \\nof the hyperplane from the origin toward the support vectors highlighted in the figure. \\nThere are two main types of margins that help in separating out the data points belonging \\nto the different classes.\\nWhen the data is linearly separable, as in Figure\\xc2\\xa04-3, we can have hard margins that \\nare basically represented by the two parallel hyperplanes depicted by the dotted lines, \\nwhich help in separating the data points belonging to the two different classes. This is \\ndone taking into account that the distance between them is as large as possible. The \\nregion bounded by these two hyperplanes forms the margin with the max-margin \\nhyperplane being in the middle. These hyperplanes are shown in the figure having the \\nequations w\\nx\\nb\\n\\x1f\\x1e\\x1f \\x1f\\x1e\\x1f\\n\\xc3\\x97\\n+\\n=1  andw\\nx\\nb\\n\\x1f\\x1e\\x1f \\x1f\\x1e\\x1f\\n\\xc3\\x97\\n+\\n= -1 .\\nOften the data points are not linearly separable, for which we can use the hinge loss \\nfunction, which can be represented as max( ,0 1-\\n\\xc3\\x97\\n+\\n(\\n)\\ny w\\nx\\nb\\ni\\ni\\n\\x1f\\x1e\\x1f \\x1f \\x1e\\x1f\\n and in fact the scikit-\\nlearn implementation of SVM can be found in SVC, LinearSVC, or SGDClassifier where \\nwe will use the \\'hinge\\' loss function (set by default) defined previously to optimize and \\nbuild the model. This loss function helps us in getting the soft margins and is often known \\nas a soft-margin SVM.\\nFigure 4-3.\\xe2\\x80\\x82 Two-class SVM depicting hyperplane and support vectors (courtesy: \\nWikipedia)\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n199\\nFor a multi-class classification problem, if we have n classes, for each class a binary \\nclassifier is trained and learned that helps in separating between each class and the other \\nn-1 classes. During prediction, the scores (distances to hyperplanes) for each classifier \\nare computed, and the maximum score is chosen for selecting the class label. Also often \\nstochastic gradient descent is used for minimizing the loss function in SVM algorithms. \\nFigure\\xc2\\xa04-4 shows how three classifiers are trained in total for a three-class SVM problem \\nover the very popular iris dataset. This figure is built using a scikit-learn model and is \\nobtained from the official documentation available at http://scikit-learn.org.\\nIn Figure\\xc2\\xa04-4 you can clearly see that a total of three SVM classifiers have been \\ntrained for each of the three classes and are then combined for the final predictions \\nso that data points belonging to each class can be labeled correctly. There are a lot \\nof resources and books dedicated entirely towards supervised ML and classification. \\nInterested readers should check them out to gain more in-depth knowledge on how these \\ntechniques work and how they can be applied to various problems in analytics.\\nEvaluating Classification Models\\nTraining, tuning, and building models are an important part of the whole analytics \\nlifecycle, but even more important is knowing how well these models are performing. \\nPerformance of classification models is usually based on how well they predict outcomes \\nfor new data points. Usually this performance is measured against a test or holdout \\ndataset that consists of data points which was not used to influence or train the classifier \\nin any way. This test dataset usually has several observations and corresponding labels. \\nFigure 4-4.\\xe2\\x80\\x82 Multi-class SVM on three classes (courtesy: scikit-learn.org)\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n200\\nWe extract features in the same way as it was followed when training the model. These \\nfeatures are fed to the already trained model, and we obtain predictions for each data \\npoint. These predictions are then matched with the actual labels to see how well or how \\naccurately the model has predicted.\\nSeveral metrics determine a model\\xe2\\x80\\x99s prediction performance, but we will mainly \\nfocus on the following metrics:\\n\\xe2\\x80\\xa2 \\nAccuracy\\n\\xe2\\x80\\xa2 \\nPrecision\\n\\xe2\\x80\\xa2 \\nRecall\\n\\xe2\\x80\\xa2 \\nF1 score\\nLet us look at a practical example to see how these metrics can be computed. \\nConsider a binary classification problem of classifying emails as either \\'spam\\' or \\'ham\\'. \\nAssuming we have a total of 20 emails, for which we already have the actual manual \\nlabels, we pass it through our built classifier to get predicted labels for each email. This \\ngives us 20 predicted labels. Now we want to measure the classifier performance by \\ncomparing each prediction with its actual label. The following code snippet sets up the \\ninitial dependencies and the actual and predicted labels:\\nfrom sklearn import metrics\\nimport numpy as np\\nimport pandas as pd\\nfrom collections import Counter\\nactual_labels = [\\'spam\\', \\'ham\\', \\'spam\\', \\'spam\\', \\'spam\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\'ham\\', \\xc2\\xa0\\'ham\\', \\'spam\\', \\'ham\\', \\xc2\\xa0\\'spam\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\'spam\\', \\'ham\\', \\'ham\\', \\xc2\\xa0\\'ham\\', \\xc2\\xa0\\'spam\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\'ham\\', \\xc2\\xa0\\'ham\\', \\'spam\\', \\'spam\\', \\'ham\\']\\npredicted_labels = [\\'spam\\', \\'spam\\', \\'spam\\', \\'ham\\', \\xc2\\xa0\\'spam\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\'spam\\', \\'ham\\', \\xc2\\xa0\\'ham\\', \\xc2\\xa0\\'spam\\', \\'spam\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\'ham\\', \\xc2\\xa0\\'ham\\', \\xc2\\xa0\\'spam\\', \\'ham\\', \\xc2\\xa0\\'ham\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\'ham\\', \\xc2\\xa0\\'spam\\', \\'ham\\', \\xc2\\xa0\\'spam\\', \\'spam\\']\\nac = Counter(actual_labels)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\npc = Counter(predicted_labels)\\xc2\\xa0\\xc2\\xa0\\nLet us now see the total number of emails belonging to either \\'spam\\' or \\'ham\\' based \\non the actual labels and our predicted labels using the following snippet:\\nIn [517]: print \\'Actual counts:\\', ac.most_common()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'Predicted counts:\\', pc.most_common()\\nActual counts: [(\\'ham\\', 10), (\\'spam\\', 10)]\\nPredicted counts: [(\\'spam\\', 11), (\\'ham\\', 9)]\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n201\\nThus we see that there are a total of 10 emails that are \\'spam\\' and 10 emails that are \\n\\'ham\\'. Our classifier has predicted a total of 11 emails as \\'spam\\' and 9 as \\'ham\\'. How \\ndo we now compare which email was actually \\'spam\\' and what it was classified as? A \\nconfusion matrix is an excellent way to measure this performance across the two classes. \\nA confusion matrix is a tabular structure that helps visualize the performance of classifiers. \\nEach column in the matrix represents classified instances based on predictions, and each \\nrow of the matrix represents classified instances based on the actual class labels. (It can \\nbe vice-versa if needed.) We usually have a class label defined as the positive class, which \\ncould be typically the class of our interest. Figure\\xc2\\xa04-5 shows a typical two-class confusion \\nmatrix where (p) denotes the positive class and (n) denotes the negative class.\\nYou can see some terms in the matrix depicted in Figure\\xc2\\xa04-5. True Positive (TP) \\nindicates the number of correct hits or predictions for our positive class. False Negative \\n(FN) indicates the number of instances we missed for that class by predicting it falsely as \\nthe negative class. False Positive (FP) is the number of instances we predicted wrongly as \\nthe positive class when it was actually not. True Negative (TN) is the number of instances \\nwe correctly predicted as the negative class.\\nThe following code snippet constructs a confusion matrix with our data:\\nIn [519]: cm = metrics.confusion_matrix(y_true=actual_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0y_pred=predicted_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0labels=[\\'spam\\',\\'ham\\'])\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print pd.DataFrame(data=cm,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0columns=pd.MultiIndex(levels=[[\\'Predicted:\\'],\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0[\\'spam\\',\\'ham\\']],\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0labels=[[0,0],[0,1]]),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0index=pd.MultiIndex(levels=[[\\'Actual:\\'],\\nFigure 4-5.\\xe2\\x80\\x82 A confusion matrix from a two-class classification problem\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n202\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0[\\'spam\\',\\'ham\\']],\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0labels=[[0,0],[0,1]]))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0Predicted:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0spam ham\\nActual: spam\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa0\\xc2\\xa05\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ham\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa06\\xc2\\xa0\\xc2\\xa0\\xc2\\xa04\\nWe now get a confusion matrix similar to the figure. In our case, let us consider \\n\\'spam\\' to be the positive class. We can now define the preceding metrics in the following \\nsnippet:\\npositive_class = \\'spam\\'\\ntrue_positive = 5.\\nfalse_positive = 6.\\nfalse_negative = 5.\\ntrue_negative = 4.\\nNow that we have the necessary values from the confusion matrix, we can calculate \\nour four performance metrics one by one. We have taken the values from earlier as \\nfloats to help with computations involving divisions. We will use the metrics module \\nfrom scikit-learn, which is very powerful and helps in computing these metrics with a \\nsingle function. And we will define and compute these metrics manually so that you can \\nunderstand them clearly and see what goes on behind the scenes of those functions from \\nthe metrics module.\\nAccuracy is defined as the overall accuracy or proportion of correct predictions of the \\nmodel, which can be depicted by the formula\\nAccuracy\\nTP\\nTN\\nTP\\nFP\\nFN\\nTN\\n=\\n+\\n+\\n+\\n+\\nwhere we have our correct predictions in the numerator divided by all the outcomes in \\nthe denominator. The following snippet shows the computations for accuracy:\\nIn [522]: accuracy = np.round(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0metrics.accuracy_score(y_true=actual_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0y_pred=predicted_labels),2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: accuracy_manual = np.round(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0(true_positive + true_negative) /\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0(true_positive + true_negative +\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0false_negative + false_positive),2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'Accuracy:\\', accuracy\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'Manually computed accuracy:\\', accuracy_manual\\nAccuracy: 0.45\\nManually computed accuracy: 0.45\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n203\\nPrecision is defined as the number of predictions made that are actually correct \\nor relevant out of all the predictions based on the positive class. This is also known as \\npositive predictive value and can be depicted by the formula\\nPrecision\\nTP\\nTP\\nFP\\n=\\n+\\nwhere we have our correct predictions in the numerator for the positive class divided \\nby all the predictions for the positive class including the false positives. The following \\nsnippet shows the computations for precision:\\nIn [523]: precision = np.round(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0metrics.precision_score(y_true=actual_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0y_pred=predicted_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07pos_label=positive_\\nclass),2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: precision_manual = np.round(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0(true_positive) /\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0(true_positive + false_positive),2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'Precision:\\', precision\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'Manually computed precision:\\', precision_manual\\nPrecision: 0.45\\nManually computed precision: 0.45\\nRecall is defined as the number of instances of the positive class that were correctly \\npredicted. This is also known as hit rate, coverage, or sensitivity and can be depicted by \\nthe formula\\nRecall\\nTP\\nTP\\nFN\\n=\\n+\\nwhere we have our correct predictions for the positive class in the numerator divided by \\ncorrect and missed instances for the positive class, giving us the hit rate. The following \\nsnippet shows the computations for recall:\\nIn [524]: recall = np.round(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0metrics.recall_score(y_true=actual_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0y_pred=predicted_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0pos_label=positive_class),2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: recall_manual = np.round(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0(true_positive) /\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0(true_positive + false_negative),2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'Recall:\\', recall\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'Manually computed recall:\\', recall_manual\\nRecall: 0.5\\nManually computed recall: 0.5\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n204\\nF1 score is another accuracy measure that is computed by taking the harmonic mean \\nof the precision and recall and can be represented as follows:\\nF Score\\nPrecision\\nRecall\\nPrecision\\nRecall\\n1\\n2\\n=\\n\\xc2\\xb4\\n\\xc2\\xb4\\n+\\nWe can compute the same using the following code snippet:\\nIn [526]: f1_score = np.round(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0metrics.f1_score(y_true=actual_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0y_pred=predicted_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0pos_label=positive_class),2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: f1_score_manual = np.round(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0(2 * precision * recall) /\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0(precision + recall),2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'F1 score:\\', f1_score\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'Manually computed F1 score:\\', f1_score_manual\\xc2\\xa0\\xc2\\xa0\\nF1 score: 0.48\\nManually computed F1 score: 0.47\\nThis should give you a pretty good idea about the main metrics used most often \\nwhen evaluating classification models. We will be measuring the performance of our \\nmodels using the very same metrics, and you may remember seeing these metrics from \\nChapter 3, when we were building some of our taggers and parsers.\\nBuilding a Multi-Class Classification System\\nWe have gone through all the steps necessary for building a classification system, from \\nnormalization to feature extraction, model building, and evaluation. In this section, we \\nwill be putting everything together and applying it on some real-world data to build a \\nmulti-class text classification system. For this, we will be using the 20 newsgroups dataset \\navailable for download using scikit-learn. The 20 newsgroups dataset comprises \\naround 18,000 newsgroups posts spread across 20 different categories or topics, thus \\nmaking this a 20-class classification problem! Remember the more classes, the more \\ncomplex or difficult trying to build an accurate classifier gets. It is recommended that \\nyou remove the headers, footers, and quotes from the text documents to prevent the \\nmodel from overfitting or not generalizing well due to certain specific headers or email \\naddresses, so we will make sure we take care of this. We will also remove documents \\nthat are empty or have no content after removing these three items because it would be \\npointless to try and extract features from empty documents.\\nLet us start with loading the necessary dataset and defining functions for building \\nthe training and testing datasets:\\nfrom sklearn.datasets import fetch_20newsgroups\\nfrom sklearn.cross_validation import train_test_split\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n205\\ndef get_data():\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0data = fetch_20newsgroups(subset=\\'all\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0shuffle=True,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0remove=(\\'headers\\', \\'footers\\', \\'quotes\\'))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return data\\ndef prepare_datasets(corpus, labels, test_data_proportion=0.3):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0test_size=0.33, \\nrandom_state=42)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return train_X, test_X, train_Y, test_Y\\ndef remove_empty_docs(corpus, labels):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0filtered_corpus = []\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0filtered_labels = []\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for doc, label in zip(corpus, labels):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if doc.strip():\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0filtered_corpus.append(doc)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0filtered_labels.append(label)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return filtered_corpus, filtered_labels\\nWe can now get the data, see the total number of classes in our dataset, and split our \\ndata into training and test datasets using the following snippet (in case you do not have \\nthe data downloaded, feel free to connect to the Internet and take some time to download \\nthe complete corpus):\\n# get the data\\nIn [529]: dataset = get_data()\\n# print all the classes\\nIn [530]: print dataset.target_names\\n[\\'alt.atheism\\', \\'comp.graphics\\', \\'comp.os.ms-windows.misc\\', \\'comp.sys.ibm.\\npc.hardware\\', \\'comp.sys.mac.hardware\\', \\'comp.windows.x\\', \\'misc.forsale\\', \\n\\'rec.autos\\', \\'rec.motorcycles\\', \\'rec.sport.baseball\\', \\'rec.sport.hockey\\', \\n\\'sci.crypt\\', \\'sci.electronics\\', \\'sci.med\\', \\'sci.space\\', \\'soc.religion.\\nchristian\\', \\'talk.politics.guns\\', \\'talk.politics.mideast\\', \\'talk.politics.\\nmisc\\', \\'talk.religion.misc\\']\\n# get corpus of documents and their corresponding labels\\nIn [531]: corpus, labels = dataset.data, dataset.target\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: corpus, labels = remove_empty_docs(corpus, labels)\\n# see sample document and its label index, name\\nIn [548]: print \\'Sample document:\\', corpus[10]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'Class label:\\',labels[10]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'Actual class label:\\', dataset.target_names[labels[10]]\\nSample document: the blood of the lamb.\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n206\\nThis will be a hard task, because most cultures used most animals\\nfor blood sacrifices. It has to be something related to our current\\npost-modernism state. Hmm, what about used computers?\\nCheers,\\nKent\\nClass label: 19\\nActual class label: talk.religion.misc\\n# prepare train and test datasets\\nIn [549]: train_corpus, test_corpus, train_labels, test_labels = prepare_\\ndatasets(corpus,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0labels, test_\\ndata_proportion=0.3)\\nYou can see from the preceding snippet how a sample document and label looks. \\nEach document has its own class label, which is one of the 20 topics it is categorized into. \\nThe labels obtained are numbers, but we can easily map it back to the original category \\nname if needed using the preceding snippet. We also split our data into train and test \\ndatasets, where the test dataset is 30 percent of the total data. We will build our model on \\nthe training data and test its performance on the test data. In the following snippet, we \\nwill use the normalization module we built earlier to normalize our datasets:\\nfrom normalization import normalize_corpus\\nnorm_train_corpus = normalize_corpus(train_corpus)\\nnorm_test_corpus = normalize_corpus(test_corpus)\\xc2\\xa0\\xc2\\xa0\\nRemember, a lot of normalization steps take place that we implemented earlier \\nfor each document in the corpora, so it may take some time to complete. Once we have \\nnormalized documents, we will use our feature extractor module built earlier to start \\nextracting features from our documents. We will build models for Bag of Words, TF-IDF, \\naveraged word vector, and TF-IDF weighted averaged word vector features separately and \\ncompare their performances.\\nThe following snippet extracts necessary features based on the different techniques:\\nfrom feature_extractors import bow_extractor, tfidf_extractor\\nfrom feature_extractors import averaged_word_vectorizer\\nfrom feature_extractors import tfidf_weighted_averaged_word_vectorizer\\nimport nltk\\nimport gensim\\n# bag of words features\\nbow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)\\xc2\\xa0\\xc2\\xa0\\nbow_test_features = bow_vectorizer.transform(norm_test_corpus)\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n207\\n# tfidf features\\ntfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)\\xc2\\xa0\\xc2\\xa0\\ntfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n# tokenize documents\\ntokenized_train = [nltk.word_tokenize(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for text in norm_train_corpus]\\ntokenized_test = [nltk.word_tokenize(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for text in norm_test_corpus]\\xc2\\xa0\\xc2\\xa0\\n# build word2vec model\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\nmodel = gensim.models.Word2Vec(tokenized_train,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0size=500,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0window=100,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0min_count=30,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sample=1e-3)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n# averaged word vector features\\navg_wv_train_features = averaged_word_vectorizer(corpus=tokenized_train,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0model=model,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_features=500)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\navg_wv_test_features = averaged_word_vectorizer(corpus=tokenized_test,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0model=model,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_features=500)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n# tfidf weighted averaged word vector features\\nvocab = tfidf_vectorizer.vocabulary_\\ntfidf_wv_train_features = \\ntfidf_weighted_averaged_word_vectorizer(corpus=tokenized_train,\\xc2\\xa0\\ntfidf_vectors=tfidf_train_features,\\ntfidf_vocabulary=vocab, model=model,\\nnum_features=500)\\ntfidf_wv_test_features =  \\ntfidf_weighted_averaged_word_vectorizer(corpus=tokenized_test,\\ntfidf_vectors=tfidf_test_features,\\ntfidf_vocabulary=vocab, model=model,\\nnum_features=500)\\nOnce we extract all the necessary features from our text documents using the preceding \\nfeature extractors, we define a function that will be useful for evaluation our classification \\nmodels based on the four metrics discussed earlier, as shown in the following snippet:\\nfrom sklearn import metrics\\nimport numpy as np\\ndef get_metrics(true_labels, predicted_labels):\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n208\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Accuracy:\\', np.round(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0metrics.accuracy_score(true_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0predicted_labels),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa02)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Precision:\\', np.round(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0metrics.precision_score(true_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0predicted_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0average=\\'weighted\\'),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa02)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Recall:\\', np.round(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0metrics.recall_score(true_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0predicted_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0average=\\'weighted\\'),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa02)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'F1 Score:\\', np.round(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0metrics.f1_score(true_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0predicted_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0average=\\'weighted\\'),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa02)\\nWe now define a function that trains the model using an ML algorithm and the \\ntraining data, performs predictions on the test data using the trained model, and then \\nevaluates the predictions using the preceding function to give us the model performance:\\ndef train_predict_evaluate_model(classifier,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0train_features, train_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0test_features, test_labels):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# build model\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0classifier.fit(train_features, train_labels)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# predict using model\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0predictions = classifier.predict(test_features)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# evaluate model prediction performance\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0get_metrics(true_labels=test_labels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0predicted_labels=predictions)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return predictions\\nWe now import two ML algorithms (discussed in detail earlier) so that we can start \\nbuilding our models with them based on our extracted features. We will be using scikit-\\nlearn as mentioned to import the necessary classification algorithms, saving us the time \\nand effort that would have been spent otherwise reinventing the wheel:\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn.linear_model import SGDClassifier\\nmnb = MultinomialNB()\\nsvm = SGDClassifier(loss=\\'hinge\\', n_iter=100)\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n209\\nNow we will train, predict, and evaluate models for all the different types of features \\nusing both multinomial na\\xc3\\xafve Bayes and support vector machines using the following \\nsnippet:\\n# Multinomial Naive Bayes with bag of words features\\nIn [558]: mnb_bow_predictions = train_predict_evaluate_model(classifier=mnb,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07train_features=bow_\\ntrain_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07train_labels=train_\\nlabels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07test_features=bow_test_\\nfeatures,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07test_labels=test_\\nlabels)\\nAccuracy: 0.67\\nPrecision: 0.72\\nRecall: 0.67\\nF1 Score: 0.65\\n# Support Vector Machine with bag of words features\\nIn [559]: svm_bow_predictions = train_predict_evaluate_model(classifier=svm,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07train_features=bow_\\ntrain_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07train_labels=train_\\nlabels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07test_features=bow_test_\\nfeatures,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07test_labels=test_\\nlabels)\\nAccuracy: 0.61\\nPrecision: 0.66\\nRecall: 0.61\\nF1 Score: 0.62\\n# Multinomial Naive Bayes with tfidf features\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\nIn [560]: mnb_tfidf_predictions = train_predict_evaluate_\\nmodel(classifier=mnb,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07train_features=tfidf_\\ntrain_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07train_labels=train_\\nlabels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07test_features=tfidf_\\ntest_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07test_labels=test_\\nlabels)\\nAccuracy: 0.72\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n210\\nPrecision: 0.78\\nRecall: 0.72\\nF1 Score: 0.7\\n# Support Vector Machine with tfidf features\\nIn [561]: svm_tfidf_predictions = train_predict_evaluate_\\nmodel(classifier=svm,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07train_features=tfidf_\\ntrain_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07train_labels=train_\\nlabels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07test_features=tfidf_\\ntest_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07test_labels=test_\\nlabels)\\nAccuracy: 0.77\\nPrecision: 0.77\\nRecall: 0.77\\nF1 Score: 0.77\\n# Support Vector Machine with averaged word vector features\\nIn [562]: svm_avgwv_predictions = train_predict_evaluate_\\nmodel(classifier=svm,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07train_features=avg_wv_\\ntrain_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07train_labels=train_\\nlabels,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07test_features=avg_wv_\\ntest_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07test_labels=test_\\nlabels)\\nAccuracy: 0.55\\nPrecision: 0.55\\nRecall: 0.55\\nF1 Score: 0.52\\n# Support Vector Machine with tfidf weighted averaged word vector features\\nIn [563]: svm_tfidfwv_predictions = train_predict_evaluate_model(classifier\\n=svm,\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0 \\ntrain_features=tfidf_wv_train_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0 \\ntrain_labels=train_labels, test_features=tfidf_wv_test_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0test_labels=test_labels)\\nAccuracy: 0.53\\nPrecision: 0.55\\nRecall: 0.53\\nF1 Score: 0.52\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n211\\nWe built a total of six models using various types of extracted features and evaluated \\nthe performance of the model on the test data. From the preceding results, we can see \\nthat the SVM-based model built using TF-IDF features yielded the best results of 77 \\npercent accuracy as well as precision, recall, and F1 score. We can build the confusion \\nmatrix for our SVM TF-IDF\\xe2\\x80\\x93based model to get an idea of the classes for which our model \\nmight not be performing well:\\nIn [597]: import pandas as pd\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: cm = metrics.confusion_matrix(test_labels, svm_tfidf_predictions)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: pd.DataFrame(cm, index=range(0,20), columns=range(0,20))\\xc2\\xa0\\xc2\\xa0\\nOut[597]:\\nFrom the confusion matrix shown in Figure\\xc2\\xa04-6, we can see a large number of \\ndocuments for class label 0 that got misclassified to class label 15, and similarly for class \\nlabel 18, many documents got misclassified into class label 16. Many documents for class \\nlabel 19 got misclassified into class label 15. On printing the class label names for them, \\nwe can observe the following output:\\nIn [600]: class_names = dataset.target_names\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print class_names[0], \\'->\\', class_names[15]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print class_names[18], \\'->\\', class_names[16]\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print class_names[19], \\'->\\', class_names[15]\\nalt.atheism -> soc.religion.christian\\ntalk.politics.misc -> talk.politics.guns\\ntalk.religion.misc -> soc.religion.christian\\nFrom the preceding output we can see that the misclassified categories are not vastly \\ndifferent from the actual correct category. Christian, religion, and atheism are based on \\nsome concepts related to the existence of God and religion and possibly have similar \\nfeatures. Talks about miscellaneous issues and guns related to politics also must be \\nFigure 4-6.\\xe2\\x80\\x82 20-class confusion matrix for our SVM based model\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n212\\nhaving similar features. We can further analyze and look at the misclassified documents \\nin detail using the following snippet (due to space constraints I only include the first few \\nmisclassified documents in each case):\\nIn [621]: import re\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: num = 0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: for document, label, predicted_label in zip(test_corpus, test_\\nlabels, svm_tfidf_predictions):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if label == 0 and predicted_label == 15:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Actual Label:\\', class_names[label]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Predicted Label:\\', class_names[predicted_label]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Document:-\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print re.sub(\\'\\\\n\\', \\' \\', document)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num += 1\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if num == 4:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0break\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\nActual Label: alt.atheism\\nPredicted Label: soc.religion.christian\\nDocument:-\\nI would like a list of Bible contadictions from those of you who dispite \\nbeing free from Christianity are well versed in the Bible.\\nActual Label: alt.atheism\\nPredicted Label: soc.religion.christian\\nDocument:-\\n\\xc2\\xa0\\xc2\\xa0They spent quite a bit of time on the wording of the Constitution.\\xc2\\xa0\\xc2\\xa0They \\npicked words whose meanings implied the intent.\\xc2\\xa0\\xc2\\xa0We have already looked in \\nthe dictionary to define the word.\\xc2\\xa0\\xc2\\xa0Isn\\'t this sufficient?\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0But we were \\ndiscussing it in relation to the death penalty.\\xc2\\xa0\\xc2\\xa0And, the Constitution need \\nnot define each of the words within.\\xc2\\xa0\\xc2\\xa0Anyone who doesn\\'t know what cruel is \\ncan look in the dictionary (and we did).\\nActual Label: alt.atheism\\nPredicted Label: soc.religion.christian\\nDocument:-\\nOur Lord and Savior David Keresh has risen!\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0He has been seen \\nalive!\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0Spread the word!\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0-----------------------------\\n-----------------------------------------------------------------\\nActual Label: alt.atheism\\nPredicted Label: soc.religion.christian\\nDocument:-\\n\\xc2\\xa0\\xc2\\xa0\"This is your god\" (from John Carpenter\\'s \"They Live,\" natch)\\xc2\\xa0\\xc2\\xa0\\nIn [623]: num = 0\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n213\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: for document, label, predicted_label in zip(test_corpus, test_\\nlabels, svm_tfidf_predictions):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if label == 18 and predicted_label == 16:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Actual Label:\\', class_names[label]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Predicted Label:\\', class_names[predicted_label]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Document:-\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print re.sub(\\'\\\\n\\', \\' \\', document)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num += 1\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if num == 4:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0break\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\nActual Label: talk.politics.misc\\nPredicted Label: talk.politics.guns\\nDocument:-\\nAfter the initial gun battle was over, they had 50 days to come out \\npeacefully. They had their high priced lawyer, and judging by the posts here \\nthey had some public support. Can anyone come up with a rational explanation \\nwhy the didn\\'t come out (even after they negotiated coming out after the \\nradio sermon) that doesn\\'t include the Davidians wanting to commit suicide/\\nmurder/general mayhem?\\nActual Label: talk.politics.misc\\nPredicted Label: talk.politics.guns\\nDocument:-\\nYesterday, the FBI was saying that at least three of the bodies had gunshot \\nwounds, indicating that they were shot trying to escape the fire.\\xc2\\xa0\\xc2\\xa0Today\\'s \\npaper quotes the medical examiner as saying that there is no evidence of \\ngunshot wounds in any of the recovered bodies.\\xc2\\xa0\\xc2\\xa0At the beginning of this \\nsiege, it was reported that while Koresh had a class III (machine gun) \\nlicense, today\\'s paper quotes the government as saying, no, they didn\\'t have \\na license.\\xc2\\xa0\\xc2\\xa0Today\\'s paper reports that a number of the bodies were found \\nwith shoulder weapons next to them, as if they had been using them while \\ndying -- which doesn\\'t sound like the sort of action I would expect from a \\nsuicide.\\xc2\\xa0\\xc2\\xa0Our government lies, as it tries to cover over its incompetence \\nand negligence.\\xc2\\xa0\\xc2\\xa0Why should I believe the FBI\\'s claims about anything else, \\nwhen we can see that they are LYING?\\xc2\\xa0\\xc2\\xa0This system of government is beyond \\nreform.\\nActual Label: talk.politics.misc\\nPredicted Label: talk.politics.guns\\nDocument:-\\n\\xc2\\xa0\\xc2\\xa0Well, for one thing most, if not all the Dividians (depending on whether \\nthey could show they acted in self-defense and there were no illegal \\nweapons), could have gone on with their life as they were living it. No one \\nwas forcing them to give up their religion or even their legal weapons. The \\nDividians had survived a change in leadership before so even if Koresch \\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n214\\nhimself would have been convicted and sent to jail, they still could have \\ncarried on.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0I don\\'t think the Dividians were insane, but I don\\'t see a \\nreason for mass suicide (if the fire was intentional set by some of the \\nDividians.) We also don\\'t know that, if the fire was intentionally set from \\ninside, was it a generally know plan or was this something only an inner \\ncircle knew about, or was it something two or three felt they had to do \\nwith or without Koresch\\'s knowledge/blessing, etc.? I don\\'t know much about \\nMasada. Were some people throwing others over? Did mothers jump over with \\ntheir babies in their arms?\\nActual Label: talk.politics.misc\\nPredicted Label: talk.politics.guns\\nDocument:-\\nrja@mahogany126.cray.com (Russ Anderson) writes...\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0The fact is that \\nKoresh and his followers involved themselves\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in a gun battle to control \\nthe Mt Carmel complex. That is not\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in dispute. From what I remember of the \\ntrial, the authories\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0couldn\\'t reasonably establish who fired first, the \\nbig reason\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0behind the aquittal. Mitchell S Todd\\nThus you can see how to analyze and look at documents that have been misclassified \\nand then maybe go back and tune our feature extraction methods by removing certain \\nwords or weighing words differently to reduce or give prominence.\\nThis brings us to the end of our discussion and implementation of our text \\nclassification system. Feel free to implement more models using other innovative feature-\\nextraction techniques or supervised learning algorithms and compare their performance.\\nApplications and Uses\\nText classification and categorization is used in several real-world scenarios and \\napplications, including the following:\\n\\xe2\\x80\\xa2 \\nNews articles categorization\\n\\xe2\\x80\\xa2 \\nSpam filtering\\n\\xe2\\x80\\xa2 \\nMusic or movie genre categorization\\n\\xe2\\x80\\xa2 \\nSentiment analysis\\n\\xe2\\x80\\xa2 \\nLanguage detection\\nThe possibilities with text data are indeed endless, and with a little effort you can \\napply classification to solve various problems and automate otherwise time-consuming \\noperations and scenarios.\\nChapter 4 \\xe2\\x96\\xa0 Text Classification\\n215\\nSummary\\nText classification is indeed a powerful tool, and we have covered almost all aspects \\nrelated to it in this chapter. We started off our journey with look at the definition and \\nscope of text classification. Next, we defined automated text classification as a supervised \\nlearning problem and looked at the various types of text classification. We also briefly \\ncovered some ML concepts related to the various types of algorithms. A typical text \\nclassification system blueprint was also defined to describe the various modules and \\nsteps involved when building an end-to-end text classifier. Each module in the blueprint \\nwas then expanded upon. Normalization was touched upon in detail in Chapter 3, and \\nwe built a normalization module here specially for text classification. Various feature-\\nextraction techniques were explored in detail, including Bag of Words, TF-IDF, and \\nadvanced word vectorization techniques.\\nYou should now be clear about not only the mathematical representations and \\nconcepts but also ways to implement them using our code samples. Various supervised \\nlearning methods were discussed with focus on multinomial na\\xc3\\xafve Bayes and support vector \\nmachines, which work well with text data, and we looked at ways to evaluate classification \\nmodel performance and even implemented those metrics. Finally, we put everything we \\nlearned together into building a robust 20-class text classification system on real data, \\nevaluated various models, and analyzed model performance in detail. We wrapped up our \\ndiscussion by looking at some areas where text classification is used frequently.\\nWe have just scratched the surface of text analytics here with classification. We \\nwill be looking at more ways to analyze and derive insights from textual data in future \\nchapters.\\n217\\n\\xc2\\xa9 Dipanjan Sarkar 2016 \\nD. Sarkar, Text Analytics with Python, DOI 10.1007/978-1-4842-2388-8_5\\nCHAPTER 5\\nText Summarization\\nWe have come a long way on our journey through the world of text analytics and natural \\nlanguage processing (NLP). You have seen how to process and annotate textual data to \\nuse it for various applications. We have also ventured into the world of machine learning \\n(ML) and built our own multi-class text classification system by leveraging various \\nfeature-extraction techniques and supervised machine learning algorithms.\\nIn this chapter, we will tackle a slightly different problem in the world of text analytics. \\nThe world is rapidly evolving with regard to technology, commerce, business, and media. \\nGone are the days when we would wait for newspapers to come to our home and be updated \\nabout the various events around the world. We now have the Internet and various forms of \\nsocial media that we consume to stay updated about daily events and stay connected with \\nthe world as well as our friends and family. With short messages and statuses, social media \\nwebsites like Facebook and Twitter have opened up a completely different dimension to \\nsharing and consuming information. We as humans tend to have short attention spans, and \\nthis leads us to get bored when consuming or reading large text documents and articles. This \\nbrings us to text summarization, an extremely important concept in text analytics that is used \\nby businesses and analytical firms to shorten and summarize huge documents of text such \\nthat they still retain their key essence or theme and present this summarized information to \\nconsumers and clients. This is analogous to an elevator pitch, where an executive summary \\ncan describe a process, product, service, or business while retaining the core important \\nthemes and values in the time it takes to ride an elevator.\\nSay you have a whole corpus of text documents that ranges from sentences to \\nparagraphs, and you are tasked with trying to derive meaningful insights from it. At first \\nglance, this may seem difficult because you do not even know what to do with these \\ndocuments, let\\xc2\\xa0alone use some analytical or ML techniques on the data. A good way to \\nstart would be to use some unsupervised learning approaches specifically aimed at text \\nsummarization and information extraction. Here are a few of the things you could do with \\ntext documents:\\n\\xe2\\x80\\xa2 \\nExtract the key influential phrases from the documents\\n\\xe2\\x80\\xa2 \\nExtract various diverse concepts or topics present in the \\ndocuments\\n\\xe2\\x80\\xa2 \\nSummarize the documents to provide a gist that retains the \\nimportant parts of the whole corpus\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n218\\nThis chapter will cover concepts, techniques, and practical implementations of ways \\nto perform all three operations. We can describe our problem formally now, which we \\nwill try to solve in this chapter, along with some of the concepts related to it. Given a set of \\ndocuments, text summarization aims to reduce a document or set of documents in a corpus \\nto a summary of user-specified length such that it retains the key important concepts and \\nthemes from the corpus. We will also discuss other ways to summarize documents and \\nextract information from them, including topic models and key phrase extraction.\\nIn this chapter, we will talk about text summarization as well as information extraction \\nfrom text documents, which captures and summarizes the main themes or concepts \\nof the document corpus. We will start with a detailed discussion of the various types of \\nsummarization and information extraction techniques and discuss some concepts essential \\nfor understanding the practical implementations later. The chapter will also briefly cover \\nsome background dependencies related to text processing and feature extraction before \\nmoving on to each technique. We will discuss the three major concepts and techniques of \\nkey phrase extraction, topic models, and automated text summarization.\\nText Summarization and Information Extraction\\nText summarization and information extraction deal with trying to extract key important \\nconcepts and themes from a huge corpus of text, essentially reducing it in the process. \\nBefore we dive deeper into the concepts and techniques, we should first understand the \\nneed for text summarization. The concept of information overload is one of the prime \\nreasons behind the demand for text summarization. Since print and verbal media came \\ninto prominence, there has been an abundance of books, articles, audio, and video. \\nThis began all the way back in the 3rd or 4th century B.C., when people referred to a \\nhuge quantity of books, as there seemed to be no end to the production of books, and \\nthis overload of information was often met with disapproval. The Renaissance gave us \\nthe invention of the printing press by Gutenberg around 1440 A.D., which led to the \\nmass production of books, manuscripts, articles, and pamphlets. This greatly increased \\ninformation overload, with scholars complaining about an excess of information, which \\nwas becoming extremely difficult to consume, process, and manage.\\nIn the 20th century, advances in computers and technology ushered in the digital \\nage, culminating in the Internet. The Internet opened up a whole window of possibilities \\ninto producing and consuming information with social media, news web sites, email, and \\ninstant messaging capabilities. This in turn has led to an explosive increase in the amount \\nof information and to unwanted information in the form of spam, unwanted statuses, and \\ntweets\\xe2\\x80\\x94and even to bots posting more unwanted content across the Web.\\nInformation overload, then, is the presence of excess data or information, which \\nconsumers find difficult to process in making well-informed decisions. The overload \\noccurs when the amount of information as input to the system starts exceeding the \\nprocessing capability of the system. We as humans have limited cognitive processing \\ncapabilities and are also wired in such a way that we cannot spend a long time reading \\na single piece of information or data because the mind tends to wander every now and \\nthen. Thus when we get loaded with information, it leads to a reduction in making \\nqualitative decisions.\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n219\\nBy now you can probably guess where I am going with this concept and why we \\nneed summarization and information extraction. Businesses thrive on making key and \\nwell-informed decisions and usually they have a huge amount of data and information. \\nGetting insights from it is no piece of cake, and automating it is tough because what to \\ndo with all that data is often unclear. Executives rarely have time to listen to long talks \\nor go through pages and pages of important information. The idea of summarization \\nand information extraction is to get an idea of the key important topics and themes \\nof huge documents of information and summarize them into a few lines that can be \\nread, understood, and interpreted easily, thus easing the process of making well-\\ninformed decisions in shorter time frames. We need efficient and scalable processes \\nand techniques that can perform this on text data, and the most popular techniques are \\nkeyphrase extraction, topic modeling, and automated document summarization. The \\nfirst two techniques are more into extracting key information in the form of concepts, \\ntopics, and themes from documents, thus reducing them, and the last technique is all \\nabout summarizing large text documents into a few lines that give the key essence or \\ninformation which the document is trying to convey. We will cover each technique in \\ndetail in future sections along with practical examples but right now, we will briefly talk \\nabout what each technique entails and their scope:\\n\\xe2\\x80\\xa2 \\nKeyphrase extraction is perhaps the simplest out of the three \\ntechniques. It involves extracting keywords or phrases from a text \\ndocument or corpus that capture its main concepts or themes. \\nThis can be said to be a simplistic form of topic modeling. You \\nmight have seen keywords or phrases described in a research \\npaper or even some product in an online store that describes \\nthe entity in a few words or phrases, capturing its main idea or \\nconcept.\\n\\xe2\\x80\\xa2 \\nTopic modeling usually involves using statistical and \\nmathematical modeling techniques to extract main topics, \\nthemes, or concepts from a corpus of documents. Note here the \\nemphasis on corpus of documents because the more diverse set \\nof documents you have, the more topics or concepts you can \\ngenerate\\xe2\\x80\\x94unlike with a single document where you will not get \\ntoo many topics or concepts if it talks about a singular concept. \\nTopic models are also often known as probabilistic statistical \\nmodels, which use specific statistical techniques including \\nsingular valued decomposition and latent dirichlet\\xc2\\xa0allocation \\nto discover connected latent semantic structures in text data \\nthat yield topics and concepts. They are used extensively in text \\nanalytics and even bioinformatics.\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n220\\n\\xe2\\x80\\xa2 \\nAutomated document summarization is the process of using a \\ncomputer program or algorithm based on statistical and ML \\ntechniques to summarize a document or corpus of documents \\nsuch that we obtain a short summary that captures all the \\nessential concepts and themes of the original document or \\ncorpus. A wide variety of techniques for building automated \\ndocument summarizers exist, including various extraction- and \\nabstraction-based techniques. The key concept behind all these \\nalgorithms is to find a representative subset of the original dataset \\nsuch that the core essence of the dataset from the semantic and \\nconceptual standpoints is contained in this subset. Document \\nsummarization usually involves trying to extract and construct \\nan executive summary from a single document. But the same \\nalgorithms can be extended to multiple documents, though \\nusually the idea is not to combine several diverse documents \\ntogether, which would defeat the purpose of the algorithm. The \\nsame concept is not only applied in text analytics but also to \\nimage and video summarization.\\nWe will discuss some important mathematical and ML concepts, text normalization, \\nand feature extraction processes in the following sections, before moving to cover each \\ntechnique in further detail.\\nImportant Concepts\\nSeveral important mathematical and ML-based concepts will be useful later on because \\nwe will be basing several of our implementations on them. Some will be familiar to you, \\nbut I will briefly touch on them again for the sake of completeness so that you can refresh \\nyour memory. We will also cover some concepts from natural language processing in this \\nsection.\\nDocuments\\nA document is usually an entity containing a whole body of text data with optional \\nheaders and other metadata information. A corpus usually consists of a collection of \\ndocuments. These documents can be simple sentences or complete paragraphs of textual \\ninformation. Tokenized corpus refers to a corpus where each document is tokenized or \\nbroken down into tokens, which are usually words.\\nText Normalization\\nText normalization is the process of cleaning, normalizing, and standardizing textual \\ndata with techniques like removing special symbols and characters, removing extraneous \\nHTML tags, removing stopwords, correcting spellings, stemming, and lemmatization.\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n221\\nFeature Extraction\\nFeature extraction is a process whereby we extract meaningful features or attributes \\nfrom raw textual data for feeding it into a statistical or ML algorithm. This process is \\nalso known as vectorization because usually the end transformation of this process is \\nnumerical vectors from raw text tokens. The reason is that conventional algorithms \\nwork on numerical vectors and cannot work directly on raw text data. There are various \\nfeature-extraction methods including Bag of Words\\xe2\\x80\\x93based binary features that tell us \\nwhether a word or group of words exist or not in the document, Bag of Words\\xe2\\x80\\x93based \\nfrequency features that tell us the frequency of occurrence of a word or group of words in \\na document, and term frequency and inverse document frequency or TF-IDF\\xe2\\x80\\x93weighted \\nfeatures that take into account the term frequency and inverse document frequency when \\nweighing each term. Refer to Chapter 4 for more on feature extraction.\\nFeature Matrix\\nA feature matrix usually refers to a mapping from a collection of documents to features \\nwhere each row indicates a document and each column indicates a particular feature, \\nusually a word or a set of words. We will represent collections of documents or sentences \\nthrough feature matrices after feature extraction and we will often apply statistical and \\nML techniques on these matrices later on in our practical examples.\\nSingular Value Decomposition\\nSingular Value Decomposition (SVD) is a technique from linear algebra that is used quite \\nfrequently in summarization algorithms. SVD is the process of factorization of a matrix \\nthat is real or complex. Formally we can define SVD as follows. Consider a matrix M that \\nhas dimensions of m\\nn\\n\\xc2\\xb4\\n where m denotes the number of rows and n denotes the \\nnumber of columns. Mathematically the matrix M can be represented using SVD as a \\nfactorization such that\\nM\\nU\\nS\\nV\\nm n\\nm m\\nm n\\nn n\\nT\\n\\xc2\\xb4\\n\\xc2\\xb4\\n\\xc2\\xb4\\n\\xc2\\xb4\\n=\\nwhere we have the following decompositions:\\n\\xe2\\x80\\xa2 \\nU is an m m\\n\\xc2\\xb4\\n unitary matrix such that U U\\nI\\nT\\nm m\\n=\\n\\xc2\\xb4  where I is the \\nidentity matrix. The columns of U indicate left singular vectors.\\n\\xe2\\x80\\xa2 \\nS is a diagonal m\\xc2\\xa0x\\xc2\\xa0n matrix with positive real numbers on the \\ndiagonal of the matrix. This is also often also represented as a \\nvector of m values that indicate the singular values.\\n\\xe2\\x80\\xa2 \\nVT is a n\\nn\\n\\xc2\\xb4\\n unitary matrix such that V V\\nI\\nT\\nn n\\n=\\n\\xc2\\xb4  where I is the \\nidentity matrix. The rows of V indicate right singular vectors.\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n222\\nThis tells us that U and V are orthogonal. The singular values of S are particularly \\nimportant in summarization algorithms. We will be using SVD particularly for low rank \\nmatrix approximation where we approximate the original matrix M with a matrix M\\n\\xef\\x80\\xa4\\n such \\nthat this new matrix is a truncated version of the original matrix M with a rank k and can \\nbe represented by SVD as M\\nU SV T\\n\\xef\\x80\\xa4\\n\\xef\\x80\\xa4\\n=\\n where S\\n\\xef\\x80\\xa4\\n is a truncated version of the original S \\nmatrix, which now consists of only the top k largest singular values, and the other singular \\nvalues are represented by zero. We will be using a nice implementation from scipy to \\nextract the top k singular values and also return the corresponding U, S and V matrices. \\nThe following code snippet we will be using is in the utils.py file:\\nfrom scipy.sparse.linalg import svds\\ndef low_rank_svd(matrix, singular_count=2):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0u, s, vt = svds(matrix, k=singular_count)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return u, s, vt\\nWe will be using this implementation in topic modeling as well as document \\nsummarization in future sections. Figure\\xc2\\xa05-1 gives a nice depiction of the preceding \\nprocess, which yields k singular vectors from the original SVD decomposition, and shows \\nhow we can get the low rank matrix approximation from the same.\\nYou can clearly see that k singular values are retained in the low rank matrix \\napproximation and how the original matrix M is decomposed into U, S, and V using SVD. \\nIn our computations, usually the rows of the matrix M will denote terms, and the columns \\nwill denote documents. This matrix, also known as the term-document matrix, is usually \\nobtained after feature extraction by converting a document-term matrix into its transpose \\nbefore applying SVD. I will try to keep the math to a minimum in the rest of the chapter \\nFigure 5-1.\\xe2\\x80\\x82 Singular Value Decomposition with low rank matrix approximation\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n223\\nunless it is absolutely essential to understand how the algorithms work. The following \\nsections will briefly touch upon text normalization and feature extraction to highlight the \\ntechniques and methods that we will be using in this chapter.\\nText Normalization\\nChapter 3 covered text normalization in detail, and we built our own normalization \\nmodule in Chapter 4. We will be reusing the same module in this chapter but will be \\nadding a couple of enhancements specifically for the benefit of some of our algorithms. \\nYou can find all the text normalization\\xe2\\x80\\x93related code in the normalization.py file. The \\nmain steps performed in text normalization include the following:\\n\\t\\n1.\\t\\nSentence extraction\\n\\t\\n2.\\t\\nUnescape HTML escape sequences\\n\\t\\n3.\\t\\nExpand contractions\\n\\t\\n4.\\t\\nLemmatize text\\n\\t\\n5.\\t\\nRemove special characters\\n\\t\\n6.\\t\\nRemove stopwords\\nSteps 3\\xe2\\x80\\x936 remain the same from Chapter 4, except step 5 where we substitute each \\nspecial character with a blank space depicted by the code pattern.sub(\\' \\', token) \\ninstead of the empty string in Chapter 4.\\nStep 1 is a new function where we take in a text document, remove its newlines, \\nparse the text, converting it into ASCII format, and break it down into its sentence \\nconstituents. The function is depicted in the following snippet:\\ndef parse_document(document):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0document = re.sub(\\'\\\\n\\', \\' \\', document)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if isinstance(document, str):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0document = document\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0elif isinstance(document, unicode):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07return unicodedata.normalize(\\'NFKD\\', document).encode(\\'ascii\\', \\n\\'ignore\\')\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0raise ValueError(\\'Document is not string or unicode!\\')\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0document = document.strip()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sentences = nltk.sent_tokenize(document)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sentences = [sentence.strip() for sentence in sentences]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return sentences\\nStep 2 deals with unescaping special HTML characters that are escaped or encoded. \\nThe full list at www.theukwebdesigncompany.com/articles/entity-escape-characters.\\nphp basically shows how some special symbols or even regular characters are escaped \\ninto a different code, for example, & is escaped as &#38;. So we use the following function \\nto unescape them and bring them back to their original unescaped form so we can \\nnormalize them properly in the subsequent stages:\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n224\\nfrom HTMLParser import HTMLParser\\nhtml_parser = HTMLParser()\\ndef unescape_html(parser, text):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return parser.unescape(text)\\nWe also parameterize our lemmatization operation in our final normalization \\nfunction so as to make it optional because in some scenarios it works perfectly while in \\nother scenarios we may not want to use lemmatization. The complete normalization \\nfunction is depicted as follows:\\ndef normalize_corpus(corpus, lemmatize=True, tokenize=False):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0normalized_corpus = []\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for text in corpus:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = html_parser.unescape(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = expand_contractions(text, CONTRACTION_MAP)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if lemmatize:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = lemmatize_text(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = text.lower()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = remove_special_characters(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = remove_stopwords(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if tokenize:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = tokenize_text(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0normalized_corpus.append(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0normalized_corpus.append(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return normalized_corpus\\nWe will be using this function for most of our normalization needs. Refer to the \\nnormalization.py file for all the detailed helper functions we use for normalizing text \\nwhich we also discussed in Chapter 4.\\nFeature Extraction\\nWe will use a generic function here to perform various types of feature extraction from \\ntext data. The types of features which we will be working with are as follows:\\n\\xe2\\x80\\xa2 \\nBinary term occurrence\\xe2\\x80\\x93based features\\n\\xe2\\x80\\xa2 \\nFrequency bag of words\\xe2\\x80\\x93based features\\n\\xe2\\x80\\xa2 \\nTF-IDF\\xe2\\x80\\x93weighted features\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n225\\nWe will use the following function in most of our practical examples in future \\nsections for feature extraction from text documents. You can also find this function in the \\nutils.py module in the code files associated with this chapter:\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\ndef build_feature_matrix(documents, feature_type=\\'frequency\\'):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type = feature_type.lower().strip()\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if feature_type == \\'binary\\':\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vectorizer = CountVectorizer(binary=True, min_df=1,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ngram_range=(1, 1))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0elif feature_type == \\'frequency\\':\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vectorizer = CountVectorizer(binary=False, min_df=1,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ngram_range=(1, 1))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0elif feature_type == \\'tfidf\\':\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vectorizer = TfidfVectorizer(min_df=1,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ngram_range=(1, 1))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07raise Exception(\"Wrong feature type entered. Possible values: \\n\\'binary\\', \\'frequency\\', \\'tfidf\\'\")\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_matrix = vectorizer.fit_transform(documents).astype(float)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return vectorizer, feature_matrix\\nNow that we have covered the necessary background concepts and dependencies \\nneeded for this chapter, we will be deep diving into each text summarization and \\ninformation extraction technique in detail.\\nKeyphrase Extraction\\nOne of the simplest yet most powerful techniques of extracting important information \\nfrom unstructured text documents is keyphrase extraction. Keyphrase extraction, also \\nknown as terminology extraction, is defined as the process or technique of extracting \\nkey important and relevant terms or phrases from a body of unstructured text such that \\nthe core topics or themes of the text document(s) are captured in these key phrases. \\nThis technique falls under the broad umbrella of information retrieval and extraction. \\nKeyphrase extraction finds its uses in many areas, including the following:\\n\\xe2\\x80\\xa2 \\nSemantic web\\n\\xe2\\x80\\xa2 \\nQuery-based search engines and crawlers\\n\\xe2\\x80\\xa2 \\nRecommendation systems\\n\\xe2\\x80\\xa2 \\nTagging systems\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n226\\n\\xe2\\x80\\xa2 \\nDocument similarity\\n\\xe2\\x80\\xa2 \\nTranslation\\nKeyphrase extraction is often the starting point for carrying out more complex \\ntasks in text analytics or NLP, and the output from this can itself act as features for more \\ncomplex systems. There are various approaches for keyphrase extraction. We will be \\ncovering the following two techniques:\\n\\xe2\\x80\\xa2 \\nCollocations\\n\\xe2\\x80\\xa2 \\nWeighted tag\\xe2\\x80\\x93based phrase extraction\\nAn important thing to remember here is that we will be extracting phrases that are usually \\ncollections of words, though sometimes that can include a single word. If you are extracting \\nkeywords, that is also known as keyword extraction, and it is a subset of keyphrase extraction.\\nCollocations\\nThe term collocation is actually a concept borrowed from analyzing corpora and \\nlinguistics. A collocation is a sequence or group of words that tend to occur frequently \\nsuch that this frequency tends to be more than what could be termed as a random or \\nchance occurrence. Various types of collocations can be formed based on the parts of \\nspeech of the various terms like nouns, verbs, and so on. There are various ways to extract \\ncollocations, and one of the best is to use an n-gram grouping or segmentation approach \\nwhere we construct n-grams out of a corpus, count the frequency of each n-gram, and rank \\nthem based on their frequency of occurrence to get the most frequent n-gram collocations.\\nThe idea is to have a corpus of documents, which could be paragraphs or sentences, \\ntokenize them to form sentences, flatten the list of sentences to form one large sentence \\nor string, over which we slide a window of size n based on the n-gram range, and \\ncompute n-grams across the string. Once computed, we count each n-gram based on its \\nfrequency of occurrence and then rank them based on their frequency. This yields the \\nmost frequent collocations on the basis of frequency.\\nWe will implement this from scratch initially so that you can understand the \\nalgorithm better and then we will use some of nltk\\'s built-in capabilities to show the \\nsame. We will start by loading some necessary dependencies and a corpus on which we \\nwill be computing collocations. We will use the nltk Gutenberg corpus\\'s book, Lewis \\nCarroll\\xe2\\x80\\x99s Alice in Wonderland as our corpus. We also normalize the corpus to standardize \\nthe text content using our normalization module specified earlier:\\nfrom nltk.corpus import gutenberg\\nfrom normalization import normalize_corpus\\nimport nltk\\nfrom operator import itemgetter\\n# load corpus\\nalice = gutenberg.sents(fileids=\\'carroll-alice.txt\\')\\nalice = [\\' \\'.join(ts) for ts in alice]\\nnorm_alice = filter(None, normalize_corpus(alice, lemmatize=False))\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n227\\n# print first line\\nIn [772]: print norm_alice[0]\\nalice adventures wonderland lewis carroll 1865\\nNow that we have loaded our corpus, we will define a function to flatten the corpus into \\none big string of text. The following function will help us do that for a corpus of documents:\\ndef flatten_corpus(corpus):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return \\' \\'.join([document.strip()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for document in corpus])\\nWe will define a function to compute n-grams based on some input list of tokens and \\nthe parameter n, which determines the degree of the n-gram like a unigram, bigram, and \\nso on. The following code snippet computes n-grams for an input sequence:\\ndef compute_ngrams(sequence, n):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return zip(*[sequence[index:]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index in range(n)])\\nThis function basically takes in a sequence of tokens and computes a list of lists \\nhaving sequences where each list contains all items from the previous list except the \\nfirst item removed from the previous list. It constructs n such lists and then zips them all \\ntogether to give us the necessary n-grams. We can see the function in action on a sample \\nsequence in the following snippet:\\nIn [802]: compute_ngrams([1,2,3,4], 2)\\nOut[802]: [(1, 2), (2, 3), (3, 4)]\\nIn [803]: compute_ngrams([1,2,3,4], 3)\\nOut[803]: [(1, 2, 3), (2, 3, 4)]\\nThe preceding output shows bigrams and trigrams for an input sequence. We will \\nnow utilize this function and build upon it to generate the top n-grams based on their \\nfrequency of occurrence. The following code snippet helps us in getting the top n-grams:\\ndef get_top_ngrams(corpus, ngram_val=1, limit=5):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0corpus = flatten_corpus(corpus)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tokens = nltk.word_tokenize(corpus)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ngrams = compute_ngrams(tokens, ngram_val)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ngrams_freq_dist = nltk.FreqDist(ngrams)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sorted_ngrams_fd = sorted(ngrams_freq_dist.items(),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0key=itemgetter(1), reverse=True)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sorted_ngrams = sorted_ngrams_fd[0:limit]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sorted_ngrams = [(\\' \\'.join(text), freq)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for text, freq in sorted_ngrams]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return sorted_ngrams\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n228\\nWe make use of nltk\\'s FreqDist class to create a counter of all the n-grams based \\non their frequency and then we sort them based on their frequency and return the top \\nn-grams based on the specified user limit. We will now compute the top bigrams and \\ntrigrams on our corpus using the following code snippet:\\n# top 10 bigrams\\nIn [805]: get_top_ngrams(corpus=norm_alice, ngram_val=2,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0limit=10)\\nOut[805]:\\n[(u\\'said alice\\', 123),\\n\\xc2\\xa0(u\\'mock turtle\\', 56),\\n\\xc2\\xa0(u\\'march hare\\', 31),\\n\\xc2\\xa0(u\\'said king\\', 29),\\n\\xc2\\xa0(u\\'thought alice\\', 26),\\n\\xc2\\xa0(u\\'said hatter\\', 22),\\n\\xc2\\xa0(u\\'white rabbit\\', 22),\\n\\xc2\\xa0(u\\'said mock\\', 20),\\n\\xc2\\xa0(u\\'said gryphon\\', 18),\\n\\xc2\\xa0(u\\'said caterpillar\\', 18)]\\n# top 10 trigrams\\nIn [806]: get_top_ngrams(corpus=norm_alice, ngram_val=3,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0limit=10)\\nOut[806]:\\n[(u\\'said mock turtle\\', 20),\\n\\xc2\\xa0(u\\'said march hare\\', 10),\\n\\xc2\\xa0(u\\'poor little thing\\', 6),\\n\\xc2\\xa0(u\\'white kid gloves\\', 5),\\n\\xc2\\xa0(u\\'little golden key\\', 5),\\n\\xc2\\xa0(u\\'march hare said\\', 5),\\n\\xc2\\xa0(u\\'certainly said alice\\', 5),\\n\\xc2\\xa0(u\\'mock turtle said\\', 5),\\n\\xc2\\xa0(u\\'mouse mouse mouse\\', 4),\\n\\xc2\\xa0(u\\'join dance join\\', 4)]\\nThe preceding output shows sequences of two and three words generated by \\nn-grams along with the number of times they occur throughout the corpus. We can see \\nmost of the collocations point to people who are speaking something as \\xe2\\x80\\x9csaid <person>\\xe2\\x80\\x9d. \\nWe also see the people who are popular characters in \\xe2\\x80\\x9cAlice in Wonderland\\xe2\\x80\\x9d like the mock \\nturtle, the king, the rabbit, the hatter, and of course Alice herself being depicted in the \\naforementioned collocations. \\nWe will now look at nltk\\xe2\\x80\\x99s collocation finders, which enable us to find collocations \\nusing various measures like raw frequencies, pointwise mutual information, and so on. \\nJust to explain briefly, pointwise mutual information can be computed for two events or \\nterms as the logarithm of the ratio of the probability of them occurring together by the \\nproduct of their individual probabilities assuming that they are independent of each \\nother. Mathematically we can represent it like this:\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n229\\npmi x y\\np x y\\np x p y\\n,\\n,\\n(\\n) =\\n(\\n)\\n( ) ( )\\nlog\\nThis measure is symmetric. The following code snippet shows how to compute these \\ncollocations using these measures:\\n# bigrams\\nfrom nltk.collocations import BigramCollocationFinder\\nfrom nltk.collocations import BigramAssocMeasures\\nfinder = BigramCollocationFinder.from_documents([item.split()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for item\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in norm_alice])\\nbigram_measures = BigramAssocMeasures()\\n# raw frequencies\\nIn [813]: finder.nbest(bigram_measures.raw_freq, 10)\\nOut[813]:\\n[(u\\'said\\', u\\'alice\\'),\\n\\xc2\\xa0(u\\'mock\\', u\\'turtle\\'),\\n\\xc2\\xa0(u\\'march\\', u\\'hare\\'),\\n\\xc2\\xa0(u\\'said\\', u\\'king\\'),\\n\\xc2\\xa0(u\\'thought\\', u\\'alice\\'),\\n\\xc2\\xa0(u\\'said\\', u\\'hatter\\'),\\n\\xc2\\xa0(u\\'white\\', u\\'rabbit\\'),\\n\\xc2\\xa0(u\\'said\\', u\\'mock\\'),\\n\\xc2\\xa0(u\\'said\\', u\\'caterpillar\\'),\\n\\xc2\\xa0(u\\'said\\', u\\'gryphon\\')]\\n# pointwise mutual information\\nIn [814]: finder.nbest(bigram_measures.pmi, 10)\\xc2\\xa0\\xc2\\xa0\\nOut[814]: \\n[(u\\'abide\\', u\\'figures\\'),\\n\\xc2\\xa0(u\\'acceptance\\', u\\'elegant\\'),\\n\\xc2\\xa0(u\\'accounting\\', u\\'tastes\\'),\\n\\xc2\\xa0(u\\'accustomed\\', u\\'usurpation\\'),\\n\\xc2\\xa0(u\\'act\\', u\\'crawling\\'),\\n\\xc2\\xa0(u\\'adjourn\\', u\\'immediate\\'),\\n\\xc2\\xa0(u\\'adoption\\', u\\'energetic\\'),\\n\\xc2\\xa0(u\\'affair\\', u\\'trusts\\'),\\n\\xc2\\xa0(u\\'agony\\', u\\'terror\\'),\\n\\xc2\\xa0(u\\'alarmed\\', u\\'proposal\\')]\\n# trigrams\\nfrom nltk.collocations import TrigramCollocationFinder\\nfrom nltk.collocations import TrigramAssocMeasures\\nfinder = TrigramCollocationFinder.from_documents([item.split()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for item\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in norm_alice])\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n230\\ntrigram_measures = TrigramAssocMeasures()\\n# raw frequencies \\nIn [817]: finder.nbest(trigram_measures.raw_freq, 10)\\nOut[817]:\\n[(u\\'said\\', u\\'mock\\', u\\'turtle\\'),\\n\\xc2\\xa0(u\\'said\\', u\\'march\\', u\\'hare\\'),\\n\\xc2\\xa0(u\\'poor\\', u\\'little\\', u\\'thing\\'),\\n\\xc2\\xa0(u\\'little\\', u\\'golden\\', u\\'key\\'),\\n\\xc2\\xa0(u\\'march\\', u\\'hare\\', u\\'said\\'),\\n\\xc2\\xa0(u\\'mock\\', u\\'turtle\\', u\\'said\\'),\\n\\xc2\\xa0(u\\'white\\', u\\'kid\\', u\\'gloves\\'),\\n\\xc2\\xa0(u\\'beau\\', u\\'ootiful\\', u\\'soo\\'),\\n\\xc2\\xa0(u\\'certainly\\', u\\'said\\', u\\'alice\\'),\\n\\xc2\\xa0(u\\'might\\', u\\'well\\', u\\'say\\')]\\n# pointwise mutual information\\nIn [818]: finder.nbest(trigram_measures.pmi, 10)\\xc2\\xa0\\xc2\\xa0\\nOut[818]:\\n[(u\\'accustomed\\', u\\'usurpation\\', u\\'conquest\\'),\\n\\xc2\\xa0(u\\'adjourn\\', u\\'immediate\\', u\\'adoption\\'),\\n\\xc2\\xa0(u\\'adoption\\', u\\'energetic\\', u\\'remedies\\'),\\n\\xc2\\xa0(u\\'ancient\\', u\\'modern\\', u\\'seaography\\'),\\n\\xc2\\xa0(u\\'apple\\', u\\'roast\\', u\\'turkey\\'),\\n\\xc2\\xa0(u\\'arithmetic\\', u\\'ambition\\', u\\'distraction\\'),\\n\\xc2\\xa0(u\\'brother\\', u\\'latin\\', u\\'grammar\\'),\\n\\xc2\\xa0(u\\'canvas\\', u\\'bag\\', u\\'tied\\'),\\n\\xc2\\xa0(u\\'cherry\\', u\\'tart\\', u\\'custard\\'),\\n\\xc2\\xa0(u\\'circle\\', u\\'exact\\', u\\'shape\\')]\\nNow you know how to compute collocations for a corpus using an n-gram generative \\napproach. We will now look at a better way of generating key phrases based on parts of \\nspeech tagging and term weighing in the next section.\\nWeighted Tag\\xe2\\x80\\x93Based Phrase Extraction\\nHere\\xe2\\x80\\x99s a slightly different approach to extracting keyphrases. This method borrows \\nconcepts from a couple of papers, namely K. Barker and N. Cornachhia\\xe2\\x80\\x99s \\xe2\\x80\\x9cUsing Noun \\nPhrase Heads to Extract Document Keyphrases\\xe2\\x80\\x9d and \\xe2\\x80\\x9cKEA: Practical Automatic Keyphrase \\nExtraction\\xe2\\x80\\x9d by Ian Witten et\\xc2\\xa0al., which you can refer to for further details on their \\nexperimentations and approaches. We follow a two-step process in our algorithm here:\\n\\t\\n1.\\t\\nExtract all noun phrases chunks using shallow parsing\\n\\t\\n2.\\t\\nCompute TF-IDF weights for each chunk and return the top \\nweighted phrases\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n231\\nFor the first step, we will use a simple pattern based on parts of speech (POS) tags \\nto extract noun phrase chunks. You will be familiar with this from Chapter 3 where we \\nexplored chunking and shallow parsing. Before discussing our algorithm, let us define the \\ncorpus on which we will be testing our implementation. We use a sample description of \\nelephants taken from Wikipedia as shown in the following code:\\ntoy_text = \"\"\"\\nElephants are large mammals of the family Elephantidae\\nand the order Proboscidea. Two species are traditionally recognised,\\nthe African elephant and the Asian elephant. Elephants are scattered\\nthroughout sub-Saharan Africa, South Asia, and Southeast Asia. Male\\nAfrican elephants are the largest extant terrestrial animals. All\\nelephants have a long trunk used for many purposes,\\nparticularly breathing, lifting water and grasping objects. Their\\nincisors grow into tusks, which can serve as weapons and as tools\\nfor moving objects and digging. Elephants\\' large ear flaps help\\nto control their body temperature. Their pillar-like legs can\\ncarry their great weight. African elephants have larger ears\\nand concave backs while Asian elephants have smaller ears\\nand convex or level backs.\\xc2\\xa0\\xc2\\xa0\\n\"\"\"\\nNow that we have our corpus ready, we will use the pattern, \" NP: {<DT>? <JJ>* \\n<NN.*>+}\" for extracting all possible noun phrases from our corpus of documents/\\nsentences. You can always experiment with more sophisticated patterns later, \\nincorporating verb, adjective, or even adverb phrases. However, I will keep things simple \\nand concise here to focus on the core logic. Once we have our pattern, we will define a \\nfunction to parse and extract these phrases using the following snippet (we also load the \\nnecessary dependencies at this point):\\nfrom normalization import parse_document\\nimport itertools\\nimport nltk\\nfrom normalization import stopword_list\\nfrom gensim import corpora, models\\ndef get_chunks(sentences, grammar = r\\'NP: {<DT>? <JJ>* <NN.*>+}\\'):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# build chunker based on grammar pattern\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0all_chunks = []\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0chunker = nltk.chunk.regexp.RegexpParser(grammar)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for sentence in sentences:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# POS tag sentences\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tagged_sents = nltk.pos_tag_sents(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0[nltk.word_tokenize(sentence)])\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n232\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# extract chunks\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0chunks = [chunker.parse(tagged_sent)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for tagged_sent in tagged_sents]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get word, pos tag, chunk tag triples\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0wtc_sents = [nltk.chunk.tree2conlltags(chunk)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for chunk in chunks]\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0flattened_chunks = list(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0itertools.chain.from_iterable(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0wtc_sent for wtc_sent in wtc_sents)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get valid chunks based on tags\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0valid_chunks_tagged = [(status, [wtc for wtc in chunk])\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for status, chunk\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in itertools.groupby(flattened_chunks,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07lambda (word,pos,chunk): chunk \\n!= \\'O\\')]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# append words in each chunk to make phrases\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0valid_chunks = [\\' \\'.join(word.lower()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for word, tag, chunk\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in wtc_group\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if word.lower()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0not in stopword_list)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for status, wtc_group\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in valid_chunks_tagged\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if status]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# append all valid chunked phrases\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0all_chunks.append(valid_chunks)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return all_chunks\\nThe comments in the preceding function are self-explanatory. Basically, we have a \\ndefined grammar pattern for chunking or extracting noun phrases. We define a chunker \\nover the same pattern, and for each sentence in the document, we first annotate it with \\nits POS tags (hence, we should not normalize the text) and then build a shallow parse tree \\nwith noun phrases as the chunks and all other POS tag\\xe2\\x80\\x93based words as chinks, which \\nare not parts of any chunks. Once this is done, we use the tree2conlltags function to \\ngenerate (w,t,c) triples, which are words, POS tags, and the IOB-formatted chunk tags \\ndiscussed in Chapter 3. We remove all tags with chunk tag of \\'O\\' since they are basically \\nwords or terms that do not belong to any chunk (if you remember our discussion of \\nshallow parsing in Chapter 3). Finally, from these valid chunks, we combine the chunked \\nterms to generate phrases from each chunk group. We can see this function in action on \\nour corpus in the following snippet:\\nsentences = parse_document(toy_text)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\nvalid_chunks = get_chunks(sentences)\\n# print all valid chunks\\nIn [834]: print valid_chunks\\n\\xc2\\xa0[[\\'elephants\\', \\'large mammals\\', \\'family elephantidae\\', \\'order \\nproboscidea\\'], [\\'species\\', \\'african elephant\\', \\'asian elephant\\'], \\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n233\\n[\\'elephants\\', \\'sub-saharan africa\\', \\'south asia\\', \\'southeast asia\\'], \\n[\\'male african elephants\\', \\'extant terrestrial animals\\'], [\\'elephants\\', \\n\\'long trunk\\', \\'many purposes\\', \\'breathing\\', \\'water\\', \\'grasping objects\\'], \\n[\\'incisors\\', \\'tusks\\', \\'weapons\\', \\'tools\\', \\'objects\\', \\'digging\\'], \\n[\\'elephants\\', \\'large ear flaps\\', \\'body temperature\\'], [\\'pillar-like legs\\', \\n\\'great weight\\'], [\\'african elephants\\', \\'ears\\', \\'backs\\', \\'asian elephants\\', \\n\\'ears\\', \\'convex\\', \\'level backs\\']]\\nThe preceding output shows all the valid keyphrases per sentence of our document. \\nYou can already see, since we targeted noun phrases, all phrases talk about noun based \\nentities. We will now build on top of our get_chunks() function by implementing the \\nnecessary logic for step 2, where we will build a TF-IDF\\xe2\\x80\\x93based model on our keyphrases \\nusing gensim and then compute TF-IDF\\xe2\\x80\\x93based weights for each keyphrase based on its \\noccurrence in the corpus. Finally, we will sort these keyphrases based on their TF-IDF \\nweights and show the top n keyphrases where n is specified by the user:\\ndef get_tfidf_weighted_keyphrases(sentences,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0grammar=r\\'NP: {<DT>? <JJ>* <NN.*>+}\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_n=10):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get valid chunks\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0valid_chunks = get_chunks(sentences, grammar=grammar)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# build tf-idf based model\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0dictionary = corpora.Dictionary(valid_chunks)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tfidf = models.TfidfModel(corpus)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0corpus_tfidf = tfidf[corpus]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get phrases and their tf-idf weights\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0weighted_phrases = {dictionary.get(id): round(value,3)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for doc in corpus_tfidf\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for id, value in doc}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0weighted_phrases = sorted(weighted_phrases.items(),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0key=itemgetter(1), reverse=True)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# return top weighted phrases\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return weighted_phrases[:top_n]\\nWe can now test this function on our toy corpus from before by using the following \\ncode snippet to generate the top ten keyphrases:\\n# top 10 tf-idf weighted keyphrases for toy_text\\nIn [836]: get_tfidf_weighted_keyphrases(sentences, top_n=10)\\nOut[836]:\\n[(u\\'pillar-like legs\\', 0.707),\\n\\xc2\\xa0(u\\'male african elephants\\', 0.707),\\n\\xc2\\xa0(u\\'great weight\\', 0.707),\\n\\xc2\\xa0(u\\'extant terrestrial animals\\', 0.707),\\n\\xc2\\xa0(u\\'large ear flaps\\', 0.684),\\n\\xc2\\xa0(u\\'body temperature\\', 0.684),\\n\\xc2\\xa0(u\\'ears\\', 0.667),\\n\\xc2\\xa0(u\\'species\\', 0.577),\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n234\\n\\xc2\\xa0(u\\'african elephant\\', 0.577),\\n\\xc2\\xa0(u\\'asian elephant\\', 0.577)]\\nInterestingly we see various types of elephants being depicted in the keyphrases, \\nlike Asian and African elephants, and also typical attributes of elephants like \"great \\nweight\", \"large ear flaps\", and \"pillar like legs\". Thus you can get an idea of \\nhow keyphrase extraction can extract key important concepts from text documents and \\nsummarize them. Try out these functions on other corpora to see interesting results!\\nTopic Modeling\\nWe have seen how keyphrases can be extracted using a couple of techniques. Though \\nthese phrases point out key pivotal points from a document or corpus, it is simplistic and \\noften does not portray the various themes or concepts in a corpus, particularly when we \\nhave different distinguishing themes or concepts in a corpus of documents. Topic models \\nhave been designed specifically for the purpose of extracting various distinguishing \\nconcepts or topics from a large corpus containing various types of documents, where \\neach document talks about one or more concepts. These concepts can be anything \\nfrom thoughts to opinions, facts, outlooks, statements, and so on. The main aim of topic \\nmodeling is to use mathematical and statistical techniques to discover hidden and latent \\nsemantic structures in a corpus.\\nTopic modeling involves extracting features from document terms and using \\nmathematical structures and frameworks like matrix factorization and SVD to generate \\nclusters or groups of terms that are distinguishable from each other, and these cluster of \\nwords form topics or concepts. These concepts can be used to interpret the main themes \\nof a corpus and also make semantic connections among words that co-occur together \\nfrequently in various documents. There are various frameworks and algorithms to build \\ntopic models. We will cover the following three methods:\\n\\xe2\\x80\\xa2 \\nLatent semantic indexing\\n\\xe2\\x80\\xa2 \\nLatent Dirichlet\\xc2\\xa0allocation\\n\\xe2\\x80\\xa2 \\nNon-negative matrix factorization\\nThe first two methods are quite popular and have been around a long time. The last \\ntechnique, non-negative matrix factorization, is a very recent technique that is extremely \\neffective and gives excellent results. We will leverage gensim and scikit-learn for our \\npractical implementations and also look at how to build our own topic model based on \\nlatent semantic indexing. This will give you an idea of how these techniques work and \\nalso how to convert mathematical frameworks into practical implementations. We will \\nuse the following toy corpus initially to test our topic models:\\ntoy_corpus = [\"The fox jumps over the dog\",\\n\"The fox is very clever and quick\",\\n\"The dog is slow and lazy\",\\n\"The cat is smarter than the fox and the dog\",\\n\"Python is an excellent programming language\",\\n\"Java and Ruby are other programming languages\",\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n235\\n\"Python and Java are very popular programming languages\",\\n\"Python programs are smaller than Java programs\"]\\nYou can see that we have eight documents in the preceding corpus: the first four \\ntalk about various animals, and the last four are about programming languages. Thus \\nthis shows that there are two distinct topics in the corpus. We generalized that using \\nour brains, but the following sections will try to extract that same information using \\ncomputational methods. Once we build some topic modeling frameworks, we will use the \\nsame to generate topics on real product reviews from Amazon.\\nLatent Semantic Indexing\\nOur first technique is latent semantic indexing (LSI), which has been around since the \\n1970s when it was first developed as a statistical technique to correlate and find out \\nsemantically linked terms from corpora. LSI is not just used for text summarization \\nbut also in information retrieval and search. LSI uses the very popular SVD technique \\ndiscussed earlier in the \\xe2\\x80\\x9cImportant Concepts\\xe2\\x80\\x9d section. The main principle behind LSI is \\nthat similar terms tend to be used in the same context and hence tend to co-occur more. \\nThe term LSI comes from the fact that this technique has the ability to uncover latent \\nhidden terms which correlate semantically to form topics.\\nWe will now try to implement an LSI by leveraging gensim and extract topics from the \\ntoy corpus. To start, we load the necessary dependencies and normalize the toy corpus \\nusing the following code snippet:\\nfrom gensim import corpora, models\\nfrom normalization import normalize_corpus\\nimport numpy as np\\nnorm_tokenized_corpus = normalize_corpus(toy_corpus, tokenize=True)\\n# view the normalized tokenized corpus\\nIn [841]: norm_tokenized_corpus\\nOut[841]:\\n[[u\\'fox\\', u\\'jump\\', u\\'dog\\'],\\n\\xc2\\xa0[u\\'fox\\', u\\'clever\\', u\\'quick\\'],\\n\\xc2\\xa0[u\\'dog\\', u\\'slow\\', u\\'lazy\\'],\\n\\xc2\\xa0[u\\'cat\\', u\\'smarter\\', u\\'fox\\', u\\'dog\\'],\\n\\xc2\\xa0[u\\'python\\', u\\'excellent\\', u\\'programming\\', u\\'language\\'],\\n\\xc2\\xa0[u\\'java\\', u\\'ruby\\', u\\'programming\\', u\\'language\\'],\\n\\xc2\\xa0[u\\'python\\', u\\'java\\', u\\'popular\\', u\\'programming\\', u\\'language\\'],\\n\\xc2\\xa0[u\\'python\\', u\\'program\\', u\\'small\\', u\\'java\\', u\\'program\\']]\\nWe now build a dictionary or vocabulary, which gensim uses to map each unique \\nterm into a numeric value. Once built, we convert the preceding tokenized corpus into \\na numeric Bag of Words vector representation where each term and its frequency in a \\nsentence is depicted by a tuple (term, frequency), as seen in the following snippet:\\n# build the dictionary\\ndictionary = corpora.Dictionary(norm_tokenized_corpus)\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n236\\n# view the dictionary mappings\\nIn [846]: print dictionary.token2id\\n{u\\'program\\': 17, u\\'lazy\\': 5, u\\'clever\\': 4, u\\'java\\': 13, u\\'programming\\': 10, \\nu\\'language\\': 11, u\\'python\\': 9, u\\'smarter\\': 7, u\\'fox\\': 1, u\\'dog\\': 2, u\\'cat\\': \\n8, u\\'jump\\': 0, u\\'popular\\': 15, u\\'slow\\': 6, u\\'excellent\\': 12, u\\'quick\\': 3, \\nu\\'small\\': 16, u\\'ruby\\': 14}\\n# convert tokenized documents into bag of words vectors\\ncorpus = [dictionary.doc2bow(text) for text in norm_tokenized_corpus]\\n# view the converted vectorized corpus\\nIn [849]: corpus\\nOut[849]: \\n[[(0, 1), (1, 1), (2, 1)],\\n\\xc2\\xa0[(1, 1), (3, 1), (4, 1)],\\n\\xc2\\xa0[(2, 1), (5, 1), (6, 1)],\\n\\xc2\\xa0[(1, 1), (2, 1), (7, 1), (8, 1)],\\n\\xc2\\xa0[(9, 1), (10, 1), (11, 1), (12, 1)],\\n\\xc2\\xa0[(10, 1), (11, 1), (13, 1), (14, 1)],\\n\\xc2\\xa0[(9, 1), (10, 1), (11, 1), (13, 1), (15, 1)],\\n\\xc2\\xa0[(9, 1), (13, 1), (16, 1), (17, 2)]]\\nWe will now build a TF-IDF\\xe2\\x80\\x93weighted model over this corpus where each term in \\neach document will contain its TF-IDF weight. This is analogous to feature extraction or \\nvector space transformation where each document is represented by a TF-IDF vector of \\nits terms, as we have done in the past. Once this is done, we build an LSI model on these \\nfeatures and take an input of the number of topics we want to generate. This number \\nis based on intuition and trial and error, so feel free to play around with this parameter \\nwhen you build topic models on corpora. We will set this parameter to 2, based on the \\nnumber of topics we expect our toy corpus to contain:\\n# build tf-idf feature vectors\\ntfidf = models.TfidfModel(corpus)\\ncorpus_tfidf = tfidf[corpus]\\n# fix the number of topics\\ntotal_topics = 2\\n# build the topic model\\nlsi = models.LsiModel(corpus_tfidf,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0id2word=dictionary,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_topics=total_topics)\\nNow that our topic modeling framework is built, we can see the generated topics in \\nthe following code snippet:\\nIn [855]: for index, topic in lsi.print_topics(total_topics):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Topic #\\'+str(index+1)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print topic\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print\\xc2\\xa0\\xc2\\xa0\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n237\\nTopic #1\\n-0.459*\"language\" + -0.459*\"programming\" + -0.344*\"java\" + -0.344*\"python\" + \\n-0.336*\"popular\" + -0.318*\"excellent\" + -0.318*\"ruby\" + -0.148*\"program\" + \\n-0.074*\"small\" + -0.000*\"clever\"\\nTopic #2\\n0.459*\"dog\" + 0.459*\"fox\" + 0.444*\"jump\" + 0.322*\"smarter\" + 0.322*\"cat\" + \\n0.208*\"lazy\" + 0.208*\"slow\" + 0.208*\"clever\" + 0.208*\"quick\" + -0.000*\"ruby\"\\nLet\\xe2\\x80\\x99s take a moment to understand those results. At first, ignoring the weights, \\nyou can see that the first topic contains terms related to programming languages and \\nthe second topic contains terms related to animals, which is in line with the main two \\nconcepts from our toy corpus mentioned earlier. If you now look at the weights, higher \\nweightage and same sign exists for the terms that contribute toward each of the topics. \\nThe first topic has related terms with negative weights, and the second topic has related \\nterms with positive weights. The sign just indicates the direction of the topic, that is, \\nsimilar correlated terms in the topics will have the same sign or direction. The following \\nfunction helps display the topics in a better way with or without thresholds:\\ndef print_topics_gensim(topic_model, total_topics=1,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0weight_threshold=0.0001,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=False,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_terms=None):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index in range(total_topics):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0topic = topic_model.show_topic(index)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0topic = [(word, round(wt,2))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for word, wt in topic\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if abs(wt) >= weight_threshold]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if display_weights:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Topic #\\'+str(index+1)+\\' with weights\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print topic[:num_terms] if num_terms else topic\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Topic #\\'+str(index+1)+\\' without weights\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tw = [term for term, wt in topic]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print tw[:num_terms] if num_terms else tw\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print\\nWe can try out this function on our toy corpus topic model using the following \\nsnippet to see how we can get the topics and play around with the parameters:\\n# print topics without weights\\nIn [860]: print_topics_gensim(topic_model=lsi,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_terms=5,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=False)\\nTopic #1 without weights\\n[u\\'language\\', u\\'programming\\', u\\'java\\', u\\'python\\', u\\'popular\\']\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n238\\nTopic #2 without weights\\n[u\\'dog\\', u\\'fox\\', u\\'jump\\', u\\'smarter\\', u\\'cat\\']\\n# print topics with their weights\\nIn [861]: print_topics_gensim(topic_model=lsi,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_terms=5,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=True)\\nTopic #1 with weights\\n[(u\\'language\\', -0.46), (u\\'programming\\', -0.46), (u\\'java\\', -0.34), \\n(u\\'python\\', -0.34), (u\\'popular\\', -0.34)]\\nTopic #2 with weights\\n[(u\\'dog\\', 0.46), (u\\'fox\\', 0.46), (u\\'jump\\', 0.44), (u\\'smarter\\', 0.32), \\n(u\\'cat\\', 0.32)]\\nWe have successfully built a topic modeling framework using LSI that can distinguish \\nand show topics from a corpus of documents. Now we will use SVD to build our own LSI \\ntopic model framework from the ground up using the mathematical concepts discussed \\nat the beginning of this chapter. We will start by building a TF-IDF feature matrix, which \\nis actually a document-term matrix (if you remember from our classification exercise in \\nChapter 4). We will transpose this to form a term-document matrix before computing \\nSVD using the following snippet. Besides this, we also fix the number of topics we want \\nto generate and extract the term names from the features so we can map them with their \\nweights:\\nfrom utils import build_feature_matrix, low_rank_svd\\n# build the term document tf-idf weighted matrix\\nnorm_corpus = normalize_corpus(toy_corpus)\\nvectorizer, tfidf_matrix = build_feature_matrix(norm_corpus,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type=\\'tfidf\\')\\ntd_matrix = tfidf_matrix.transpose()\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\ntd_matrix = td_matrix.multiply(td_matrix > 0)\\n# fix total topics and get the terms used in the term-document matrix\\ntotal_topics = 2\\nfeature_names = vectorizer.get_feature_names()\\nOnce this is done, we compute the SVD for our term-document matrix using our low_\\nrank_svd() function such that we build a low ranked matrix approximation taking only the \\ntop k singular vectors, which will be equal to our number of topics in this case. Using the \\nS and U components, we multiply them together to generate each term and its weightage \\nper topic giving us the necessary weights per topic similar to what you saw earlier:\\nu, s, vt = low_rank_svd(td_matrix, singular_count=total_topics)\\nweights = u.transpose() * s[:, None]\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n239\\nNow that we have our term weights, we need to connect them back to our terms. We \\ndefine two utility functions for generating these topics by connecting the terms with their \\nweights and then printing these topics using a function with configurable parameters:\\n# get topics with their terms and weights\\ndef get_topics_terms_weights(weights, feature_names):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_names = np.array(feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sorted_indices = np.array([list(row[::-1])\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for row\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in np.argsort(np.abs(weights))])\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sorted_weights = np.array([list(wt[index])\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for wt, index\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in zip(weights,sorted_indices)])\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sorted_terms = np.array([list(feature_names[row])\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for row\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in sorted_indices])\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0topics = [np.vstack((terms.T,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0term_weights.T)).T\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for terms, term_weights\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in zip(sorted_terms, sorted_weights)]\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return topics\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n# print all the topics from a corpus\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\ndef print_topics_udf(topics, total_topics=1,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0weight_threshold=0.0001,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=False,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_terms=None):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index in range(total_topics):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0topic = topics[index]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0topic = [(term, float(wt))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for term, wt in topic]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0topic = [(word, round(wt,2))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for word, wt in topic\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if abs(wt) >= weight_threshold]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if display_weights:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Topic #\\'+str(index+1)+\\' with weights\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print topic[:num_terms] if num_terms else topic\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Topic #\\'+str(index+1)+\\' without weights\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tw = [term for term, wt in topic]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print tw[:num_terms] if num_terms else tw\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n240\\nWe are now ready to see our function in action. The following snippet utilizes the \\npreviously defined functions to generate topics using our LSI implementation using SVD \\nby connecting the terms with their weights for each topic:\\nIn [871]: topics = get_topics_terms_weights(weights, feature_names)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print_topics_udf(topics=topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0weight_threshold=0,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=True)\\nTopic #1 with weights\\n[(u\\'dog\\', 0.72), (u\\'fox\\', 0.72), (u\\'jump\\', 0.43), (u\\'smarter\\', 0.34), \\n(u\\'cat\\', 0.34), (u\\'slow\\', 0.23), (u\\'lazy\\', 0.23), (u\\'quick\\', 0.23), \\n(u\\'clever\\', 0.23), (u\\'program\\', 0.0), (u\\'java\\', 0.0), (u\\'excellent\\', -0.0), \\n(u\\'small\\', 0.0), (u\\'popular\\', 0.0), (u\\'python\\', 0.0), (u\\'programming\\', \\n-0.0), (u\\'language\\', -0.0), (u\\'ruby\\', 0.0)]\\nTopic #2 with weights\\n[(u\\'programming\\', -0.73), (u\\'language\\', -0.73), (u\\'python\\', -0.56), \\n(u\\'java\\', -0.56), (u\\'popular\\', -0.34), (u\\'ruby\\', -0.33), (u\\'excellent\\', \\n-0.33), (u\\'program\\', -0.21), (u\\'small\\', -0.11), (u\\'fox\\', 0.0), (u\\'dog\\', \\n0.0), (u\\'jump\\', 0.0), (u\\'clever\\', 0.0), (u\\'quick\\', 0.0), (u\\'lazy\\', 0.0), \\n(u\\'slow\\', 0.0), (u\\'smarter\\', 0.0), (u\\'cat\\', 0.0)]\\nFrom the preceding output we see that both topics have all the terms, but notice \\nthe weights more minutely. Do you see any difference? Of course, the terms in topic one \\nrelated to programming have zero value, indicating they do not contribute to the topic at \\nall. Let us put a proper threshold and get only the relevant terms per topic as follows:\\n# applying a scoring threshold\\nIn [874]: topics = get_topics_terms_weights(weights, feature_names)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print_topics_udf(topics=topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0weight_threshold=0.15,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=True)\\nTopic #1 with weights\\n[(u\\'dog\\', 0.72), (u\\'fox\\', 0.72), (u\\'jump\\', 0.43), (u\\'smarter\\', 0.34), \\n(u\\'cat\\', 0.34), (u\\'slow\\', 0.23), (u\\'lazy\\', 0.23), (u\\'quick\\', 0.23), \\n(u\\'clever\\', 0.23)]\\nTopic #2 with weights\\n[(u\\'programming\\', -0.73), (u\\'language\\', -0.73), (u\\'python\\', -0.56), \\n(u\\'java\\', -0.56), (u\\'popular\\', -0.34), (u\\'ruby\\', -0.33), (u\\'excellent\\', \\n-0.33), (u\\'program\\', -0.21)]\\n\\xc2\\xa0In [875]: topics = get_topics_terms_weights(weights, feature_names)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print_topics_udf(topics=topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0weight_threshold=0.15,\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n241\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=False)\\nTopic #1 without weights\\n[u\\'dog\\', u\\'fox\\', u\\'jump\\', u\\'smarter\\', u\\'cat\\', u\\'slow\\', u\\'lazy\\', u\\'quick\\', \\nu\\'clever\\']\\nTopic #2 without weights\\n[u\\'programming\\', u\\'language\\', u\\'python\\', u\\'java\\', u\\'popular\\', u\\'ruby\\', \\nu\\'excellent\\', u\\'program\\']\\nThis gives us much better depiction of the topics, similar to the ones obtained \\nearlier, where each topic clearly has distinguishable concepts from the other. Thus you \\ncan see how simple matrix computations helped us in implementing a powerful topic \\nmodel framework! We define the following function as a generic reusable topic modeling \\nframework using LSI:\\ndef train_lsi_model_gensim(corpus, total_topics=2):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0norm_tokenized_corpus = normalize_corpus(corpus, tokenize=True)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0dictionary = corpora.Dictionary(norm_tokenized_corpus)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0mapped_corpus = [dictionary.doc2bow(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for text in norm_tokenized_corpus]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tfidf = models.TfidfModel(mapped_corpus)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0corpus_tfidf = tfidf[mapped_corpus]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0lsi = models.LsiModel(corpus_tfidf,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0id2word=dictionary,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_topics=total_topics)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return lsi\\nWe will use the preceding function later to extract topics from product reviews. Let us \\nnow look at the next technique to build topic models using latent Dirichlet\\xc2\\xa0allocation.\\nLatent Dirichlet Allocation\\nThe latent Dirichlet\\xc2\\xa0allocation (LDA) technique is a generative probabilistic model where \\neach document is assumed to have a combination of topics similar to a probabilistic \\nlatent semantic indexing model\\xe2\\x80\\x94but in this case, the latent topics contain a Dirichlet \\nprior over them. The math behind in this technique is pretty involved, so I will try \\nto summarize it because going it specific detail would be out of the current scope. I \\nrecommend readers to go through this excellent talk by Christine Doig available at \\nhttp://chdoig.github.io/pygotham-topic-modeling/#/, from which we will be \\nborrowing some excellent pictorial representations. The plate notation for the LDA model \\nis depicted in Figure\\xc2\\xa05-2.\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n242\\nFigure\\xc2\\xa05-3 shows a good representation of how each of the parameters connects back \\nto the text documents and terms. It is assumed that we have M documents, N number of \\nwords in the documents, and K total number of topics we want to generate.\\nFigure 5-2.\\xe2\\x80\\x82 LDA plate notation (courtesy of C. Doig, Introduction to Topic Modeling in \\nPython)\\nFigure 5-3.\\xe2\\x80\\x82 End-to-end LDA framework (courtesy of C. Doig, Introduction to Topic \\nModeling in Python)\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n243\\nThe black box in the figure represents the core algorithm that makes use of the previously \\nmentioned parameters to extract K topics from the documents. The following steps give a very \\nsimplistic explanation of what happens in the algorithm for everyone\\'s benefit:\\n\\t\\n1.\\t\\nInitialize the necessary parameters.\\n\\t\\n2.\\t\\nFor each document, randomly initialize each word to one of \\nthe K topics.\\n\\t\\n3.\\t\\nStart an iterative process as follows and repeat it several times.\\n\\t\\n4.\\t\\nFor each document D:\\n\\t\\na.\\t\\nFor each word W in document:\\n\\xe2\\x80\\xa2 \\nFor each topic T:\\n\\xe2\\x80\\xa2 \\nCompute P T D\\n|\\n(\\n) , which is proportion of words in  \\nD assigned to topic T.\\n\\xe2\\x80\\xa2 \\nCompute P W T\\n|\\n(\\n) , which is proportion of \\nassignments to topic T over all documents having \\nthe word W.\\n\\xe2\\x80\\xa2 \\nReassign word W with topic T with probability \\nP T D\\nP W T\\n|\\n|\\n(\\n)\\xc2\\xb4 (\\n)  considering all other words and \\ntheir topic assignments.\\nOnce this runs for several iterations, we should have topic mixtures for each document \\nand then generate the constituents of each topic from the terms that point to that topic. We \\nuse gensim in the following implementation to build an LDA-based topic model:\\ndef train_lda_model_gensim(corpus, total_topics=2):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0norm_tokenized_corpus = normalize_corpus(corpus, tokenize=True)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0dictionary = corpora.Dictionary(norm_tokenized_corpus)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0mapped_corpus = [dictionary.doc2bow(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for text in norm_tokenized_corpus]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tfidf = models.TfidfModel(mapped_corpus)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0corpus_tfidf = tfidf[mapped_corpus]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0lda = models.LdaModel(corpus_tfidf,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0id2word=dictionary,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0iterations=1000,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_topics=total_topics)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return lda\\xc2\\xa0\\xc2\\xa0\\n# use the function to generate topics on toy corpus\\nIn [922]: lda_gensim = train_lda_model_gensim(toy_corpus,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print_topics_gensim(topic_model=lda_gensim,\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n244\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=2,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_terms=5,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=True)\\xc2\\xa0\\xc2\\xa0\\nTopic #1 with weights\\n[(u\\'fox\\', 0.08), (u\\'dog\\', 0.08), (u\\'jump\\', 0.07), (u\\'clever\\', 0.07), \\n(u\\'quick\\', 0.07)]\\nTopic #2 with weights\\n[(u\\'programming\\', 0.08), (u\\'language\\', 0.08), (u\\'java\\', 0.07), (u\\'python\\', \\n0.07), (u\\'ruby\\', 0.07)]\\nYou can play around with various model parameters in the LdaModel class, which \\nbelongs to gensim\\'s ldamodel module. This implementation works best with a corpus \\nthat has many documents. We see how the concepts are quite distinguishing across the \\ntwo topics just as before, but note in this case the weights are positive, making it easier \\nto interpret than LSI. Even scikit-learn has finally included an LDA-based topic model \\nimplementation in its library. The following snippet makes use of the same to build an \\nLDA topic model:\\nfrom sklearn.decomposition import LatentDirichletAllocation\\n# get tf-idf based features\\nnorm_corpus = normalize_corpus(toy_corpus)\\nvectorizer, tfidf_matrix = build_feature_matrix(norm_corpus,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type=\\'tfidf\\')\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n# build LDA model\\ntotal_topics = 2\\nlda = LatentDirichletAllocation(n_topics=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0max_iter=100,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0learning_method=\\'online\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0learning_offset=50.,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0random_state=42)\\nlda.fit(tfidf_matrix)\\n# get terms and their weights\\nfeature_names = vectorizer.get_feature_names()\\nweights = lda.components_\\n# generate topics from their terms and weights\\ntopics = get_topics_terms_weights(weights, feature_names)\\nIn that snippet, the LDA model is applied on the document-term TF-IDF feature \\nmatrix, which is decomposed into two matrices, namely a document-topic matrix and a \\ntopic-term matrix. We use the topic-term matrix stored in lda.components_ to retrieve \\nthe weights for each term per topic. Once we have these weights, we use our get_topics_\\nterms_weights() function from our LSI modeling to build the topics based on the \\nterms and weights per topic. We can now view the topics using our print_topics_udf() \\nfunction, which we implemented earlier:\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n245\\nIn [926]: topics = get_topics_terms_weights(weights, feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print_topics_udf(topics=topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_terms=8,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=True)\\nTopic #1 with weights\\n[(u\\'fox\\', 1.86), (u\\'dog\\', 1.86), (u\\'jump\\', 1.19), (u\\'clever\\', 1.12), \\n(u\\'quick\\', 1.12), (u\\'lazy\\', 1.12), (u\\'slow\\', 1.12), (u\\'cat\\', 1.06)]\\nTopic #2 with weights\\n[(u\\'programming\\', 1.8), (u\\'language\\', 1.8), (u\\'java\\', 1.64), (u\\'python\\', \\n1.64), (u\\'program\\', 1.3), (u\\'ruby\\', 1.11), (u\\'excellent\\', 1.11), \\n(u\\'popular\\', 1.06)]\\nWe can now see similar results for the two topics with distinguishable concepts \\nwhere the first topic is about the animals and their characteristics from the first four \\ndocuments and the second topic is all about programming languages and their attributes \\nfrom the last four documents.\\nNon-negative Matrix Factorization\\nThe last technique we will look at is non-negative matrix factorization (NNMF), which is \\nanother matrix decomposition technique similar to SVD, though NNMF operates on non-\\nnegative matrices and works well for multivariate data. NNMF can be formally defined \\nlike so: Given a non-negative matrix V, the objective is to find two non-negative matrix \\nfactors W and H such that when they are multiplied, they can approximately reconstruct \\nV. Mathematically this is represented by \\nV\\nWH\\n\\xc2\\xbb\\nsuch that all three matrices are non-negative. To get to this approximation, we usually \\nuse a cost function like the Euclidean distance or L2 norm between two matrices, or the \\nFrobenius norm which is a slight modification of the L2 norm. This can be represented as\\nargmin\\nW H\\nV\\nWH\\n,\\n1\\n2\\n2\\n-\\nwhere we have our three non-negative matrices V, W, and H. This can be further \\nsimplified as follows:\\n1\\n2\\n2\\ni j\\nij\\nij\\nV\\nWH\\n,\\xc3\\xa5\\n-\\n(\\n)\\nThis implementation is available in the NMF class in the scikit-learn \\ndecomposition module that we will be using in the section.\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n246\\nWe can build an NNMF-based topic model using the following snippet on our toy \\ncorpus which gives us the feature names and their weights just like in LDA:\\nfrom sklearn.decomposition import NMF\\n# build tf-idf document-term matrix\\nnorm_corpus = normalize_corpus(toy_corpus)\\nvectorizer, tfidf_matrix = build_feature_matrix(norm_corpus,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type=\\'tfidf\\')\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n# build topic model\\ntotal_topics = 2\\nnmf = NMF(n_components=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0random_state=42, alpha=.1, l1_ratio=.5)\\nnmf.fit(tfidf_matrix)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n# get terms and their weights\\nfeature_names = vectorizer.get_feature_names()\\nweights = nmf.components_\\nNow that we have our terms and their weights, we can use our defined functions \\nfrom before to print the topics as follows:\\nIn [928]: topics = get_topics_terms_weights(weights, feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print_topics_udf(topics=topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_terms=None,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=True)\\nTopic #1 with weights\\n[(u\\'programming\\', 0.55), (u\\'language\\', 0.55), (u\\'python\\', 0.4), (u\\'java\\', \\n0.4), (u\\'popular\\', 0.24), (u\\'ruby\\', 0.23), (u\\'excellent\\', 0.23), \\n(u\\'program\\', 0.09), (u\\'small\\', 0.03)]\\nTopic #2 with weights\\n[(u\\'dog\\', 0.57), (u\\'fox\\', 0.57), (u\\'jump\\', 0.35), (u\\'smarter\\', 0.26), \\n(u\\'cat\\', 0.26), (u\\'quick\\', 0.13), (u\\'slow\\', 0.13), (u\\'clever\\', 0.13), \\n(u\\'lazy\\', 0.13)]\\nWhat we have observed is that non-negative matrix factorization works the best even \\nwith small corpora with few documents compared to the other methods, but again, this \\ndepends on the type of data you are dealing with.\\nExtracting Topics from Product Reviews\\nWe will now utilize our earlier functions and build topic models using the three \\ntechniques on some real-world data. For this, I have extracted some reviews for a \\nparticular product from Amazon. Data enthusiasts can get more information about the \\nsource of this data from http://jmcauley.ucsd.edu/data/amazon/, which contains \\nvarious product reviews based on product types and categories. The product of our \\ninterest is the very popular video game The Elder Scrolls V: Skyrim developed by Bethesda \\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n247\\nSoftworks. It is perhaps one of the best role-playing games out there. (You can view the \\nproduct information and its reviews on Amazon at www.amazon.com/dp/B004HYK956 if \\nyou are interested.) In our case, the extracted reviews are available in a CSV file named \\namazon_skyrim_reviews.csv, available along with the code files of this chapter. Let us \\nfirst load the reviews before extracting topics:\\nimport pandas as pd\\nimport numpy as np\\n# load reviews\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\nCORPUS = pd.read_csv(\\'amazon_skyrim_reviews.csv\\')\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\nCORPUS = np.array(CORPUS[\\'Reviews\\'])\\n# view sample review\\nIn [946]: print CORPUS[12]\\nI base the value of a game on the amount of enjoyable gameplay I can get out \\nof it and this one was definitely worth the price!\\nNow that we have our corpus of product reviews loaded, let us set the number of \\ntopics to 5 and extract topics using all the three techniques implemented in the earlier \\nsections. The following code snippet achieves the same:\\n# set number of topics\\ntotal_topics = 5\\n# Technique 1: Latent Semantic Indexing\\nIn [958]: lsi_gensim = train_lsi_model_gensim(CORPUS,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=total_topics)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print_topics_gensim(topic_model=lsi_gensim,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_terms=10,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=False)\\nTopic #1 without weights\\n[u\\'skyrim\\', u\\'one\\', u\\'quest\\', u\\'like\\', u\\'play\\', u\\'oblivion\\', u\\'go\\', u\\'get\\', \\nu\\'time\\', u\\'level\\']\\nTopic #2 without weights\\n[u\\'recommend\\', u\\'love\\', u\\'ever\\', u\\'best\\', u\\'great\\', u\\'level\\', u\\'highly\\', \\nu\\'play\\', u\\'elder\\', u\\'scroll\\']\\nTopic #3 without weights\\n[u\\'recommend\\', u\\'highly\\', u\\'fun\\', u\\'love\\', u\\'ever\\', u\\'wonderful\\', u\\'best\\', \\nu\\'everyone\\', u\\'series\\', u\\'scroll\\']\\nTopic #4 without weights\\n[u\\'fun\\', u\\'scroll\\', u\\'elder\\', u\\'recommend\\', u\\'highly\\', u\\'wonderful\\', u\\'fan\\', \\nu\\'graphic\\', u\\'series\\', u\\'cool\\']\\nTopic #5 without weights\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n248\\n[u\\'fun\\', u\\'love\\', u\\'elder\\', u\\'scroll\\', u\\'highly\\', u\\'5\\', u\\'dont\\', u\\'hour\\', \\nu\\'series\\', u\\'hundred\\']\\n# Technique 2a: Latent Dirichlet Allocation (gensim)\\nIn [959]: lda_gensim = train_lda_model_gensim(CORPUS,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=total_topics)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print_topics_gensim(topic_model=lda_gensim,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_terms=10,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=False)\\nTopic #1 without weights\\n[u\\'quest\\', u\\'good\\', u\\'skyrim\\', u\\'love\\', u\\'make\\', u\\'best\\', u\\'time\\', u\\'go\\', \\nu\\'play\\', u\\'every\\']\\nTopic #2 without weights\\n[u\\'good\\', u\\'play\\', u\\'get\\', u\\'really\\', u\\'like\\', u\\'one\\', u\\'hour\\', u\\'buy\\', \\nu\\'go\\', u\\'skyrim\\']\\nTopic #3 without weights\\n[u\\'fun\\', u\\'gameplay\\', u\\'skyrim\\', u\\'best\\', u\\'want\\', u\\'time\\', u\\'one\\', u\\'play\\', \\nu\\'review\\', u\\'like\\']\\nTopic #4 without weights\\n[u\\'love\\', u\\'play\\', u\\'one\\', u\\'much\\', u\\'great\\', u\\'ever\\', u\\'like\\', u\\'fun\\', \\nu\\'recommend\\', u\\'level\\']\\nTopic #5 without weights\\n[u\\'great\\', u\\'long\\', u\\'love\\', u\\'scroll\\', u\\'elder\\', u\\'oblivion\\', u\\'play\\', \\nu\\'month\\', u\\'never\\', u\\'skyrim\\']\\n# Technique 2b: Latent Dirichlet Allocation (scikit-learn)\\nIn [960]: norm_corpus = normalize_corpus(CORPUS)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: vectorizer, tfidf_matrix = build_feature_matrix(norm_corpus,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type=\\'tfidf\\')\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: feature_names = vectorizer.get_feature_names()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: lda = LatentDirichletAllocation(n_topics=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0max_iter=100,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0learning_method=\\'online\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0learning_offset=50.,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0random_state=42)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: lda.fit(tfidf_matrix)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: weights = lda.components_\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: topics = get_topics_terms_weights(weights, feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print_topics_udf(topics=topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_terms=10,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=False)\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n249\\nTopic #1 without weights\\n[u\\'statrs\\', u\\'expression\\', u\\'demand\\', u\\'unnecessary\\', u\\'mining\\', u\\'12yr\\', \\nu\\'able\\', u\\'snowy\\', u\\'shopkeepers\\', u\\'arpg\\']\\nTopic #2 without weights\\n[u\\'game\\', u\\'play\\', u\\'get\\', u\\'one\\', u\\'skyrim\\', u\\'great\\', u\\'like\\', u\\'time\\', \\nu\\'quest\\', u\\'much\\']\\nTopic #3 without weights\\n[u\\'de\\', u\\'pagar\\', u\\'cr\\\\xe9dito\\', u\\'momento\\', u\\'responsabilidad\\', u\\'compras\\', \\nu\\'para\\', u\\'futuras\\', u\\'recomiendo\\', u\\'skyrimseguridad\\']\\nTopic #4 without weights\\n[u\\'booklet\\', u\\'proudly\\', u\\'ending\\', u\\'destiny\\', u\\'estatic\\', u\\'humungous\\', \\nu\\'chirstmas\\', u\\'bloodthey\\', u\\'accolade\\', u\\'scaled\\']\\nTopic #5 without weights\\n[u\\'game\\', u\\'play\\', u\\'fun\\', u\\'good\\', u\\'buy\\', u\\'one\\', u\\'whatnot\\', u\\'titles\\', \\nu\\'haveseen\\', u\\'best\\']\\n# Technique 3: Non-negative Matrix Factorization\\nIn [961]: nmf = NMF(n_components=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0random_state=42, alpha=.1, l1_ratio=.5)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: nmf.fit(tfidf_matrix)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: feature_names = vectorizer.get_feature_names()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: weights = nmf.components_\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: topics = get_topics_terms_weights(weights, feature_names)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print_topics_udf(topics=topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_topics=total_topics,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_terms=10,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0display_weights=False)\\nTopic #1 without weights\\n[u\\'game\\', u\\'get\\', u\\'skyrim\\', u\\'play\\', u\\'time\\', u\\'like\\', u\\'quest\\', u\\'one\\', \\nu\\'go\\', u\\'much\\']\\nTopic #2 without weights\\n[u\\'game\\', u\\'best\\', u\\'ever\\', u\\'fun\\', u\\'play\\', u\\'hour\\', u\\'great\\', u\\'rpg\\', \\nu\\'definitely\\', u\\'one\\']\\nTopic #3 without weights\\n[u\\'write\\', u\\'review\\', u\\'describe\\', u\\'justice\\', u\\'word\\', u\\'game\\', u\\'simply\\', \\nu\\'try\\', u\\'period\\', u\\'really\\']\\nTopic #4 without weights\\n[u\\'scroll\\', u\\'elder\\', u\\'series\\', u\\'always\\', u\\'love\\', u\\'pass\\', u\\'buy\\', \\nu\\'franchise\\', u\\'game\\', u\\'best\\']\\nTopic #5 without weights\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n250\\n[u\\'recommend\\', u\\'love\\', u\\'game\\', u\\'highly\\', u\\'great\\', u\\'play\\', u\\'wonderful\\', \\nu\\'like\\', u\\'oblivion\\', u\\'would\\']\\nThe preceding outputs show five topics per technique. If you observe them closely, \\nyou will notice that there will always be some overlap between topics, but they bring out \\ndistinguishing concepts from the review. We can conclude a few observations:\\n\\xe2\\x80\\xa2 \\nAll topic modeling techniques bring out concepts related to \\npeople describing this game with adjectives like wonderful, great, \\nand highly recommendable.\\n\\xe2\\x80\\xa2 \\nThey also describe the game\\'s genre as RPG (role-playing game) \\nor ARPG (action role-playing game).\\n\\xe2\\x80\\xa2 \\nGame features like gameplay and graphics are associated with \\npositive words like good, great, fun, and cool.\\n\\xe2\\x80\\xa2 \\nThe word oblivion comes up in many of the topic models. This is \\nin reference to the previous game of the Elder Scrolls series, called \\nThe Elder Scrolls IV: Oblivion. This is an indication of customers \\ncomparing this game with its predecessor in the reviews.\\nGo ahead and play around with these functions and the data. You might even try \\nbuilding topic models on new data sources. Remember, topic modeling often acts as \\na starting point to digging deeper into the data to uncover patterns by querying with \\nspecific topic concepts or even clustering and grouping text documents and analyzing \\ntheir similarity.\\nAutomated Document Summarization\\nWe briefly talked about document summarization at the beginning of this chapter, \\nin trying to extract the gist from a large document or corpus such that it retains the \\ncore essence or meaning of the corpus. The idea of document summarization is a bit \\ndifferent from keyphrase extraction or topic modeling. The end result is still in the form \\nof some document, but with a few sentences based on the length we might want the \\nsummary to be. This is similar to having a research paper with an abstract or an executive \\nsummary. The main objective of automated document summarization is to perform \\nthis summarization without involving human inputs except for running any computer \\nprograms. Mathematical and statistical models help in building and automating the task \\nof summarizing documents by observing their content and context.\\nThere are mainly two broad approaches towards document summarization using \\nautomated techniques:\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n251\\n\\xe2\\x80\\xa2 \\nExtraction-based techniques: These methods use mathematical \\nand statistical concepts like SVD to extract some key subset of \\ncontent from the original document such that this subset of \\ncontent contains the core information and acts as the focal point \\nof the entire document. This content could be words, phrases, \\nor sentences. The end result from this approach is a short \\nexecutive summary of a couple of lines are taken or extracted \\nfrom the original document. No new content is generated in this \\ntechnique\\xe2\\x80\\x94hence the name extraction-based.\\n\\xe2\\x80\\xa2 \\nAbstraction-based techniques: These methods are more complex \\nand sophisticated and leverage language semantics to create \\nrepresentations. They also make use of NLG techniques where the \\nmachine uses knowledge bases and semantic representations to \\ngenerate text on its own and creates summaries just like a human \\nwould write them.\\nMost research today exists for extraction-based techniques because it is \\ncomparatively harder to build abstraction-based summarizers. But some advances have \\nbeen made in that area with regard to creating abstract summaries mimicking humans. \\nLet us look at an implementation of document summarization by leveraging gensim\\'s \\nsummarization module. We will be using our Wikipedia description of elephants as the \\ndocument on which we will test all our summarization techniques. We start by loading \\nthe necessary dependencies and the corpus as follows:\\nfrom normalization import normalize_corpus, parse_document\\nfrom utils import build_feature_matrix, low_rank_svd\\nimport numpy as np\\ntoy_text = \"\"\"\\nElephants are large mammals of the family Elephantidae\\nand the order Proboscidea. Two species are traditionally recognised,\\nthe African elephant and the Asian elephant. Elephants are scattered\\nthroughout sub-Saharan Africa, South Asia, and Southeast Asia. Male\\nAfrican elephants are the largest extant terrestrial animals. All\\nelephants have a long trunk used for many purposes,\\nparticularly breathing, lifting water and grasping objects. Their\\nincisors grow into tusks, which can serve as weapons and as tools \\nfor moving objects and digging. Elephants\\' large ear flaps help\\nto control their body temperature. Their pillar-like legs can\\ncarry their great weight. African elephants have larger ears\\nand concave backs while Asian elephants have smaller ears\\nand convex or level backs.\\xc2\\xa0\\xc2\\xa0\\n\"\"\"\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n252\\nWe now define a function to summarize an input document to a fraction of its \\noriginal size, which will be taken as a user input parameter summary_ratio in the \\nfollowing function. The output will be the summarized document:\\nfrom gensim.summarization import summarize, keywords\\ndef text_summarization_gensim(text, summary_ratio=0.5):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0summary = summarize(text, split=True, ratio=summary_ratio)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for sentence in summary:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print sentence\\nWe will now parse our input document to remove the newlines and extract sentences \\nand then pass the complete document to the preceding function where gensim takes care \\nof normalization and summarizes the document, as shown in the following snippet:\\nIn [978]: docs = parse_document(toy_text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: text = \\' \\'.join(docs)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: text_summarization_gensim(text, summary_ratio=0.4)\\nTwo species are traditionally recognised,\\xc2\\xa0\\xc2\\xa0the African elephant and the \\nAsian elephant.\\nAll\\xc2\\xa0\\xc2\\xa0elephants have a long trunk used for many purposes,\\xc2\\xa0\\xc2\\xa0particularly \\nbreathing, lifting water and grasping objects.\\nAfrican elephants have larger ears\\xc2\\xa0\\xc2\\xa0and concave backs while Asian elephants \\nhave smaller ears\\xc2\\xa0\\xc2\\xa0and convex or level backs.\\nIf you observe the preceding output and compare it with the original document, \\nwe had a total of nine sentences in the original document, and it has been summarize \\nto a total of three sentences. But if you read the summarized document, you will see the \\ncore meaning and themes of the document have been retained, which include the two \\nspecies of elephants, how they are distinguishable from each other, and their common \\ncharacteristics. This summarization implementation from gensim is based on a popular \\nalgorithm called TextRank.\\nNow that we have seen how interesting text summarization can be, let us look at a \\ncouple of extraction-based summarization algorithms. We will be mainly focusing on the \\nfollowing two techniques:\\n\\xe2\\x80\\xa2 \\nLatent semantic analysis\\n\\xe2\\x80\\xa2 \\nTextRank\\nWe will first explore the concepts and math behind each technique and then \\nimplement those using Python. Finally, we will test them on our toy document from \\nbefore. Before we deep dive into the techniques, let us prepare our toy document by \\nparsing and normalizing it as follows:\\n# parse and normalize document\\nsentences = parse_document(toy_text)\\nnorm_sentences = normalize_corpus(sentences,lemmatize=True)\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n253\\n# check total sentences in document\\nIn [992]: total_sentences = len(norm_sentences)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'Total Sentences in Document:\\', total_sentences\\nTotal Sentences in Document: 9\\nOnce we have a working summarization algorithm, we will also construct a generic \\nfunction for each technique and test it on a real product description from Wikipedia in a \\nfuture section.\\nLatent Semantic Analysis\\nHere, we will be looking at summarizing text documents by utilizing document sentences, \\nthe terms in each sentence of the document, and applying SVD to them using some sort \\nof feature weights like Bag of Words or TF-IDF weights. The core principle behind latent \\nsemantic analysis (LSA) is that in any document, there exists a latent structure among \\nterms which are related contextually and hence should also be correlated in the same \\nsingular space. The approach we follow in our implementation is taken from the popular \\npaper published in 2004 by J. Steinberger and K. Jezek, \\xe2\\x80\\x9cUsing latent semantic analysis in \\ntext summarization and summary evaluation,\\xe2\\x80\\x9d which proposes some improvements over \\nsome excellent work done by Y. Gong and X. Liu\\xe2\\x80\\x99s \\xe2\\x80\\x9cGeneric Text Summarization Using \\nRelevance Measure and Latent Semantic Analysis,\\xe2\\x80\\x9d published in 2001. I recommend you \\nto read these two papers if you are interested in learning more about this technique. \\nThe main idea in our implementation is to use SVD such that, if you remember the \\nequation from SVD where M\\nUSV T\\n=\\n such that U and V are the orthogonal matrices and S \\nwas the diagonal matrix, which can also be represented as a vector of the singular values. \\nThe original matrix can be represented as a term-document matrix, where the rows will be \\nterms and each column will be a document, that is, a sentence from our document in this \\ncase. The values can be any type of weighting, like Bag of Words model-based frequencies, \\nTF-IDFS, or binary occurrences. We will use our low_rank_svd() function to create a low \\nrank matrix approximation for M based on the number of concepts k, which will be our \\nnumber of singular values. The same k columns from matrix U will point to the term \\nvectors for each of the k concepts, and in case of matrix V, the k rows based on the top k \\nsingular values point to sentence vectors. Once we have U, S, and VT from the SVD for the \\ntop k singular values based on the number of concepts k, we perform the following \\ncomputations. Remember, the input parameters we need are the number of concepts k \\nand the number of sentences n which we want the final summary to contain:\\n\\xe2\\x80\\xa2 \\nGet the sentence vectors from the matrix V (k rows).\\n\\xe2\\x80\\xa2 \\nGet the top k singular values from S.\\n\\xe2\\x80\\xa2 \\nApply a threshold-based approach to remove singular values that \\nare less than half of the largest singular value if any exist. This is \\na heuristic, and you can play around with this value if you want. \\nMathematically, S\\niff S\\nS\\ni\\ni\\n=\\n<\\n( )\\n0\\n1\\n2 max\\n.\\n\\xe2\\x80\\xa2 \\nMultiply each term sentence column from V squared with its \\ncorresponding singular value from S also squared, to get sentence \\nweights per topic.\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n254\\n\\xe2\\x80\\xa2 \\nCompute the sum of the sentence weights across the topics and \\ntake the square root of the final score to get the salience scores for \\neach sentence in the document.\\nThe preceding salience score computations for each sentence can be mathematically \\nrepresented as\\nSS\\nSV\\ni\\nk\\ni\\ni\\nT\\n=\\n=\\xc3\\xa5\\n1\\nwhere SS denotes the saliency score for each sentence by taking the dot product between \\nthe singular values and the sentence vectors from VT. Once we have these scores, we sort \\nthem in descending order, pick the top n sentences corresponding to the highest scores, \\nand combine them to form our final summary based on the order in which they were \\npresent in the original document. Let us implement the above steps in our code using the \\nfollowing snippet:\\n# set the number of sentences and topics for summarized document\\nnum_sentences = 3\\nnum_topics = 3\\n# build document term matrix based on bag of words features\\nvec, dt_matrix = build_feature_matrix(sentences,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type=\\'frequency\\')\\n# convert to term document matrix\\ntd_matrix = dt_matrix.transpose()\\ntd_matrix = td_matrix.multiply(td_matrix > 0)\\n# get low rank SVD components\\nu, s, vt = low_rank_svd(td_matrix, singular_count=num_topics)\\xc2\\xa0\\xc2\\xa0\\n# remove singular values below threshold\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\nsv_threshold = 0.5\\nmin_sigma_value = max(s) * sv_threshold\\ns[s < min_sigma_value] = 0\\n# compute salience scores for all sentences in document\\nsalience_scores = np.sqrt(np.dot(np.square(s), np.square(vt)))\\n# print salience score for each sentence\\nIn [996]: print np.round(salience_scores, 2)\\n[ 2.93\\xc2\\xa0\\xc2\\xa03.28\\xc2\\xa0\\xc2\\xa01.67\\xc2\\xa0\\xc2\\xa01.8\\xc2\\xa0\\xc2\\xa0\\xc2\\xa02.24\\xc2\\xa0\\xc2\\xa04.51\\xc2\\xa0\\xc2\\xa00.71\\xc2\\xa0\\xc2\\xa01.22\\xc2\\xa0\\xc2\\xa05.24]\\n# rank sentences based on their salience scores\\ntop_sentence_indices = salience_scores.argsort()[-num_sentences:][::-1]\\ntop_sentence_indices.sort()\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n255\\n# view top sentence index positions\\nIn [997]: print top_sentence_indices\\n[1 5 8]\\n# get document summary by combining above sentences\\nIn [998]: for index in top_sentence_indices:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print sentences[index]\\nTwo species are traditionally recognised,\\xc2\\xa0\\xc2\\xa0the African elephant and the \\nAsian elephant.\\nTheir\\xc2\\xa0\\xc2\\xa0incisors grow into tusks, which can serve as weapons and as \\ntools\\xc2\\xa0\\xc2\\xa0for moving objects and digging.\\nAfrican elephants have larger ears\\xc2\\xa0\\xc2\\xa0and concave backs while Asian elephants \\nhave smaller ears\\xc2\\xa0\\xc2\\xa0and convex or level backs.\\nYou can see how a few matrix operations give us a concise and excellent summarized \\ndocument that covers the main topics from the document about elephants. Compare \\nit with the one generated earlier using gensim. Do you see some similarity between the \\nsummaries?\\nWe will now build a generic reusable function for LSA using the previous algorithm \\nso that we can use it on our product description document later on and you can also use \\nthis function on your own data:\\ndef lsa_text_summarizer(documents, num_sentences=2,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_topics=2, feature_type=\\'frequency\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sv_threshold=0.5):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vec, dt_matrix = build_feature_matrix(documents,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type=feature_type)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0td_matrix = dt_matrix.transpose()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0td_matrix = td_matrix.multiply(td_matrix > 0)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0u, s, vt = low_rank_svd(td_matrix, singular_count=num_topics)\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0min_sigma_value = max(s) * sv_threshold\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0s[s < min_sigma_value] = 0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0salience_scores = np.sqrt(np.dot(np.square(s), np.square(vt)))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_sentence_indices = salience_scores.argsort()[-num_sentences:][::-1]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_sentence_indices.sort()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index in top_sentence_indices:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print sentences[index]\\nThis concludes our discussion on LSA, and we will move on to the next technique for \\nextraction-based document summarization.\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n256\\nTextRank\\nThe TextRank summarization algorithm internally uses the popular PageRank algorithm, \\nwhich is used by Google for ranking web sites and pages and measures their importance. \\nIt is used by the Google search engine when providing relevant web pages based on \\nsearch queries. To understand TextRank better, we need to understand some of the \\nconcepts surrounding PageRank.\\nThe core algorithm in PageRank is a graph-based scoring or ranking algorithm, where \\npages are scored or ranked based on their importance. Web sites and pages contain further \\nlinks embedded in them, which link to more pages with more links, and this continues \\nacross the Internet. This can be represented as a graph-based model where vertices \\nindicate the web pages, and edges indicate links among them. This can be used to form a \\nvoting or recommendation system such that when one vertex links to another one in the \\ngraph, it is basically casting a vote. Vertex importance is decided not only on the number \\nof votes or edges but also the importance of the vertices that are connected to it and their \\nimportance. This helps in determining the score or rank for each vertex or page. This is \\nevident from Figure\\xc2\\xa05-4, which represents a sample of pages with their importance.\\nIn Figure\\xc2\\xa05-4, we can see that vertex denoting Page B has a higher score than Page C, \\neven if it has fewer edges compared to Page C, because Page A is an important page \\nwhich is connected to Page B. Thus we can now formally define PageRank as follows. \\nFigure 5-4.\\xe2\\x80\\x82 PageRank scores for a simple network\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n257\\nConsider a directed graph represented as G\\nV E\\n=(\\n)\\n,\\n such that V represents the set of \\nvertices or pages and E represents the set of edges or links, and E is a subset ofV\\nV\\n\\xc2\\xb4\\n. \\nAssuming we have a given page Vi for which we want to compute the PageRank, we can \\nmathematically define it as\\nPR V\\nd\\nd\\nPR V\\nOut V\\ni\\nj\\nIn V\\nj\\nj\\ni\\n(\\n) =\\n-\\n(\\n)+\\n\\xc2\\xb4\\n(\\n)\\n\\xc3\\x8e\\n(\\n)\\n\\xc3\\xa5\\n1\\n(\\nwhere for the vertex/page Vi we have PR(Vi), which indicates the PageRank score, In(Vi) \\nrepresents the set of pages which point to this vertex/page, Out(Vi) represents the set of \\npages which the vertex/page Vi points to, and d is the damping factor usually having a \\nvalue between 0 to 1\\xe2\\x80\\x94ideally it is set to 0.85. \\nComing back to the TextRank algorithm, when summarizing a document, we will \\nhave sentences, keywords, or phrases as the vertices of the algorithm based on the type of \\nsummarization we are trying to do. We might have multiple links between these vertices, \\nand the modification which we make from the original PageRank algorithm is to have a \\nweight coefficient say wij between the edge connecting two vertices Vi and Vj such that \\nthis weight indicates the strength of this connection between them. Thus we now formally \\ndefine the new function for computing TextRank of vertices as\\nTR V\\nd\\nd\\nw TR V\\nw\\ni\\nV\\nIn V\\nji\\nj\\nV\\nOut V\\njk\\nj\\ni\\nk\\nj\\n(\\n) =\\n-\\n(\\n)+\\n\\xc2\\xb4\\n(\\n)\\n\\xc3\\x8e\\n(\\n)\\n\\xc3\\x8e\\n(\\n)\\n\\xc3\\xa5\\n\\xc3\\xa5\\n1\\nwhere TR indicates the weighted PageRank score for a vertex now defined as the TextRank \\nfor that vertex. Thus we can now formulate the algorithm and identify the main steps we \\nwill be following:\\n\\t\\n1.\\t\\nTokenize and extract sentences from the document to be \\nsummarized.\\n\\t\\n2.\\t\\nDecide on the number of sentences k that we want in the final \\nsummary.\\n\\t\\n3.\\t\\nBuild document term feature matrix using weights like TF-IDF \\nor Bag of Words.\\n\\t\\n4.\\t\\nCompute a document similarity matrix by multiplying the \\nmatrix with its transpose.\\n\\t\\n5.\\t\\nUse these documents (sentences in our case) as the vertices \\nand the similarities between each pair of documents as the \\nweight or score coefficient mentioned earlier and feed them to \\nthe PageRank algorithm.\\n\\t\\n6.\\t\\nGet the score for each sentence.\\n\\t\\n7.\\t\\nRank the sentences based on score and return the top k \\nsentences.\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n258\\nThe following code snippet shows how to construct the connected graph among all \\nthe sentences from our toy document by making use of the document similarity scores \\nand the documents themselves as the vertices. We will use the networkx library to help \\nus plot this graph. Remember, each document is a sentence in our case which will also be \\nthe vertices in the graph:\\nimport networkx\\n# define number of sentences in final summary\\nnum_sentences = 3\\n# construct weighted document term matrix\\nvec, dt_matrix = build_feature_matrix(norm_sentences,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type=\\'tfidf\\')\\n# construct the document similarity matrix\\nsimilarity_matrix = (dt_matrix * dt_matrix.T)\\n# view the document similarity matrix\\nIn [1011]: print np.round(similarity_matrix.todense(), 2)\\n[[ 1.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.03\\xc2\\xa0\\xc2\\xa00.05\\xc2\\xa0\\xc2\\xa00.03\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.15\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.06]\\n\\xc2\\xa0[ 0.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.07\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.11]\\n\\xc2\\xa0[ 0.03\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.03\\xc2\\xa0\\xc2\\xa00.02\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.03\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.04]\\n\\xc2\\xa0[ 0.05\\xc2\\xa0\\xc2\\xa00.07\\xc2\\xa0\\xc2\\xa00.03\\xc2\\xa0\\xc2\\xa01.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.03\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.04\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.11]\\n\\xc2\\xa0[ 0.03\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.02\\xc2\\xa0\\xc2\\xa00.03\\xc2\\xa0\\xc2\\xa01.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.07\\xc2\\xa0\\xc2\\xa00.03\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.04]\\n\\xc2\\xa0[ 0.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.07\\xc2\\xa0\\xc2\\xa01.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0]\\n\\xc2\\xa0[ 0.15\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.03\\xc2\\xa0\\xc2\\xa00.04\\xc2\\xa0\\xc2\\xa00.03\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.05]\\n\\xc2\\xa0[ 0.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0]\\n\\xc2\\xa0[ 0.06\\xc2\\xa0\\xc2\\xa00.11\\xc2\\xa0\\xc2\\xa00.04\\xc2\\xa0\\xc2\\xa00.11\\xc2\\xa0\\xc2\\xa00.04\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa00.05\\xc2\\xa0\\xc2\\xa00.\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa01.\\xc2\\xa0\\xc2\\xa0]]\\n# build the similarity graph\\nsimilarity_graph = networkx.from_scipy_sparse_matrix(similarity_matrix)\\n# view the similarity graph\\nIn [1013]: networkx.draw_networkx(similarity_graph)\\nOut [1013]:\\nIn Figure\\xc2\\xa05-5, we can see how the sentences of our toy document are now linked to \\neach other based on document similarities. The graph gives an idea how well connected \\nsome sentences are to other sentences.\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n259\\nWe will now compute the PageRank scores for all the sentences, rank them, and \\nbuild our summary using the top three sentences:\\n# compute pagerank scores for all the sentences\\nscores = networkx.pagerank(similarity_graph)\\n# rank sentences based on their scores\\nranked_sentences = sorted(((score, index)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index, score\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in scores.items()),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0reverse=True)\\n# view the ranked sentences\\nIn [1030]: ranked_sentences\\nOut[1030]:\\nFigure 5-5.\\xe2\\x80\\x82 Similarity graph showing connections between sentences\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n260\\n[(0.11889477617125277, 8),\\n\\xc2\\xa0(0.11456045476451866, 3),\\n\\xc2\\xa0(0.11285293843138654, 0),\\n\\xc2\\xa0(0.11210156056437962, 6),\\n\\xc2\\xa0(0.11139550507847462, 4),\\n\\xc2\\xa0(0.1111111111111111, 7),\\n\\xc2\\xa0(0.10709498606197024, 5),\\n\\xc2\\xa0(0.10610242758495998, 2),\\n\\xc2\\xa0(0.10588624023194664, 1)]\\n# get the top sentence indices for our summary\\ntop_sentence_indices = [ranked_sentences[index][1]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index in range(num_sentences)]\\ntop_sentence_indices.sort()\\n# view the top sentence indices\\nIn [1032]: print top_sentence_indices\\n\\xc2\\xa0[0, 3, 8]\\n# construct the document summary\\nIn [1033]: for index in top_sentence_indices:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print sentences[index]\\nElephants are large mammals of the family Elephantidae\\xc2\\xa0\\xc2\\xa0and the order \\nProboscidea.\\nMale\\xc2\\xa0\\xc2\\xa0African elephants are the largest extant terrestrial animals.\\nAfrican elephants have larger ears\\xc2\\xa0\\xc2\\xa0and concave backs while Asian elephants \\nhave smaller ears\\xc2\\xa0\\xc2\\xa0and convex or level backs.\\nWe finally get our desired summary by using the TextRank algorithm. The content \\nis also quite meaningful where it talks about elephants being mammals, their taxonomy, \\nand how Asian and African elephants can be distinguished.\\nWe will now define a generic function as follows to compute TextRank-based \\nsummaries on any document:\\ndef textrank_text_summarizer(documents, num_sentences=2,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type=\\'frequency\\'):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vec, dt_matrix = build_feature_matrix(norm_sentences,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type=\\'tfidf\\')\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0similarity_matrix = (dt_matrix * dt_matrix.T)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0similarity_graph = networkx.from_scipy_sparse_matrix(similarity_matrix)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0scores = networkx.pagerank(similarity_graph)\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ranked_sentences = sorted(((score, index)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index, score\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in scores.items()),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0reverse=True)\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n261\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_sentence_indices = [ranked_sentences[index][1]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index in range(num_sentences)]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_sentence_indices.sort()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index in top_sentence_indices:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print sentences[index]\\xc2\\xa0\\xc2\\xa0\\nWe have covered two document-summarization techniques and also built generic \\nreusable functions to compute automated document summaries for any text document. \\nIn the following section, we will summarize a product description from a wiki page.\\nSummarizing a Product Description\\nBuilding on what we talked about in the product reviews from the topic modeling section, \\nhere we will be summarizing a description for the same product\\xe2\\x80\\x94a role-playing video \\ngame named The Elder Scrolls V: Skyrim. We have taken several lines from the Wikipedia \\npage containing the product\\'s detailed description. In this section, we will perform \\nautomated document summarization on the product description utilizing our functions \\nfrom the previous section. We will start with loading the product description and \\nnormalizing the content:\\n# load the document\\nDOCUMENT = \"\"\"\\nThe Elder Scrolls V: Skyrim is an open world action role-playing video game\\ndeveloped by Bethesda Game Studios and published by Bethesda Softworks.\\nIt is the fifth installment in The Elder Scrolls series, following\\nThe Elder Scrolls IV: Oblivion. Skyrim\\'s main story revolves around\\nthe player character and their effort to defeat Alduin the World-Eater,\\na dragon who is prophesied to destroy the world.\\nThe game is set two hundred years after the events of Oblivion\\nand takes place in the fictional province of Skyrim. The player completes \\nquests\\nand develops the character by improving skills.\\nSkyrim continues the open world tradition of its predecessors by allowing the\\nplayer to travel anywhere in the game world at any time, and to\\nignore or postpone the main storyline indefinitely. The player may freely roam\\nover the land of Skyrim, which is an open world environment consisting\\nof wilderness expanses, dungeons, cities, towns, fortresses and villages.\\nPlayers may navigate the game world more quickly by riding horses, \\nor by utilizing a fast-travel system which allows them to warp to previously\\nPlayers have the option to develop their character. At the beginning of the game,\\nplayers create their character by selecting one of several races,\\nincluding humans, orcs, elves and anthropomorphic cat or lizard-like \\ncreatures,\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n262\\nand then customizing their character\\'s appearance.discovered locations. Over the\\ncourse of the game, players improve their character\\'s skills, which are \\nnumerical\\nrepresentations of their ability in certain areas. There are eighteen skills\\ndivided evenly among the three schools of combat, magic, and stealth.\\nSkyrim is the first entry in The Elder Scrolls to include Dragons in the game\\'s\\nwilderness. Like other creatures, Dragons are generated randomly in the world\\nand will engage in combat.\\n\"\"\"\\n# normalize the document\\nIn [1045]: sentences = parse_document(DOCUMENT)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: norm_sentences = normalize_corpus(sentences,lemmatize=True)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \"Total Sentences:\", len(norm_sentences)\\nTotal Sentences: 13\\nWe can see that there are a total of 13 sentences in this description. Let us now \\ngenerate the document summaries using our functions in the following code snippet:\\n# LSA document summarization\\nIn [1053]: lsa_text_summarizer(norm_sentences, num_sentences=3,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_topics=5, feature_type=\\'frequency\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sv_threshold=0.5)\\xc2\\xa0\\xc2\\xa0\\nThe Elder Scrolls V: Skyrim is an open world action role-playing video \\ngame\\xc2\\xa0\\xc2\\xa0developed by Bethesda Game Studios and published by Bethesda \\nSoftworks.\\nPlayers may navigate the game world more quickly by riding horses,\\xc2\\xa0\\xc2\\xa0or \\nby utilizing a fast-travel system which allows them to warp to \\npreviously\\xc2\\xa0\\xc2\\xa0Players have the option to develop their character.\\nAt the beginning of the game,\\xc2\\xa0\\xc2\\xa0players create their character by selecting \\none of several races,\\xc2\\xa0\\xc2\\xa0including humans, orcs, elves and anthropomorphic \\ncat or lizard-like creatures,\\xc2\\xa0\\xc2\\xa0and then customizing their character\\'s \\nappearance.discovered locations.\\n# TextRank document summarization\\nIn [1054]: textrank_text_summarizer(norm_sentences, num_sentences=3,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type=\\'tfidf\\')\\xc2\\xa0\\xc2\\xa0\\nThe Elder Scrolls V: Skyrim is an open world action role-playing video \\ngame\\xc2\\xa0\\xc2\\xa0developed by Bethesda Game Studios and published by Bethesda \\nSoftworks.\\nPlayers may navigate the game world more quickly by riding horses,\\xc2\\xa0\\xc2\\xa0or \\nby utilizing a fast-travel system which allows them to warp to \\npreviously\\xc2\\xa0\\xc2\\xa0Players have the option to develop their character.\\nSkyrim is the first entry in The Elder Scrolls to include Dragons in the \\ngame\\'s\\xc2\\xa0\\xc2\\xa0wilderness.\\nChapter 5 \\xe2\\x96\\xa0 Text Summarization\\n263\\nYou can see from the preceding outputs that we were successfully able to summarize \\nour product description from 13 to 3 lines, and this short summary depicts the core \\nessence of the product description, like the name of the game and its various features \\nregarding its gameplay and characters.\\nThis concludes our discussion on automated text summarization. I encourage you to \\ntry out these techniques on more documents and test it with various different parameters \\nlike more number of topics, different feature types like TF-IDF, Bag of Words, binary \\noccurrences, and even word vectors.\\nSummary\\nIn this chapter, we covered some interesting areas in NLP and text analytics with \\nregard to information extraction, document summarization, and topic modeling. We \\nstarted with an overview of the evolution of information and learned about concepts \\nlike information overload leading to the need for text summarization and information \\nretrieval. We talked about the various ways we can extract key information from textual \\ndata and ways of summarizing large documents. We covered important mathematical \\nconcepts like SVD and low rank matrix approximation and utilized them in several of our \\nalgorithms. We mainly covered three approaches towards reducing information overload, \\nincluding keyphrase extraction, topic models, and automated document summarization. \\nKeyphrase extraction includes methods like collocations and weighted tagged term\\xe2\\x80\\x93\\nbased approaches for getting keyphrases or terms from corpora. We built several topic \\nmodeling techniques, including latent semantic indexing, latent Dirichlet\\xc2\\xa0allocation, \\nand the very recently implemented non-negative matrix factorization. Finally, we looked \\nat two extraction-based techniques for automated document summarization: LSA and \\nTextRank. We implemented each method and observed results on real-world data to \\nget a good idea of how these methods worked and how effective simple mathematical \\noperations can be in generating actionable insights.\\n265\\n\\xc2\\xa9 Dipanjan Sarkar 2016 \\nD. Sarkar, Text Analytics with Python, DOI 10.1007/978-1-4842-2388-8_6\\nCHAPTER 6\\nText Similarity and \\nClustering\\nPrevious chapters have covered several techniques of analyzing text and extracting interesting \\ninsights. We have looked at supervised machine learning (ML) techniques that are used to \\nclassify or categorize text documents into several pre-assumed categories. Unsupervised \\ntechniques like topic models and document summarization have also been also covered, \\nwhich involved trying to extract and retrieve key themes and information from large text \\ndocuments and corpora. In this chapter, we will be looking at several other techniques and \\nuse-cases that leverage unsupervised learning and information retrieval concepts.\\nIf you refresh your memory of Chapter 4, text categorization is indeed an interesting \\nproblem that has several applications, most notably in the classification of news articles \\nand email. But one constraint in text classification is that we need some training data with \\nmanually labeled categories because we use supervised learning algorithms to build our \\nclassification model. The efforts of building this dataset are definitely not easy, because \\nto build a good model, you need a sizeable amount of training data. For this, we need to \\nspend time and manual effort in labeling data, building a model, and then finally using it to \\nclassify new documents. Can we instead make the machine do it? Yes, as a matter of fact, we \\ncan. This chapter specifically addresses looking at the content of text documents, analyzing \\ntheir similarity using various measures, and clustering similar documents together.\\nText data is unstructured and highly noisy. We get the benefits of well-labeled \\ntraining data and supervised learning when performing text classification. But document \\nclustering is an unsupervised learning process, where we are trying to segment and \\ncategorize documents into separate categories by making the machine learn about the \\nvarious text documents, their features, similarities, and the differences among them. \\nThis makes document clustering more challenging, albeit interesting. Consider having \\na corpus of documents that talk about various different concepts and ideas. Humans are \\nwired in such a way that we use our learning from the past and apply it to distinguish \\ndocuments from each other. For example, the sentence The fox is smarter than the \\ndog is more similar to The fox is faster than the dog than it is to Python is an excellent \\nprogramming language. We can easily spot and intuitively figure out specific keyphrases \\nlike Python, fox, dog, programming, and so on, which help us determine which sentences \\nor documents are more similar. But can we do that programmatically? In this chapter, \\nwe will focus on several concepts related to text similarity, distance metrics, and \\nunsupervised ML algorithms to answer the following questions:\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n266\\n\\xe2\\x80\\xa2 \\nHow do we measure similarity between documents?\\n\\xe2\\x80\\xa2 \\nHow can we use distance measures to find the most relevant \\ndocuments?\\n\\xe2\\x80\\xa2 \\nWhen is a distance measure called a metric?\\n\\xe2\\x80\\xa2 \\nHow do we cluster or group similar documents?\\n\\xe2\\x80\\xa2 \\nCan we visualize document clusters?\\nAlthough we will be focused on trying to answer these questions, we will cover \\nessential concepts and information needed to understand various techniques for \\nsolving these problems. We will also use some practical examples to illustrate concepts \\nrelated to text similarity, distance metrics, and document clustering. Also, many of these \\ntechniques can be combined with some of the techniques we learned previously and \\nvice versa. For example, concepts of text similarity using distance metrics are also used \\nto build document clusters. You can also use features from topic models for measuring \\ntext similarity. Besides this, clustering is often a starting point to get a feel for the possible \\ngroups or categories that your data might consist of, or to even visualize these clusters \\nor groups of similar text documents. This can then be plugged in to other systems \\nlike supervised classification systems, or you can even combine them both and build \\nweighted classifiers. The possibilities are indeed endless!\\nIn this chapter, we will first cover some important concepts related to distance \\nmeasures, metrics, and unsupervised learning and brush up on text normalization and \\nfeature extraction. Once the basics are covered, our objective will be to understand and \\nanalyze term similarity, document similarity, and finally document clustering.\\nImportant Concepts\\nOur main objectives in this chapter are to understand text similarity and clustering. \\nBefore moving on to the actual techniques and algorithms, this section will discuss some \\nimportant concepts related to information retrieval, document similarity measures, and \\nmachine learning. Even though some of these concepts might be familiar to you from the \\nprevious chapters, all of them will be useful to us as we gradually journey through this \\nchapter. Without further ado, let\\xe2\\x80\\x99s get started.\\nInformation Retrieval (IR)\\nInformation retrieval (IR) is the process of retrieving or fetching relevant sources of \\ninformation from a corpus or set of entities that hold information based on some \\ndemand. For example, it could be a query or search that users enter in a search engine \\nand then get relevant search items pertaining to their query. In fact, search engines are \\nthe most popular use-case or application of IR.\\nThe relevancy of documents with information compared to the demand can \\nbe measured in several ways. It can include looking for specific keywords from the \\nsearch text or using some similarity measures to see the similarity rank or score of the \\ndocuments with respect to the entered query. This makes is quite different from string \\nmatching or matching regular expressions because more than often the words in a search \\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n267\\nstring can have different order, context, and semantics in the collection of documents \\n(entities), and these words can even have multiple different resolutions or possibilities \\nbased on synonyms, antonyms, and negation modifiers.\\nFeature Engineering\\nFeature engineering or feature extraction is something which you know quite well by \\nnow. Methods like Bag of Words, TF-IDF, and word vectorization models are typically \\nused to represent or model documents in the form of numeric vectors so that applying \\nmathematical or machine learning techniques become much easier. You can use various \\ndocument representations using these feature-extraction techniques or even map each \\nletter or a word to a corresponding unique numeric identifier.\\nSimilarity Measures\\nSimilarity measures are used frequently in text similarity analysis and clustering. Any \\nsimilarity or distance measure usually measures the degree of closeness between two \\nentities, which can be any text format like documents, sentences, or even terms. This \\nmeasure of similarity can be useful in identifying similar entities and distinguishing \\nclearly different entities from each other. Similarity measures are very effective, and \\nsometimes choosing the right measure can make a lot of difference in the performance \\nof your final analytics system. Various scoring or ranking algorithms have also been \\ninvented based on these distance measures. Two main factors determine the degree of \\nsimilarity between entities:\\n\\xe2\\x80\\xa2 \\nInherent properties or features of the entities\\n\\xe2\\x80\\xa2 \\nMeasure formula and properties\\nThere are several distance measures that measure similarity, and we will be covering \\nseveral of them in future sections. However, an important thing to remember is that all \\ndistance measures of similarity are not distance metrics of similarity. The excellent paper \\nby A. Huang, \\xe2\\x80\\x9cSimilarity Measures for Text Document Clustering,\\xe2\\x80\\x9d talks about this in \\ndetail. Consider a distance measure d and two entities (say they are documents in our \\ncontext) x and y. The distance between x and y, which is used to determine the degree of \\nsimilarity between them, can be represented as d(x,\\xc2\\xa0y), but the measure d can be called as \\na distance metric of similarity if and only if it satisfies the following four conditions:\\n\\t\\n1.\\t\\nThe distance measured between any two entities, say x and y, \\nmust be always non-negative, that is, d x y\\n,\\n(\\n)\\xc2\\xb3 0 .\\n\\t\\n2.\\t\\nThe distance between two entities should always be zero if \\nand only if they are both identical, that is, d x y\\niff x\\ny\\n,\\n(\\n)\\xc2\\xb3\\n=\\n0\\n.\\n\\t\\n3.\\t\\nThis distance measure should always be symmetric, which \\nmeans that the distance from x to y is always the same as the \\ndistance from y to x. Mathematically this is represented as \\nd x y\\nd y x\\n,\\n,\\n(\\n) = (\\n) .\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n268\\n\\t\\n4.\\t\\nThis distance measure should satisfy the triangle inequality \\nproperty, which can be mathematically represented \\nd x z\\nd x y\\nd y z\\n,\\n,\\n,\\n(\\n)\\xc2\\xa3 (\\n)+ (\\n) .\\nThis tells us important criteria and gives us a good framework we can use to check \\nwhether a distance measure can be used as a distance metric for measuring similarity. I \\ndon\\xe2\\x80\\x99t have room here to go into more detail, but you may be interested in knowing that \\nthe very popular KL-divergence measure, also known as Kullback-Leibler divergence, is \\na distance measure that violates the third property, where this measure is asymmetric, \\nhence it kind of does not make sense to use it as a measure of similarity for text \\ndocuments\\xe2\\x80\\x94but otherwise, this is extremely useful in differentiating between various \\ndistributions and patterns.\\nUnsupervised Machine Learning Algorithms\\nUnsupervised machine learning algorithms are the family of ML algorithms that try to \\ndiscover latent hidden structures and patterns in data from their various attributes and \\nfeatures. Besides this, several unsupervised learning algorithms are also used to reduce \\nthe feature space, which is often of a higher dimension to one with a lower dimension. \\nThe data on which these algorithms operate is essentially unlabeled data that does not \\nhave any pre-determined category or class. We apply these algorithms with the intent \\nof finding patterns and distinguishing features that might help us in grouping various \\ndata points into groups or clusters. These algorithms are popularly known as clustering \\nalgorithms. Even the topic models covered in Chapter 5 belong to the unsupervised \\nlearning family of algorithms.\\nThis concludes our discussion on the important concepts and background \\ninformation necessary for this chapter. We will now move on to a brief coverage of text \\nnormalization and feature extraction, where we introduce a few things which are specific \\nto this chapter.\\nText Normalization\\nWe will need to normalize our text documents and corpora as usual before we perform \\nany further analyses or NLP. For this we will reuse our normalization module from \\nChapter 5 but with a few more additions specifically aimed toward this chapter. The \\ncomplete normalization module is available in the code files for this chapter in the file \\nnormalization.py, but I will still be highlighting the new additions in our normalization \\nmodule in this section for your benefit.\\nTo start, we have updated our stopwords list with several new words that have been \\ncarefully selected after analyzing many corpora. The following code snippet illustrates:\\nstopword_list = nltk.corpus.stopwords.words(\\'english\\')\\nstopword_list = stopword_list + [\\'mr\\', \\'mrs\\', \\'come\\', \\'go\\', \\'get\\', \\'tell\\', \\n\\'listen\\', \\'one\\', \\'two\\', \\'three\\', \\'four\\', \\'five\\',  \\n\\'six\\', \\'seven\\', \\'eight\\',  \\n\\'nine\\', \\'zero\\', \\'join\\', \\'find\\', \\'make\\', \\'say\\', \\'ask\\',  \\n\\'tell\\', \\'see\\', \\'try\\', \\'back\\', \\'also\\']\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n269\\nYou can see the new additions are words that are mostly generic verbs or nouns without \\na lot of significance. This will be useful to us in feature extraction during text clustering. We \\nalso add a new function in our normalization pipeline, which is to only extract text tokens \\nfrom a body of text for which we use regular expressions, as depicted in the following function:\\nimport re\\ndef keep_text_characters(text):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0filtered_tokens = []\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tokens = tokenize_text(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for token in tokens:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if re.search(\\'[a-zA-Z]\\', token):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0filtered_tokens.append(token)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0filtered_text = \\' \\'.join(filtered_tokens)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return filtered_text\\nWe add this in our final normalization function along with the other functions that \\nwe have reused from previous chapters, including expanding contractions, unescaping \\nHTML, tokenization, removing stopwords, special characters, and lemmatization. The \\nupdated normalization function is shown in the following snippet:\\ndef normalize_corpus(corpus, lemmatize=True,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0only_text_chars=False,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tokenize=False):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0normalized_corpus = []\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for text in corpus:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = html_parser.unescape(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = expand_contractions(text, CONTRACTION_MAP)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if lemmatize:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = lemmatize_text(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = text.lower()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = remove_special_characters(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = remove_stopwords(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if only_text_chars:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = keep_text_characters(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if tokenize:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0text = tokenize_text(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0normalized_corpus.append(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0normalized_corpus.append(text)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return normalized_corpus\\nThus, as you can see, the preceding function is very similar to the one from Chapter 5  \\nwith only the addition of keeping text characters using the keep_text_characters() \\nfunction, which can be executed by setting the only_text_chars parameter to True.\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n270\\nFeature Extraction\\nWe will also be using a feature-extraction function similar to the one used in Chapter 5. \\nThe code will be very similar to our previous feature extractor, except we will be adding \\nsome new parameters in this chapter. The function can be found in the utils.py file and \\nis also shown in the following snippet:\\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\\ndef build_feature_matrix(documents, feature_type=\\'frequency\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ngram_range=(1, 1), min_df=0.0, max_df=1.0):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type = feature_type.lower().strip()\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if feature_type == \\'binary\\':\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vectorizer = CountVectorizer(binary=True, min_df=min_df,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0max_df=max_df, ngram_range=ngram_range)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0elif feature_type == \\'frequency\\':\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vectorizer = CountVectorizer(binary=False, min_df=min_df,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0max_df=max_df, ngram_range=ngram_range)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0elif feature_type == \\'tfidf\\':\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ngram_range=ngram_range)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0raise Exception(\"Wrong feature type entered. Possible values: \\n\\'binary\\', \\'frequency\\',\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\'tfidf\\'\")\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_matrix = vectorizer.fit_transform(documents).astype(float)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return vectorizer, feature_matrix\\nYou can see from the function definition that we have capabilities for Bag of Words \\nfrequency, occurrences, and also TF-IDF\\xe2\\x80\\x93based features. The new additions in this \\nfunction include the addition of the min_df, max_df and ngram_range parameters and \\nalso accepting them as optional arguments. The ngram_range is useful when we want \\nto add bigrams, trigrams, and so on as additional features. The min_df parameter can \\nbe expressed by a threshold value within a range of [0.0, 1.0] and it will ignore terms \\nas features that will have a document frequency strictly lower than the input threshold \\nvalue. The max_df parameter can also be expressed by a threshold value within a range \\nof [0.0, 1.0] and it will ignore terms as features that will have a document frequency \\nstrictly higher than the input threshold value. The intuition behind this would be that \\nthese words, if they occur in almost all the documents, tend to have little value that would \\nhelp us in distinguishing among various types of documents. We will now deep dive into \\nthe various techniques for text similarity.\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n271\\nText Similarity\\nThe main objective of text similarity is to analyze and measure how two entities of text are \\nclose or far apart from each other. These entities of text can be simple tokens or terms, \\nlike words, or whole documents, which may include sentences or paragraphs of text. \\nThere are various ways of analyzing text similarity, and we can classify the intent of text \\nsimilarity broadly into the following two areas:\\n\\xe2\\x80\\xa2 \\nLexical similarity: This involves observing the contents of the \\ntext documents with regard to syntax, structure, and content and \\nmeasuring their similarity based on these parameters.\\n\\xe2\\x80\\xa2 \\nSemantic similarity: This involves trying to find out the semantics, \\nmeaning, and context of the documents and then trying to see \\nhow close they are to each other. Dependency grammars and \\nentity recognition are handy tools that can help in this.\\nNote that the most popular area is lexical similarity, because the techniques \\nare more straightforward, easy to implement, and you can also cover several parts of \\nsemantic similarity using simple models like the Bag of Words. Usually distance metrics \\nwill be used to measure similarity scores between text entities, and we will be mainly \\ncovering the following two broad areas of text similarity:\\n\\xe2\\x80\\xa2 \\nTerm similarity: Here we will measure similarity between \\nindividual tokens or words.\\n\\xe2\\x80\\xa2 \\nDocument similarity: Here we will be measuring similarity \\nbetween entire text documents.\\nThe idea is to implement and use several distance metrics and see how we can \\nmeasure and analyze similarity among entities that are just simple words, and then \\nhow things change when we measure similarity among documents that are groups of \\nindividual words.\\nAnalyzing Term Similarity\\nWe will start with analyzing term similarity\\xe2\\x80\\x94or similarity between individual word \\ntokens, to be more precise. Even though this is not used a lot in practical applications, \\nit can be used as an excellent starting point for understanding text similarity. Of course, \\nseveral applications and use-cases like autocompleters, spell check, and correctors use \\nsome of these techniques to correct misspelled terms. Here we will be taking a couple of \\nwords and measuring the similarity between then using different word representations as \\nwell as distance metrics. The word representations we will be using are as follows:\\n\\xe2\\x80\\xa2 \\nCharacter vectorization\\n\\xe2\\x80\\xa2 \\nBag of Characters vectorization\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n272\\nFor character vectorization, it is an extremely simple process of just mapping each \\ncharacter of the term to a corresponding unique number. We can do that using the \\nfunction depicted in the following snippet:\\nimport numpy as np\\ndef vectorize_terms(terms):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0terms = [term.lower() for term in terms]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0terms = [np.array(list(term)) for term in terms]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0terms = [np.array([ord(char) for char in term])\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for term in terms]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return terms\\nThe function takes input a list of words or terms and returns the corresponding \\ncharacter vectors for the words. Bag of Characters vectorization is very similar to the Bag \\nof Words model except here we compute the frequency of each character in the word. \\nSequence or word orders are not taken into account. The following function helps in \\ncomputing this:\\nfrom scipy.stats import itemfreq\\ndef boc_term_vectors(word_list):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0word_list = [word.lower() for word in word_list]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0unique_chars = np.unique(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0np.hstack([list(word)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for word in word_list]))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0word_list_term_counts = [{char: count for char, count in \\nitemfreq(list(word))}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for word in word_list]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0boc_vectors = [np.array([int(word_term_counts.get(char, 0))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for char in unique_chars])\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for word_term_counts in word_list_term_counts]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return list(unique_chars), boc_vectors\\nIn that function, we take in a list of words or terms and then extract the unique \\ncharacters from all the words. This becomes our feature list, just like we do in Bag of \\nWords, where instead of characters, unique words are our features. Once we have this list \\nof unique_chars, we get the count for each of the characters in each word and build our \\nBag of Characters vectors.\\nWe can now see our previous functions in action in the following snippet. We will be \\nusing a total of four example terms and computing the similarity among them later on:\\nroot = \\'Believe\\'\\nterm1 = \\'beleive\\'\\nterm2 = \\'bargain\\'\\nterm3 = \\'Elephant\\'\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n273\\nterms = [root, term1, term2, term3]\\n# Character vectorization\\nvec_root, vec_term1, vec_term2, vec_term3 = vectorize_terms(terms)\\n# show vector representations\\nIn [103]: print \\'\\'\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: root: {}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: term1: {}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: term2: {}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: term3: {}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: \\'\\'\\'.format(vec_root, vec_term1, vec_term2, vec_term3)\\nroot: [ 98 101 108 105 101 118 101]\\nterm1: [ 98 101 108 101 105 118 101]\\nterm2: [ 98\\xc2\\xa0\\xc2\\xa097 114 103\\xc2\\xa0\\xc2\\xa097 105 110]\\nterm3: [101 108 101 112 104\\xc2\\xa0\\xc2\\xa097 110 116]\\n# Bag of characters vectorization\\nfeatures, (boc_root, boc_term1, boc_term2, boc_term3) = boc_term_\\nvectors(terms)\\n# show features and vector representations\\nIn [105]: print \\'Features:\\', features\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'\\'\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: root: {}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: term1: {}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: term2: {}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: term3: {}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: \\'\\'\\'.format(boc_root, boc_term1, boc_term2, boc_term3)\\nFeatures: [\\'a\\', \\'b\\', \\'e\\', \\'g\\', \\'h\\', \\'i\\', \\'l\\', \\'n\\', \\'p\\', \\'r\\', \\'t\\', \\'v\\']\\nroot: [0 1 3 0 0 1 1 0 0 0 0 1]\\nterm1: [0 1 3 0 0 1 1 0 0 0 0 1]\\nterm2: [2 1 0 1 0 1 0 1 0 1 0 0]\\nterm3: [1 0 2 0 1 0 1 1 1 0 1 0]\\nThus you can see how we can easily transform text terms into numeric vector \\nrepresentations. We will now be using several distance metrics to compute similarity \\nbetween the root word and the other three words mentioned in the preceding snippet. \\nThere are a lot of distance metrics out there that you can use to compute and measure \\nsimilarities. We will be covering the following five metrics in this section:\\n\\xe2\\x80\\xa2 \\nHamming distance\\n\\xe2\\x80\\xa2 \\nManhattan distance\\n\\xe2\\x80\\xa2 \\nEuclidean distance\\n\\xe2\\x80\\xa2 \\nLevenshtein edit distance\\n\\xe2\\x80\\xa2 \\nCosine distance and similarity\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n274\\nWe will be looking at the concepts for each distance metric and using the power of \\nnumpy arrays to implement the necessary computations and mathematical formulae. \\nOnce we do that, we will put them in action by measuring the similarity of our example \\nterms. First, though, we will set up some necessary variables storing the root term, \\nthe other terms with which its similarity will be measures, and their various vector \\nrepresentations using the following snippet:\\nroot_term = root\\nroot_vector = vec_root\\nroot_boc_vector = boc_root\\nterms = [term1, term2, term3]\\nvector_terms = [vec_term1, vec_term2, vec_term3]\\nboc_vector_terms = [boc_term1, boc_term2, boc_term3]\\nWe are now ready to start computing similarity metrics and will be using the \\npreceding terms and their vector representations to measure similarities.\\nHamming Distance\\nThe Hamming distance is a very popular distance metric used frequently in information \\ntheory and communication systems. It is distance measured between two strings under \\nthe assumption that they are of equal length. Formally, it is defined as the number of \\npositions that have different characters or symbols between two strings of equal length. \\nConsidering two terms u and v of length n, we can mathematically denote Hamming \\ndistance as\\nhd u v\\nu\\nv\\ni\\nn\\ni\\ni\\n,\\n(\\n) =\\n\\xc2\\xb9\\n(\\n)\\n=\\xc3\\xa5\\n1\\nand you can also normalize it if you want by dividing the number of mismatches by the \\ntotal length of the terms to give the normalized hamming distance, which is represented \\nas\\nnorm hd u v\\nu\\nv\\nn\\ni\\nn\\ni\\ni\\n_\\n,\\n(\\n) =\\n\\xc2\\xb9\\n(\\n)\\n=\\xc3\\xa5\\n1\\nwhereas you already know n denotes the length of the terms.\\nThe following function computes the Hamming distance between two terms and \\nalso has the capability to compute the normalized distance:\\ndef hamming_distance(u, v, norm=False):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if u.shape != v.shape:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0raise ValueError(\\'The vectors must have equal lengths.\\')\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return (u != v).sum() if not norm else (u != v).mean()\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n275\\nWe will now measure the Hamming distance between our root term and the other \\nterms using the following code snippet:\\n# compute Hamming distance\\nIn [115]: for term, vector_term in zip(terms, vector_terms):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Hamming distance between root: {} and term: {} is {}\\'.\\nformat(root_term,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0term, hamming_distance(root_vector, vector_\\nterm, norm=False))\\nHamming distance between root: Believe and term: believe is 2\\nHamming distance between root: Believe and term: bargain is 6\\nTraceback (most recent call last):\\n\\xc2\\xa0\\xc2\\xa0File \"<ipython-input-115-3391bd2c4b7e>\", line 4, in <module>\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0hamming_distance(root_vector, vector_term, norm=False))\\nValueError: The vectors must have equal lengths.\\n# compute normalized Hamming distance\\nIn [117]: for term, vector_term in zip(terms, vector_terms):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Normalized Hamming distance between root: {} and term: \\n{} is\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07{}\\'.format(root_term, \\nterm,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07round(hamming_distance(root_vector, vector_term, \\nnorm=True), 2))\\nNormalized Hamming distance between root: Believe and term: believe is 0.29\\nNormalized Hamming distance between root: Believe and term: bargain is 0.86\\nTraceback (most recent call last):\\n\\xc2\\xa0\\xc2\\xa0File \"<ipython-input-117-7dfc67d08c3f>\", line 4, in <module>\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0round(hamming_distance(root_vector, vector_term, norm=True), 2))\\nValueError: The vectors must have equal lengths\\nYou can see from the preceding output that terms \\'Believe\\' and \\'believe\\' \\nignoring their case are most similar to each other with the Hamming distance of 2 or 0.29, \\ncompared to the term \\'bargain\\' giving scores of 6 or 0.86 (here, the smaller the score, \\nthe more similar are the terms). The term \\'Elephant\\' throws an exception because the \\nlength of that term (term3) is 8 compared to length 7 of the root term \\'Believe\\', hence \\nHamming distance can\\xe2\\x80\\x99t be computed because the base assumption of strings being of \\nequal length is violated.\\nManhattan Distance\\nThe Manhattan distance metric is similar to the Hamming distance conceptually, where \\ninstead of counting the number of mismatches, we subtract the difference between each \\npair of characters at each position of the two strings. Formally, Manhattan distance is \\nalso known as city block distance, L1 norm, taxicab metric and is defined as the distance \\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n276\\nbetween two points in a grid based on strictly horizontal or vertical paths instead of \\nthe diagonal distance conventionally calculated by the Euclidean distance metric. \\nMathematically it can be denoted as\\nmd u v\\nu v\\nu\\nv\\ni\\nn\\ni\\ni\\n,\\n(\\n) =\\n-\\n=\\n-\\n=\\xc3\\xa5\\n1\\n1\\nwhere u and v are the two terms of length n. The same assumption of the two terms \\nhaving equal length from Hamming distance holds good here. We can also compute the \\nnormalized Manhattan distance by dividing the sum of the absolute differences by the \\nterm length. This can be denoted by\\nnorm md u v\\nu v\\nn\\nu\\nv\\nn\\ni\\nn\\ni\\ni\\n_\\n,\\n(\\n) =\\n-\\n=\\n-\\n=\\xc3\\xa5\\n1\\n1\\nwhere n is the length of each of the terms u and v. The following function helps us in \\nimplementing Manhattan distance with the capability to also compute the normalized \\nManhattan distance:\\ndef manhattan_distance(u, v, norm=False):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if u.shape != v.shape:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0raise ValueError(\\'The vectors must have equal lengths.\\')\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return abs(u - v).sum() if not norm else abs(u - v).mean()\\nWe will now compute the Manhattan distance between our root term and the other \\nterms using the previous function, as shown in the following code snippet:\\n# compute Manhattan distance\\nIn [120]: for term, vector_term in zip(terms, vector_terms):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07print \\'Manhattan distance between root: {} and term: {} is \\n{}\\'.format(root_term,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07term, manhattan_distance(root_vector,  \\nvector_term, norm=False))\\nManhattan distance between root: Believe and term: believe is 8\\nManhattan distance between root: Believe and term: bargain is 38\\nTraceback (most recent call last):\\n\\xc2\\xa0\\xc2\\xa0File \"<ipython-input-120-b228f24ad6a2>\", line 4, in <module>\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0manhattan_distance(root_vector, vector_term, norm=False))\\nValueError: The vectors must have equal lengths.\\n# compute normalized Manhattan distance\\nIn [122]: for term, vector_term in zip(terms, vector_terms):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07print \\'Normalized Manhattan distance between root: {} and \\nterm: {} is {}\\'.format(root_term,\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n277\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0term,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07round(manhattan_distance(root_vector, vector_term,  \\nnorm=True),2))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\nNormalized Manhattan distance between root: Believe and term: believe is 1.14\\nNormalized Manhattan distance between root: Believe and term: bargain is 5.43\\nTraceback (most recent call last):\\n\\xc2\\xa0\\xc2\\xa0File \"<ipython-input-122-d13a48d56a22>\", line 4, in <module>\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0round(manhattan_distance(root_vector, vector_term, norm=True),2))\\nValueError: The vectors must have equal lengths.\\nFrom those results you can see that as expected, the distance between \\'Believe\\' \\nand \\'believe\\' ignoring their case is most similar to each other, with a score of 8 or \\n1.14, as compared to \\'bargain\\', which gives a score of 38 or 5.43 (here the smaller the \\nscore, the more similar the words). The term \\'Elephant\\' yields an error because it has \\na different length compared to the base term just as we noticed earlier when computing \\nHamming distances.\\nEuclidean Distance\\nWe briefly mentioned the Euclidean distance when comparing it with the Manhattan \\ndistance in the earlier section. Formally, the Euclidean distance is also known as the \\nEuclidean norm, L2 norm, or L2 distance and is defined as the shortest straight-line \\ndistance between two points. Mathematically this can be denoted as\\ned u v\\nu v\\nu\\nv\\ni\\nn\\ni\\ni\\n,\\n(\\n) =\\n-\\n=\\n-\\n(\\n)\\n=\\xc3\\xa5\\n2\\n1\\n2\\nwhere the two points u and v are vectorized text terms in our scenario, each having \\nlength n. The following function helps us in computing the Euclidean distance between \\ntwo terms:\\ndef euclidean_distance(u, v):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if u.shape != v.shape:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0raise ValueError(\\'The vectors must have equal lengths.\\')\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0distance = np.sqrt(np.sum(np.square(u - v)))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return distance\\nWe can now compare the Euclidean distance among our terms by using the \\npreceding function as depicted in the following code snippet:\\n# compute Euclidean distance\\nIn [132]: for term, vector_term in zip(terms, vector_terms):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Euclidean distance between root: {} and term: {} is \\n{}\\'.format(root_term,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07term, round(euclidean_distance(root_\\nvector, vector_term),2))\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n278\\nEuclidean distance between root: Believe and term: believe is 5.66\\nEuclidean distance between root: Believe and term: bargain is 17.94\\nTraceback (most recent call last):\\n\\xc2\\xa0\\xc2\\xa0File \"<ipython-input-132-90a4dbe8ce60>\", line 4, in <module>\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0round(euclidean_distance(root_vector, vector_term),2))\\nValueError: The vectors must have equal lengths.\\nFrom the preceding outputs you can see that the terms \\'Believe\\' and \\'believe\\' \\nare the most similar with a score of 5.66 compared to \\'bargain\\' giving us a score of 17.94, \\nand \\'Elephant\\' throws a ValueError because the base assumption that strings being \\ncompared should have equal lengths holds good for this distance metric also.\\nSo far, all the distance metrics we have used work on strings or terms of the same \\nlength and fail when they are not of equal length. So how do we deal with this problem? \\nWe will now look at a couple of distance metrics that work even with strings of unequal \\nlength to measure similarity.\\nLevenshtein Edit Distance\\nThe Levenshtein edit distance, often known as just Levenshtein distance, belongs to the \\nfamily of edit distance\\xe2\\x80\\x93based metrics and is used to measure the distance between two \\nsequence of strings based on their differences\\xe2\\x80\\x94similar to the concept behind Hamming \\ndistance. The Levenshtein edit distance between two terms can be defined as the \\nminimum number of edits needed in the form of additions, deletions, or substitutions \\nto change or convert one term to the other. These substitutions are character-based \\nsubstitutions, where a single character can be edited in a single operation. Also, as \\nmentioned before, the length of the two terms need not be equal here. Mathematically, \\nwe can represent the Levenshtein edit distance between two terms as ldu,\\xc2\\xa0v(|u|,\\xc2\\xa0|v|) such \\nthat u and v are our two terms where |u| and |v| are their lengths. This distance can be \\nrepresented by the following formula\\nld\\ni j\\ni j\\nif\\ni j\\nld\\ni\\nj\\nld\\ni j\\nu v\\nu v\\nu v\\n,\\n,\\n,\\n,\\n,\\n,\\n,\\n,\\n(\\n) =\\n(\\n)\\n(\\n) =\\n-\\n(\\n)+\\n-\\nmax\\nmin\\nmin\\n0\\n1\\n1\\n1\\n(\\n)+\\n-\\n-\\n(\\n)+\\n\\xc3\\xac\\n\\xc3\\xad\\n\\xc3\\xaf\\n\\xc3\\xae\\n\\xc3\\xaf\\n\\xc3\\xbc\\n\\xc3\\xbd\\n\\xc3\\xaf\\n\\xc3\\xbe\\n\\xc3\\xaf\\n\\xc3\\xac\\n\\xc3\\xad\\n\\xc3\\xaf\\n\\xc3\\xaf\\n\\xc3\\xae\\n\\xc3\\xaf\\n\\xc3\\xaf\\n\\xc3\\xbc\\n\\xc3\\xbd\\n\\xc3\\xaf\\n\\xc3\\xaf\\n\\xc3\\xbe\\n\\xc3\\xaf\\n\\xc2\\xb9\\n1\\n1\\n1\\nld\\ni\\nj\\nC\\notherwise\\nu v\\nu\\nv\\ni\\nj\\n,\\n,\\n\\xc3\\xaf\\nwhere i and j are basically indices for the terms u and v. The third equation in the minimum \\nabove has a cost function denoted by Cu\\nv\\ni\\nj\\n\\xc2\\xb9\\n such that it has the following conditions\\nC\\nif u\\nv\\nif u\\nv\\nu\\nv\\ni\\nj\\ni\\nj\\ni\\nj\\n\\xc2\\xb9\\n=\\n\\xc2\\xb9\\n=\\n\\xc3\\xac\\n\\xc3\\xad\\xc3\\xaf\\n\\xc3\\xae\\xc3\\xaf\\n\\xc3\\xbc\\n\\xc3\\xbd\\xc3\\xaf\\n\\xc3\\xbe\\xc3\\xaf\\n1\\n0\\nand this denotes the indicator function, which depicts the cost associated with two \\ncharacters being matched for the two terms (the equation represents the match or \\nmismatch operation). The first equation in the previous minimum stands for the deletion \\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n279\\noperation, and the second equation represents the insertion operation. The function \\nldu,\\xc2\\xa0v(i,\\xe2\\x80\\x89j) thus covers all the three operations of insertion, deletion, and addition as we \\nmentioned earlier and it denotes the Levenshtein distance as measured between the first \\ni characters for the term u and the first j characters of the term v. There are also several \\ninteresting boundary conditions with regard to the Levenshtein edit distance:\\n\\xe2\\x80\\xa2 \\nThe minimum value that the edit distance between two terms can \\ntake is the difference in length of the two terms.\\n\\xe2\\x80\\xa2 \\nThe maximum value of the edit distance between two terms can \\nbe the length of the term that is larger.\\n\\xe2\\x80\\xa2 \\nIf the two terms are equal, the edit distance is zero.\\n\\xe2\\x80\\xa2 \\nHamming distance between two terms is an upper bound for \\nLevenshtein edit distance if and only if the two terms have equal \\nlength.\\n\\xe2\\x80\\xa2 \\nThis being a distance metric also satisfies the triangle inequality \\nproperty, discussed earlier when we talked about distance \\nmetrics.\\nThere are various ways of implementing Levenshtein distance computations for \\nterms. Here we will start with an example of two of our terms. Considering the root term \\n\\'believe\\' and another term \\'beleive\\' (we ignore case in our computations). The edit \\ndistance would be 2 because we would need the following two operations:\\n\\xe2\\x80\\xa2 \\n\\'beleive\\' \\xe2\\x86\\x92 \\'beliive\\' (substitution of e to i)\\n\\xe2\\x80\\xa2 \\n\\'beliive\\' \\xe2\\x86\\x92 \\'believe\\' (substitution of i to e)\\nTo implement this, we build a matrix that will basically compute the Levenshtein \\ndistance between all the characters of both terms by comparing each character of the \\nfirst term with the characters of the second term. For computation, we follow a dynamic \\nprogramming approach to get the edit distance between the two terms based on the \\nlast computed value. For the given two terms, the Levenshtein edit distance matrix our \\nalgorithm should generate is shown in Figure\\xc2\\xa06-1.\\nFigure 6-1.\\xe2\\x80\\x82 Levenshtein edit distance matrix between terms\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n280\\nYou can see in Figure\\xc2\\xa06-1 that the edit distances are computed for each pair of \\ncharacters in the terms, as mentioned earlier, and the final edit distance value highlighted \\nin the figure gives us the actual edit distance between the two terms. This algorithm is \\nalso known as the Wagner-Fischer algorithm and is available in the paper by R. Wagner \\nand M. Fischer titled \\xe2\\x80\\x9cThe String-to-String Correction Problem,\\xe2\\x80\\x9d which you can refer to \\nif you are more interested in the details. The pseudocode for the same is shown in the \\nsnippet below, courtesy of the paper:\\nfunction levenshtein_distance(char u[1..m], char v[1..n]):\\n# for all i and j, d[i,j] will hold the Levenshtein distance between the \\nfirst i characters of\\xc2\\xa0\\xc2\\xa0\\n# u and the first j characters of v, note that d has (m+1)*(n+1) values\\nint d[0..m, 0..n]\\n# set each element in d to zero\\nd[0..m, 0..n] := 0\\n# source prefixes can be transformed into empty string by dropping all \\ncharacters\\nfor i from 1 to m:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0d[i, 0] := i\\n# target prefixes can be reached from empty source prefix by inserting every \\ncharacter\\nfor j from 1 to n:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0d[0, j] := j\\n# build the edit distance matrix\\nfor j from 1 to n:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for i from 1 to m:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if s[i] = t[j]:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0substitutionCost := 0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0substitutionCost := 1\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0d[i, j] := minimum(d[i-1, j] + 1,\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# deletion\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0d[i, j-1] + 1,\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# insertion\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0d[i-1, j-1] + substitutionCost)\\xc2\\xa0\\xc2\\xa0# substitution\\n# the final value of the matrix is the edit distance between the terms\\nreturn d[m, n]\\nYou can see from the preceding function definition pseudocode how we have \\ncaptured the necessary formulae we used earlier to define Levenshtein edit distance.\\nWe will now implement this pseudocode in Python. The preceding algorithm uses \\nO(mn) space because it stores the entire distance matrix, but it is enough to just store the \\nprevious and current row of distances to get to the final result. We will do the same in our \\ncode but we will also store the results in a matrix so that we can visualize it in the end. The \\nfollowing function implements Levenshtein edit distance as mentioned:\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n281\\nimport copy\\nimport pandas as pd\\ndef levenshtein_edit_distance(u, v):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# convert to lower case\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0u = u.lower()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0v = v.lower()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# base cases\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0if u == v: return 0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0elif len(u) == 0: return len(v)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0elif len(v) == 0: return len(u)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# initialize edit distance matrix\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0edit_matrix = []\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# initialize two distance matrices\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0du = [0] * (len(v) + 1)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0dv = [0] * (len(v) + 1)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# du: the previous row of edit distances\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for i in range(len(du)):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0du[i] = i\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# dv : the current row of edit distances\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for i in range(len(u)):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0dv[0] = i + 1\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# compute cost as per algorithm\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for j in range(len(v)):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cost = 0 if u[i] == v[j] else 1\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0dv[j + 1] = min(dv[j] + 1, du[j + 1] + 1, du[j] + cost)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# assign dv to du for next iteration\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for j in range(len(du)):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0du[j] = dv[j]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# copy dv to the edit matrix\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0edit_matrix.append(copy.copy(dv))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# compute the final edit distance and edit matrix\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0distance = dv[len(v)]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0edit_matrix = np.array(edit_matrix)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0edit_matrix = edit_matrix.T\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0edit_matrix = edit_matrix[1:,]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0edit_matrix = pd.DataFrame(data=edit_matrix,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0index=list(v),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0columns=list(u))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return distance, edit_matrix\\nThat function returns both the final Levenshtein edit distance and the complete edit \\nmatrix between the two terms u and v, which are taken as input. Remember, we need to \\npass the terms directly in their raw string format and not their vector representations. \\nAlso, we do not consider case of strings here and convert them to lowercase.\\nThe following snippet computes the Levenshtein edit distance between our example \\nterms using the preceding function:\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n282\\nIn [223]: for term in terms:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0edit_d, edit_m = levenshtein_edit_distance(root_term, term)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07print \\'Computing distance between root: {} and term: {}\\'. \\nformat(root_term,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0term)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Levenshtein edit distance is {}\\'.format(edit_d)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'The complete edit distance matrix is depicted below\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print edit_m\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'-\\'*30\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\nComputing distance between root: Believe and term: beleive\\nLevenshtein edit distance is 2\\nThe complete edit distance matrix is depicted below\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0b\\xc2\\xa0\\xc2\\xa0e\\xc2\\xa0\\xc2\\xa0l\\xc2\\xa0\\xc2\\xa0i\\xc2\\xa0\\xc2\\xa0e\\xc2\\xa0\\xc2\\xa0v\\xc2\\xa0\\xc2\\xa0e\\nb\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa06\\ne\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa05\\nl\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa04\\ne\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa03\\ni\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa03\\nv\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa03\\ne\\xc2\\xa0\\xc2\\xa06\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa02\\n------------------------------\\nComputing distance between root: Believe and term: bargain\\nLevenshtein edit distance is 6\\nThe complete edit distance matrix is depicted below\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0b\\xc2\\xa0\\xc2\\xa0e\\xc2\\xa0\\xc2\\xa0l\\xc2\\xa0\\xc2\\xa0i\\xc2\\xa0\\xc2\\xa0e\\xc2\\xa0\\xc2\\xa0v\\xc2\\xa0\\xc2\\xa0e\\nb\\xc2\\xa0\\xc2\\xa00\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa06\\na\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa06\\nr\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa06\\ng\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa06\\na\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa06\\ni\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa06\\nn\\xc2\\xa0\\xc2\\xa06\\xc2\\xa0\\xc2\\xa06\\xc2\\xa0\\xc2\\xa06\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa06\\xc2\\xa0\\xc2\\xa06\\n------------------------------\\nComputing distance between root: Believe and term: Elephant\\nLevenshtein edit distance is 7\\nThe complete edit distance matrix is depicted below\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0b\\xc2\\xa0\\xc2\\xa0e\\xc2\\xa0\\xc2\\xa0l\\xc2\\xa0\\xc2\\xa0i\\xc2\\xa0\\xc2\\xa0e\\xc2\\xa0\\xc2\\xa0v\\xc2\\xa0\\xc2\\xa0e\\ne\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa06\\nl\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa01\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa05\\ne\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa02\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa04\\np\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa03\\xc2\\xa0\\xc2\\xa04\\nh\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa04\\xc2\\xa0\\xc2\\xa04\\na\\xc2\\xa0\\xc2\\xa06\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa05\\xc2\\xa0\\xc2\\xa05\\nn\\xc2\\xa0\\xc2\\xa07\\xc2\\xa0\\xc2\\xa06\\xc2\\xa0\\xc2\\xa06\\xc2\\xa0\\xc2\\xa06\\xc2\\xa0\\xc2\\xa06\\xc2\\xa0\\xc2\\xa06\\xc2\\xa0\\xc2\\xa06\\nt\\xc2\\xa0\\xc2\\xa08\\xc2\\xa0\\xc2\\xa07\\xc2\\xa0\\xc2\\xa07\\xc2\\xa0\\xc2\\xa07\\xc2\\xa0\\xc2\\xa07\\xc2\\xa0\\xc2\\xa07\\xc2\\xa0\\xc2\\xa07\\n------------------------------\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n283\\nYou can see from the preceding outputs that \\'Believe\\' and \\'beleive\\' are the \\nclosest to each other, with an edit distance of 2 and the distances between \\'Believe\\', \\n\\'bargain\\', and \\'Elephant\\' are 6, indicating a total of 6 edit operations needed. The edit \\ndistance matrices provide a more detailed insight into how the algorithm computes the \\ndistances per iteration.\\nCosine Distance and Similarity\\nThe Cosine distance is a metric that can be actually derived from the Cosine similarity and \\nvice versa. Considering we have two terms such that they are represented in their \\nvectorized forms, Cosine similarity gives us the measure of the cosine of the angle \\nbetween them when they are represented as non-zero positive vectors in an inner \\nproduct space. Thus term vectors having similar orientation will have scores closer to 1 \\n( cos0\\xef\\x81\\xaf) indicating the vectors are very close to each other in the same direction (near to \\nzero degree angle between them). Term vectors having a similarity score close to 0 \\n( cos90\\xef\\x81\\xaf) indicate unrelated terms with a near orthogonal angle between then. Term \\nvectors with a similarity score close to \\xe2\\x80\\x931 ( cos180\\xef\\x81\\xaf) indicate terms that are completely \\noppositely oriented to each other. Figure\\xc2\\xa06-2 illustrates this more clearly, where u and v \\nare our term vectors in the vector space.\\nThus you can see from the position of the vectors, the plots show more clearly how \\nthe vectors are close or far apart from each other, and the cosine of the angle between \\nthem gives us the Cosine similarity metric. Now we can formally define Cosine similarity \\nas the dot product of the two term vectors u and v, divided by the product of their L2 \\nnorms. Mathematically, we can represent the dot product between two vectors as\\nu v\\nu\\nv\\n\\xc3\\x97 =\\n( )\\ncos q\\nwhere \\xce\\xb8 is the angle between u and v and u  represents the L2 norm for vector u and v  \\nis the L2 norm for vector v. Thus we can derive the Cosine similarity from the above \\nformula as\\nFigure 6-2.\\xe2\\x80\\x82 Cosine similarity representations for term vectors\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n284\\ncs u v\\nu v\\nu\\nv\\nu v\\nu\\nv\\ni\\nn\\ni\\ni\\ni\\nn\\ni\\ni\\nn\\ni\\n,\\ncos\\n(\\n) =\\n( ) =\\n\\xc3\\x97\\n=\\n=\\n=\\n=\\n\\xc3\\xa5\\n\\xc3\\xa5\\n\\xc3\\xa5\\nq\\n1\\n1\\n2\\n1\\n2\\nwhere cs(u,\\xc2\\xa0v) is the Cosine similarity score between u and v. Here ui and vi are the \\nvarious features or components of the two vectors, and the total number of these features \\nor components is n. In our case, we will be using the Bag of Characters vectorization to \\nbuild these term vectors, and n will be the number of unique characters across the terms \\nunder analysis. An important thing to note here is that the Cosine similarity score usually \\nranges from \\xe2\\x80\\x931 to +1, but if we use the Bag of Characters\\xe2\\x80\\x93based character frequencies for \\nterms or Bag of Words\\xe2\\x80\\x93based word frequencies for documents, the score will range from 0 \\nto 1 because the frequency vectors can never be negative, and hence the angle between \\nthe two vectors cannot exceed 90\\xef\\x81\\xaf. The Cosine distance is complimentary to the \\nsimilarity score can be computed by the formula,\\ncd u v\\ncs u v\\nu v\\nu\\nv\\nu v\\nu\\ni\\nn\\ni\\ni\\ni\\nn\\ni\\ni\\nn\\n,\\n,\\ncos\\n(\\n) = -\\n(\\n) = -\\n( ) = -\\n\\xc3\\x97\\n=\\n-\\n=\\n=\\n=\\n\\xc3\\xa5\\n\\xc3\\xa5\\n1\\n1\\n1\\n1\\n1\\n1\\n2\\n1\\nq\\n\\xc3\\xa5vi\\n2\\nwhere cd(u,\\xc2\\xa0v) denotes the Cosine distance between the term vectors u and v. The \\nfollowing function implements computation of Cosine distance based on the preceding \\nformulae:\\ndef cosine_distance(u, v):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0distance = 1.0 - (np.dot(u, v) /\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07(np.sqrt(sum(np.square(u))) * np.sqrt(sum(np.\\nsquare(v))))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return distance\\nWe will now test the similarity between our example terms using their Bag of \\nCharacter representations, which we created earlier, available in the boc_root_vector \\nand the boc_vector_terms variables, as depicted in the following code snippet:\\nIn [235]: for term, boc_term in zip(terms, boc_vector_terms):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Analyzing similarity between root: {} and term: {}\\'. \\nformat(root_term,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0term)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0distance = round(cosine_distance(root_boc_vector, boc_term),2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0similarity = 1 - distance\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Cosine distance\\xc2\\xa0\\xc2\\xa0is {}\\'.format(distance)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Cosine similarity\\xc2\\xa0\\xc2\\xa0is {}\\'.format(similarity)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'-\\'*40\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n285\\nAnalyzing similarity between root: Believe and term: believe\\nCosine distance\\xc2\\xa0\\xc2\\xa0is -0.0\\nCosine similarity\\xc2\\xa0\\xc2\\xa0is 1.0\\n----------------------------------------\\nAnalyzing similarity between root: Believe and term: bargain\\nCosine distance\\xc2\\xa0\\xc2\\xa0is 0.82\\nCosine similarity\\xc2\\xa0\\xc2\\xa0is 0.18\\n----------------------------------------\\nAnalyzing similarity between root: Believe and term: Elephant\\nCosine distance\\xc2\\xa0\\xc2\\xa0is 0.39\\nCosine similarity\\xc2\\xa0\\xc2\\xa0is 0.61\\n----------------------------------------\\nThese vector representations do not take order of characters into account, hence \\nthe similarity between the terms \"Believe\" and \"believe\" is 1.0 or a perfect 100 percent \\nbecause it contains the same characters with the same frequency. You can see how this \\ncan be used in combination with a semantic dictionary like WordNet to provide correct \\nspelling suggestions by suggesting semantically and syntactically correct words from \\na vocabulary when users type a misspelled word, by measuring the similarity between \\nthe words. You can even try our different features here instead of single character \\nfrequencies, like taking two characters at a time and computing their frequencies to build \\nthe term vectors. This takes into account some of the sequences that characters maintain \\nin various terms. Try out different possibilities and compare the results! This distance \\nmeasure works very well when measuring similarity between large documents or \\nsentences, and we will see that in the next section when we discuss document similarity.\\nAnalyzing Document Similarity\\nWe analyzed similarity between terms using various similarity and distance metrics in \\nthe previous sections. We also saw how vectorization was useful so that mathematical \\ncomputations become much easier, especially when computing distances between \\nvectors. In this section, we will try to analyze similarities between documents. By now, \\nyou must already know that a document is defined as a body of text which can be \\ncomprised of sentences or paragraphs of text. For analyzing document similarity, we will \\nbe using our utils module to extract features from document using the build_feature_\\nmatrix() function. We will vectorize documents using their TF-IDFs similarly to what \\nwe did previously when we classified text documents or summarized entire documents. \\nOnce we have the vector representations of the various documents, we will compute \\nsimilarity between the documents using several distance or similarity metrics. The \\nmetrics we will cover in this section are as follows:\\n\\xe2\\x80\\xa2 \\nCosine similarity\\n\\xe2\\x80\\xa2 \\nHellinger-Bhattacharya distance\\n\\xe2\\x80\\xa2 \\nOkapi BM25 ranking\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n286\\nAs usual, we will cover the concepts behind each metric, look at its mathematical \\nrepresentations and definitions, and then implement it using Python. We will also test \\nour metrics on a toy corpus here with nine documents and a separate corpus with three \\ndocuments, which will be our query documents. For each of these three documents, we \\nwill try to find out the most similar documents from the corpus of nine documents, which \\nwill act as our index. Consider this to be a mini-simulation of what happens in a search \\nengine when you search with a sentence and the most relevant results are returned to you \\nfrom its index of web pages. In our case, the queries are in the form of three documents, \\nand relevant documents for each of these three will be returned from the index of nine \\ndocuments based on similarity metrics.\\nWe will start with loading the necessary dependencies and the corpus of documents \\non which we will be testing our various metrics, as shown in the following code snippet:\\nfrom normalization import normalize_corpus\\nfrom utils import build_feature_matrix\\nimport numpy as np\\n# load the toy corpus index\\ntoy_corpus = [\\'The sky is blue\\',\\n\\'The sky is blue and beautiful\\',\\n\\'Look at the bright blue sky!\\',\\n\\'Python is a great Programming language\\',\\n\\'Python and Java are popular Programming languages\\',\\n\\'Among Programming languages, both Python and Java are the most used in \\nAnalytics\\',\\n\\'The fox is quicker than the lazy dog\\',\\n\\'The dog is smarter than the fox\\',\\n\\'The dog, fox and cat are good friends\\']\\n# load the docs for which we will be measuring similarities\\nquery_docs = [\\'The fox is definitely smarter than the dog\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\'Java is a static typed programming language unlike Python\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\'I love to relax under the beautiful blue sky!\\']\\xc2\\xa0\\xc2\\xa0\\nFrom that snippet you can see that we have various documents in our corpus index \\nthat talk about the sky, programming languages, and animals. We also have three query \\ndocuments for which we want to get the most relevant documents from the toy_corpus \\nindex, based on similarity computations. Before we start looking at metrics, we will \\nnormalize the documents and vectorize them by extracting their TF-IDF features, as \\nshown in the following snippet:\\n# normalize and extract features from the toy corpus\\nnorm_corpus = normalize_corpus(toy_corpus, lemmatize=True)\\ntfidf_vectorizer, tfidf_features = build_feature_matrix(norm_corpus,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07feature_\\ntype=\\'tfidf\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ngram_range=(1, 1),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07min_df=0.0, max_\\ndf=1.0)\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n287\\n# normalize and extract features from the query corpus\\nnorm_query_docs =\\xc2\\xa0\\xc2\\xa0normalize_corpus(query_docs, lemmatize=True)\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\nquery_docs_tfidf = tfidf_vectorizer.transform(norm_query_docs)\\nNow that we have our documents normalized and vectorized with TF-IDF\\xe2\\x80\\x93based \\nvector representations, we will look at how to compute similarity for each of the metrics \\nwe specified at the beginning of this section.\\nCosine Similarity\\nWe have seen the concepts with regards to computing Cosine similarity and also \\nimplemented the same for term similarity. Here, we will reuse the same concepts to \\ncompute the Cosine similarity scores for documents instead of terms. The document \\nvectors will be the Bag of Words model\\xe2\\x80\\x93based vectors with TF-IDF values instead of \\nterm frequencies. We have also taken only unigrams here, but you can experiment with \\nbigrams and so on as document features during the vectorization process. For each of \\nthe three query documents, we will compute its similarity with the nine documents in \\ntoy_corpus and return the n most similar documents where n is a user input parameter.\\nWe will define a function that will take in the vectorized corpus and the document \\ncorpus for which we want to compute similarities. We will get the similarity scores using the \\ndot product operation as before and finally we will sort them in reverse order and get the \\ntop n documents with the highest similarity score. The following function implements this:\\ndef compute_cosine_similarity(doc_features, corpus_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_n=3):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get document vectors\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0doc_features = doc_features.toarray()[0]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0corpus_features = corpus_features.toarray()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# compute similarities\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0similarity = np.dot(doc_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0corpus_features.T)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get docs with highest similarity scores\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_docs = similarity.argsort()[::-1][:top_n]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_docs_with_score = [(index, round(similarity[index], 3))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index in top_docs]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return top_docs_with_score\\nIn that function, corpus_features are the vectorized documents belonging to the \\ntoy_corpus index from which we want to retrieve similar documents. These documents \\nwill be retrieved on the basis of their similarity score with doc_features, which basically \\nrepresents the vectorized document belonging to each of the query_docs, as shown in the \\nfollowing snippet:\\n# get Cosine similarity results for our example documents\\nIn [243]: print \\'Document Similarity Analysis using Cosine Similarity\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'=\\'*60\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: for index, doc in enumerate(query_docs):\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n288\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0doc_tfidf = query_docs_tfidf[index]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_similar_docs = compute_cosine_similarity(doc_tfidf,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tfidf_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_n=2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Document\\',index+1 ,\\':\\', doc\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Top\\', len(top_similar_docs), \\'similar docs:\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'-\\'*40\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for doc_index, sim_score in top_similar_docs:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07print \\'Doc num: {} Similarity Score: {}\\\\nDoc: {}\\'.\\nformat(doc_index+1,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0 \\nsim_score, toy_corpus[doc_index])\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'-\\'*40\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print\\xc2\\xa0\\xc2\\xa0\\nDocument Similarity Analysis using Cosine Similarity\\n============================================================\\nDocument 1 : The fox is definitely smarter than the dog\\nTop 2 similar docs:\\n----------------------------------------\\nDoc num: 8 Similarity Score: 1.0\\nDoc: The dog is smarter than the fox\\n----------------------------------------\\nDoc num: 7 Similarity Score: 0.426\\nDoc: The fox is quicker than the lazy dog\\n----------------------------------------\\nDocument 2 : Java is a static typed programming language unlike Python\\nTop 2 similar docs:\\n----------------------------------------\\nDoc num: 5 Similarity Score: 0.837\\nDoc: Python and Java are popular Programming languages\\n----------------------------------------\\nDoc num: 6 Similarity Score: 0.661\\nDoc: Among Programming languages, both Python and Java are the most used in \\nAnalytics\\n----------------------------------------\\nDocument 3 : I love to relax under the beautiful blue sky!\\nTop 2 similar docs:\\n----------------------------------------\\nDoc num: 2 Similarity Score: 1.0\\nDoc: The sky is blue and beautiful\\n----------------------------------------\\nDoc num: 1 Similarity Score: 0.72\\nDoc: The sky is blue\\n----------------------------------------\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n289\\nThe preceding output depicts the top two most relevant documents for each of the \\nquery documents based on Cosine similarity scores, and you can see that the outputs \\nare quite what were expected. Documents about animals are similar to the document \\nthat mentions the fox and the dog; documents about Python and Java are most similar to \\nthe query document talking about them; and the beautiful blue sky is indeed similar to \\ndocuments that talk about the sky being blue and beautiful! \\nAlso note the Cosine similarity scores in the preceding outputs, where 1.0 indicates \\nperfect similarity, 0.0 indicates no similarity, and any score between them indicates some \\nlevel of similarity based on how large that score is. For instance, in the last example, \\nthe main document vectors are [\\'sky\\', \\'blue\\', \\'beautiful\\'] and because they all \\nmatch with the first document from the toy corpus, we get a 1.0 or 100 percent similarity \\nscore, and only [\\'sky\\', \\'blue\\'] match from the second most similar document, and \\nwe get a 0.72 or 72 percent similarity score. And you should remember our discussion \\nfrom earlier where I mentioned briefly that Cosine similarity using Bag of Words\\xe2\\x80\\x93based \\nvectors only looks at token weights and does not consider order or sequence of the terms, \\nwhich is quite desirable in large documents because the same content may be depicted \\nin different ways, and capturing sequences there might lead to loss of information due to \\nunwanted mismatches. \\nWe recommend using scikit-learn\\xe2\\x80\\x99s cosine_similarity() utility function, which \\nyou can find under the sklearn.metrics.pairwise module. It uses similar logic as our \\nimplementation but is much more optimized and performs well on large corpora of \\ndocuments. You can also use gensim\\xe2\\x80\\x99s similarities module or the cossim() function \\ndirectly available in the gensim.matutils module.\\nHellinger-Bhattacharya Distance\\nThe Hellinger-Bhattacharya distance (HB-distance) is also called the Hellinger distance or the \\nBhattacharya distance. The Bhattacharya distance, originally introduced by A. Bhattacharya, \\nis used to measure the similarity between two discrete or continuous probability \\ndistributions. E. Hellinger introduced the Hellinger integral in 1909, which is used in the \\ncomputation of the Hellinger distance. Overall, the Hellinger-Bhattacharya distance is an \\nf-divergence, which in the theory of probability is defined as a function D\\nP Q\\nf\\n||\\n(\\n) , which \\ncan be used to measure the difference between P and Q probability distributions. There are \\nmany instances of f-divergences, including KL-divergence and HB-distance. Remember that \\nKL-divergence is not a distance metric because it violates the symmetric condition from the \\nfour conditions necessary for a distance measure to be a metric.\\nHB-distance is computable for both continuous and discrete probability \\ndistributions. In our case, we will be using the TF-IDF\\xe2\\x80\\x93based vectors as our document \\ndistributions. This makes it discrete distributions because we have specific TF-IDF values \\nfor specific feature terms, unlike continuous distributions. We can define the Hellinger-\\nBhattacharya distance mathematically as\\nhbd u v\\nu\\nv\\n,\\n(\\n) =\\n-\\n1\\n2\\n2\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n290\\nwhere hbd(u,\\xc2\\xa0v) denotes the Hellinger-Bhattacharya distance between the document \\nvectors u and v, and it is equal to the Euclidean or L2 norm of the difference of the square \\nroot of the vectors divided by the square root of 2. Considering the document vectors u and \\nv to be discrete with n number of features, we can further expand the above formula into\\nhbd u v\\nu\\nv\\ni\\nn\\ni\\ni\\n,\\n(\\n) =\\n-\\n(\\n)\\n=\\xc3\\xa5\\n1\\n2\\n1\\n2\\nsuch that u\\nu u\\nun\\n=\\n\\xc2\\xbc\\n(\\n)\\n1\\n2\\n,\\n,\\n,\\n and v\\nv v\\nvn\\n=\\n\\xc2\\xbc\\n(\\n)\\n1\\n2\\n,\\n,\\n,\\n are the document vectors having \\nlength n indicating n features, which are the TF-IDF weights of the various terms in the \\ndocuments. As with the previous computation of Cosine similarity, we will build our \\nfunction on the same principles; basically we will accept as input a corpus of document \\nvectors and a single document vector for which we want to get the n most similar \\ndocuments from the corpus based on their HB-distances. The function implements the \\npreceding concepts in Python in the following snippet:\\ndef compute_hellinger_bhattacharya_distance(doc_features, corpus_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_n=3):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get document vectors\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0doc_features = doc_features.toarray()[0]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0corpus_features = corpus_features.toarray()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# compute hb distances\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0distance = np.hstack(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0np.sqrt(0.5 *\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0np.sum(\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0np.square(np.sqrt(doc_features) -\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0np.sqrt(corpus_features)),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0axis=1)))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get docs with lowest distance scores\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_docs = distance.argsort()[:top_n]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_docs_with_score = [(index, round(distance[index], 3))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index in top_docs]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return top_docs_with_score\\nFrom the preceding implementation, you case see that we sort the documents based \\non their scores in ascending order, unlike Cosine similarity, where 1.0 indicates perfect \\nsimilarity\\xe2\\x80\\x94since this is a distance metric between distributions, a value of 0 indicates \\nperfect similarity, and higher values indicate some dissimilarity being present. We can \\nnow apply this function to our example corpora, compute their HB-distances, and see the \\nresults in the following snippet:\\n# get Hellinger-Bhattacharya distance based similarities for our example \\ndocuments\\nIn [246]: print \\'Document Similarity Analysis using Hellinger-Bhattacharya \\ndistance\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'=\\'*60\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: for index, doc in enumerate(query_docs):\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n291\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0doc_tfidf = query_docs_tfidf[index]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07top_similar_docs = compute_hellinger_bhattacharya_\\ndistance(doc_tfidf,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0tfidf_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_n=2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Document\\',index+1 ,\\':\\', doc\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Top\\', len(top_similar_docs), \\'similar docs:\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'-\\'*40\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for doc_index, sim_score in top_similar_docs:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07print \\'Doc num: {} Distance Score: {}\\\\nDoc: {}\\'.\\nformat(doc_index+1,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07sim_score, toy_corpus[doc_\\nindex])\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'-\\'*40\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\nDocument Similarity Analysis using Hellinger-Bhattacharya distance\\n============================================================\\nDocument 1 : The fox is definitely smarter than the dog\\nTop 2 similar docs:\\n----------------------------------------\\nDoc num: 8 Distance Score: 0.0\\nDoc: The dog is smarter than the fox\\n----------------------------------------\\nDoc num: 7 Distance Score: 0.96\\nDoc: The fox is quicker than the lazy dog\\n----------------------------------------\\nDocument 2 : Java is a static typed programming language unlike Python\\nTop 2 similar docs:\\n----------------------------------------\\nDoc num: 5 Distance Score: 0.53\\nDoc: Python and Java are popular Programming languages\\n----------------------------------------\\nDoc num: 4 Distance Score: 0.766\\nDoc: Python is a great Programming language\\n----------------------------------------\\nDocument 3 : I love to relax under the beautiful blue sky!\\nTop 2 similar docs:\\n----------------------------------------\\nDoc num: 2 Distance Score: 0.0\\nDoc: The sky is blue and beautiful\\n----------------------------------------\\nDoc num: 1 Distance Score: 0.602\\nDoc: The sky is blue\\n----------------------------------------\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n292\\nYou can see from the preceding outputs that documents with lower HB-distance \\nscores are more similar to the query documents, and the result documents are quite \\nsimilar to what we obtained using Cosine similarity. Compare the results and try out \\nthese functions with larger corpora! I recommend using gensim\\xe2\\x80\\x99s hellinger() function, \\navailable in the gensim.matutils module (which uses the same logic as our preceding \\nfunction) when building large-scale systems for analyzing similarity.\\nOkapi BM25 Ranking\\nThere are several techniques that are quite popular in information retrieval and \\nsearch engines, including PageRank and Okapi BM25. The acronym BM stands for best \\nmatching. This technique is also known as BM25, but for the sake of completeness I refer \\nto it as Okapi BM25, because originally although the concepts behind the BM25 function \\nwere merely theoretical, the City University in London built the Okapi Information \\nRetrieval system in the 1980s\\xe2\\x80\\x9390s, which implemented this technique to retrieve \\ndocuments on actual real-world data. This technique can also be called a framework \\nor model based on probabilistic relevancy and was developed by several people in the \\n1970s\\xe2\\x80\\x9380s, including computer scientists S. Robertson and K. Jones. There are several \\nfunctions that rank documents based on different factors, and BM25 is one of them. Its \\nnewer variant is BM25F; other variants include BM15 and BM25+.\\nThe Okapi BM25 can be formally defined as a document ranking and retrieval function \\nbased on a Bag of Words\\xe2\\x80\\x93based model for retrieving relevant documents based on a user \\ninput query. This query can be itself a document containing a sentence or collection of \\nsentences, or it can even be a couple of words. The Okapi BM25 is actually not just a single \\nfunction but is a framework consisting of a whole collection of scoring functions combined \\ntogether. Say we have a query document QD such that QD\\nq q\\nqn\\n=\\n\\xc2\\xbc\\n(\\n)\\n1\\n2\\n,\\n,\\n,\\n containing n \\nterms or keywords and we have a corpus document CD in the corpus of documents from \\nwhich we want to get the most relevant documents to the query document based on \\nsimilarity scores, just as we have done earlier. Assuming we have these, we can \\nmathematically define the BM25 score between these two documents as\\nbm\\nCD QD\\nidf q\\nf q\\nCD\\nk\\nf q\\nCD\\nk\\nb\\nb\\ni\\nn\\ni\\ni\\ni\\n25\\n1\\n1\\n1\\n1\\n1\\n,\\n,\\n,\\n(\\n) =\\n(\\n)\\xc3\\x97\\n(\\n)\\xc3\\x97\\n+\\n(\\n)\\n(\\n)+\\n\\xc3\\x97\\n-\\n+\\n\\xc3\\x97\\n=\\xc3\\xa5\\nCD\\navgdl\\n\\xc3\\xa6\\n\\xc3\\xa8\\n\\xc3\\xa7\\n\\xc3\\xb6\\n\\xc3\\xb8\\n\\xc3\\xb7\\nwhere the function bm25(CD,\\xc2\\xa0QD) computes the BM25 rank or score of the document \\nCD based on the query document QD. The function idf(qi)\\xc2\\xa0\\xc2\\xa0gives us the inverse document \\nfrequency (IDF) of the term qi in the corpus that contains CD and from which we want \\nto retrieve the relevant documents. If you remember, we computed IDFs in Chapter 4 \\nwhen we implemented the TF-IDF feature extractor. Just to refresh your memory, it can \\nrepresented by\\nidf t\\nC\\ndf t\\n( ) = +\\n+\\n( )\\n1\\n1\\nlog\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n293\\nwhere idf(t) represents the idf for the term t and C represents the count of the total \\nnumber of documents in our corpus and df(t) represents the frequency of the number \\nof documents in which the term t is present. There are various other methods of \\nimplementing IDF, but we will be using this one, and on a side note the end outcome \\nfrom the different implementations is very similar. The function f(qi,\\xe2\\x80\\x82 CD) gives us the \\nfrequency of the term qi in the corpus document CD. The expression |CD| indicates the \\ntotal length of the document CD which is measured by its number of words, and the \\nterm avgdl represents the average document length of the corpus from which we will be \\nretrieving documents. Besides that, you will also observe there are two free parameters, \\nk1, which is usually in the range of [1.2, 2.0], and b, which is usually taken as 0.75. We \\nwill be taking the value of k1 to be 1.5 in our implementation.\\nThere are several steps we must go through to successfully implement and compute \\nBM25 scores for documents:\\n\\t\\n1.\\t\\nBuild a function to get inverse document frequency (IDF) \\nvalues for terms in corpus.\\n\\t\\n2.\\t\\nBuild a function for computing BM25 scores for query \\ndocument and corpus documents.\\n\\t\\n3.\\t\\nGet Bag of Words\\xe2\\x80\\x93based features for corpus documents and \\nquery documents.\\n\\t\\n4.\\t\\nCompute average length of corpus documents and IDFs of the \\nterms in the corpus documents using function from point 1.\\n\\t\\n5.\\t\\nCompute BM25 scores, rank relevant documents, and fetch \\nthe n most relevant documents for each query document \\nusing the function in point 2.\\nWe will start with implementing a function to extract and compute inverse document \\nfrequencies of all the terms in a corpus of documents by using its Bag of Words features, \\nwhich will contain the term frequencies, and then convert them to IDFs using the formula \\nmentioned earlier. The following function implements this:\\nimport scipy.sparse as sp\\ndef compute_corpus_term_idfs(corpus_features, norm_corpus):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0dfs = np.diff(sp.csc_matrix(corpus_features, copy=True).indptr)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0dfs = 1 + dfs # to smoothen idf later\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0total_docs = 1 + len(norm_corpus)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0idfs = 1.0 + np.log(float(total_docs) / dfs)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return idfs\\nWe will now implement the main function for computing BM25 score for all \\nthe documents in our corpus based on the query document and retrieving the top n \\nrelevant documents from the corpus based on their BM25 score. The following function \\nimplements the BM25 scoring framework:\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n294\\ndef compute_bm25_similarity(doc_features, corpus_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0corpus_doc_lengths, avg_doc_length,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0term_idfs, k1=1.5, b=0.75, top_n=3):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get corpus bag of words features\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0corpus_features = corpus_features.toarray()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# convert query document features to binary features\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# this is to keep a note of which terms exist per document\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0doc_features = doc_features.toarray()[0]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0doc_features[doc_features >= 1] = 1\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# compute the document idf scores for present terms\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0doc_idfs = doc_features * term_idfs\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# compute numerator expression in BM25 equation\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0numerator_coeff = corpus_features * (k1 + 1)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0numerator = np.multiply(doc_idfs, numerator_coeff)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# compute denominator expression in BM25 equation\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0denominator_coeff =\\xc2\\xa0\\xc2\\xa0k1 * (1 - b +\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0(b * (corpus_doc_lengths /\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0avg_doc_length)))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0denominator_coeff = np.vstack(denominator_coeff)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0denominator = corpus_features + denominator_coeff\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# compute the BM25 score combining the above equations\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0bm25_scores = np.sum(np.divide(numerator,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0denominator),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0axis=1)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get top n relevant docs with highest BM25 score\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_docs = bm25_scores.argsort()[::-1][:top_n]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_docs_with_score = [(index, round(bm25_scores[index], 3))\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index in top_docs]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return top_docs_with_score\\nThe comments in the function are self-explanatory and explain how the BM25 \\nscoring function is implemented. In simple terms, we first compute the numerator \\nexpression in the BM25 mathematical equation we specified earlier and then compute \\nthe denominator expression. Finally, we divide the numerator by the denominator to get \\nthe BM25 scores for all the corpus documents. Then we sort them in descending order \\nand return the top n relevant documents with the highest BM25 score. In the following \\nsnippet, we will test our function on our example corpora and see how it performs for \\neach of the query documents:\\n# build bag of words based features first\\nvectorizer, corpus_features = build_feature_matrix(norm_corpus,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type=\\'frequency\\')\\nquery_docs_features = vectorizer.transform(norm_query_docs)\\n# get average document length of the corpus (avgdl)\\ndoc_lengths = [len(doc.split()) for doc in norm_corpus]\\xc2\\xa0\\xc2\\xa0\\navg_dl = np.average(doc_lengths)\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n295\\n# Get the corpus term idfs\\ncorpus_term_idfs = compute_corpus_term_idfs(corpus_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0norm_corpus)\\n# analyze document similarity using BM25 framework\\nIn [253]: print \\'Document Similarity Analysis using BM25\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'=\\'*60\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: for index, doc in enumerate(query_docs):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0doc_features = query_docs_features[index]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_similar_docs = compute_bm25_similarity(doc_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0corpus_features,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0doc_lengths,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0avg_dl,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0corpus_term_idfs,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0k1=1.5, b=0.75,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top_n=2)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Document\\',index+1 ,\\':\\', doc\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Top\\', len(top_similar_docs), \\'similar docs:\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'-\\'*40\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for doc_index, sim_score in top_similar_docs:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07print \\'Doc num: {} BM25 Score: {}\\\\nDoc: {}\\'.format(doc_\\nindex+1,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07sim_score, toy_corpus[doc_\\nindex])\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'-\\'*40\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print\\nDocument Similarity Analysis using BM25\\n============================================================\\nDocument 1 : The fox is definitely smarter than the dog\\nTop 2 similar docs:\\n----------------------------------------\\nDoc num: 8 BM25 Score: 7.334\\nDoc: The dog is smarter than the fox\\n----------------------------------------\\nDoc num: 7 BM25 Score: 3.88\\nDoc: The fox is quicker than the lazy dog\\n----------------------------------------\\nDocument 2 : Java is a static typed programming language unlike Python\\nTop 2 similar docs:\\n----------------------------------------\\nDoc num: 5 BM25 Score: 7.248\\nDoc: Python and Java are popular Programming languages\\n----------------------------------------\\nDoc num: 6 BM25 Score: 6.042\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n296\\nDoc: Among Programming languages, both Python and Java are the most used in \\nAnalytics\\n----------------------------------------\\nDocument 3 : I love to relax under the beautiful blue sky!\\nTop 2 similar docs:\\n----------------------------------------\\nDoc num: 2 BM25 Score: 7.334\\nDoc: The sky is blue and beautiful\\n----------------------------------------\\nDoc num: 1 BM25 Score: 4.984\\nDoc: The sky is blue\\n----------------------------------------\\nYou can now see how for each query document, we get expected and relevant \\ndocuments that have similar concepts just like the query documents. You can see that \\nthe results are quite similar to the previous methods\\xe2\\x80\\x94because, of course, they are \\nall similarity and ranking metrics and are expected to return similar results. Notice \\nthe BM25 scores of the relevant documents. The higher the score, the more relevant \\nis the document. Unfortunately, I was not able to find any production-ready scalable \\nimplementation of the BM25 ranking framework in nltk or scikit-learn. However, \\ngensim seems to have a bm25 module under the gensim.summarization package and if \\nyou are interested you can give it a try. But the core of the algorithm is based on what we \\nimplemented, and this should work pretty well on its own! \\nTry loading a bigger corpus of documents and test out these functions on some \\nsample query strings and documents. In fact, information retrieval frameworks like Solr \\nand Elasticsearch are built on top of Lucene, which use these types of ranking algorithms \\nto return relevant documents from an index of stored documents\\xe2\\x80\\x94and you can build \\nyour own search engine using them! Interested readers can check out www.elastic.co/\\nblog/found-bm-vs-lucene-default-similarity by elastic.co, the company behind the \\npopular Elasticsearch product, which tells that the performance of BM25 is much better \\nthan the default similarity ranking implementation of Lucene.\\nDocument Clustering\\nDocument clustering or cluster analysis is an interesting area in NLP and text analytics \\nthat applies unsupervised ML concepts and techniques. The main premise of document \\nclustering is similar to that of document categorization, where you start with a whole \\ncorpus of documents and are tasked with segregating them into various groups based \\non some distinctive properties, attributes, and features of the documents. Document \\nclassification needs pre-labeled training data to build a model and then categorize \\ndocuments. Document clustering uses unsupervised ML algorithms to group the \\ndocuments into various clusters. The properties of these clusters are such that documents \\ninside one cluster are more similar and related to each other compared to documents \\nbelonging to other clusters. Figure\\xc2\\xa06-3, courtesy of scikit-learn, visualizes an example \\nof clustering data points into three clusters based on its features.\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n297\\nThe cluster analysis in Figure\\xc2\\xa06-3 depicts three clusters among the data points, \\nwhich are visualized using different colors. An important thing to remember here is \\nthat clustering is an unsupervised learning technique, and from Figure\\xc2\\xa06-3 it is pretty \\nclear that there will always be some overlap among the clusters because there is no such \\ndefinition of a perfect cluster. All the techniques are based on math, heuristics, and some \\ninherent attributes toward generating clusters, and they are never a 100 percent perfect. \\nHence, there are several techniques or methods for finding clusters. Some popular \\nclustering algorithms are briefly described as follows:\\n\\xe2\\x80\\xa2 \\nHierarchical clustering models: These clustering models are also \\nknown as connectivity-based clustering methods and are based on \\nthe concept that similar objects will be closer to related objects \\nin the vector space than unrelated objects, which will be farther \\naway from them. Clusters are formed by connecting objects based \\non their distance and they can be visualized using a dendrogram. \\nThe output of these models is a complete, exhaustive hierarchy \\nof clusters. They are mainly subdivided into agglomerative and \\ndivisive clustering models.\\nFigure 6-3.\\xe2\\x80\\x82 Sample cluster analysis results (courtesy: scikit-learn)\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n298\\n\\xe2\\x80\\xa2 \\nCentroid-based clustering models: These models build clusters in \\nsuch a way that each cluster has a central representative member \\nthat represents each cluster and has the features that distinguish \\nthat particular cluster from the rest. There are various algorithms \\nin this, like k-means, k-medoids, and so on, where we need to set \\nthe number of clusters \\'k\\' in advance, and distance metrics like \\nsquares of distances from each data point to the centroid need \\nto be minimized. The disadvantage of these models is that you \\nneed to specify the \\'k\\' number of clusters in advance, which \\nmay lead to local minima, and you may not get a true clustered \\nrepresentation of your data.\\n\\xe2\\x80\\xa2 \\nDistribution-based clustering models: These models make use \\nof concepts from probability distributions when clustering data \\npoints. The idea is that objects having similar distributions can \\nbe clustered into the same group or cluster. Gaussian mixture \\nmodels (GMM) use algorithms like the Expectation-Maximization \\nalgorithm for building these clusters. Feature and attribute \\ncorrelations and dependencies can also be captured using these \\nmodels, but it is prone to overfitting.\\n\\xe2\\x80\\xa2 \\nDensity-based clustering models: These clustering models \\ngenerate clusters from data points that are grouped together at \\nareas of high density compared to the rest of the data points, \\nwhich may occur randomly across the vector space in sparsely \\npopulated areas. These sparse areas are treated as noise and \\nare used as border points to separate clusters. Two popular \\nalgorithms in this area include DBSCAN and OPTICS.\\nSeveral other clustering models have been recently introduced, including algorithms \\nlike BIRCH and CLARANS. Entire books and journals have been written just for clustering \\nalone\\xe2\\x80\\x94it is a very interesting topic offering a lot of value. Covering each and every \\nmethod would be impossible for us in the current scope, so we will cover a total of \\nthree different clustering algorithms, illustrating them with real-world data for better \\nunderstanding:\\n\\xe2\\x80\\xa2 \\nK-means clustering\\n\\xe2\\x80\\xa2 \\nAffinity propagation\\n\\xe2\\x80\\xa2 \\nWard\\xe2\\x80\\x99s agglomerative hierarchical clustering\\nFor each algorithm, we will be covering its theoretical concepts as we have done \\npreviously with other methods. We will also illustrate how each method works by \\napplying each clustering algorithm on some real-world data pertaining to movies and \\ntheir synopses. We will also look at detailed cluster statistics and focus on visualizing the \\nclusters using tried-and-tested methods, because it is often difficult to visualize results \\nfrom clustering, and practitioners often face challenges in this area.\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n299\\nClustering Greatest Movies of All Time\\nWe will be clustering a total of 100 different popular movies based on their IMDb synopses \\nas our raw data. IMDb, also known as the Internet Movie Database (www.imdb.com), is an \\nonline database that hosts extensive detailed information about movies, video games, \\nand television shows. It also aggregates reviews and synopses for movies and shows and \\nhas several curated lists. The list we are interested in is available at www.imdb.com/list/\\nls055592025/, titled Top 100 Greatest Movies of All Time (The Ultimate List). We will be \\nclustering these movies into groups using the IMDb synopsis and description of each movie.\\nBefore we begin our analysis, I would like to thank Brandon Rose for helping me out \\nwith getting this data, which he personally retrieved and curated, and also for giving me \\nsome excellent pointers on visualizing clusters. He has done some detailed clustering \\nanalysis with this data himself. If you are interested, you can get the raw data and also see \\nhis document clustering analysis in his repository at https://github.com/brandomr/\\ndocument_cluster, which is also described in further detail in his personal blog, which is \\ndedicated to analytics, at http://brandonrose.org.\\nWe have downloaded data pertaining to the top 100 movie titles and their synopses \\nfrom IMDb from the repository mentioned earlier. We parsed and cleaned it up and also \\nadded the synopses for a few movies that were missing from the original data. We added \\nthese synopses and movie descriptions from Wikipedia. Once parsed, we stored them \\nin a data frame and saved it as a .csv file called movie_data.csv, which you can find in \\nthe code files for this chapter. We will be loading and using the data from this file in our \\nclustering analysis, starting with loading and looking at the contents of our movie data \\npoints in the following snippet:\\nimport pandas as pd\\nimport numpy as np\\n# load movie data\\nmovie_data = pd.read_csv(\\'movie_data.csv\\')\\n# view movie data\\nIn [256]: print movie_data.head()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0Title\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0Synopsis\\n0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0The Godfather\\xc2\\xa0\\xc2\\xa0In late summer 1945, guests are gathered...\\n1\\xc2\\xa0\\xc2\\xa0The Shawshank Redemption\\xc2\\xa0\\xc2\\xa0In 1947, Andy Dufresne (Tim Robbins),...\\n2\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0Schindler\\'s List\\xc2\\xa0\\xc2\\xa0The relocation of Polish Jews from...\\n3\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0Raging Bull\\xc2\\xa0\\xc2\\xa0The film opens in 1964, where an older...\\n4\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0Casablanca\\xc2\\xa0\\xc2\\xa0In the early years of World War II...\\n# print sample movie and its synopsis\\nIn [268]: print \\'Movie:\\', movie_titles[0]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'Movie Synopsis:\\', movie_synopses[0][:1000]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\nMovie: The Godfather\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n300\\nMovie Synopsis: In late summer 1945, guests are gathered for the wedding \\nreception of Don Vito Corleone\\'s daughter Connie (Talia Shire) and Carlo \\nRizzi (Gianni Russo). Vito (Marlon Brando), the head of the Corleone Mafia \\nfamily, is known to friends and associates as \"Godfather.\" He and Tom Hagen \\n(Robert Duvall), the Corleone family lawyer, are hearing requests for favors \\nbecause, according to Italian tradition, \"no Sicilian can refuse a request \\non his daughter\\'s wedding day.\" One of the men who asks the Don for a favor \\nis Amerigo Bonasera, a successful mortician and acquaintance of the Don, \\nwhose daughter was brutally beaten by two young men because she refused \\ntheir advances; the men received minimal punishment. The Don is disappointed \\nin Bonasera, who\\'d avoided most contact with the Don due to Corleone\\'s \\nnefarious business dealings. The Don\\'s wife is godmother to Bonasera\\'s \\nshamed daughter, a relationship the Don uses to extract new loyalty from the \\nundertaker. The Don agrees to have his men punish\\nYou can see that we have our movie titles and their corresponding synopses, which \\nwe load into a data frame and then store them in variables. A sample movie and a part of \\nits corresponding synopsis are also depicted in the preceding output. The main idea is to \\ncluster these movies into groups using their synopsis as raw input. We will extract features \\nfrom these synopses and use unsupervised learning algorithms on them to cluster them \\ntogether. The movie titles are just for representation and will be useful when we would \\nwant to visualize and display clusters and their statistics. The data to be fed to the clustering \\nalgorithms will be features extracted from the movie synopses just to make things clearer. \\nBefore we can jump into each of the clustering methods, we will follow the same process of \\nnormalization and feature extraction that we have followed in all our other processes:\\nfrom normalization import normalize_corpus\\nfrom utils import build_feature_matrix\\n# normalize corpus\\nnorm_movie_synopses = normalize_corpus(movie_synopses,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0lemmatize=True,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0only_text_chars=True)\\n# extract tf-idf features\\nvectorizer, feature_matrix = build_feature_matrix(norm_movie_synopses,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_type=\\'tfidf\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0min_df=0.24, max_df=0.85,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ngram_range=(1, 2))\\n# view number of features\\nIn [275]: print feature_matrix.shape\\n(100, 307)\\n# get feature names\\nfeature_names = vectorizer.get_feature_names()\\n# print sample features\\nIn [277]: print feature_names[:20]\\xc2\\xa0\\xc2\\xa0\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n301\\n[u\\'able\\', u\\'accept\\', u\\'across\\', u\\'act\\', u\\'agree\\', u\\'alive\\', u\\'allow\\', \\nu\\'alone\\', u\\'along\\', u\\'already\\', u\\'although\\', u\\'always\\', u\\'another\\', \\nu\\'anything\\', u\\'apartment\\', u\\'appear\\', u\\'approach\\', u\\'arm\\', u\\'army\\', \\nu\\'around\\']\\nWe keep text tokens in our normalized text and extract TF-IDF\\xe2\\x80\\x93based features \\nfor unigrams and bigrams such that each feature occurs in at least in 25 percent of the \\ndocuments and at most 85 percent of the documents using the terms min_df and max_df. \\nWe can see that we have a total of 100 rows for the 100 movies and a total of 307 features \\nfor each movie. Some sample features are also printed in the preceding snippet. We will \\nstart our clustering analysis next, now that we have our features and documents ready.\\nK-means Clustering\\nThe k-means clustering algorithm is a centroid-based clustering model that tries to cluster \\ndata into groups or clusters of equal variance. The criteria or measure that this algorithm \\ntries to minimize is inertia, also known as within-cluster sum-of-squares. Perhaps the one \\nmain disadvantage of this algorithm is that the number of clusters k need to be specified \\nin advance, as is the case with all other centroid-based clustering models. This algorithm \\nis perhaps the most popular clustering algorithm out there and is frequently used due to \\nits ease of use as well as the fact that it is scalable with large amounts of data.\\nWe can now formally define the k-means clustering algorithm along with its \\nmathematical notations. Consider that we have a dataset X with N data points or samples \\nand we want to group them into K clusters where K is a user-specified parameter. The \\nk-means clustering algorithm will segregate the N data points into K disjoint separate \\nclusters Ck, and each of these clusters can be described by the means of the cluster \\nsamples. These means become the cluster centroids \\xce\\xbck such that these centroids are not \\nbound by the condition that they have to be actual data points from the N samples in \\nX. The algorithm chooses these centroids and builds the clusters in such a way that the \\ninertia or within-cluster sums of squares are minimized. Mathematically, this can be \\nrepresented as\\nmin\\ni\\nK\\nx\\nC\\nn\\ni\\nn\\ni\\nx\\n=\\n\\xc3\\x8e\\n\\xc3\\xa5 \\xc3\\xa5\\n-\\n1\\n2\\nm\\nwith regard to clusters Ci and centroids \\xce\\xbci such that i\\nk\\n\\xc3\\x8e\\n\\xc2\\xbc\\n{\\n}\\n1 2\\n, ,\\n,\\n. This optimization is \\nan NP hard problem for all you algorithm enthusiasts out there. Lloyd\\xe2\\x80\\x99s algorithm is a \\nsolution to this problem, which is an iterative procedure consisting of the following steps.\\n\\t\\n1.\\t\\nChoose initial k centroids \\xce\\xbck by taking k random samples from \\nthe dataset X.\\n\\t\\n2.\\t\\nUpdate clusters by assigning each data point or sample to its \\nnearest centroid point. Mathematically, we can represent this \\nas C\\nx\\nx\\nall x\\nk\\nn\\nn\\nk\\nn\\nl\\n=\\n-\\n\\xc2\\xa3\\n-\\n{\\n}\\n:\\nm\\nm\\n where Ck denotes the \\nclusters.\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n302\\n\\t\\n3.\\t\\nRecalculate and update clusters based on the new cluster data \\npoints for each cluster obtained from step 2. Mathematically, \\nthis can be represented as\\n mk\\nk x\\nC\\nn\\nC\\nx\\nn\\nk\\n=\\n\\xc3\\x8e\\xc3\\xa5\\n1\\n \\nwhere \\xce\\xbck denotes the centroids.\\nThe preceding steps are repeated in an iterative fashion till the outputs of steps 2 and \\n3 do not change anymore. One caveat of this method is that even though the optimization \\nis guaranteed to converge, it might lead to a local minimum, hence in reality, this algorithm \\nis run multiple times with several epochs and iterations, and the results might be averaged \\nfrom them if needed. The convergence and occurrence of local minimum are highly \\ndependent on the initialization of the initial centroids in step 1. One way is to make multiple \\niterations with multiple random initializations and take the average. Another way would be \\nto use the kmeans++ scheme as implemented in scikit-learn, which initializes the initial \\ncentroids to be far apart from each other and has proven to be effective. We will now use \\nk-means clustering to cluster the movie data from earlier, in the following code snippet:\\nfrom sklearn.cluster import KMeans\\n# define the k-means clustering function\\ndef k_means(feature_matrix, num_clusters=5):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0km = KMeans(n_clusters=num_clusters,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0max_iter=10000)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0km.fit(feature_matrix)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0clusters = km.labels_\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return km, clusters\\n# set k = 5, lets say we want 5 clusters from the 100 movies\\nnum_clusters = 5\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n# get clusters and assigned the cluster labels to the movies\\nkm_obj, clusters = k_means(feature_matrix=feature_matrix,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0num_clusters=num_clusters)\\nmovie_data[\\'Cluster\\'] = clusters\\nThat snippet uses our implemented k-means function to cluster the movies based \\non the TF-IDF features from the movie synopses, and we assign the cluster label for each \\nmovie from the outcome of this cluster analysis by storing it in the movie_data dataframe \\nin the \\'Cluster\\' column. You can see that we have taken k to be 5 in our analysis. We can \\nnow see the total number of movies for each of the 5 clusters using the following snippet:\\nIn [284]: from collections import Counter\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: # get the total number of movies per cluster\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: c = Counter(clusters)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print c.items()\\n[(0, 29), (1, 5), (2, 21), (3, 15), (4, 30)]\\nYou can see that there are five cluster labels as expected, from 0 to 5, and each of \\nthem has some movies belonging to the cluster whose counts are mentioned as the \\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n303\\nsecond element of each tuple in the preceding list. But can we do more than just see \\ncluster counts? Of course we can! We will now define some functions to extract detailed \\ncluster analysis information, print them, and then visualize the clusters. We will start by \\ndefining a function to extract important information from our cluster analysis:\\ndef get_cluster_data(clustering_obj, movie_data,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_names, num_clusters,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0topn_features=10):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_details = {}\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get cluster centroids\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ordered_centroids = clustering_obj.cluster_centers_.argsort()[:, ::-1]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get key features for each cluster\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get movies belonging to each cluster\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for cluster_num in range(num_clusters):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_details[cluster_num] = {}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_details[cluster_num][\\'cluster_num\\'] = cluster_num\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0key_features = [feature_names[index]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0in ordered_centroids[cluster_num, :topn_features]]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_details[cluster_num][\\'key_features\\'] = key_features\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07movies = movie_data[movie_data[\\'Cluster\\'] == cluster_num][\\'Title\\'].\\nvalues.tolist()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_details[cluster_num][\\'movies\\'] = movies\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return cluster_details\\nThe preceding function is pretty self-explanatory. What it does is basically extract the \\nkey features per cluster that were essential in defining the cluster from the centroids. It also \\nretrieves the movie titles that belong to each cluster and stores everything in a dictionary.\\nWe will now define a function that uses this data structure and prints the results in a \\nclear format:\\ndef print_cluster_data(cluster_data):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# print cluster details\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for cluster_num, cluster_details in cluster_data.items():\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Cluster {} details:\\'.format(cluster_num)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'-\\'*20\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Key features:\\', cluster_details[\\'key_features\\']\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'Movies in this cluster:\\'\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\', \\'.join(cluster_details[\\'movies\\'])\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0print \\'=\\'*40\\nBefore we analyze the results of our k-means clustering algorithm, we will also \\ndefine a function to visualize the clusters. If you remember, we talked earlier about \\nchallenges associated with visualizing clusters. This happens because we deal with \\nmultidimensional feature spaces and unstructured text data. Numeric feature vectors \\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n304\\nthemselves may not make any sense to readers if they were visualized directly. So, there \\nare some techniques like principal component analysis (PCA) or multidimensional scaling \\n(MDS) to reduce the dimensionality such that we can visualize these clusters in 2- or \\n3-dimensional plots. We will be using MDS in our implementation for visualizing clusters.\\nMDS is an approach towards non-linear dimensionality reduction such that the \\nresults can be visualized better in lower dimensional systems. The main idea is having a \\ndistance matrix such that distances between various data points are captured. We will be \\nusing Cosine similarity for this. MDS tries to build a lower-dimensional representation \\nof our data with higher numbers of features in the vector space such that the distances \\nbetween the various data points obtained using Cosine similarity in the higher \\ndimensional feature space is still similar in this lower-dimensional representation.\\nThe scikit-learn implementation for MDS has two types of algorithms: metric and \\nnon-metric. We will be using the metric approach because we will use the Cosine \\nsimilarity\\xe2\\x80\\x93based distance metric to build the input similarity matrix between the various \\nmovies. Mathematically, MDS can be defined as follows: Let S be our similarity matrix \\nbetween the various data points (movies) obtained using Cosine similarity on the feature \\nmatrix and X be the coordinates of the n input data points (movies). Disparities are \\nrepresented by \\xcb\\x86d\\nt S\\nij\\nij\\n= (\\n) , which is usually some optimal transformation of the similarity \\nvalues or could even be the raw similarity values themselves. The objective function for \\nMDS, called stress, is defined as sum\\nd\\nX\\nX\\nd\\ni j\\nij\\nij\\n<\\n(\\n)-\\n(\\n)\\n\\xcb\\x86\\n. We implement MDS-based \\nvisualization for clusters in the following function:\\nimport matplotlib.pyplot as plt\\nfrom sklearn.manifold import MDS\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nimport random\\nfrom matplotlib.font_manager import FontProperties\\ndef plot_clusters(num_clusters, feature_matrix,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_data, movie_data,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0plot_size=(16,8)):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# generate random color for clusters\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0def generate_random_color():\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0color = \\'#%06x\\' % random.randint(0, 0xFFFFFF)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return color\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# define markers for clusters\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0markers = [\\'o\\', \\'v\\', \\'^\\', \\'<\\', \\'>\\', \\'8\\', \\'s\\', \\'p\\', \\'*\\', \\'h\\', \\'H\\', \\'D\\', \\'d\\']\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# build cosine distance matrix\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cosine_distance = 1 - cosine_similarity(feature_matrix)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# dimensionality reduction using MDS\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0mds = MDS(n_components=2, dissimilarity=\"precomputed\",\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0random_state=1)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# get coordinates of clusters in new low-dimensional space\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0plot_positions = mds.fit_transform(cosine_distance)\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0x_pos, y_pos = plot_positions[:, 0], plot_positions[:, 1]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# build cluster plotting data\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n305\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_color_map = {}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_name_map = {}\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for cluster_num, cluster_details in cluster_data.items():\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# assign cluster features to unique label\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_color_map[cluster_num] = generate_random_color()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07cluster_name_map[cluster_num] = \\', \\'.join(cluster_details[\\'key_\\nfeatures\\'][:5]).strip()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# map each unique cluster label with its coordinates and movies\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_plot_frame = pd.DataFrame({\\'x\\': x_pos,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\'y\\': y_pos,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07\\'label\\': movie_data[\\'Cluster\\'].\\nvalues.tolist(),\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07\\'title\\': movie_data[\\'Title\\'].values.\\ntolist()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0})\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0grouped_plot_frame = cluster_plot_frame.groupby(\\'label\\')\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# set plot figure size and axes\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0fig, ax = plt.subplots(figsize=plot_size)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ax.margins(0.05)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# plot each cluster using co-ordinates and movie titles\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for cluster_num, cluster_frame in grouped_plot_frame:\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0marker = markers[cluster_num] if cluster_num < len(markers) \\\\\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0else np.random.choice(markers, size=1)[0]\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ax.plot(cluster_frame[\\'x\\'], cluster_frame[\\'y\\'],\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0marker=marker, linestyle=\\'\\', ms=12,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0label=cluster_name_map[cluster_num],\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0color=cluster_color_map[cluster_num], mec=\\'none\\')\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ax.set_aspect(\\'auto\\')\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ax.tick_params(axis= \\'x\\', which=\\'both\\', bottom=\\'off\\', top=\\'off\\',\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0labelbottom=\\'off\\')\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ax.tick_params(axis= \\'y\\', which=\\'both\\', left=\\'off\\', top=\\'off\\',\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0labelleft=\\'off\\')\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0fontP = FontProperties()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0fontP.set_size(\\'small\\')\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07ax.legend(loc=\\'upper center\\', bbox_to_anchor=(0.5, -0.01), \\nfancybox=True,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0shadow=True, ncol=5, numpoints=1, prop=fontP)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0#add labels as the film titles\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0for index in range(len(cluster_plot_frame)):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ax.text(cluster_plot_frame.ix[index][\\'x\\'],\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_plot_frame.ix[index][\\'y\\'],\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_plot_frame.ix[index][\\'title\\'], size=8)\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# show the plot\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0plt.show()\\nThe function is quite big, but the self-explanatory comments explain each step \\nclearly. We build our similarity matrix first using the Cosine similarity between \\ndocuments, get the cosine distances, and then transform the high dimensional feature \\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n306\\nspace into 2 dimensions using MDS. Then we plot the clusters using matplotlib with \\na bit of necessary formatting to view the results in a nice way. This function is a generic \\nfunction and will work with any clustering algorithm with a dynamic number of clusters. \\nEach cluster will have its own color, symbol, and label in the terms of top distinguishing \\nfeatures in the legend. The actual plot will plot each movie with its corresponding cluster \\nlabel with its own color and symbol.\\nWe are now ready to analyze the cluster results of our k-means clustering using the \\npreceding functions. The following code snippet depicts the detailed analysis results for \\nk-means clustering:\\n# get clustering analysis data\\ncluster_data =\\xc2\\xa0\\xc2\\xa0get_cluster_data(clustering_obj=km_obj, movie_data=movie_\\ndata,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07feature_names=feature_names, num_\\nclusters=num_clusters,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0topn_features=5)\\xc2\\xa0\\xc2\\xa0\\n# print clustering analysis results\\nIn [294]: print_cluster_data(cluster_data)\\nCluster 0 details:\\n--------------------\\nKey features: [u\\'car\\', u\\'police\\', u\\'house\\', u\\'father\\', u\\'room\\']\\nMovies in this cluster:\\nPsycho, Sunset Blvd., Vertigo, West Side Story, E.T. the Extra-Terrestrial, \\n2001: A Space Odyssey, The Silence of the Lambs, Singin\\' in the Rain, It\\'s \\na Wonderful Life, Some Like It Hot, Gandhi, To Kill a Mockingbird, Butch \\nCassidy and the Sundance Kid, The Exorcist, The French Connection, It \\nHappened One Night, Rain Man, Fargo, Close Encounters of the Third Kind, \\nNashville, The Graduate, American Graffiti, Pulp Fiction, The Maltese \\nFalcon, A Clockwork Orange, Rebel Without a Cause, Rear Window, The Third \\nMan, North by Northwest\\n========================================\\nCluster 1 details:\\n--------------------\\nKey features: [u\\'water\\', u\\'attempt\\', u\\'cross\\', u\\'death\\', u\\'officer\\']\\nMovies in this cluster:\\nChinatown, Apocalypse Now, Jaws, The African Queen, Mutiny on the Bounty\\n========================================\\nCluster 2 details:\\n--------------------\\nKey features: [u\\'family\\', u\\'love\\', u\\'marry\\', u\\'war\\', u\\'child\\']\\nMovies in this cluster:\\nThe Godfather, Gone with the Wind, The Godfather: Part II, The Sound of \\nMusic, A Streetcar Named Desire, The Philadelphia Story, An American in \\nParis, Ben-Hur, Doctor Zhivago, High Noon, The Pianist, Goodfellas, The \\nKing\\'s Speech, A Place in the Sun, Out of Africa, Terms of Endearment, \\nGiant, The Grapes of Wrath, Wuthering Heights, Double Indemnity, Yankee \\nDoodle Dandy\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n307\\n========================================\\nCluster 3 details:\\n--------------------\\nKey features: [u\\'apartment\\', u\\'new\\', u\\'woman\\', u\\'york\\', u\\'life\\']\\nMovies in this cluster:\\nCitizen Kane, Titanic, 12 Angry Men, Rocky, The Best Years of Our Lives, My \\nFair Lady, The Apartment, City Lights, Midnight Cowboy, Mr. Smith Goes to \\nWashington, Annie Hall, Good Will Hunting, Tootsie, Network, Taxi Driver\\n========================================\\nCluster 4 details:\\n--------------------\\nKey features: [u\\'kill\\', u\\'soldier\\', u\\'men\\', u\\'army\\', u\\'war\\']\\nMovies in this cluster:\\nThe Shawshank Redemption, Schindler\\'s List, Raging Bull, Casablanca, One \\nFlew Over the Cuckoo\\'s Nest, The Wizard of Oz, Lawrence of Arabia, On the \\nWaterfront, Forrest Gump, Star Wars, The Bridge on the River Kwai, Dr. \\nStrangelove or: How I Learned to Stop Worrying and Love the Bomb, Amadeus, \\nThe Lord of the Rings: The Return of the King, Gladiator, From Here to \\nEternity, Saving Private Ryan, Unforgiven, Raiders of the Lost Ark, Patton, \\nBraveheart, The Good, the Bad and the Ugly, The Treasure of the Sierra \\nMadre, Platoon, Dances with Wolves, The Deer Hunter, All Quiet on the \\nWestern Front, Shane, The Green Mile, Stagecoach\\n========================================\\n# visualize the clusters\\nIn [295]: plot_clusters(num_clusters=num_clusters,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0feature_matrix=feature_matrix,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_data=cluster_data,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0movie_data=movie_data,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0plot_size=(16,8))\\nFigure 6-4.\\xe2\\x80\\x82 Visualizing the output of K-means clustering on IMDb movie data\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n308\\nThe preceding output shows the key features for each cluster and the movies in each \\ncluster, and you can also see the same in the visualization in Figure\\xc2\\xa06-4 (there is a lot \\nin that figure\\xe2\\x80\\x94if the text appears too small, check out the kmeans_clustering.png file, \\navailable along with the code files for this chapter). Each cluster is depicted by the main \\nthemes that define that cluster by its top features, and you can see popular movies like \\nThe Godfather and The Godfather: Part II in the same cluster along with other movies \\nlike Ben-Hur and so on which talk about \\'family\\', \\'love\\', \\'war\\', and so on. Movies \\nlike Star Wars, The Lord of the Rings, The Deer Hunter, Gladiator, Forrest Gump, and so \\non are clustered together associated with themes like \\'kill\\', \\'soldier\\', \\'army\\', and \\n\\'war\\'. Definitely interesting results considering the data used for clustering was just a few \\nparagraphs of synopsis per movie. Look more closely at the results and the visualization. \\nCan you notice any other interesting patterns?\\nAffinity Propagation\\nThe k-means algorithm, although very popular, has the drawback that the user has to pre-\\ndefine the number of clusters. What if in reality there are more clusters or lesser clusters? \\nThere are some ways of checking the cluster quality and seeing what the value of the \\noptimum k might be. Interested readers can check out the elbow method and the silhouette \\ncoefficient, which are popular methods of determining the optimum k. Here we will talk \\nabout an algorithm that tries to build clusters based on inherent properties of the data \\nwithout any pre-assumptions about the number of clusters. The affinity propagation (AP) \\nalgorithm is based on the concept of \\xe2\\x80\\x9cmessage passing\\xe2\\x80\\x9d among the various data points to \\nbe clustered, and no pre-assumption is needed about the number of possible clusters.\\nAP creates these clusters from the data points by passing messages between pairs \\nof data points until convergence is achieved. The entire dataset is then represented by a \\nsmall number of exemplars that act as representatives for samples. These exemplars are \\nanalogous to the centroids you obtain from k-means or k-medoids. The messages that \\nare sent between pairs represent how suitable one of the points might be in being the \\nexemplar or representative of the other data point. This keeps getting updated in every \\niteration until convergence is achieved, with the final exemplars being the representatives \\nof each cluster. Remember, one drawback of this method is that it is computationally \\nintensive because messages are passed between each pair of data points across the entire \\ndataset and can take substantial time to converge for large datasets.\\nWe can now define the steps involved in the AP algorithm (courtesy of Wikipedia and \\nscikit-learn). Consider that we have a dataset X with n data points such that \\nX\\nx x\\nxn\\n=\\n\\xc2\\xbc\\n{\\n}\\n1\\n2\\n,\\n,\\n,\\n, and let sim(x,\\xc2\\xa0y) be the similarity function that quantifies the similarity \\nbetween two points x and y. In our implementation, we will be using Cosine similarity \\nagain for this. The AP algorithm iteratively proceeds by executing two message-passing \\nsteps as follows:\\n\\t\\n1.\\t\\nResponsibility updates are sent around, which can be \\nmathematically represented as \\nr i k\\nsim i k\\na i k\\nsim i k\\nk\\nk\\n,\\n,\\nmax\\n,\\n,\\n(\\n)\\xc2\\xac\\n(\\n)-\\n(\\n)+\\n(\\n)\\n{\\n}\\n\\xc2\\xa2\\n\\xc2\\xa2\\n\\xc2\\xa2\\xc2\\xb9\\n \\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n309\\nwhere the responsibility matrix is R and r(i,\\xc2\\xa0k) is a measure which quantifies \\nhow well xk can serve as being the representative or exemplar for xi in \\ncomparison to the other candidates.\\n\\t\\n2.\\t\\nAvailability updates are then sent around which can be \\nmathematically represented as\\na i k\\nr k k\\nr i k\\ni\\ni k\\n,\\nmin\\n,\\n,\\nmax\\n,\\n,\\n,\\n(\\n)\\xc2\\xac\\n(\\n)+\\n(\\n)\\n(\\n)\\n\\xc3\\xa6\\n\\xc3\\xa8\\n\\xc3\\xa7\\xc3\\xa7\\n\\xc3\\xb6\\n\\xc3\\xb8\\n\\xc3\\xb7\\xc3\\xb7\\n\\xc2\\xa2\\n\\xc2\\xa2\\xc3\\x8f{\\n}\\n\\xc3\\xa5\\n0\\n0\\n for i \\xe2\\x89\\xa0 k and \\navailability for i\\nk\\n=\\n is represented as \\na k k\\nr i k\\ni\\nk\\n,\\nmax ( ,\\n, )\\n(\\n)\\xc2\\xac\\n(\\n)\\n\\xc2\\xa2\\n\\xc2\\xa2\\xc2\\xb9\\xc3\\xa5\\n0\\n \\nwhere the availability matrix is A and a(i,\\xc2\\xa0k) represents \\nhow appropriate it would be for xi to pick xk as its exemplar, \\nconsidering all the other points\\xe2\\x80\\x99 preference to pick xk as an \\nexemplar.\\nThose two steps keep occurring per iteration until convergence is achieved. The \\nfollowing function implements AP such that it takes in a feature matrix and returns the \\nnecessary clusters for each sample based on its features and the other samples:\\nfrom sklearn.cluster import AffinityPropagation\\ndef affinity_propagation(feature_matrix):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sim = feature_matrix * feature_matrix.T\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0sim = sim.todense()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ap = AffinityPropagation()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ap.fit(sim)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0clusters = ap.labels_\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return ap, clusters\\nWe will now use this function to cluster our movies based on their synopses and \\nthen we will print the number of movies in each cluster and the total number of clusters \\nformed by this algorithm:\\n# get clusters using affinity propagation\\nap_obj, clusters = affinity_propagation(feature_matrix=feature_matrix)\\nmovie_data[\\'Cluster\\'] = clusters\\n# get the total number of movies per cluster\\nIn [299]: c = Counter(clusters)\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print c.items()\\n[(0, 5), (1, 6), (2, 12), (3, 6), (4, 2), (5, 7), (6, 10), (7, 7), (8, 4), \\n(9, 8), (10, 3), (11, 4), (12, 5), (13, 7), (14, 4), (15, 3), (16, 7)]\\n# get total clusters\\nIn [300]: total_clusters = len(c)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: print \\'Total Clusters:\\', total_clusters\\nTotal Clusters: 17\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n310\\nFrom the preceding results, we can see that a total of 17 clusters have been created \\nby AP on our movie data containing 100 movies. Each cluster has movies ranging from as \\nlow as 2 to as high as 12 movies. We shall now extract detailed cluster information, display \\ncluster statistics, and visualize the clusters similar to what we did for k-means clustering, \\nusing our utility functions that we implemented in the K-means clustering section:\\n# get clustering analysis data\\ncluster_data =\\xc2\\xa0\\xc2\\xa0get_cluster_data(clustering_obj=ap_obj, movie_data=movie_\\ndata,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\x07feature_names=feature_names, num_\\nclusters=total_clusters,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0topn_features=5)\\n# print clustering analysis results\\nIn [302]: print_cluster_data(cluster_data)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\nCluster 0 details:\\n--------------------\\nKey features: [u\\'able\\', u\\'always\\', u\\'cover\\', u\\'end\\', u\\'charge\\']\\nMovies in this cluster:\\nThe Godfather, The Godfather: Part II, Doctor Zhivago, The Pianist, \\nGoodfellas\\n========================================\\nCluster 1 details:\\n--------------------\\nKey features: [u\\'alive\\', u\\'accept\\', u\\'around\\', u\\'agree\\', u\\'attack\\']\\nMovies in this cluster:\\nCasablanca, One Flew Over the Cuckoo\\'s Nest, Titanic, 2001: A Space Odyssey, \\nThe Silence of the Lambs, Good Will Hunting\\n========================================\\nCluster 2 details:\\n--------------------\\nKey features: [u\\'apartment\\', u\\'film\\', u\\'final\\', u\\'fall\\', u\\'due\\']\\nMovies in this cluster:\\nThe Shawshank Redemption, Vertigo, West Side Story, Rocky, Tootsie, \\nNashville, The Graduate, The Maltese Falcon, A Clockwork Orange, Taxi \\nDriver, Rear Window, The Third Man\\n========================================\\nCluster 3 details:\\n--------------------\\nKey features: [u\\'arrest\\', u\\'film\\', u\\'evening\\', u\\'final\\', u\\'fall\\']\\nMovies in this cluster:\\nThe Wizard of Oz, Psycho, E.T. the Extra-Terrestrial, My Fair Lady, Ben-Hur, \\nClose Encounters of the Third Kind\\n========================================\\nCluster 4 details:\\n--------------------\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n311\\nKey features: [u\\'become\\', u\\'film\\', u\\'city\\', u\\'army\\', u\\'die\\']\\nMovies in this cluster:\\n12 Angry Men, Mr. Smith Goes to Washington\\n========================================\\nCluster 5 details:\\n--------------------\\nKey features: [u\\'behind\\', u\\'city\\', u\\'father\\', u\\'appear\\', u\\'allow\\']\\nMovies in this cluster:\\nForrest Gump, Amadeus, Gladiator, Braveheart, The Exorcist, A Place in the \\nSun, Double Indemnity\\n========================================\\nCluster 6 details:\\n--------------------\\nKey features: [u\\'body\\', u\\'allow\\', u\\'although\\', u\\'city\\', u\\'break\\']\\nMovies in this cluster:\\nSchindler\\'s List, Gone with the Wind, Lawrence of Arabia, Star Wars, The \\nLord of the Rings: The Return of the King, From Here to Eternity, Raiders of \\nthe Lost Ark, The Best Years of Our Lives, The Deer Hunter, Stagecoach\\n========================================\\nCluster 7 details:\\n--------------------\\nKey features: [u\\'brother\\', u\\'bring\\', u\\'close\\', u\\'although\\', u\\'car\\']\\nMovies in this cluster:\\nGandhi, Unforgiven, To Kill a Mockingbird, The Good, the Bad and the Ugly, \\nButch Cassidy and the Sundance Kid, High Noon, Shane\\n========================================\\nCluster 8 details:\\n--------------------\\nKey features: [u\\'child\\', u\\'everyone\\', u\\'attempt\\', u\\'fall\\', u\\'face\\']\\nMovies in this cluster:\\nChinatown, Jaws, The African Queen, Mutiny on the Bounty\\n========================================\\nCluster 9 details:\\n--------------------\\nKey features: [u\\'continue\\', u\\'bring\\', u\\'daughter\\', u\\'break\\', u\\'allow\\']\\nMovies in this cluster:\\nThe Bridge on the River Kwai, Dr. Strangelove or: How I Learned to Stop \\nWorrying and Love the Bomb, Apocalypse Now, Saving Private Ryan, Patton, \\nPlatoon, Dances with Wolves, All Quiet on the Western Front\\n========================================\\nCluster 10 details:\\n--------------------\\nKey features: [u\\'despite\\', u\\'drop\\', u\\'family\\', u\\'confront\\', u\\'drive\\']\\nMovies in this cluster:\\nThe Treasure of the Sierra Madre, City Lights, Midnight Cowboy\\n========================================\\nCluster 11 details:\\n--------------------\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n312\\nKey features: [u\\'discover\\', u\\'always\\', u\\'feel\\', u\\'city\\', u\\'act\\']\\nMovies in this cluster:\\nRaging Bull, It Happened One Night, Rain Man, Rebel Without a Cause\\n========================================\\nCluster 12 details:\\n--------------------\\nKey features: [u\\'discuss\\', u\\'alone\\', u\\'drop\\', u\\'business\\', u\\'consider\\']\\nMovies in this cluster:\\nSingin\\' in the Rain, An American in Paris, The Apartment, Annie Hall, \\nNetwork\\n========================================\\nCluster 13 details:\\n--------------------\\nKey features: [u\\'due\\', u\\'final\\', u\\'day\\', u\\'ever\\', u\\'eventually\\']\\nMovies in this cluster:\\nOn the Waterfront, It\\'s a Wonderful Life, Some Like It Hot, The French \\nConnection, Fargo, Pulp Fiction, North by Northwest\\n========================================\\nCluster 14 details:\\n--------------------\\nKey features: [u\\'early\\', u\\'able\\', u\\'end\\', u\\'charge\\', u\\'allow\\']\\nMovies in this cluster:\\nA Streetcar Named Desire, The King\\'s Speech, Giant, The Grapes of Wrath\\n========================================\\nCluster 15 details:\\n--------------------\\nKey features: [u\\'enter\\', u\\'eventually\\', u\\'cut\\', u\\'accept\\', u\\'even\\']\\nMovies in this cluster:\\nThe Philadelphia Story, The Green Mile, American Graffiti\\n========================================\\nCluster 16 details:\\n--------------------\\nKey features: [u\\'far\\', u\\'allow\\', u\\'apartment\\', u\\'anything\\', u\\'car\\']\\nMovies in this cluster:\\nCitizen Kane, Sunset Blvd., The Sound of Music, Out of Africa, Terms of \\nEndearment, Wuthering Heights, Yankee Doodle Dandy\\n========================================\\n# visualize the clusters\\nIn [304]: plot_clusters(num_clusters=num_clusters, feature_matrix=feature_\\nmatrix,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cluster_data=cluster_data, movie_data=movie_data,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0plot_size=(16,8))\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n313\\nThe preceding outputs show the contents of the different clusters and their \\nvisualization. If the visual text in Figure\\xc2\\xa06-5 is too small, you can always refer to the file \\naffinity_prop_clustering.png, which contains the plot depicted in higher resolution. \\nYou can see from the results that we now have a total of 17 clusters, and there are some \\nsimilarities where you will see similar movies that were grouped together in k-means \\nclustering are in similar clusters here also, and there are also notable differences where \\nmany movies now have their own cluster. Are these clustering results better than the \\nprevious one? Well a lot depends on human perspective, and since I have yet to watch \\nseveral of these movies, I leave this decision to you, dear reader! An important point to \\nnote here is that a few keywords from the exemplars or centroids for each cluster may not \\nalways depict the true essence or theme of that cluster, so a good idea here would be to \\nbuild topic models on each cluster and see the kind of topics you can extract from each \\ncluster that would make a better representation of each cluster (another example where \\nyou can see how we can connect various text analytics techniques together).\\nWard\\xe2\\x80\\x99s Agglomerative Hierarchical Clustering\\nThe hierarchical clustering family of algorithms is a bit different from the other clustering \\nmodels we\\xe2\\x80\\x99ve discussed. Hierarchical clustering tries to build a nested hierarchy of \\nclusters by either merging or splitting them in succession. There are two main strategies \\nfor Hierarchical clustering:\\n\\xe2\\x80\\xa2 \\nAgglomerative: These algorithms follow a bottom-up approach \\nwhere initially all data points belong to their own individual \\ncluster, and then from this bottom layer, we start merging clusters \\ntogether, building a hierarchy of clusters as we go up.\\nFigure 6-5.\\xe2\\x80\\x82 Visualizing the output of Affinity Propagation clustering on IMDb movie data\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n314\\n\\xe2\\x80\\xa2 \\nDivisive: These algorithms follow a top-down approach where \\ninitially all the data points belong to a single huge cluster and \\nthen we start recursively dividing them up as we move down \\ngradually, and this produces a hierarchy of clusters going from the \\ntop-down.\\nMerges and splits normally happen using a greedy algorithm, and the end result \\nof the hierarchy of clusters can be visualized as a tree structure, called a dendrogram. \\nFigure\\xc2\\xa06-6 shows an example of how a dendrogram is constructed using agglomerative \\nhierarchical clustering for a sample of documents.\\nFigure\\xc2\\xa06-6 clearly highlights how six separate data points start off as six clusters, \\nand then we slowly start grouping them in each step following a bottom-up approach. \\nWe will be using an agglomerative hierarchical clustering algorithm in this section. In \\nagglomerative clustering, for deciding which clusters we should combine when starting \\nfrom the individual data point clusters, we need two things:\\n\\xe2\\x80\\xa2 \\nA distance metric to measure the similarity or dissimilarity degree \\nbetween data points. We will be using the Cosine distance/\\nsimilarity in our implementation.\\n\\xe2\\x80\\xa2 \\nA linkage criterion that determines the metric to be used for the \\nmerging strategy of clusters. We will be using Ward\\xe2\\x80\\x99s method here.\\nFigure 6-6.\\xe2\\x80\\x82 Agglomerative hierarchical clustering representation\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n315\\nThe Ward\\xe2\\x80\\x99s linkage criterion minimizes the sum of squared differences within all the \\nclusters and is a variance minimizing approach. This is also known as Ward\\xe2\\x80\\x99s minimum \\nvariance method and was initially presented by J. Ward. The idea is to minimize the \\nvariances within each cluster using an objective function like the L2 norm distance \\nbetween two points. We can start with computing the initial cluster distances between \\neach pair of points using the formula\\nd\\nd\\nC C\\nC\\nC\\nij\\ni\\nj\\ni\\nj\\n=\\n{\\n}\\n(\\n) =\\n-\\n,\\n2\\nwhere initially Ci indicates cluster i with one document, and at each iteration, we find the \\npairs of clusters that lead to the least increase in variance for that cluster once merged. A \\nweighted squared Euclidean distance or L2 norm as depicted in the preceding formula \\nwould suffice for this algorithm. We use Cosine similarity to compute the cosine distances \\nbetween each pair of movies for our dataset. The following function implements Ward\\xe2\\x80\\x99s \\nagglomerative hierarchical clustering.:\\nfrom scipy.cluster.hierarchy import ward, dendrogram\\ndef ward_hierarchical_clustering(feature_matrix):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0cosine_distance = 1 - cosine_similarity(feature_matrix)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0linkage_matrix = ward(cosine_distance)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0return linkage_matrix\\nTo view the results of the hierarchical clustering, we need to plot a dendrogram using \\nthe preceding linkage matrix, and so we implement the following function to build and \\nplot a dendrogram from the hierarchical clustering linkage matrix:\\ndef plot_hierarchical_clusters(linkage_matrix, movie_data, figure_\\nsize=(8,12)):\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# set size\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0fig, ax = plt.subplots(figsize=figure_size)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0movie_titles = movie_data[\\'Title\\'].values.tolist()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0# plot dendrogram\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0ax = dendrogram(linkage_matrix, orientation=\"left\", labels=movie_titles)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0plt.tick_params(axis= \\'x\\',\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0which=\\'both\\',\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0bottom=\\'off\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0top=\\'off\\',\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0labelbottom=\\'off\\')\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0plt.tight_layout()\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0plt.savefig(\\'ward_hierachical_clusters.png\\', dpi=200)\\nWe are now ready to perform hierarchical clustering on our movie data! The \\nfollowing code snippet shows Ward\\xe2\\x80\\x99s clustering in action:\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n316\\nIn [307]:# build ward\\'s linkage matrix\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:linkage_matrix = ward_hierarchical_clustering(feature_matrix)\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: # plot the dendrogram\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...: plot_hierarchical_clusters(linkage_matrix=linkage_matrix,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0movie_data=movie_data,\\n\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0...:\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0\\xc2\\xa0figure_size=(8,10))\\nFigure 6-7.\\xe2\\x80\\x82 Ward\\'s clustering dendrogram on our IMDb movie data\\nChapter 6 \\xe2\\x96\\xa0 Text Similarity and Clustering\\n317\\nThe dendrogram in Figure\\xc2\\xa06-7 shows the clustering analysis results. The colors \\nindicate that there are three main clusters, which further get subdivided into more \\ngranular clusters maintaining a hierarchy. (If you have trouble reading the small fonts, look \\nat the file ward_hierachical_clusters.png available with the code files in this chapter). \\nYou will notice a lot of similarities with the results of the previous clustering algorithms.\\nThe green colored movies like Raiders of the Lost Ark, The Lord of the Rings, Star \\nWars, The Godfather, The Godfather: Part II, Pulp Fiction, A Clockwork Orange, and \\nPlatoon are definitely some of the top movies and in fact classics in the action, adventure, \\nwar, and crime-based genres. \\nThe red colored movies include comedy-based movies like City Lights, The \\nApartment, and My Fair Lady, and also several movies that belong to the drama genre \\nincluding Mutiny on the Bounty, 12 Angry Men, Annie Hall, Midnight Cowboy, Titanic, \\nand An American in Paris, with several of them having romantic plots too. Several of them \\nare even musicals, including Yankee Doodle Dandy, An American in Paris, Singin\\' in the \\nRain, and My Fair Lady. It is definitely interesting indeed that with just movie synopses, \\nour algorithm has clustered movies with similar attributes and genres together!\\nThe blue colored movies give us similar results, in that Braveheart and Gladiator are \\naction, drama, and war classics. We also have some classics related to drama, romance, \\nand biographies like The Sound of Music, Wuthering Heights, Terms of Endearment, and \\nOut of Africa. Toward the top of the dendrogram you will observe movies related to \\nscience fiction and fantasy, like 2001: A Space Odyssey, Close Encounters of the Third Kind, \\nand E.T. the Extra-Terrestrial, all close to each other. \\nCan you find more interesting patterns? Which movies do you think do not belong \\ntogether in the same clusters? Can we build better clusters? Can we recommend similar \\nmovies to watch based on clustering movies together? These are some interesting \\nquestions to ponder, and I will leave them for you to look at and explore further.\\nSummary\\nI would like to really commend your efforts on staying with me till the end of this \\nchapter. We covered a lot here, including several topics in the challenging but very \\ninteresting unsupervised machine learning domain. You now know how text similarity \\ncan be computed and you learned about various kinds of distance measures and \\nmetrics. We also looked at important concepts related to distance metrics and measures \\nand properties that make a measure into a metric. We explored concepts related to \\nunsupervised ML and saw how we can incorporate such techniques in document \\nclustering. Various ways of measuring term and document similarity were also \\ncovered, and we implemented several of these techniques by successfully converting \\nmathematical equations into code using the power of Python and several open source \\nlibraries. We touched on document clustering in detail, looking at the various concepts \\nand types of clustering models. Finally, we took a real-world example of clustering the top \\nhundred greatest movies of all time using IMDb movie synopses data and used different \\nclustering models like k-means, affinity propagation, and Ward\\xe2\\x80\\x99s hierarchical clustering \\nto build, analyze, and visualize clusters. This should be enough for you to get started \\nwith analyzing document similarity and clustering, and you can even start combining \\nvarious techniques from the chapters covered so far. (Hint: Topic models with clustering, \\nbuilding classifiers by combining supervised and unsupervised learning, and augmenting \\nrecommendation systems using document clusters\\xe2\\x80\\x94just to name a few!)\\n319\\n\\xc2\\xa9 Dipanjan Sarkar 2016 \\nD. Sarkar, Text Analytics with Python, DOI 10.1007/978-1-4842-2388-8_7\\n CHAPTER 7  \\n Semantic and Sentiment \\nAnalysis \\n Natural language understanding has gained significant importance in the last decade \\nwith the advent of machine learning (ML) and further advances like   deep learning and \\nartificial intelligence. Computers and other machines can be programmed to learn \\nthings and perform specific operations. The key limitation is their inability to perceive, \\nunderstand, and comprehend things like humans do. With the resurgence in popularity \\nof neural networks and advances made in computer architecture, we now have deep \\nlearning and artificial intelligence evolving rapidly to make some efforts into trying to \\nengineer machines into learning, perceiving, understanding, and performing actions on \\ntheir own. You may have seen or heard several of these efforts, such as self-driving cars, \\ncomputers beating experienced players in games like chess and Go, and the proliferation \\nof chatbots on the Internet. \\n In Chapters   4 \\xe2\\x80\\x93  6 , we have looked at various computational, language processing, and \\nML techniques to classify, cluster, and summarize text. Back in Chapter   3 we developed \\ncertain methods and programs to analyze and understand text syntax and structure. \\nThis chapter will deal with methods that try to answer the question  Can we analyze and \\nunderstand the meaning and sentiment behind a body of text? \\n Natural Language Processing (NLP) has a wide variety of applications that try to use \\nnatural language understanding to infer the meaning and context behind text and use it to \\nsolve various problems. We discussed several of these applications briefly in Chapter   1 . \\nTo refresh your memory, the following applications require extensive understanding of \\ntext from the semantic perspective:\\n\\xe2\\x80\\xa2 \\n Question Answering Systems \\n\\xe2\\x80\\xa2 \\n Contextual recognition \\n\\xe2\\x80\\xa2 \\n Speech recognition (for some applications) \\n Text semantics specifically deals with understanding the meaning of text or language. \\nWhen combined into sentences, words have lexical relations and contextual relations \\nbetween them lead to various types of relationships and hierarchies, and semantics sits \\nat the heart of all this in trying to analyze and understand these relationships and infer \\nmeaning from them. We will be exploring various types of semantic relationships in natural \\nlanguage and look at some NLP-based techniques for inferring and extracting meaningful \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n320\\nsemantic information from text. Semantics is purely concerned with context and meaning, \\nand the structure or format of text holds little significance here. But sometimes even the \\nsyntax or arrangement of words helps us in inferring the context of words and helps us \\ndifferentiate things like  lead as a metal from  lead as in the lead of a movie. \\n Sentiment analysis is perhaps the most popular application of text analytics, with a \\nvast number of tutorials, web sites, and applications that focus on analyzing sentiment of \\nvarious text resources ranging from corporate surveys to movie reviews. The key aspect of \\nsentiment analysis is to analyze a body of text for understanding the opinion expressed by \\nit and other factors like mood and modality. Usually sentiment analysis works best on text \\nthat has a subjective context than on that with only an objective context. This is because \\nwhen a body of text has an objective context or perspective to it, the text usually depicts some \\nnormal statements or facts without expressing any emotion, feelings, or mood. Subjective \\ntext contains text that is usually expressed by a human having typical moods, emotions, and \\nfeelings. Sentiment analysis is widely used, especially as a part of social media analysis for \\nany domain, be it a business, a recent movie, or a product launch, to understand its reception \\nby the people and what they think of it based on their opinions or, you guessed it, sentiment. \\n In this chapter, we will be covering several aspects from both semantic and \\nsentiment analysis for textual data. We will start with exploring WordNet, a lexical \\ndatabase, and introduce a new concept called  synsets . We will also explore various \\nsemantic relationships and representations in natural language and we will cover \\ntechniques such as  word sense disambiguation and  named entity recognition . In \\nsentiment analysis, we will be looking at how to use supervised ML techniques to analyze \\nsentiment and also at several unsupervised lexical techniques with more detailed insights \\ninto natural language sentiment, mood, and modality. \\n Semantic Analysis \\n We have seen how terms or words get grouped into phrases that further form clauses \\nand finally sentences. Chapter   3 showed various structural components in natural \\nlanguage, including  parts of speech (POS), chunking, and grammars . All these concepts \\nfall under the syntactic and structural analysis of text data. Whereas we do explore \\nrelationships of words, phrases, and clauses, these are purely based on their position, \\nsyntax, and structure. Semantic analysis is more about understanding the actual context \\nand meaning behind words in text and how they relate to other words to convey some \\ninformation as a whole. As mentioned in Chapter   1 , the definition of semantics itself is \\nthe study of meaning, and linguistic semantics is a complete branch under linguistics \\nthat deals with the study of meaning in  natural language , including exploring various \\nrelationships between words, phrases and symbols. Besides this, there are also various \\nways to represent semantics associated with statements and propositions. We will be \\nbroadly covering the following topics under semantic analysis:\\n\\xe2\\x80\\xa2 \\n Exploring WordNet and synsets \\n\\xe2\\x80\\xa2 \\n  Analyzing lexical semantic relations \\n\\xe2\\x80\\xa2 \\n Word sense disambiguation \\n\\xe2\\x80\\xa2 \\n Named entity recognition \\n\\xe2\\x80\\xa2 \\n Analyzing semantic representations \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n321\\n The main objective of these topics is to give you a clear understanding of the \\nresources you can leverage for semantic analysis as well as how to use these resources. \\nWe will explore various concepts related to semantic analysis, which was covered \\nin Chapter   1 , with actual examples. You can refresh your memory by revisiting the \\n\\xe2\\x80\\x9cLanguage Semantics\\xe2\\x80\\x9d section in Chapter   1 . Without any further delay, let\\'s get started! \\n Exploring  WordNet  \\n WordNet is a huge lexical database for the English Language. The database is a part of \\nPrinceton University, and you can read more about it at   https://wordnet.princeton.edu . \\nIt was originally created in around 1985, in Princeton University\\xe2\\x80\\x99s Cognitive Science \\nLaboratory under the direction of Professor G. A. Miller. This lexical database consists \\nof nouns, adjective, verbs, and adverbs, and related lexical terms are grouped together \\nbased on some common concepts into sets, known as  cognitive synonym sets or  synsets . \\nEach synset expresses a unique, distinct concept. At a high level, WordNet can be \\ncompared to a thesaurus or a dictionary that provides words and their synonyms. On a \\nlower level, it is much more than that, with synsets and their corresponding terms having \\ndetailed relationships and hierarchies based on their semantic meaning and similar \\nconcepts. WordNet is used extensively as a lexical database, in text analytics, NLP, and \\nartificial intelligence (AI)-based applications. \\n The WordNet database consists of over 155,000 words, represented in more than \\n117,000 synsets, and contains over 206,000 word-sense pairs. The database is roughly 12 \\nMB in size and can be accessed through various interfaces and APIs. The official web site \\nhas a  web application interface for accessing various details related to words, synsets, \\nand concepts related to the entered word. You can access it at   http://wordnetweb.\\nprinceton.edu/perl/webwn  or download it from   https://wordnet.princeton.edu/\\nwordnet/download/  . The download contains various packages, files, and tools related to \\nWordNet. We will be accessing WordNet programmatically using the interface provided \\nby the  nltk package. We will start by exploring synsets and then various semantic \\nrelationships using synsets. \\n Understanding  Synsets \\n We will start exploring WordNet by looking at synsets since they are perhaps one of the \\nmost important concepts and structures that tie everything together. In general, based on \\nconcepts from NLP and information retrieval, a synset is a collection or set of data entities \\nthat are considered to be semantically similar. This doesn\\xe2\\x80\\x99t mean that they will be exactly \\nthe same, but they will be centered on similar context and concepts. Specifically in the \\ncontext of WordNet, a synset is a set or collection of synonyms that are interchangeable \\nand revolve around a specific concept. Synsets not only consist of simple words, but \\nalso collocations.   Polysemous word forms (words that sound and look the same but \\nhave different but relatable meanings) are assigned to different synsets based on their \\nmeaning. Synsets are connected to other synsets using semantic relations, which we shall \\nexplore in a future section. Typically each synset has the term, a definition explaining \\nthe meaning of the term, and some optional examples and related lemmas (collection \\nof synonyms) to the term. Some terms may have multiple synsets associated with them, \\nwhere each synset has a particular context. \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n322\\n Let\\xe2\\x80\\x99s look at a real example by using  nltk \\xe2\\x80\\x99s WordNet interface to explore synsets \\nassociated with the term,  \\'fruit\\' . We can do this using the following code snippet: \\n from nltk.corpus import wordnet as wn \\n import pandas as pd \\n term = \\'fruit\\' \\n synsets = wn.synsets(term) \\n # display total synsets \\n In [75]: print \\'Total Synsets:\\', len(synsets) \\n Total Synsets: 5 \\n We can see that there are a total of five synsets associated with the term  \\'fruit\\' . \\nWhat can these synsets indicate? We can dig deeper into each synset and its components \\nusing the following code snippet: \\n In [76]: for synset in synsets: \\n    ...:     print \\'Synset:\\', synset \\n    ...:     print \\'Part of speech:\\', synset.lexname() \\n    ...:     print \\'Definition:\\', synset.definition() \\n    ...:     print \\'Lemmas:\\', synset.lemma_names() \\n    ...:     print \\'Examples:\\', synset.examples() \\n    ...:     print \\n    ...:  \\n    ...:  \\n Synset: Synset(\\'fruit.n.01\\') \\n Part of speech: noun.plant \\n Definition: the ripened reproductive body of a seed plant \\n Lemmas: [u\\'fruit\\'] \\n Examples: [] \\n Synset: Synset(\\'yield.n.03\\') \\n Part of speech: noun.artifact \\n Definition: an amount of a product \\n Lemmas: [u\\'yield\\', u\\'fruit\\'] \\n Examples: [] \\n Synset: Synset(\\'fruit.n.03\\') \\n Part of speech: noun.event \\n Definition: the consequence of some effort or action \\n Lemmas: [u\\'fruit\\'] \\n Examples: [u\\'he lived long enough to see the fruit of his policies\\'] \\n Synset: Synset(\\'fruit.v.01\\') \\n Part of speech: verb.creation \\n Definition: cause to bear fruit \\n Lemmas: [u\\'fruit\\'] \\n Examples: [] \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n323\\n Synset: Synset(\\'fruit.v.02\\') \\n Part of speech: verb.creation \\n Definition: bear fruit \\n Lemmas: [u\\'fruit\\'] \\n Examples: [u\\'the trees fruited early this year\\'] \\n  The preceding output shows us details pertaining to each synset associated with \\nthe term  \\'fruit\\' , and the definitions give us the sense of each synset and the lemma \\nassociated with it. The part of speech for each synset is also mentioned, which includes \\nnouns and verbs. Some examples are also depicted in the preceding output that show \\nhow the term is used in actual sentences. Now that we understand synsets better, let\\xe2\\x80\\x99s \\nstart exploring various semantic relationships as mentioned. \\n Analyzing  Lexical Semantic Relations \\n Text semantics refers to the study of meaning and context. Synsets give a nice abstraction \\nover various terms and provide useful information like definition, examples, POS, and \\nlemmas. But can we explore semantic relationships among entities using synsets? The \\nanswer is definitely yes. We will be talking about many of the concepts related to semantic \\nrelations (covered in detail in the \\xe2\\x80\\x9cLexical Semantic Relations\\xe2\\x80\\x9d subsection under the \\n\\xe2\\x80\\x9cLanguage Semantics\\xe2\\x80\\x9d section in Chapter   1 . It would be useful for you to review that \\nsection to better understand each of the concepts when we illustrate them with real-world \\nexamples here. We will be using  nltk \\'s  wordnet resource here, but you can use the same \\nWordNet resource from the  pattern package, which includes an interface similar to  nltk . \\n Entailments \\n The term  entailment usually refers to some event or action that logically involves or is \\nassociated with some other action or event that has taken place or will take place. Ideally \\nthis applies very well to verbs indicating some specific action. The following snippet \\nshows how to get entailments: \\n # entailments \\n In [80]: for action in [\\'walk\\', \\'eat\\', \\'digest\\']: \\n    ...:     action_syn = wn.synsets(action, pos=\\'v\\')[0] \\n    ...:     print action_syn, \\'-- entails -->\\', action_syn.entailments() \\n Synset(\\'walk.v.01\\') -- entails --> [Synset(\\'step.v.01\\')] \\n Synset(\\'eat.v.01\\') -- entails --> [Synset(\\'chew.v.01\\'), \\nSynset(\\'swallow.v.01\\')] \\n Synset(\\'digest.v.01\\') -- entails --> [Synset(\\'consume.v.02\\')] \\n  You can see how related synsets depict the concept of entailment in that output. \\nRelated actions are depicted in entailment, where actions like  walking  involve or entail \\n stepping , and  eating entails  chewing and  swallowing . \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n324\\n Homonyms and  Homographs \\n On a high level,  homonyms refer to words or terms having the same written form or \\npronunciation but different meanings. Homonyms are a superset of homographs, which \\nare words with same spelling but may have different pronunciation and meaning. The \\nfollowing code snippet shows how we can get homonyms/homographs: \\n In [81]: for synset in wn.synsets(\\'bank\\'): \\n    ...:     print synset.name(),\\'-\\',synset.definition() \\n    ...:  \\n    ...:  \\n bank.n.01 - sloping land (especially the slope beside a body of water) \\n depository_financial_institution.n.01 - a financial institution that accepts \\ndeposits and channels the money into lending activities \\n bank.n.03 - a long ridge or pile \\n bank.n.04 - an arrangement of similar objects in a row or in tiers \\n ... \\n ... \\n deposit.v.02 - put into a bank account \\n bank.v.07 - cover with ashes so to control the rate of burning \\n trust.v.01 - have confidence or faith in \\n The preceding output shows a part of the result obtained for the various homographs \\nfor the term  \\'bank\\' . You can see that there are various different meanings associated with \\nthe word  \\'bank\\' , which is the core intuition behind homographs. \\n Synonyms and  Antonyms \\n Synonyms  are words having similar meaning and context, and  antonyms are words having \\nopposite or contrasting meaning, as you may know already. The following snippet depicts \\nsynonyms and antonyms: \\n In [82]: term = \\'large\\' \\n    ...: synsets = wn.synsets(term) \\n    ...: adj_large = synsets[1] \\n    ...: adj_large = adj_large.lemmas()[0] \\n    ...: adj_large_synonym = adj_large.synset() \\n    ...: adj_large_antonym = adj_large.antonyms()[0].synset() \\n    ...: # print synonym and antonym \\n    ...: print \\'Synonym:\\', adj_large_synonym.name() \\n    ...: print \\'Definition:\\', adj_large_synonym.definition() \\n    ...: print \\'Antonym:\\', adj_large_antonym.name() \\n    ...: print \\'Definition:\\', adj_large_antonym.definition() \\n Synonym: large.a.01 \\n Definition: above average in size or number or quantity or magnitude or \\nextent \\n Antonym: small.a.01 \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n325\\n Definition: limited or below average in number or quantity or magnitude or \\nextent \\n In [83]: term = \\'rich\\' \\n    ...: synsets = wn.synsets(term)[:3] \\n    ...: # print synonym and antonym for different synsets \\n    ...: for synset in synsets: \\n    ...:     rich = synset.lemmas()[0] \\n    ...:     rich_synonym = rich.synset() \\n    ...:     rich_antonym = rich.antonyms()[0].synset() \\n    ...:     print \\'Synonym:\\', rich_synonym.name() \\n    ...:     print \\'Definition:\\', rich_synonym.definition() \\n    ...:     print \\'Antonym:\\', rich_antonym.name() \\n    ...:     print \\'Definition:\\', rich_antonym.definition() \\n Synonym: rich_people.n.01 \\n Definition: people who have possessions and wealth (considered as a group) \\n Antonym: poor_people.n.01 \\n Definition: people without possessions or wealth (considered as a group) \\n Synonym: rich.a.01 \\n Definition: possessing material wealth \\n Antonym: poor.a.02 \\n Definition: having little money or few possessions \\n Synonym: rich.a.02 \\n Definition: having an abundant supply of desirable qualities or substances \\n(especially natural resources) \\n Antonym: poor.a.04 \\n Definition: lacking in specific resources, qualities or substances \\n  The preceding outputs show sample  synonyms and  antonyms for the term  \\'large\\' \\nand the term  \\'rich\\' . Additionally, we explore several synsets associated with the term \\nor concept  \\'rich\\' , which rightly give us distinct synonyms and their corresponding \\nantonyms. \\n Hyponyms and  Hypernyms \\n Synsets represent terms with unique semantics and concepts and are linked or related \\nto each other based on some similarity and context. Several of these synsets represent \\nabstract and generic concepts also besides concrete entities. Usually they are interlinked \\ntogether in the form of a hierarchical structure representing  is-a relationships. Hyponyms \\nand hypernyms help us explore related concepts by navigating through this hierarchy. \\nTo be more specific,  hyponyms refer to entities or concepts that are a subclass of a higher \\norder concept or entity and have very specific sense or context compared to its superclass. \\nThe following snippet shows the hyponyms for the entity  \\'tree\\' : \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n326\\n term = \\'tree\\' \\n synsets = wn.synsets(term) \\n tree = synsets[0] \\n # print the entity and its meaning \\n In [86]: print \\'Name:\\', tree.name() \\n    ...: print \\'Definition:\\', tree.definition() \\n Name: tree.n.01 \\n Definition: a tall perennial woody plant having a main trunk and branches \\nforming a distinct elevated crown; includes both gymnosperms and angiosperms \\n # print total hyponyms and some sample hyponyms for \\'tree\\' \\n In [87]: hyponyms = tree.hyponyms() \\n    ...: print \\'Total Hyponyms:\\', len(hyponyms) \\n    ...: print \\'Sample Hyponyms\\' \\n    ...: for hyponym in hyponyms[:10]: \\n    ...:     print hyponym.name(), \\'-\\', hyponym.definition() \\n Total Hyponyms: 180 \\n Sample  Hyponyms \\n aalii.n.01 - a small Hawaiian tree with hard dark wood \\n acacia.n.01 - any of various spiny trees or shrubs of the genus Acacia \\n african_walnut.n.01 - tropical African timber tree with wood that resembles \\nmahogany \\n albizzia.n.01 - any of numerous trees of the genus Albizia \\n alder.n.02 - north temperate shrubs or trees having toothed leaves and \\nconelike fruit; bark is used in tanning and dyeing and the wood is rot-\\nresistant \\n angelim.n.01 - any of several tropical American trees of the genus  Andira \\n angiospermous_tree.n.01 - any tree having seeds and ovules contained in the \\novary \\n anise_tree.n.01 - any of several evergreen shrubs and small trees of the \\ngenus Illicium \\n arbor.n.01 - tree (as opposed to shrub) \\n aroeira_blanca.n.01 - small resinous tree or shrub of Brazil \\n The preceding output tells us that there are a total of 180 hyponyms for  \\'tree\\' , \\nand we see some of the sample hyponyms and their definitions. We can see that each \\nhyponym is a specific type of tree, as expected. Hyponyms are entities or concepts that act \\nas the superclass to hyponyms and have a more generic sense or context. The following \\nsnippet shows the immediate superclass hyponym for  \\'tree\\' : \\n In [88]: hypernyms = tree.hypernyms() \\n    ...: print hypernyms \\n [Synset(\\'woody_plant.n.01\\')] \\n You can even navigate up the entire entity/concept hierarchy depicting all the \\nhyponyms or parent classes for  \\'tree\\' using the following code snippet: \\n # get total hierarchy pathways for \\'tree\\' \\n In [91]: hypernym_paths = tree.hypernym_paths() \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n327\\n    ...: print \\'Total Hypernym paths:\\', len(hypernym_paths) \\n Total Hypernym paths: 1 \\n # print the entire hypernym hierarchy \\n In [92]: print \\'Hypernym Hierarchy\\' \\n    ...: print \\' -> \\'.join(synset.name() for synset in hypernym_paths[0]) \\n Hypernym Hierarchy \\n entity.n.01 -> physical_entity.n.01 -> object.n.01 -> whole.n.02 -> living_\\nthing.n.01 -> organism.n.01 -> plant.n.02 -> vascular_plant.n.01 -> woody_\\nplant.n.01 -> tree.n.01 \\n  From the preceding output, you can see that  \\'entity\\' is the most generic concept \\nin which  \\'tree\\'  is present, and the complete hypernym hierarchy showing the \\ncorresponding hypernym or superclass at each level is shown. As you navigate further \\ndown, you get into more specific concepts/entities, and if you go in the reverse direction \\nyou will get into more generic concepts/entities. \\n Holonyms and  Meronyms \\n Holonyms are entities that contain a specific entity of our interest. Basically  holonym refers \\nto the relationship between a term or entity that denotes the whole and a term denoting a \\nspecific part of the whole. The following snippet shows the holonyms for  \\'tree\\' : \\n In [94]: member_holonyms = tree.member_holonyms()     \\n    ...: print \\'Total Member Holonyms:\\', len(member_holonyms) \\n    ...: print \\'Member Holonyms for [tree]:-\\' \\n    ...: for holonym in member_holonyms: \\n    ...:     print holonym.name(), \\'-\\', holonym.definition() \\n Total Member Holonyms: 1 \\n Member Holonyms for [tree]:- \\n forest.n.01 - the trees and other plants in a large densely wooded area \\n From the output, we can see that  \\'forest\\' is a holonym for  \\'tree\\' , which is \\nsemantically correct because, of course, a forest is a collection of trees.  Meronyms are \\nsemantic relationships that relate a term or entity as a part or constituent of another term \\nor entity. The following snippet depicts different types of meronyms for  \\'tree\\' : \\n # part based meronyms for tree \\n In [95]: part_meronyms = tree.part_meronyms() \\n    ...: print \\'Total Part Meronyms:\\', len(part_meronyms) \\n    ...: print \\'Part Meronyms for [tree]:-\\' \\n    ...: for meronym in part_meronyms: \\n    ...:     print meronym.name(), \\'-\\', meronym.definition() \\n Total Part Meronyms: 5 \\n Part Meronyms for [tree]:- \\n burl.n.02 - a large rounded outgrowth on the trunk or branch of a tree \\n crown.n.07 - the upper branches and leaves of a tree or other plant \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n328\\n limb.n.02 - any of the main branches arising from the trunk or a bough of a \\ntree \\n stump.n.01 - the base part of a tree that remains standing after the tree \\nhas been felled \\n trunk.n.01 - the main stem of a tree; usually covered with bark; the bole is \\nusually the part that is commercially useful for lumber \\n # substance based meronyms for tree \\n In [96]: substance_meronyms = tree.substance_meronyms()     \\n    ...: print \\'Total Substance Meronyms:\\', len(substance_meronyms) \\n    ...: print \\'Substance Meronyms for [tree]:-\\' \\n    ...: for meronym in substance_meronyms: \\n    ...:     print meronym.name(), \\'-\\', meronym.definition() \\n Total Substance Meronyms: 2 \\n Substance Meronyms for [tree]:- \\n heartwood.n.01 - the older inactive central wood of a tree or woody plant; \\nusually darker and denser than the surrounding sapwood \\n sapwood.n.01 - newly formed outer wood lying between the cambium and the \\nheartwood of a tree or woody plant; usually light colored; active in water \\nconduction \\n The preceding output shows various meronyms that include various constituents of \\ntrees like  stump and  trunk and also various derived substances from trees like  heartwood \\nand  sapwood . \\n Semantic Relationships and Similarity \\n In the previous sections, we have looked at various concepts related to lexical semantic \\nrelationships. We will now look at ways to connect similar entities based on their \\nsemantic relationships and also measure semantic similarity between them. Semantic \\nsimilarity is different from the conventional similarity metrics discussed in Chapter   6 . We \\nwill use some sample synsets related to living entities as shown in the following snippet \\nfor our analysis: \\n tree = wn.synset(\\'tree.n.01\\') \\n lion = wn.synset(\\'lion.n.01\\') \\n tiger = wn.synset(\\'tiger.n.02\\') \\n cat = wn.synset(\\'cat.n.01\\') \\n dog = wn.synset(\\'dog.n.01\\') \\n # create entities and extract names and definitions \\n entities = [tree, lion, tiger, cat, dog] \\n entity_names = [entity.name().split(\\'.\\')[0] for entity in entities] \\n entity_definitions = [entity.definition() for entity in entities] \\n # print entities and their definitions \\n In [99]: for entity, definition in zip(entity_names, entity_definitions): \\n    ...:     print entity, \\'-\\', definition \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n329\\n tree - a tall perennial woody plant having a main trunk and branches forming \\na distinct elevated crown; includes both gymnosperms and angiosperms \\n lion - large gregarious predatory feline of Africa and India having a tawny \\ncoat with a shaggy mane in the male \\n tiger - large feline of forests in most of Asia having a tawny coat with \\nblack stripes; endangered \\n cat - feline mammal usually having thick soft fur and no ability to roar: \\ndomestic cats; wildcats \\n dog - a member of the genus Canis (probably descended from the common wolf) \\nthat has been domesticated by man since prehistoric times; occurs in many \\nbreeds \\n Now that we know our entities a bit better from these definitions explaining them, we \\nwill try to correlate the entities based on common hypernyms. For each pair of entities, \\nwe will try to find the lowest common hypernym in the relationship hierarchy tree. \\nCorrelated entities are expected to have very specific hypernyms, and unrelated entities \\nshould have very abstract or generic hypernyms. The following code snippet illustrates: \\n common_hypernyms = [] \\n for entity in entities: \\n    # get pairwise lowest common hypernyms \\n    common_hypernyms.append([entity.lowest_common_hypernyms(compared_entity)[0] \\n                                      .name().split(\\'.\\')[0] \\n                             for compared_entity in entities]) \\n # build pairwise lower common hypernym matrix \\n common_hypernym_frame = pd.DataFrame(common_hypernyms, \\n                                     index=entity_names,  \\n                                     columns=entity_names) \\n # print the matrix \\n In [101]: print common_hypernym_frame     \\n     ...:  \\n           tree       lion      tiger        cat        dog \\n tree       tree   organism   organism   organism   organism \\n lion   organism       lion    big_cat     feline  carnivore \\n tiger  organism    big_cat      tiger     feline  carnivore \\n cat    organism     feline     feline        cat  carnivore \\n dog    organism  carnivore  carnivore  carnivore        dog \\n Ignoring the main diagonal of the matrix, for each pair of entities, we can see their \\nlowest common hypernym which depicts the nature of relationship between them.  Trees are \\nunrelated to the other animals except that they are all living organisms. Hence we get the \\n \\'organism\\' relationship amongst them.  Cats are related to  lions and  tigers  with respect to \\nbeing feline creatures, and we can see the same in the preceding output.  Tigers and  lions are \\nconnected to each other with the  \\'big cat\\'  relationship. Finally, we can see  dogs  having the \\nrelationship of  \\'carnivore\\' with the other animals since they all typically eat meat. \\n We can also measure the semantic similarity between these entities using various \\nsemantic concepts. We will use  \\'path similarity\\' , which returns a value between  [0, 1]  \\nbased on the shortest path connecting two terms based on their hypernym/hyponym based \\ntaxonomy. The following snippet shows us how to generate this similarity matrix: \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n330\\n similarities = [] \\n for entity in entities: \\n    # get pairwise similarities \\n    similarities.append([round(entity.path_similarity(compared_entity), 2) \\n                         for compared_entity in entities])         \\n # build pairwise similarity matrix                              \\n similarity_frame = pd.DataFrame(similarities, \\n                                index=entity_names,  \\n                                columns=entity_names) \\n # print the matrix \\n print similarity_frame  \\n       tree  lion  tiger   cat   dog \\n tree   1.00  0.07   0.07  0.08  0.13 \\n lion   0.07  1.00   0.33  0.25  0.17 \\n tiger  0.07  0.33   1.00  0.25  0.17 \\n cat    0.08  0.25   0.25  1.00  0.20 \\n dog    0.13  0.17   0.17  0.20  1.00 \\n From the preceding output, as expected,  lion and  tiger  are the most similar with a \\nvalue of 0.33, followed by their semantic similarity with  cat  having a value of 0.25. And \\n tree has the lowest semantic similarity values when compared with other animals.    \\n This concludes our discussion on analyzing lexical semantic relations. I encourage \\nyou to try exploring more concepts with different examples by leveraging WordNet.    \\n Word Sense Disambiguation \\n In the previous section, we looked at homographs and homonyms, which are basically words \\nthat look or sound similar but have very different meanings. This meaning is contextual \\nbased on how it has been used and also depends on the word semantics, also called  word \\nsense . Identifying the correct sense or semantics of a word based on its usage is called  word \\nsense disambiguation with the assumption that the word has multiple meanings based on its \\ncontext. This is a very popular problem in NLP and is used in various applications, such as \\nimproving the relevance of search engine results, coherence, and so on. \\n There are various ways to solve this problem, including lexical and dictionary-based \\nmethods and supervised and unsupervised ML methods. Covering everything would be \\nout of the current scope, so I will be showing word sense disambiguation using the Lesk \\nalgorithm, a classic algorithm invented by M. E. Lesk in 1986. The basic principle behind \\nthis algorithm is to leverage dictionary or vocabulary definitions for a word we want to \\ndisambiguate in a body of text and compare the words in these definitions with a section \\nof text surrounding our word of interest. We will be using the WordNet definitions for \\nwords instead of a dictionary. The main objective for us would be to return the synset \\nwith the maximum number of overlapping words or terms between the context sentence \\nand the different definitions from each synset for the word we target for disambiguation. \\nThe following snippet leverages  nltk to depict how to use word sense disambiguation for \\nvarious examples: \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n331\\n from nltk.wsd import lesk \\n from nltk import word_tokenize \\n # sample text and word to disambiguate \\n samples = [(\\'The fruits on that plant have ripened\\', \\'n\\'), \\n            (\\'He finally reaped the fruit of his hard work as he won the \\nrace\\', \\'n\\')] \\n word = \\'fruit\\' \\n # perform word sense disambiguation \\n In [106]: for sentence, pos_tag in samples: \\n     ...:     word_syn = lesk(word_tokenize(sentence.lower()), word, pos_tag) \\n     ...:     print \\'Sentence:\\', sentence \\n     ...:     print \\'Word synset:\\', word_syn \\n     ...:     print \\'Corresponding definition:\\', word_syn.definition() \\n     ...:     print \\n Sentence: The fruits on that plant have ripened \\n Word synset: Synset(\\'fruit.n.01\\') \\n Corresponding definition: the ripened reproductive body of a seed plant \\n Sentence: He finally reaped the fruit of his hard work as he won the race \\n Word synset: Synset(\\'fruit.n.03\\') \\n Corresponding definition: the consequence of some effort or  action \\n # sample text and word to disambiguate \\n samples = [(\\'Lead is a very soft, malleable metal\\', \\'n\\'), \\n           (\\'John is the actor who plays the lead in that movie\\', \\'n\\'), \\n           (\\'This road leads to nowhere\\', \\'v\\')] \\n word = \\'lead\\' \\n # perform word sense disambiguation \\n In [108]: for sentence, pos_tag in samples: \\n     ...:      word_syn = lesk(word_tokenize(sentence.lower()), word, \\npos_tag) \\n     ...:     print \\'Sentence:\\', sentence \\n     ...:     print \\'Word synset:\\', word_syn \\n     ...:     print \\'Corresponding definition:\\', word_syn.definition() \\n     ...:     print \\n Sentence: Lead is a very soft, malleable metal \\n Word synset: Synset(\\'lead.n.02\\') \\n Corresponding definition: a soft heavy toxic malleable metallic element; \\nbluish white when freshly cut but tarnishes readily to dull grey \\n Sentence: John is the actor who plays the lead in that movie \\n Word synset: Synset(\\'star.n.04\\') \\n Corresponding definition: an actor who plays a principal role \\n Sentence: This road leads to nowhere \\n Word synset: Synset(\\'run.v.23\\') \\n Corresponding definition: cause something to pass or lead  somewhere \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n332\\n We try to disambiguate two words,  \\'fruit\\' and  \\'lead\\' in various text documents \\nin the preceding examples. You can see how we use the Lesk algorithm to get the correct \\nword sense for the word we are disambiguating based on its usage and context in each \\ndocument. This tells you how  fruit can mean both an entity that is consumed as well as \\nsome consequence one faces on applying efforts. We also see how  lead  can mean the soft \\nmetal, causing something/someone to go somewhere, or even an actor who plays the \\nmain role in a play or movie. \\n Named Entity Recognition  \\n In any text document, there are particular terms that represent entities that are more \\ninformative and have a unique context compared to the rest of the text. These entities are \\nknown as  named entities , which more specifically refers to terms that represent real-world \\nobjects like people, places, organizations, and so on, which are usually denoted by proper \\nnames. We can find these typically by looking at the noun phrases in text documents. \\n Named entity recognition , also known as  entity chunking/extraction , is a popular technique \\nused in information extraction to identify and segment named entities and classify or \\ncategorize them under various predefined classes. Some of these classes that are used \\nmost frequently are shown in Figure\\xc2\\xa0 7-1 (courtesy of  nltk  and The Stanford NLP group).  \\n There is some overlap between  GPE and  LOCATION . The  GPE  entities are usually more \\ngeneric and represent geo-political entities like cities, states, countries, and continents. \\n LOCATION  can also refer to these entities (it varies across different NER systems) along \\nwith very specific locations like a mountain, river, or hill-station.  FACILITY on the other \\nhand refers to popular monuments or artifacts that are usually man-made. The remaining \\ncategories are pretty self-explanatory from their names and the examples depicted in \\nFigure\\xc2\\xa0 7-1 . \\n The Bundesliga is perhaps the most popular top-level professional association \\nfootball league in Germany, and FC Bayern Munchen is one of the most popular clubs \\nin this league with a global presence. We will now take a sample description of this club \\n Figure 7-1.  Common named entities with  examples \\n \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n333\\nfrom Wikipedia and try to extract named entities from it. We will reuse our normalization \\nmodule (accessible as  normalization.py in the code files) from the last chapter in \\nthis section to parse the document to remove unnecessary new lines. We will start by \\nleveraging  nltk \\xe2\\x80\\x99s Named Entity Chunker: \\n # sample document \\n text = \"\"\" \\n Bayern Munich, or FC Bayern, is a German sports club based in Munich,  \\n Bavaria, Germany. It is best known for its professional football team,  \\n which plays in the Bundesliga, the top tier of the German football  \\n league system, and is the most successful club in German football  \\n history, having won a record 26 national titles and 18 national cups.  \\n FC Bayern was founded in 1900 by eleven football players led by Franz John.  \\n Although Bayern won its first national championship in 1932, the club  \\n was not selected for the Bundesliga at its inception in 1963. The club  \\n had its period of greatest success in the middle of the 1970s when,  \\n under the captaincy of Franz Beckenbauer, it won the European Cup three  \\n times in a row (1974-76). Overall, Bayern has reached ten UEFA Champions  \\n League finals, most recently winning their fifth title in 2013 as part  \\n of a continental treble.  \\n \"\"\" \\n import nltk \\n from normalization import parse_document \\n import pandas as pd \\n # tokenize sentences \\n sentences = parse_document(text) \\n tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in \\nsentences]  \\n # tag sentences and use nltk\\'s Named Entity Chunker \\n tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_\\nsentences] \\n ne_chunked_sents = [nltk.ne_chunk(tagged) for tagged in tagged_sentences] \\n # extract all named entities \\n named_entities = [] \\n for ne_tagged_sentence in ne_chunked_sents: \\n    for tagged_tree in ne_tagged_sentence: \\n        # extract only chunks having NE labels \\n        if hasattr(tagged_tree, \\'label\\'):  \\n                 entity_name = \\' \\'.join(c[0] for c in tagged_tree.leaves()) # \\nget NE name \\n                entity_type = tagged_tree.label() # get NE category \\n                named_entities.append((entity_name, entity_type)) \\n # get unique named entities                 \\n named_entities = list(set(named_entities)) \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n334\\n # store named entities in a data frame \\n entity_frame = pd.DataFrame(named_entities,  \\n                            columns=[\\'Entity Name\\', \\'Entity Type\\']) \\n # display results \\n In [116]: print entity_frame     \\n          Entity Name   Entity Type \\n 0              Bayern        PERSON \\n 1          Franz John        PERSON \\n 2   Franz Beckenbauer        PERSON \\n 3              Munich  ORGANIZATION \\n 4            European  ORGANIZATION \\n 5          Bundesliga  ORGANIZATION \\n 6              German           GPE \\n 7             Bavaria           GPE \\n 8             Germany           GPE \\n 9           FC Bayern  ORGANIZATION \\n 10               UEFA  ORGANIZATION \\n 11             Munich           GPE \\n 12             Bayern           GPE \\n 13            Overall           GPE \\n The Named Entity Chunker identifies named entities from the preceding text \\ndocument, and we extract these named entities from the tagged annotated sentences \\nand display them in the data frame as shown. You can clearly see how it has correctly \\nidentified  PERSON ,  ORGANIZATION , and  GPE related named entities, although a few of them \\nare incorrectly identified. \\n We will now use the Stanford NER tagger on the same text and compare the results. \\nFor this, you need to have Java installed and then download the Stanford NER resources \\nfrom   http://nlp.stanford.edu/software/stanford-ner-2014-08-27.zip . Unzip them \\nto a location of your choice (I used  E:/stanford in my system). Once done, you can use \\n nltk \\xe2\\x80\\x99s interface to access this, similar to what we did in Chapter   3 for constituency and \\ndependency parsing. For more details on Stanford NER, visit   http://nlp.stanford.edu/\\nsoftware/CRF-NER.shtml  , the official web site, which also contains the latest version of \\ntheir Named Entity Recognizer (I used an older version):      \\n from nltk.tag import StanfordNERTagger \\n import os \\n # set java path in environment variables \\n java_path = r\\'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_102\\\\bin\\\\java.exe\\' \\n os.environ[\\'JAVAHOME\\'] = java_path \\n # load stanford NER \\n sn = StanfordNERTagger(\\'E:/stanford/stanford-ner-2014-08-27/classifiers/\\nenglish.all.3class.distsim.crf.ser.gz\\', \\n                        path_to_jar=\\'E:/stanford/stanford-ner-2014-08-27/\\nstanford-ner.jar\\') \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n335\\n # tag sentences                        \\n ne_annotated_sentences = [sn.tag(sent) for sent in tokenized_sentences] \\n # extract named entities \\n named_entities = [] \\n for sentence in ne_annotated_sentences: \\n    temp_entity_name = \\'\\' \\n    temp_named_entity = None \\n    for term, tag in sentence: \\n        # get terms with NE tags \\n        if tag != \\'O\\':  \\n             temp_entity_name = \\' \\'.join([temp_entity_name, term]).strip() # \\nget NE name \\n             temp_named_entity = (temp_entity_name, tag) # get NE and its \\ncategory \\n        else: \\n            if temp_named_entity: \\n                named_entities.append(temp_named_entity) \\n                temp_entity_name = \\'\\' \\n                temp_named_entity = None \\n # get unique named entities \\n named_entities = list(set(named_entities)) \\n # store named entities in a data frame \\n entity_frame = pd.DataFrame(named_entities,  \\n                            columns=[\\'Entity Name\\', \\'Entity Type\\']) \\n # display results \\n In [118]: print entity_frame                        \\n         Entity Name   Entity Type \\n 0         Franz John        PERSON \\n 1  Franz Beckenbauer        PERSON \\n 2            Germany      LOCATION \\n 3             Bayern  ORGANIZATION \\n 4            Bavaria      LOCATION \\n 5             Munich      LOCATION \\n 6          FC Bayern  ORGANIZATION \\n 7               UEFA  ORGANIZATION \\n 8      Bayern Munich  ORGANIZATION \\n The preceding output depicts various named entities obtained from our document. \\nYou can compare this with the results obtained from  nltk \\xe2\\x80\\x99s NER chunker. The results here \\nare definitely better\\xe2\\x80\\x94there are no misclassifications and each category is also assigned \\ncorrectly. Some really interesting points: It has correctly identified  Munich as a  LOCATION \\nand  Bayern Munich as an  ORGANIZATION . Does this mean the second NER tagger is better? \\nNot really. It depends on the type of corpus you are analyzing, and you can even build \\nyour own NER tagger using supervised learning by training on pre-tagged corpora similar \\nto what we did in Chapter   3 . In fact, both the taggers just discussed have been trained on \\npre-tagged corpora like CoNLL, MUC, and Penn Treebank.  \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n336\\n Analyzing Semantic Representations \\n We usually communicate in the form of messages in spoken form or in written form \\nwith other people or interfaces. Each of these  messages is typically a collection of words, \\nphrases, or sentences, and they have their own semantics and context. So far, we\\xe2\\x80\\x99ve talked \\nabout semantics and relations between various lexical units. But how do we represent the \\nmeaning of semantics conveyed by a message or messages? How do humans understand \\nwhat someone is telling them? How do we believe in statements and propositions and \\nevaluate outcomes and what action to take? It feels easy because the brain helps us with \\nlogic and reasoning\\xe2\\x80\\x94but computationally can we do the same? \\n The answer is yes we can.  Frameworks  like propositional logic and first-order logic \\nhelp us in representation of semantics. We discussed this in detail in Chapter   1 in the \\nsubsection \\xe2\\x80\\x9cRepresentation of Semantics\\xe2\\x80\\x9d under the \\xe2\\x80\\x9cLanguage Semantics\\xe2\\x80\\x9d section. I \\nencourage you to go through that once more to refresh your memory. In the following \\nsections, we will look at ways to represent propositional and first order logic and prove or \\ndisprove propositions, statements, and predicates using practical examples and code. \\n Propositional  Logic \\n We have already discussed propositional logic (PL) as the study of propositions, \\nstatements, and sentences. A  proposition  is usually declarative, having a binary value \\nof being either true or false. There also exist various logical operators like conjunction, \\ndisjunction, implication, and equivalence, and we also study the effects of applying these \\noperators on multiple propositions to understand their behavior and outcome. \\n Let us consider our example from Chapter   1 with regard to two propositions  P and  Q \\nsuch that they can be represented as follows: \\n P : He is hungry \\n Q : He will eat a sandwich \\n We will now try to build the truth tables for various operations on these propositions \\nusing  nltk based on the various logical operators discussed in Chapter   1  (refer to the \\n\\xe2\\x80\\x9cPropositional Logic\\xe2\\x80\\x9d section for more details) and derive outcomes computationally: \\n import nltk \\n import pandas as pd \\n import os \\n # assign symbols and propositions \\n symbol_P = \\'P\\' \\n symbol_Q = \\'Q\\' \\n proposition_P = \\'He is hungry\\' \\n propositon_Q = \\'He will eat a sandwich\\' \\n # assign various truth values to the propositions \\n p_statuses = [False, False, True, True] \\n q_statuses = [False, True, False, True] \\n # assign the various expressions combining the logical operators \\n conjunction = \\'(P & Q)\\' \\n disjunction = \\'(P | Q)\\' \\n implication = \\'(P -> Q)\\' \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n337\\n equivalence = \\'(P <-> Q)\\' \\n expressions = [conjunction, disjunction, implication, equivalence] \\n # evaluate each expression using propositional logic \\n results = [] \\n for status_p, status_q in zip(p_statuses, q_statuses): \\n    dom = set([]) \\n    val = nltk.Valuation([(symbol_P, status_p),  \\n                          (symbol_Q, status_q)]) \\n    assignments = nltk.Assignment(dom) \\n    model = nltk.Model(dom, val) \\n    row = [status_p, status_q] \\n    for expression in expressions: \\n        # evaluate each expression based on proposition truth values \\n        result = model.evaluate(expression, assignments)  \\n        row.append(result) \\n    results.append(row) \\n # build the result table \\n columns = [symbol_P, symbol_Q, conjunction,  \\n           disjunction, implication, equivalence]            \\n result_frame = pd.DataFrame(results, columns=columns) \\n # display results \\n In [125]: print \\'P:\\', proposition_P \\n     ...: print \\'Q:\\', propositon_Q \\n     ...: print \\n     ...: print \\'Expression Outcomes:-\\' \\n     ...: print result_frame  \\n P: He is hungry \\n Q: He will eat a sandwich \\n Expression Outcomes:- \\n       P      Q (P & Q) (P | Q) (P -> Q) (P <-> Q) \\n 0  False  False   False   False     True      True \\n 1  False   True   False    True     True     False \\n 2   True  False   False    True    False     False \\n 3   True   True    True    True     True      True \\n The preceding output depicts the various truth values of the two propositions, and \\nwhen we combine them with various logical operators, you will find the results matching \\nwith what we manually evaluated in Chapter   1  . For example,  P & Q indicates  He is hungry \\nand he will eat a sandwich is  True only when both of the individual propositions is  True . \\nWe use  nltk \\xe2\\x80\\x99s  Valuation class to create a dictionary of the propositions and their various \\noutcome states. We use the  Model class to evaluate each expression, where the  evaluate()  \\nfunction internally calls the recursive function  satisfy() , which helps in evaluating the \\noutcome of each expression with the propositions based on the assigned truth values.  \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n338\\n First Order  Logic \\n PL has several limitations, like the inability to represent facts or complex relationships \\nand inferences. PL also has limited expressive power because for each new proposition \\nwe would need a unique symbolic representation, and it becomes very difficult to \\ngeneralize facts. This is where first order logic (FOL) works really well with features \\nlike functions, quantifiers, relations, connectives, and symbols. It definitely provides a \\nricher and more powerful representation for semantic information. The \\xe2\\x80\\x9cFirst Order \\nLogic\\xe2\\x80\\x9d subsection under \\xe2\\x80\\x9cRepresentation of Semantics\\xe2\\x80\\x9d in Chapter   1 provides detailed \\ninformation about how  FOL works. \\n In this section, we will build several FOL representations similar to what we did \\nmanually in Chapter   1  using mathematical representations. Here we will build them \\nin our code using similar syntax and leverage  nltk and some theorem provers to prove \\nthe outcome of various expressions based on predefined conditions and relationships, \\nsimilar to what we did for PL. The key takeaway for you from this section should be \\ngetting to know how to represent FOL representations in Python and how to perform FOL \\ninference using proofs based on some goal and predefined rules and events. There are \\nseveral theorem provers you can use for evaluating expressions and proving theorems. \\nThe  nltk  package has three main different types of provers:  Prover9 ,  TableauProver , and \\n ResolutionProver . The first one is a free-to-use prover available for download at    www.\\ncs.unm.edu/~mccune/prover9/download/   . You can extract the contents in a location of \\nyour choice (I used  E:/prover9 ). We will be using both  ResolutionProver and  Prover9 \\nin our examples. The following snippet helps in setting up the necessary dependencies \\nfor FOL expressions and evaluations: \\n import  nltk \\n import os \\n # for reading FOL expressions \\n read_expr = nltk.sem.Expression.fromstring \\n # initialize theorem provers (you can choose any) \\n os.environ[\\'PROVER9\\'] = r\\'E:/prover9/bin\\' \\n prover = nltk.Prover9() \\n # I use the following one for our examples \\n prover = nltk.ResolutionProver()    \\n Now that we have our dependencies ready, let us evaluate a few FOL expressions. \\nConsider a simple expression that  If an entity jumps over another entity, the reverse cannot \\nhappen . Assuming the entities to be  x and  y , we can represent this is FOL as \\xe2\\x88\\x80 x \\xe2\\x88\\x80 y \\n(jumps_over(x, y) \\xe2\\x86\\x92 \\xc2\\xac jumps_over(y, x)) which signifies that for all  x and  y , if  x jumps \\nover  y , it implies that  y cannot jump over  x . Consider now that we have two entities  fox \\nand  dog such that the  fox jumps over the  dog is an event which has taken place and can \\nbe represented by  jumps_over(fox, dog) . Our end goal or objective is to evaluate the \\noutcome of  jumps_over(dog, fox) considering the preceding expression and the event \\nthat has occurred. The following snippet shows us how we can do this: \\n # set the rule expression \\n rule = read_expr(\\'all x. all y. (jumps_over(x, y) -> -jumps_over(y, x))\\') \\n # set the event occured \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n339\\n event = read_expr(\\'jumps_over(fox, dog)\\') \\n # set the outcome we want to evaluate -- the goal \\n test_outcome = read_expr(\\'jumps_over(dog, fox)\\') \\n # get the  result \\n In [132]: prover.prove(goal=test_outcome,  \\n     ...:              assumptions=[event, rule], \\n     ...:              verbose=True) \\n [1] {-jumps_over(dog,fox)}                    A  \\n [2] {jumps_over(fox,dog)}                     A  \\n [3] {-jumps_over(z4,z3), -jumps_over(z3,z4)}  A  \\n [4] {-jumps_over(dog,fox)}                    (2, 3)  \\n Out[132]: False \\n The preceding output depicts the final result for our goal  test_outcome is  False , that \\nis, the  dog cannot jump over the  fox if the  fox has already jumped over the  dog based on \\nour rule expression and the events assigned to the assumptions parameter in the prover \\nalready given. The sequence of steps that lead to the result is also shown in the output. \\nLet us now consider another FOL expression rule \\xe2\\x88\\x80 x studies(x, exam) \\xe2\\x86\\x92  pass(x, \\nexam) , which tells us that for all instances of  x , if  x studies for the exam, he/she will pass \\nthe exam. Let us represent this rule and consider two students,  John and  Pierre , such \\nthat  John  does not study for the exam and  Pierre does. Can we then find out the outcome \\nwhether they will pass the exam based on the given expression rule? The following \\nsnippet shows us how: \\n # set the rule expression                           \\n rule = read_expr(\\'all x. (studies(x, exam) -> pass(x, exam))\\')  \\n # set the events and outcomes we want to determine \\n event1 = read_expr(\\'-studies(John, exam)\\')   \\n test_outcome1 = read_expr(\\'pass(John, exam)\\')  \\n event2 = read_expr(\\'studies(Pierre, exam)\\')   \\n test_outcome2 = read_expr(\\'pass(Pierre, exam)\\') \\n # get results \\n In [134]: prover.prove(goal=test_outcome1,  \\n     ...:              assumptions=[event1, rule], \\n     ...:              verbose=True)  \\n [1] {-pass(John,exam)}                  A  \\n [2] {-studies(John,exam)}               A  \\n [3] {-studies(z6,exam), pass(z6,exam)}  A  \\n [4] {-studies(John,exam)}               (1, 3)  \\n Out[134]: False \\n  In [135]: prover.prove(goal=test_outcome2,  \\n     ...:              assumptions=[event2, rule], \\n     ...:              verbose=True)    \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n340\\n [1] {-pass(Pierre,exam)}                A  \\n [2] {studies(Pierre,exam)}              A  \\n [3] {-studies(z8,exam), pass(z8,exam)}  A  \\n [4] {-studies(Pierre,exam)}             (1, 3)  \\n [5] {pass(Pierre,exam)}                 (2, 3)  \\n [6] {}                                  (1, 5)  \\n Out[135]:  True \\n Thus you can see from the above evaluations that  Pierre  does pass the exam \\nbecause he studied for the exam, unlike  John who doesn\\'t pass the exam since he did not \\nstudy for it. \\n Let us consider a more complex example with several entities. They perform several \\nactions as follows:\\n\\xe2\\x80\\xa2 \\n There are two dogs  rover ( r ) and  alex ( a ) \\n\\xe2\\x80\\xa2 \\n There is one cat  garfield ( g ) \\n\\xe2\\x80\\xa2 \\n  There is one fox  felix ( f ) \\n\\xe2\\x80\\xa2 \\n Two animals, alex ( a ) and felix ( f ) run, denoted by function \\n runs() \\n\\xe2\\x80\\xa2 \\n Two animals  rover ( r ) and  garfield ( g ) sleep, denoted by \\nfunction  sleeps() \\n\\xe2\\x80\\xa2 \\n Two animals,  felix ( f ) and  alex ( a ) can jump over the other two, \\ndenoted by function  jumps_over() \\n Taking all these assumptions, the following snippet builds an FOL-based model \\nwith the previously mentioned domain and assignment values based on the entities \\nand functions. Once we build this model, we evaluate various FOL-based expressions to \\ndetermine their outcome and prove some theorems like we did earlier: \\n # define symbols (entities\\\\functions) and their values \\n rules = \"\"\" \\n    rover => r \\n    felix => f \\n    garfield => g \\n    alex => a \\n    dog => {r, a} \\n    cat => {g} \\n     fox => {f} \\n    runs => {a, f} \\n    sleeps => {r, g} \\n    jumps_over => {(f, g), (a, g), (f, r), (a, r)} \\n    \"\"\" \\n val = nltk.Valuation.fromstring(rules) \\n # view the valuation object of symbols and their assigned values \\n(dictionary) \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n341\\n In [143]: print val \\n {\\'rover\\': \\'r\\', \\'runs\\': set([(\\'f\\',), (\\'a\\',)]), \\'alex\\': \\'a\\', \\'sleeps\\': \\nset([(\\'r\\',), (\\'g\\',)]), \\'felix\\': \\'f\\', \\'fox\\': set([(\\'f\\',)]), \\'dog\\': \\nset([(\\'a\\',), (\\'r\\',)]), \\'jumps_over\\': set([(\\'a\\', \\'g\\'), (\\'f\\', \\'g\\'), (\\'a\\', \\n\\'r\\'), (\\'f\\', \\'r\\')]), \\'cat\\': set([(\\'g\\',)]), \\'garfield\\': \\'g\\'} \\n # define domain and build FOL based model \\n dom = {\\'r\\', \\'f\\', \\'g\\', \\'a\\'} \\n m = nltk.Model(dom, val) \\n # evaluate various expressions \\n In [148]: print m.evaluate(\\'jumps_over(felix, rover) & dog(rover) & \\nruns(rover)\\', None) \\n False \\n In [149]: print m.evaluate(\\'jumps_over(felix, rover) & dog(rover) & \\n-runs(rover)\\', None) \\n True \\n In [150]: print m.evaluate(\\'jumps_over(alex, garfield) & dog(alex) & \\ncat(garfield) & sleeps(garfield)\\', None) \\n True \\n # assign rover to x and felix to y in the domain \\n g = nltk.Assignment(dom, [(\\'x\\', \\'r\\'), (\\'y\\', \\'f\\')])    \\n # evaluate more expressions based on above assigned symbols \\n In [152]: print m.evaluate(\\'runs(y) & jumps_over(y, x) & sleeps(x)\\', g)    \\n True \\n In [153]: print m.evaluate(\\'exists y. (fox(y) & runs(y))\\', g)  \\n True \\n The preceding snippet depicts the evaluation of various expressions based on the \\nvaluation of different symbols based on the rules and domain. We create various FOL-\\nbased expressions and see their outcome based on the predefined assumptions. For \\nexample, the first expression gives us  False because  rover  never  runs() and the second \\nand third expressions are  True because they satisfy all the conditions like  felix and  alex \\ncan  jump over rover or  garfield and  rover is a  dog that does not  run and  garfield is \\na  cat . The second set of expressions is evaluated based on assigning  felix and  rover to \\nspecific symbols in our domain ( dom ), and we pass that variable ( g ) when evaluating the \\nexpressions. We can even satisfy open formulae or expressions using the  satisfiers() \\nfunction as shown here: \\n # who are the animals who run? \\n In [154]: formula = read_expr(\\'runs(x)\\') \\n     ...: print m.satisfiers(formula, \\'x\\', g)  \\n set([\\'a\\', \\'f\\']) \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n342\\n # animals who run and are also a fox? \\n In [155]: formula = read_expr(\\'runs(x) & fox(x)\\') \\n     ...: print m.satisfiers(formula, \\'x\\', g) \\n set([\\'f\\']) \\n The preceding outputs are self-explanatory wherein we evaluate open-ended \\nquestions like  which animals run ? And also  which animals can run and are also foxes ? \\nWe get the relevant symbols in our outputs, which you can map back to the actual \\nanimal names (Hint:  a: alex, f: felix ). I encourage you to experiment with more \\npropositions and FOL expressions by building your own assumptions, domain, and rules.   \\n Sentiment Analysis \\n We will now discuss several concepts, techniques, and examples with regard to our second \\nmajor topic in this chapter, sentiment analysis.  Textual data , even though unstructured, \\nmainly has two broad types of data points: factual based (objective) and opinion based \\n(subjective). We briefly talked about these two categories at the beginning of this chapter \\nwhen I introduced the concept of sentiment analysis and how it works best on text that has \\na subjective context. In general, social media, surveys, and feedback data all are heavily \\nopinionated and express the beliefs, judgement, emotion, and feelings of human beings. \\nSentiment analysis, also popularly known as  opinion analysis/mining , is  defined as the \\nprocess of using techniques like NLP, lexical resources, linguistics, and machine learning \\n(ML) to extract subjective and opinion related information like emotions, attitude, mood, \\nmodality, and so on and try to use these to compute the polarity expressed by a text \\ndocument. By  polarity , I mean to find out whether the document expresses a positive, \\nnegative, or a neutral sentiment. More advanced analysis involves trying to find out more \\ncomplex emotions like sadness, happiness, anger, and sarcasm. \\n Typically, sentiment analysis for text data can be computed on several levels, \\nincluding on an individual sentence level, paragraph level, or the entire document as a \\nwhole. Often sentiment is computed on the document as a whole or some aggregations \\nare done after computing the sentiment for individual sentences.   Polarity analysis usually \\ninvolves trying to assign some scores contributing to the positive and negative emotions \\nexpressed in the document and then finally assigning a label to the document based on \\nthe aggregate score. We will depict two major  techniques for sentiment analysis here:\\n\\xe2\\x80\\xa2 \\n Supervised machine learning \\n\\xe2\\x80\\xa2 \\n Unsupervised lexicon-based \\n The key idea is to learn the various techniques typically used to tackle sentiment \\nanalysis problems so that you can apply them to solve your own problems. We will \\nsee how to re-use the concepts of supervised machine learning based classification \\nalgorithms from Chapter   4 here to classify documents to their associated sentiment. We \\nwill also use  lexicons , which are dictionaries or vocabularies specially constructed to \\nbe used for sentiment analysis, and compute sentiment without using any supervised \\ntechniques. We will be carrying out our experiments on a large real-world dataset \\npertaining to movie reviews, which will make this task more interesting. We will compare \\nthe performance of the various algorithms and also try to perform some detailed analytics \\nbesides just analyzing polarity, which includes analyzing the subjectivity, mood, and \\nmodality of the movie reviews. Without further delay, let\\xe2\\x80\\x99s get started! \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n343\\n Sentiment Analysis of  IMDb Movie Reviews \\n We will be using a dataset of movie reviews obtained from the Internet Movie Database \\n(IMDb) for sentiment analysis. This dataset, containing over 50,000 movie reviews, can be \\nobtained from   http://ai.stanford.edu/~amaas/data/sentiment/  , courtesy of Stanford \\nUniversity and A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, Andrew Ng, and C. Potts, \\nand this dataset was used in their famous paper, \\xe2\\x80\\x9cLearning Word Vectors for Sentiment \\nAnalysis.\\xe2\\x80\\x9d We will be using 50,000 movie reviews from this dataset, which contain the \\nreview and a corresponding sentiment polarity label which is either positive or negative. \\nA positive review is basically a movie review which was rated with more than six stars in \\nIMDb, and a negative  review was rated with less than five stars in IMDb. An important \\nthing to remember here before we begin our exercise is the fact that many of these reviews, \\neven though labeled positive or negative, might have some elements of negative or positive \\ncontext respectively. Hence, there is a possibility for some overlap in many reviews, which \\nmake this task harder. Sentiment is not a quantitative number that you can compute and \\nprove mathematically. It expresses complex emotions, feelings, and judgement, and hence \\nyou should never focus on trying to get a cent-percent perfect model but a model that \\ngeneralizes well on data and works decently. We will start with setting up some necessary \\ndependencies and utilities before moving on to the various techniques. \\n Setting Up Dependencies  \\n There are several utility functions, data, and package dependencies that we need to set \\nup before we jump into sentiment analysis. We will need our movie review dataset, some \\nspecific packages that we will be using in our implementations, and we will be defining \\nsome utility functions for text normalization, feature extracting, and model evaluation, \\nsimilar to what we have used in previous chapters. \\n Getting and Formatting the  Data \\n We will use the IMDb movie review dataset officially available in raw text files for each \\nset (training and testing) from   http://ai.stanford.edu/~amaas/data/sentiment/  as \\nmentioned. You can download and unzip the files to a location of your choice and use \\nthe  review_data_extractor.py file included along with the code files of this chapter to \\nextract each review from the unzipped directory, parse them, and neatly format them into \\na data frame, which is then stored as a csv file named  movie_reviews.csv . Otherwise, \\nyou can directly download the parsed and formatted file from   https://github.com/\\ndipanjanS/text-analytics-with-python/tree/master/Chapter-7  , which contains all \\ndatasets and code used and is the official repository for this book. The data frame consists \\nof two columns,  review and  sentiment , for each data point, which indicates the review \\nfor a movie and its corresponding sentiment (positive or negative). \\n Text Normalization \\n We will be normalizing and standardizing our text data similar to what we did in Chapter \\n  6 as a part of text pre-processing and normalization. For this we will be re-using our \\n normalization.py module from Chapter   6 with a few additions. This mainly includes \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n344\\nadding an HTML stripper to remove unnecessary HTML characters from text documents, \\nas shown here: \\n from HTMLParser import HTMLParser \\n class MLStripper(HTMLParser): \\n    def __init__(self): \\n        self.reset() \\n        self.fed = [] \\n    def handle_data(self, d): \\n        self.fed.append(d) \\n    def get_data(self): \\n        return \\' \\'.join(self.fed) \\n def strip_html(text): \\n    html_stripper = MLStripper() \\n    html_stripper.feed(text) \\n    return html_stripper.get_data() \\n We also add a new function to normalize special accented characters and convert \\nthem into regular ASCII characters so as to standardize the text across all documents. The \\nfollowing snippet helps us achieve this: \\n def normalize_accented_characters(text): \\n    text = unicodedata.normalize(\\'NFKD\\',  \\n                                 text.decode(\\'utf-8\\') \\n                                 ).encode(\\'ascii\\', \\'ignore\\') \\n    return  text \\n The overall text normalization function is depicted in the following snippet and it \\nre-uses the expand contractions, lemmatization, HTML unescaping, special characters \\nremoval, and stopwords removal functions from the previous chapter\\'s normalization \\nmodule: \\n def normalize_corpus(corpus, lemmatize=True,  \\n                     only_text_chars=False, \\n                     tokenize=False): \\n    normalized_corpus = []     \\n    for index, text in enumerate(corpus): \\n        text = normalize_accented_characters(text) \\n        text = html_parser.unescape(text) \\n        text = strip_html(text) \\n        text = expand_contractions(text, CONTRACTION_MAP) \\n        if lemmatize: \\n            text = lemmatize_text(text) \\n        else: \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n345\\n            text = text.lower() \\n        text = remove_special_characters(text) \\n        text = remove_stopwords(text) \\n        if only_text_chars: \\n            text = keep_text_characters(text) \\n        if tokenize: \\n            text = tokenize_text(text) \\n            normalized_corpus.append(text) \\n        else: \\n            normalized_corpus.append(text) \\n    return normalized_ corpus \\n  To re-use this code, you can make use of the  normalization.py and  contractions.\\npy files provided with the code files of this chapter. \\n Feature  Extraction \\n We will be reusing the same feature-extraction function we used in Chapter   6 , and it is \\navailable as a part of the  utils.py module. The function is shown here for the sake of \\ncompleteness: \\n from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer \\n def build_feature_matrix(documents, feature_type=\\'frequency\\', \\n                         ngram_range=(1, 1), min_df=0.0, max_df=1.0): \\n    feature_type = feature_type.lower().strip()   \\n    if feature_type == \\'binary\\': \\n        vectorizer = CountVectorizer(binary=True, min_df=min_df, \\n                                     max_df=max_df, ngram_range=ngram_range) \\n    elif feature_type == \\'frequency\\': \\n        vectorizer = CountVectorizer(binary=False, min_df=min_df, \\n                                     max_df=max_df, ngram_range=ngram_range) \\n    elif feature_type == \\'tfidf\\': \\n        vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df,  \\n                                     ngram_range=ngram_range) \\n    else: \\n         raise Exception(\"Wrong feature type entered. Possible values: \\n\\'binary\\', \\'frequency\\', \\'tfidf\\'\") \\n    feature_matrix = vectorizer.fit_transform(documents).astype(float) \\n    return vectorizer, feature_ matrix \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n346\\n You can experiment with various features provided by this function, which include \\nBag of Words-based frequencies, occurrences, and TF-IDF based features. \\n Model Performance Evaluation \\n We will be evaluating our models based on precision, recall, accuracy, and F1-score, \\nsimilar to our evaluation methods in Chapter   4 for text classification. Additionally we \\nwill be looking at the confusion matrix and detailed classification reports for each class, \\nthat is, the positive and negative classes to evaluate model performance. You can refer to \\nthe \\xe2\\x80\\x9cEvaluating Classification Models\\xe2\\x80\\x9d section in Chapter   4  to refresh your memory on \\nthe various model-evaluation metrics. The following function will help us in getting the \\nmodel accuracy, precision, recall, and F1-score: \\n from sklearn import metrics \\n import numpy as np \\n import pandas as pd \\n def display_evaluation_metrics(true_labels, predicted_labels, positive_\\nclass=1): \\n    print \\'Accuracy:\\', np.round( \\n                        metrics.accuracy_score(true_labels,  \\n                                               predicted_labels), \\n                        2) \\n    print \\'Precision:\\', np.round( \\n                        metrics.precision_score(true_labels,  \\n                                               predicted_labels, \\n                                               pos_label=positive_class, \\n                                               average=\\'binary\\'), \\n                        2) \\n    print \\'Recall:\\', np.round( \\n                        metrics.recall_score(true_labels,  \\n                                               predicted_labels, \\n                                               pos_label=positive_class, \\n                                               average=\\'binary\\'), \\n                        2) \\n    print \\'F1 Score:\\', np.round( \\n                        metrics.f1_score(true_labels,  \\n                                               predicted_labels, \\n                                               pos_label=positive_class, \\n                                               average=\\'binary\\'), \\n                        2) \\n We will also define a function to help us build the confusion matrix for evaluating \\nthe model predictions against the actual sentiment labels for the reviews. The following \\nfunction will help us achieve that: \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n347\\n def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]): \\n    cm = metrics.confusion_matrix(y_true=true_labels,  \\n                                  y_pred=predicted_labels,  \\n                                  labels=classes) \\n    cm_frame = pd.DataFrame(data=cm,  \\n                             columns=pd.MultiIndex(levels=[[\\'Predicted:\\'], \\nclasses],  \\n                                                  labels=[[0,0],[0,1]]),  \\n                             index=pd.MultiIndex(levels=[[\\'Actual:\\'], \\nclasses],  \\n                                                labels=[[0,0],[0,1]]))  \\n    print cm_frame   \\n  Finally, we will define a function for getting a detailed classification report per \\nsentiment category (positive and negative) by displaying the precision, recall, F1-score, \\nand support (number of reviews) for each of the classes: \\n def display_classification_report(true_labels, predicted_labels, \\nclasses=[1,0]): \\n    report = metrics.classification_report(y_true=true_labels,  \\n                                           y_pred=predicted_labels,  \\n                                           labels=classes)  \\n    print report \\n You will find all the preceding functions in the  utils.py  module along with the other \\ncode files for this chapter and you can re-use them as needed. Besides this, you need to \\nmake sure you have  nltk and  pattern installed\\xe2\\x80\\x94which you should already have by this \\npoint of time because we have used them numerous times in our previous chapters.      \\n Preparing  Datasets \\n We will be loading our movie reviews data and preparing two datasets, namely training \\nand testing, similar to what we did in Chapter   4 . We will train our supervised model on \\nthe training data and evaluate model performance on the testing data. For unsupervised \\nmodels, we will directly evaluate them on the testing data so as to compare their \\nperformance with the supervised model. Besides that, we will also pick some sample \\npositive and negative reviews to see how the different models perform on them: \\n import pandas as pd \\n import numpy as np \\n # load movie reviews data \\n dataset = pd.read_csv(r\\'E:/aclImdb/movie_reviews.csv\\') \\n # print sample data \\n In [235]: print dataset.head() \\n                                              review sentiment \\n 0  One of the other reviewers has mentioned that ...  positive \\n 1  A wonderful little production. <br /><br />The...  positive \\n 2  I thought this was a wonderful way to spend ti...  positive \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n348\\n 3  Basically there\\'s a family where a little boy ...  negative \\n 4  Petter Mattei\\'s \"Love in the Time of Money\" is...  positive \\n # prepare training and testing datasets \\n train_data = dataset[:35000] \\n test_data = dataset[35000:] \\n train_reviews = np.array(train_data[\\'review\\']) \\n train_sentiments = np.array(train_data[\\'sentiment\\']) \\n test_reviews = np.array(test_data[\\'review\\']) \\n test_sentiments = np.array(test_data[\\'sentiment\\']) \\n # prepare sample dataset for experiments \\n sample_docs = [100, 5817, 7626, 7356, 1008, 7155, 3533, 13010] \\n sample_data = [(test_reviews[index], \\n                test_sentiments[index]) \\n                  for index in sample_docs] \\n We have taken a total of 35,000 reviews out of the 50,000 to be our training dataset \\nand we will evaluate our models and test them on the remaining 15,000 reviews. This is in \\nline with a typical 70:30 separation used for training and testing dataset building. We have \\nalso extracted a total of eight reviews from the test dataset and we will be looking closely \\nat the results for these documents as well as evaluating the model performance on the \\ncomplete test dataset in the following sections. \\n Supervised Machine Learning  Technique \\n As mentioned before, in this section we will be building a model to analyze sentiment \\nusing supervised ML. This model will learn from past reviews and their corresponding \\nsentiment from the training dataset so that it can predict the sentiment for new reviews \\nfrom the test dataset. The basic principle here is to use the same concepts we used for \\n text classification such that the classes to predict here are positive and negative sentiment \\ncorresponding to the movie reviews. \\n We will be following the same workflow which we followed in Chapter   4  for  text \\nclassification  (refer to Figure 4-2 in Chapter   4 ) in the \\xe2\\x80\\x9cText Classification Blueprint\\xe2\\x80\\x9d \\nsection. The following points summarize these steps:\\n \\n 1. \\n Model training\\n   a. \\n   Normalize training data  \\n  b. \\n  Extract features and build feature set and feature \\nvectorizer \\n  c. \\n Use supervised learning algorithm (SVM) to build a \\npredictive model \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n349\\n \\n 2. \\n Model testing\\n   a. \\n   Normalize testing data  \\n b. \\n Extract features using training feature vectorizer \\n  c. \\n Predict the sentiment for testing reviews using training \\nmodel \\n d. \\n Evaluate model performance \\n To start, we will be building our training model using the steps in point 1. We will be \\nusing our normalization and feature- extraction modules discussed in previous sections: \\n from normalization import normalize_corpus \\n from utils import build_feature_matrix \\n # normalization \\n norm_train_reviews = normalize_corpus(train_reviews, lemmatize=True, only_\\ntext_chars=True) \\n # feature extraction                                                                             \\n vectorizer, train_features = build_feature_matrix(documents=norm_train_\\nreviews, \\n                                                   feature_type=\\'tfidf\\', \\nngram_range=(1, 1),  \\n                                                  min_df=0.0, max_df=1.0) \\n We will now build our model using the  support vector machine (SVM) algorithm which \\nwe used for text classification in Chapter   4 . Refer to the \\xe2\\x80\\x9cSupport Vector Machines\\xe2\\x80\\x9d subsection \\nunder the \\xe2\\x80\\x9cClassification Algorithms\\xe2\\x80\\x9d section in Chapter   4 to refresh your memory: \\n from sklearn.linear_model import SGDClassifier \\n # build the model \\n svm = SGDClassifier(loss=\\'hinge\\', n_iter=200) \\n svm.fit(train_features, train_sentiments) \\n The preceding snippet trainings the classifier and builds the model that is in the \\n svm variable, which we can now use for predicting sentiment for new movie reviews (not \\nused for training) from the test dataset. Let us normalize and extract  features   from the test \\ndataset first as mentioned in step 2 in our workflow: \\n # normalize reviews                         \\n norm_test_reviews = normalize_corpus(test_reviews, lemmatize=True, only_\\ntext_chars=True)   \\n # extract features                                      \\n test_features = vectorizer.transform(norm_test_reviews) \\n Now that we have our features for the entire test dataset, before we predict the \\nsentiment and measure model prediction performance for the entire test dataset, let us \\nlook at some of the  predictions for the sample documents we extracted earlier: \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n350\\n # predict sentiment for sample docs from test data \\n In [253]: for doc_index in sample_docs: \\n     ...:     print \\'Review:-\\' \\n     ...:     print test_reviews[doc_index] \\n     ...:     print \\'Actual Labeled Sentiment:\\', test_sentiments[doc_index] \\n     ...:     doc_features = test_features[doc_index] \\n     ...:     predicted_sentiment = svm.predict(doc_features)[0] \\n     ...:     print \\'Predicted Sentiment:\\', predicted_sentiment \\n     ...:     print \\n     ...:  \\n     ...:  \\n Review:- \\n Worst movie, (with the best reviews given it) I\\'ve ever seen. Over the top \\ndialog, acting, and direction. more slasher flick than thriller.With all the \\ngreat reviews this movie got I\\'m appalled that it turned out so silly. shame \\non you martin  scorsese \\n Actual Labeled Sentiment: negative \\n Predicted Sentiment: negative \\n Review:- \\n I hope this group of film-makers never re-unites. \\n Actual Labeled Sentiment: negative \\n Predicted Sentiment: negative \\n Review:- \\n no comment - stupid movie, acting average or worse... screenplay - no sense \\nat all... SKIP IT! \\n Actual Labeled Sentiment: negative \\n Predicted Sentiment: negative \\n Review:- \\n Add this little gem to your list of holiday regulars. It is<br /><br \\n/>sweet, funny, and endearing \\n Actual Labeled Sentiment: positive \\n Predicted Sentiment: positive \\n Review:- \\n a mesmerizing film that certainly keeps your attention... Ben Daniels is \\nfascinating (and courageous) to watch. \\n Actual Labeled Sentiment: positive \\n Predicted Sentiment: positive \\n Review:- \\n This movie is perfect for all the romantics in the world. John Ritter has \\nnever been better and has the best line in the movie! \"Sam\" hits close to \\nhome, is lovely to look at and so much fun to play along with. Ben Gazzara \\nwas an excellent cast and easy to fall in love with. I\\'m sure I\\'ve met \\nArthur in my travels somewhere. All around, an excellent choice to pick up \\nany evening.!:-) \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n351\\n Actual Labeled Sentiment: positive \\n Predicted Sentiment: positive \\n Review:- \\n I don\\'t care if some people voted this movie to be bad. If you want the \\nTruth this is a Very Good Movie! It has every thing a movie should have. You \\nreally should Get this one. \\n Actual Labeled Sentiment: positive \\n Predicted Sentiment: negative \\n Review:- \\n Worst horror film ever but funniest film ever rolled in one you have got \\nto see this film it is so cheap it is unbeliaveble but you have to see it \\nreally!!!! P.s watch the carrot \\n Actual Labeled Sentiment: positive \\n Predicted Sentiment:  negative \\n You can look at each review, its actual labeled sentiment, and our predicted sentiment \\nin the preceding output and see that we have some negative and positive reviews, and our \\nmodel is able to correctly identify the sentiment for most of the sampled reviews except \\nthe last two reviews. If you look closely at the last two reviews, some part of the review has \\na negative sentiment ( \"worst horror film\" ,  \"voted this movie to be bad\" ) but the \\ngeneral sentiment or opinion of the person who wrote the review was intended positive. \\nThese are the examples I mentioned earlier about the overlap of  positive and negative \\nemotions , which makes it difficult for the model to predict the actual sentiment! \\n Let us now predict the sentiment for all our  test dataset reviews   and evaluate our \\nmodel performance: \\n # predict the sentiment for test dataset movie reviews \\n predicted_sentiments = svm.predict(test_features)        \\n # evaluate model prediction performance \\n from utils import display_evaluation_metrics, display_confusion_matrix, \\ndisplay_classification_report \\n # show performance metrics \\n In [270]: display_evaluation_metrics(true_labels=test_sentiments, \\n     ...:                            predicted_labels=predicted_sentiments, \\n     ...:                            positive_class=\\'positive\\')   \\n Accuracy: 0.89 \\n Precision: 0.88 \\n Recall: 0.9 \\n F1 Score: 0.89 \\n # show confusion matrix \\n In [271]: display_confusion_matrix(true_labels=test_sentiments, \\n     ...:                          predicted_labels=predicted_sentiments, \\n     ...:                          classes=[\\'positive\\', \\'negative\\']) \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n352\\n                 Predicted:          \\n                   positive negative \\n Actual: positive       6770      740 \\n        negative        912     6578 \\n # show detailed per-class classification report \\n In [272]: display_classification_report(true_labels=test_sentiments, \\n     ...:                                predicted_labels=predicted_\\nsentiments, \\n     ...:                               classes=[\\'positive\\', \\'negative\\'])  \\n             precision    recall  f1-score   support \\n   positive       0.88      0.90      0.89      7510 \\n   negative       0.90      0.88      0.89      7490 \\n avg / total       0.89      0.89      0.89     15000 \\n The preceding outputs show the various  performance metrics   that depict the \\nperformance of our SVM model with regard to predicting sentiment for movie reviews. \\nWe have an average sentiment prediction accuracy of 89 percent, which is really good if \\nyou compare it with standard baselines for text classification using supervised techniques. \\nThe classification report also shows a per-class detailed report, and we see that our F1-\\nscore (harmonic mean of precision and recall) is 89 percent for both positive and negative \\nsentiment. The support metric shows the number of reviews having positive (7510) \\nsentiment and negative (7490) sentiment. The  confusion matrix shows how many reviews \\nfor which we predicted the correct sentiment ( positive : 6770/7510,  negative : 6578/7490) \\nand the number of reviews for which we predicted the wrong sentiment ( positive : 740/7510, \\n negative : 912/7490). Do try out building more models with different features (Chapter \\n  4  talks about different feature-extraction techniques) and different supervised learning \\nalgorithms. Can you get a better model which predicts sentiment more accurately?  \\n Unsupervised Lexicon-based  Techniques \\n So far, we used labeled training data to learn patterns using features from the movie \\nreviews and their corresponding sentiment. Then we applied this knowledge learned on \\nnew movie reviews (the testing dataset) to predict their sentiment. Often, you may not \\nhave the convenience of a well-labeled training dataset. In those situations, you need \\nto use unsupervised techniques for predicting the sentiment by using knowledgebases, \\nontologies, databases, and lexicons that have detailed information specially curated and \\nprepared just for sentiment analysis. \\n As mentioned, a  lexicon is a dictionary, vocabulary, or a book of words. In our case, \\nlexicons are special dictionaries or vocabularies that have been created for analyzing \\nsentiment. Most of these lexicons have a list of positive and negative polar words with \\nsome score associated with them, and using various techniques like the position of words, \\nsurrounding words, context, parts of speech, phrases, and so on, scores are assigned to \\nthe text documents for which we want to compute the sentiment. After aggregating these \\nscores, we get the final sentiment. More advanced analyses can also be done, including \\ndetecting the subjectivity, mood, and modality. Various popular lexicons are used for \\nsentiment analysis, including the following:\\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n353\\n\\xe2\\x80\\xa2 \\n AFINN lexicon \\n\\xe2\\x80\\xa2 \\n Bing Liu\\xe2\\x80\\x99s lexicon \\n\\xe2\\x80\\xa2 \\n MPQA subjectivity lexicon \\n\\xe2\\x80\\xa2 \\n SentiWordNet \\n\\xe2\\x80\\xa2 \\n VADER lexicon \\n\\xe2\\x80\\xa2 \\n Pattern lexicon \\n This is not an exhaustive list of lexicons that can be leveraged for sentiment analysis, \\nand there are several other lexicons which can be easily obtained from the Internet. \\nWe will briefly discuss each lexicon and will be using the last three lexicons to analyze \\nthe sentiment for our testing dataset in more detail. Although these techniques are \\nunsupervised, you can also use them to analyze and evaluate the sentiment for the \\ntraining dataset too, but for the sake of consistency and to compare model performances \\nwith the supervised model, we will be performing all our analyses on the testing dataset. \\n AFINN Lexicon \\n The AFINN lexicon was curated and created by Finn \\xc3\\x85rup Nielsen, and more details are \\nmentioned in his paper \\xe2\\x80\\x9cA New ANEW: Evaluation of a Word List for Sentiment Analysis \\nin Microblogs.\\xe2\\x80\\x9d The latest version, known as AFINN-111, consists of a total of 2477 words \\nand phrases with their own scores based on sentiment polarity. The polarity basically \\nindicates how positive, negative, or neutral the term might be with some numerical \\nscore. You can download it from    www2.imm.dtu.dk/pubdb/views/publication_details.\\nphp?id=6010   . It also talks about the lexicon in further details. The author of this lexicon \\nhas also built a Python wrapper over the AFINN lexicon, which you can directly use to \\npredict the sentiment of text data. The repository is available from GitHub at   https://\\ngithub.com/fnielsen/afinn  . You can install the  afinn library directly and start \\nanalyzing sentiment. This library even has support for emoticons and smileys. Following \\nis a sample of the AFINN-111 lexicon: \\n abandon        -2 \\n abandoned      -2 \\n abandons       -2 \\n abducted       -2 \\n abduction      -2 \\n ... \\n ... \\n youthful        2 \\n yucky          -2 \\n yummy           3 \\n zealot         -2 \\n zealots        -2 \\n zealous         2 \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n354\\n The basic idea is to load the entire list of polar words and phrases in the lexicon \\nalong with their corresponding score (sample shown above) in memory and then find the \\nsame words/phrases and score them accordingly in a text document. Finally, these scores \\nare aggregated, and the final sentiment and score can be obtained for a text document. \\nFollowing is an example snippet based on the official documentation: \\n from afinn import Afinn \\n afn = Afinn(emoticons=True)  \\n In [281]: print afn.score(\\'I really hated the plot of this movie\\') \\n -3.0 \\n In [282]: print afn.score(\\'I really hated the plot of this movie :(\\') \\n -5.0 \\n    Thus you can use the  score() function directly to evaluate the sentiment of your text \\ndocuments, and from the preceding output you can see that they even give proper weightage \\nto emoticons, which are used extensively in social media like Twitter and Facebook.  \\n Bing Liu\\xe2\\x80\\x99s Lexicon \\n This lexicon has been developed by Bing Liu over several years and is discussed in \\nfurther details in his paper, by Nitin Jindal and Bing Liu, \\xe2\\x80\\x9cIdentifying Comparative \\nSentences in Text Documents.\\xe2\\x80\\x9d You can get more details about the lexicon at   https://\\nwww.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon  , which also includes a \\nlink to download it as an archive (RAR format). This lexicon consists of over 6800 words \\ndivided into two files named  positive-words.txt , containing around 2000+ words/\\nphrases, and  negative-words.txt , which contains around 4800+ words/phrases. The \\nkey idea is to leverage these words to contribute to the positive or negative polarity of \\nany text document when they are identified in that document. This lexicon also includes \\nmany misspelled words, taking into account that words or terms are often misspelled on \\npopular social media web sites.  \\n MPQA Subjectivity Lexicon \\n MPQA stands for Multi-Perspective Question Answering, and it hosts a plethora of \\nresources maintained by the University of Pittsburgh. It contains resources including \\nopinion corpora, subjectivity lexicon, sense annotations, argument-based lexicon, and \\ndebate datasets. A lot of these can be leveraged for complex analysis of human emotions \\nand sentiment. The subjectivity lexicon is maintained by Theresa Wilson, Janyce Wiebe, \\nand Paul Hoffmann, and is discussed in detail in their paper, \\xe2\\x80\\x9cRecognizing Contextual \\nPolarity in Phrase-Level Sentiment Analysis,\\xe2\\x80\\x9d which focuses on contextual polarity. You \\ncan download the subjectivity lexicon from   http://mpqa.cs.pitt.edu/lexicons/subj_\\nlexicon/  , which is their official website. It has subjectivity clues present in the dataset \\nnamed  subjclueslen1-HLTEMNLP05.tff , which is available once you extract the archive. \\nSome sample lines from the dataset are depicted as follows: \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n355\\n type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n \\npriorpolarity=negative \\n type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n \\npriorpolarity=negative \\n type=weaksubj len=1 word1=abandon pos1=verb stemmed1=y \\npriorpolarity=negative \\n type=strongsubj len=1 word1=abase pos1=verb stemmed1=y \\npriorpolarity=negative \\n ... \\n ... \\n type=strongsubj len=1 word1=zealously pos1=anypos stemmed1=n \\npriorpolarity=negative \\n type=strongsubj len=1 word1=zenith pos1=noun stemmed1=n \\npriorpolarity=positive \\n type=strongsubj len=1 word1=zest pos1=noun stemmed1=n priorpolarity=positive  \\n  To understand this data, you can refer to the  readme file provided along with the \\ndataset. Basically, the clues in this dataset were curated and collected manually with \\nefforts by the above-mentioned maintainers of this project. The various parameters \\nmentioned above are explained briefly as follows:\\n\\xe2\\x80\\xa2 \\n type : This has values that are either  strongsubj indicating the \\npresence of a strongly subjective context or  weaksubj which \\nindicates the presence of a weak/part subjective context. \\n\\xe2\\x80\\xa2 \\n len : This points to the number of words in the term of the clue (all \\nare single words of length 1 for now). \\n\\xe2\\x80\\xa2 \\n word1 : The actual term present as a token or a stem of the actual \\ntoken. \\n\\xe2\\x80\\xa2 \\n pos1 : The part of speech for the term (clue) and it can be  noun , \\n verb ,  adj ,  adverb , or  anypos . \\n\\xe2\\x80\\xa2 \\n stemmed1 : This indicates if the clue (term) is stemmed ( y ) or not \\nstemmed ( n ). If it is stemmed, it can match all its other variants \\nhaving the same  pos1 tag. \\n\\xe2\\x80\\xa2 \\n priorpolarity : This has values of negative, positive, both, or \\nneutral, and indicates the polarity of the sentiment associated \\nwith this clue (term). \\n The idea is to load this lexicon into a database or memory (hint: Python dictionary \\nworks well) and then use it similarly to the previous lexicons to analyze the sentiment \\nassociated with any text document. \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n356\\n SentiWordNet \\n We know that WordNet is perhaps one of the most popular corpora for the English \\nlanguage, used extensively in semantic analysis, and it introduces the concept of synsets. \\nThe SentiWordNet lexicon is a lexical resource used for sentiment analysis and opinion \\nmining. For each synset present in WordNet, the SentiWordNet lexicon assigns three \\nsentiment scores to it, including a positive polarity score, a negative polarity score, \\nand an objectivity score. You can find more details on the official web site   http://\\nsentiwordnet.isti.cnr.it  , which includes research papers explaining the lexicon in \\ndetail and also a link to download the lexicon. The  nltk package in Python provides an \\ninterface directly for accessing the SentiWordNet lexicon, and we will be using this to \\nanalyze the sentiment of our movie reviews. The following snippet shows an example \\nsynset and its sentiment scores using SentiWordNet: \\n import nltk \\n from nltk.corpus import sentiwordnet as swn \\n # get synset for \\'good\\' \\n good = swn.senti_synsets(\\'good\\', \\'n\\')[0]   \\n # print synset sentiment scores \\n In [287]: print \\'Positive Polarity Score:\\', good.pos_score() \\n     ...: print \\'Negative Polarity Score:\\', good.neg_score() \\n     ...: print \\'Objective Score:\\', good.obj_score() \\n Positive Polarity Score: 0.5 \\n Negative Polarity Score: 0.0 \\n Objective Score: 0.5 \\n Now that we know how to use the  sentiwordnet interface, we define a function \\nthat can take in a body of text (movie review in our case) and analyze its sentiment by \\nleveraging  sentiwordnet : \\n from normalization import normalize_accented_characters, html_parser, strip_\\nhtml \\n def analyze_sentiment_sentiwordnet_lexicon(review, \\n                                           verbose=False): \\n    # pre-process text \\n    review = normalize_accented_characters(review) \\n    review = html_parser.unescape(review) \\n    review = strip_html(review) \\n    # tokenize and POS tag text tokens \\n    text_tokens = nltk.word_tokenize(review) \\n    tagged_text = nltk.pos_tag(text_tokens) \\n    pos_score = neg_score = token_count = obj_score = 0 \\n    # get wordnet synsets based on POS tags \\n    # get sentiment scores if synsets are found \\n    for word, tag in tagged_text: \\n        ss_set = None \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n357\\n        if \\'NN\\' in tag and swn.senti_synsets(word, \\'n\\'): \\n            ss_set = swn.senti_synsets(word, \\'n\\')[0] \\n        elif \\'VB\\' in tag and swn.senti_synsets(word, \\'v\\'): \\n            ss_set = swn.senti_synsets(word, \\'v\\')[0] \\n        elif \\'JJ\\' in tag and swn.senti_synsets(word, \\'a\\'): \\n            ss_set = swn.senti_synsets(word, \\'a\\')[0] \\n        elif \\'RB\\' in tag and swn.senti_synsets(word, \\'r\\'): \\n            ss_set = swn.senti_synsets(word, \\'r\\')[0] \\n        # if senti-synset is found     \\n        if ss_set: \\n            # add scores for all found synsets \\n            pos_score += ss_set.pos_score() \\n            neg_score += ss_set.neg_score() \\n            obj_score += ss_set.obj_score() \\n            token_count += 1 \\n    # aggregate final  scores \\n    final_score = pos_score - neg_score \\n    norm_final_score = round(float(final_score) / token_count, 2) \\n    final_sentiment = \\'positive\\' if norm_final_score >= 0 else \\'negative\\' \\n    if verbose: \\n        norm_obj_score = round(float(obj_score) / token_count, 2) \\n        norm_pos_score = round(float(pos_score) / token_count, 2) \\n        norm_neg_score = round(float(neg_score) / token_count, 2) \\n        # to display results in a nice table \\n        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score, \\n                                         norm_pos_score, norm_neg_score, \\n                                         norm_final_score]], \\n                                          columns=pd.MultiIndex(levels\\n=[[\\'SENTIMENT STATS:\\'],  \\n                                                       [\\'Predicted Sentiment\\',\\n \\'Objectivity\\', \\n                                                        \\'Positive\\', \\'Negative\\',\\n \\'Overall\\']],  \\n                                                      labels=[[0,0,0,0,0],\\n[0,1,2,3,4]])) \\n                                                     print sentiment_frame \\n    return final_ sentiment \\n  The comments in the preceding function are pretty self-explanatory. We take in a \\nbody of text (a movie review), do some initial pre-processing, and then tokenize and POS \\ntag the tokens. For each pair of (word, tag) we check if any senti-synsets exist for the same \\nword and its corresponding tag. If there is a match, we take the first senti-synset and store \\nits sentiment scores in corresponding variables, and finally we aggregate its scores. We \\ncan now see the preceding function in action for our sample reviews (in the  sample_data \\nvariable we created earlier from the test data) in the following snippet: \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n358\\n # detailed sentiment analysis for sample reviews \\n In [292]: for review, review_sentiment in sample_data:   \\n     ...:     print \\'Review:\\' \\n     ...:     print review \\n     ...:     print \\n     ...:     print \\'Labeled Sentiment:\\', review_sentiment     \\n     ...:     print     \\n     ...:      final_sentiment = analyze_sentiment_sentiwordnet_\\nlexicon(review, \\n     ...:                                                              \\nverbose=True) \\n     ...:     print \\'-\\'*60     \\n     ...:  \\n     ...:  \\n Review: \\n Worst movie, (with the best reviews given it) I\\'ve ever seen. Over the top \\ndialog, acting, and direction. more slasher flick than thriller.With all the \\ngreat reviews this movie got I\\'m appalled that it turned out so silly. shame \\non you martin scorsese \\n Labeled Sentiment:  negative \\n     SENTIMENT STATS:                                       \\n  Predicted Sentiment Objectivity Positive Negative Overall \\n 0            negative        0.83     0.08     0.09   -0.01 \\n ------------------------------------------------------------ \\n Review: \\n I hope this group of film-makers never re-unites. \\n Labeled Sentiment: negative \\n     SENTIMENT STATS:                                       \\n  Predicted Sentiment Objectivity Positive Negative Overall \\n 0            negative        0.71     0.04     0.25   -0.21 \\n ------------------------------------------------------------ \\n Review: \\n no comment - stupid movie, acting average or worse... screenplay - no sense \\nat all... SKIP IT! \\n Labeled Sentiment: negative \\n     SENTIMENT STATS:                                       \\n  Predicted Sentiment Objectivity Positive Negative Overall \\n 0            negative        0.81     0.04     0.15   -0.11 \\n ------------------------------------------------------------ \\n Review: \\n Add this little gem to your list of holiday regulars. It is<br /><br \\n/>sweet, funny, and endearing \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n359\\n Labeled Sentiment: positive \\n     SENTIMENT STATS:                                       \\n  Predicted Sentiment Objectivity Positive Negative Overall \\n 0            positive        0.76     0.18     0.06    0.13 \\n ------------------------------------------------------------ \\n Review: \\n a mesmerizing film that certainly keeps your attention... Ben Daniels is \\nfascinating (and courageous) to watch. \\n Labeled Sentiment:  positive \\n     SENTIMENT STATS:                                       \\n  Predicted Sentiment Objectivity Positive Negative Overall \\n 0            positive        0.84     0.14     0.03    0.11 \\n ------------------------------------------------------------ \\n Review: \\n This movie is perfect for all the romantics in the world. John Ritter has \\nnever been better and has the best line in the movie! \"Sam\" hits close to \\nhome, is lovely to look at and so much fun to play along with. Ben Gazzara \\nwas an excellent cast and easy to fall in love with. I\\'m sure I\\'ve met \\nArthur in my travels somewhere. All around, an excellent choice to pick up \\nany evening.!:-) \\n Labeled Sentiment: positive \\n     SENTIMENT STATS:                                       \\n  Predicted Sentiment Objectivity Positive Negative Overall \\n 0            positive        0.75      0.2     0.05    0.15 \\n ------------------------------------------------------------ \\n Review: \\n I don\\'t care if some people voted this movie to be bad. If you want the \\nTruth this is a Very Good Movie! It has every thing a movie should have. You \\nreally should Get this one. \\n Labeled Sentiment: positive \\n     SENTIMENT STATS:                                       \\n  Predicted Sentiment Objectivity Positive Negative Overall \\n 0            positive        0.73     0.21     0.06    0.15 \\n ------------------------------------------------------------ \\n Review: \\n Worst horror film ever but funniest film ever rolled in one you have got \\nto see this film it is so cheap it is unbeliaveble but you have to see it \\nreally!!!! P.s watch the carrot \\n Labeled Sentiment: positive \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n360\\n     SENTIMENT STATS:                                       \\n  Predicted Sentiment Objectivity Positive Negative Overall \\n 0            positive        0.79     0.13     0.08    0.05 \\n ------------------------------------------------------------ \\n You can see detailed statistics related to each sentiment score and also the overall \\nsentiment and compare it with the actual labeled sentiment for each review in the \\npreceding output. Interestingly, we were able to predict the sentiment correctly for all \\nour sampled reviews as compared to the supervised learning technique. But how well \\ndoes this technique perform for our complete test movie reviews dataset? The following \\nsnippet will give us the answer! \\n # predict sentiment for test movie reviews dataset \\n sentiwordnet_predictions = [analyze_sentiment_sentiwordnet_lexicon(review) \\n                            for review in test_reviews] \\n from utils import display_evaluation_metrics, display_confusion_matrix, \\ndisplay_classification_report \\n # get model performance statistics \\n In [295]: print \\'Performance metrics:\\' \\n     ...:  display_evaluation_metrics(true_labels=test_sentiments, \\n     ...:                             predicted_labels=sentiwordnet_\\npredictions, \\n     ...:                            positive_class=\\'positive\\')   \\n     ...: print \\'\\\\nConfusion Matrix:\\'                            \\n     ...: display_confusion_matrix(true_labels=test_sentiments, \\n     ...:                           predicted_labels=sentiwordnet_\\npredictions, \\n     ...:                          classes=[\\'positive\\', \\'negative\\']) \\n     ...: print \\'\\\\nClassification report:\\'                          \\n     ...: display_classification_report(true_labels=test_sentiments, \\n     ...:                                predicted_labels=sentiwordnet_\\npredictions, \\n     ...:                               classes=[\\'positive\\', \\'negative\\'])  \\n Performance metrics: \\n Accuracy: 0.59 \\n Precision: 0.56 \\n Recall: 0.92 \\n F1 Score: 0.7 \\n Confusion Matrix: \\n                 Predicted:          \\n                   positive negative \\n Actual: positive       6941      569 \\n        negative       5510     1980 \\n Classification report: \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n361\\n             precision    recall  f1-score   support \\n   positive       0.56      0.92      0.70      7510 \\n   negative       0.78      0.26      0.39      7490 \\n avg / total       0.67      0.59      0.55     15000 \\n Our model has a sentiment prediction accuracy of around 60% and an F1-score of \\n70% approximately. If you look at the detailed classification report and the confusion \\nmatrix, you will observe that we correctly classify 6941/7510 positive movie reviews as \\npositive, but we incorrectly classify 5510/7490 negative movie reviews as positive\\xe2\\x80\\x94which \\nis quite high! A way to redress this would be to change our logic slightly in our function \\nand relax the threshold for overall sentiment score to decide whether a document will \\nhave an overall positive or negative sentiment from 0 to maybe 0.1 or higher. Experiment \\nwith this threshold and see what kind of results you get.        \\n VADER Lexicon \\n VADER stands for Valence Aware Dictionary and sEntiment Reasoner. It is a lexicon \\nwith a rule-based sentiment analysis framework that was specially built for analyzing \\nsentiment from social media resources. This lexicon was developed by C. J. Hutto and \\nEric Gilbert, and you will find further details in the paper, \\xe2\\x80\\x9cVADER: A Parsimonious Rule-\\nbased Model for Sentiment Analysis of Social Media Text.\\xe2\\x80\\x9d You can read more about it \\nand even download the dataset or install the library from   https://github.com/cjhutto/\\nvaderSentiment  , which contains all the resources pertaining to the VADER lexicon. \\nThe file  vader_sentiment_lexicon.txt contains all the necessary sentiment scores \\nassociated with various terms, including words, emoticons, and even slang language-\\nbased tokens (like  lol ,  wtf ,  nah , and so on). There are over 9000 lexical features from \\nwhich it was further curated to 7500 lexical features in this lexicon with proper validated \\nvalence scores. Each feature was rated on a scale from  \"[-4] Extremely Negative\" to \\n \"[4] Extremely Positive\" , with allowance for  \"[0] Neutral (or Neither, N/A)\" . \\nThis curation was done by keeping all lexical features which had a non-zero mean rating \\nand whose standard deviation was less than 2.5, which was determined by the aggregate \\nof ten independent raters. A sample of the VADER lexicon is depicted as follows:          \\n )-:<   -2.2   0.4     [-2, -2, -2, -2, -2, -2, -3, -3, -2, -2] \\n )-:{   -2.1   0.9434  [-1, -3, -2, -1, -2, -2, -3, -4, -1, -2] \\n ):     -1.8   0.87178 [-1, -3, -1, -2, -1, -3, -1, -3, -1, -2] \\n ... \\n ... \\n resolved      0.7   0.78102  [1, 2, 0, 1, 1, 0, 2, 0, 0, 0] \\n resolvent     0.7   0.78102  [1, 0, 1, 2, 0, -1, 1, 1, 1, 1] \\n resolvents    0.4   0.66332  [2, 0, 0, 1, 0, 0, 1, 0, 0, 0] \\n ... \\n ... \\n }:-(   -2.1   0.7       [-2, -1, -2, -2, -2, -4, -2, -2, -2, -2] \\n }:-)    0.3   1.61555   [1, 1, -2, 1, -1, -3, 2, 2, 1, 1] \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n362\\n Each line in the preceding lexicon depicts a unique term, which can be a word \\nor even an emoticon. The first term indicates the word/emoticon, the second column \\nindicates the mean or average score, the third column indicates the standard deviation, \\nand the final column indicates a list of scores given by ten independent scorers. The  nltk \\npackage has a nice interface for leveraging the VADER lexicon, and the following function \\nmakes use of the same for analyzing sentiment for any text document: \\n from nltk.sentiment.vader import SentimentIntensityAnalyzer \\n def analyze_sentiment_vader_lexicon(review,  \\n                                    threshold=0.1, \\n                                    verbose=False): \\n    # pre-process text \\n    review = normalize_accented_characters(review) \\n    review = html_parser.unescape(review) \\n    review = strip_html(review) \\n    # analyze the sentiment for review \\n    analyzer = SentimentIntensityAnalyzer() \\n    scores = analyzer.polarity_scores(review) \\n    # get aggregate scores and final sentiment \\n    agg_score = scores[\\'compound\\'] \\n    final_sentiment = \\'positive\\' if agg_score >= threshold\\\\ \\n                                   else \\'negative\\' \\n    if verbose: \\n        # display detailed sentiment statistics \\n        positive = str(round(scores[\\'pos\\'], 2)*100)+\\'%\\' \\n        final = round(agg_score, 2) \\n        negative = str(round(scores[\\'neg\\'], 2)*100)+\\'%\\' \\n        neutral = str(round(scores[\\'neu\\'], 2)*100)+\\'%\\' \\n        sentiment_frame = pd.DataFrame([[final_sentiment, final, positive, \\n                                        negative, neutral]], \\n                  columns=pd.MultiIndex(levels=[[\\'SENTIMENT STATS:\\'], \\n                                               [ \\'Predicted Sentiment\\', \\n\\'Polarity Score\\', \\n                                                \\'Positive\\', \\'Negative\\', \\n                                                \\'Neutral\\']],  \\n                                       labels=[[0,0,0,0,0],[0,1,2,3,4]])) \\n        print sentiment_frame \\n    return final_ sentiment \\n That function helps in computing the sentiment and various statistics associated with \\nit for any text document (movie reviews in our case). The comments explain the main \\nsections of the function, which include text-preprocessing, getting the necessary sentiment \\nscores using the VADER lexicon, aggregating them, and computing the final sentiment \\n(positive/negative) using a specific threshold we talked about earlier. A threshold of 0.1 \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n363\\nseemed to work best on an average, but you can experiment further with it. The following \\nsnippet shows us how to use this function on our sampled test movie reviews:       \\n # get detailed sentiment statistics \\n In [301]: for review, review_sentiment in sample_data: \\n     ...:     print \\'Review:\\' \\n     ...:     print review \\n     ...:     print \\n     ...:     print \\'Labeled Sentiment:\\', review_sentiment     \\n     ...:     print     \\n     ...:     final_sentiment = analyze_sentiment_vader_lexicon(review, \\n     ...:                                                    threshold=0.1, \\n     ...:                                                    verbose=True) \\n     ...:     print \\'-\\'*60  \\n Review: \\n Worst movie, (with the best reviews given it) I\\'ve ever seen. Over the top \\ndialog, acting, and direction. more slasher flick than thriller.With all the \\ngreat reviews this movie got I\\'m appalled that it turned out so silly. shame \\non you martin scorsese \\n Labeled Sentiment: negative \\n     SENTIMENT STATS:                                          \\n  Predicted Sentiment Polarity Score Positive Negative Neutral \\n 0            negative           0.03    20.0%    18.0%   62.0% \\n ------------------------------------------------------------ \\n Review: \\n I hope this group of film-makers never re-unites. \\n Labeled Sentiment: negative \\n     SENTIMENT STATS:                                          \\n  Predicted Sentiment Polarity Score Positive Negative Neutral \\n 0            positive           0.44    33.0%     0.0%   67.0% \\n ------------------------------------------------------------ \\n Review: \\n no comment - stupid movie, acting average or worse... screenplay - no sense \\nat all... SKIP IT! \\n Labeled Sentiment: negative \\n     SENTIMENT STATS:                                          \\n  Predicted Sentiment Polarity Score Positive Negative Neutral \\n 0            negative           -0.8     0.0%    40.0%   60.0% \\n ------------------------------------------------------------ \\n Review: \\n Add this little gem to your list of holiday regulars. It is<br /><br />sweet, \\nfunny, and endearing \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n364\\n Labeled Sentiment: positive \\n     SENTIMENT STATS:                                          \\n  Predicted Sentiment Polarity Score Positive Negative Neutral \\n 0            positive           0.82    40.0%     0.0%   60.0% \\n ------------------------------------------------------------ \\n Review: \\n a mesmerizing film that certainly keeps your attention... Ben Daniels is \\nfascinating (and courageous) to watch. \\n Labeled Sentiment: positive \\n     SENTIMENT STATS:                                          \\n  Predicted Sentiment Polarity Score Positive Negative Neutral \\n 0            positive           0.71    31.0%     0.0%   69.0% \\n ------------------------------------------------------------ \\n Review: \\n This movie is perfect for all the romantics in the world. John Ritter has \\nnever been better and has the best line in the movie! \"Sam\" hits close to \\nhome, is lovely to look at and so much fun to play along with. Ben Gazzara \\nwas an excellent cast and easy to fall in love with. I\\'m sure I\\'ve met \\nArthur in my travels somewhere. All around, an excellent choice to pick up \\nany evening.!:-) \\n Labeled Sentiment: positive \\n     SENTIMENT STATS:                                          \\n  Predicted Sentiment Polarity Score Positive Negative Neutral \\n 0            positive           0.99    37.0%     2.0%   61.0% \\n ------------------------------------------------------------ \\n Review: \\n I don\\'t care if some people voted this movie to be bad. If you want the \\nTruth this is a Very Good Movie! It has every thing a movie should have. You \\nreally should Get this one. \\n Labeled Sentiment: positive \\n     SENTIMENT STATS:                                          \\n  Predicted Sentiment Polarity Score Positive Negative Neutral \\n 0            negative          -0.16    17.0%    14.0%   69.0% \\n ------------------------------------------------------------ \\n Review: \\n Worst horror film ever but funniest film ever rolled in one you have got \\nto see this film it is so cheap it is unbeliaveble but you have to see it \\nreally!!!! P.s watch the carrot \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n365\\n Labeled Sentiment: positive \\n     SENTIMENT STATS:                                          \\n  Predicted Sentiment Polarity Score Positive Negative Neutral \\n 0            positive           0.49    11.0%    11.0%   77.0% \\n ------------------------------------------------------------ \\n The preceding statistics are similar to our previous function except the  Positive , \\n Negative , and  Neutral columns indicate the percentage or proportion of the document \\nthat is positive, negative, or neutral, and the final score is determined based on the \\npolarity score and the threshold. The following snippet shows the model sentiment \\nprediction performance on the entire test movie reviews dataset: \\n # predict sentiment for test movie reviews dataset \\n vader_predictions = [analyze_sentiment_vader_lexicon(review, threshold=0.1) \\n                     for review in test_reviews] \\n # get model performance statistics \\n In [302]: print \\'Performance metrics:\\' \\n     ...: display_evaluation_metrics(true_labels=test_sentiments, \\n     ...:                            predicted_labels=vader_predictions, \\n     ...:                            positive_class=\\'positive\\')   \\n     ...: print \\'\\\\nConfusion Matrix:\\'                            \\n     ...: display_confusion_matrix(true_labels=test_sentiments, \\n     ...:                          predicted_labels=vader_predictions, \\n     ...:                          classes=[\\'positive\\', \\'negative\\']) \\n     ...: print \\'\\\\nClassification report:\\'                          \\n     ...: display_classification_report(true_labels=test_sentiments, \\n     ...:                               predicted_labels=vader_predictions, \\n     ...:                               classes=[\\'positive\\', \\'negative\\'])  \\n Performance metrics:  \\n Accuracy: 0.7 \\n Precision: 0.65 \\n Recall: 0.86 \\n F1 Score: 0.74 \\n Confusion Matrix: \\n                 Predicted:          \\n                   positive negative \\n Actual: positive       6434     1076 \\n        negative       3410     4080 \\n Classification report: \\n             precision    recall  f1-score   support \\n   positive       0.65      0.86      0.74      7510 \\n   negative       0.79      0.54      0.65      7490 \\n avg / total       0.72      0.70      0.69     15000 \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n366\\n The preceding metrics depict that our model has a sentiment prediction accuracy of \\naround 70 percent and an F1-score close to 75 percent, which is definitely better than our \\nprevious model. Also notice that we are able to correctly predict positive sentiment for \\n6434 out of 7510 positive movie reviews, and negative sentiment correctly for 4080 out of \\n7490 negative movie reviews. \\n Pattern Lexicon \\n The  pattern  package  is a complete package for NLP, text analytics, and information \\nretrieval. We discussed it in detail in previous chapters and have also used it several \\ntimes to solve several problems. This package is developed by CLiPS (Computational \\nLinguistics & Psycholinguistics), a research center associated with the Linguistics \\nDepartment of the Faculty of Arts of the University of Antwerp. It has a sentiment module \\nassociated with it, along with modules for analyzing mood and modality of a body of text. \\n For sentiment analysis, it analyzes any body of text by decomposing it into sentences \\nand then tokenizing it and tagging the various tokens with necessary parts of speech. \\nIt then uses its own subjectivity-based sentiment lexicon, which you can access from \\nits official repository at   https://github.com/clips/pattern/blob/master/pattern/\\ntext/en/en-sentiment.xml  . It contains scores like polarity, subjectivity, intensity, and \\nconfidence, along with other tags like the part of speech, WordNet identifier, and so \\non. It then leverages this lexicon to compute the overall polarity and subjectivity score \\nassociated with a text document. A threshold of 0.1 is recommended by  pattern itself to \\ncompute the final sentiment of a document as positive, and anything below it as negative. \\n You can also analyze the mood and modality of text  documents   by leveraging the \\nmood and modality functions provided by the  pattern package. The mood function \\nhelps in determining the mood expressed by a particular text document. This function \\nreturns  INDICATIVE ,  IMPERATIVE ,  CONDITIONAL , or  SUBJUNCTIVE for any text based on its \\ncontent. The table in Figure\\xc2\\xa0 7-2 talks about each type of mood in further detail, courtesy \\nof the official documentation provided by CLiPS  pattern . The column  Use talks about \\nthe typical usage patterns for each type of mood, and the examples provide some actual \\nexamples from the English language. \\n Modality for any text represents the degree of certainty expressed by the text as \\na whole. This value is a number that ranges between 0 and 1. Values > 0.5 indicate \\nfactual texts having a high certainty, and < 0.5 indicate wishes and hopes and have a low \\n Figure 7-2.  Different types of mood and their examples (figure courtesy of CLiPS pattern) \\n \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n367\\ncertainty associated with them. We will define a function now to analyze the sentiment \\nfor text documents using the  pattern lexicon: \\n from pattern.en import sentiment, mood, modality \\n def analyze_sentiment_pattern_lexicon(review, threshold=0.1, \\n                                      verbose=False): \\n    # pre-process text \\n    review = normalize_accented_characters(review) \\n    review = html_parser.unescape(review) \\n    review = strip_html(review) \\n    # analyze sentiment for the text document \\n    analysis = sentiment(review) \\n    sentiment_score = round(analysis[0], 2) \\n    sentiment_subjectivity = round(analysis[1], 2) \\n    # get final sentiment \\n    final_sentiment = \\'positive\\' if sentiment_score >= threshold\\\\ \\n                                   else \\'negative\\' \\n    if verbose: \\n        # display detailed sentiment statistics \\n        sentiment_frame = pd.DataFrame([[final_sentiment, sentiment_score, \\n                                        sentiment_subjectivity]], \\n                                         columns=pd.MultiIndex(levels\\n=[[\\'SENTIMENT STATS:\\'],  \\n                                                     [\\'Predicted Sentiment\\', \\n\\'Polarity Score\\', \\n                                                     \\'Subjectivity Score\\']],  \\n                                                      labels=[[0,0,0],\\n[0,1,2]])) \\n        print sentiment_frame \\n        assessment = analysis.assessments \\n        assessment_frame = pd.DataFrame(assessment,  \\n                                   columns=pd.MultiIndex(levels=[[\\'DETAILED \\nASSESSMENT STATS:\\'],  \\n                                                        [\\'Key Terms\\', \\'Polarity \\nScore\\', \\n                                                      \\'Subjectivity Score\\', \\n\\'Type\\']],  \\n                                                      labels=[[0,0,0,0],\\n[0,1,2,3]])) \\n                                                     print assessment_frame \\n                                                     print \\n    return final_sentiment   \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n368\\n We will now test the function we defined to analyze the sentiment of our sample \\ntest movie reviews and observe the results. We take a threshold of 0.1 as the cut-off to \\ndecide between positive and negative sentiment for a document based on the aggregated \\nsentiment polarity score, based on several experiments and recommendations from the \\nofficial documentation: \\n # get detailed sentiment statistics \\n In [303]: for review, review_sentiment in sample_data: \\n     ...:     print \\'Review:\\' \\n     ...:     print review \\n     ...:     print \\n     ...:     print \\'Labeled Sentiment:\\', review_sentiment     \\n     ...:     print     \\n     ...:     final_sentiment = analyze_sentiment_pattern_lexicon(review, \\n     ...:                                                         \\nthreshold=0.1, \\n     ...:                                                         \\nverbose=True) \\n     ...:     print \\'-\\'* 60 \\n Review: \\n Worst movie, (with the best reviews given it) I\\'ve ever seen. Over the top \\ndialog, acting, and direction. more slasher flick than thriller.With all the \\ngreat reviews this movie got I\\'m appalled that it turned out so silly. shame \\non you martin scorsese \\n Labeled Sentiment: negative \\n     SENTIMENT STATS:                                   \\n  Predicted Sentiment Polarity Score Subjectivity Score \\n 0            negative           0.06               0.62 \\n  DETAILED ASSESSMENT STATS:                                         \\n                   Key Terms Polarity Score Subjectivity Score  Type \\n 0                    [worst]           -1.0              1.000  None \\n 1                     [best]            1.0              0.300  None \\n 2                      [top]            0.5              0.500  None \\n 3                   [acting]            0.0              0.000  None \\n 4                     [more]            0.5              0.500  None \\n 5                    [great]            0.8              0.750  None \\n 6                 [appalled]           -0.8              1.000  None \\n 7                    [silly]           -0.5              0.875   None \\n ------------------------------------------------------------ \\n Review: \\n I hope this group of film-makers never re-unites. \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n369\\n Labeled Sentiment: negative \\n     SENTIMENT STATS:                                   \\n  Predicted Sentiment Polarity Score Subjectivity Score \\n 0            negative            0.0                0.0 \\n Empty DataFrame \\n Columns: [(DETAILED ASSESSMENT STATS:, Key Terms), (DETAILED ASSESSMENT \\nSTATS:, Polarity Score), (DETAILED ASSESSMENT STATS:, Subjectivity Score), \\n(DETAILED ASSESSMENT STATS:, Type)] \\n Index: [] \\n ------------------------------------------------------------ \\n Review: \\n no comment - stupid movie, acting average or worse... screenplay - no sense \\nat all... SKIP IT! \\n Labeled Sentiment: negative \\n     SENTIMENT STATS:                                   \\n  Predicted Sentiment Polarity Score Subjectivity Score \\n 0            negative          -0.36                0.5 \\n  DETAILED ASSESSMENT STATS:                                         \\n                   Key Terms Polarity Score Subjectivity Score  Type \\n 0                   [stupid]          -0.80                1.0  None \\n 1                   [acting]           0.00                0.0  None \\n 2                  [average]          -0.15                0.4  None \\n 3                 [worse, !]          -0.50                0.6  None \\n ------------------------------------------------------------ \\n Review: \\n Add this little gem to your list of holiday regulars. It is<br /><br \\n/>sweet, funny, and endearing \\n Labeled Sentiment: positive \\n     SENTIMENT STATS:                                   \\n  Predicted Sentiment Polarity Score Subjectivity Score \\n 0            positive           0.19               0.67 \\n  DETAILED ASSESSMENT STATS:                                         \\n                   Key Terms Polarity Score Subjectivity Score  Type \\n 0                   [little]        -0.1875                0.5  None \\n 1                    [funny]         0.2500                1.0  None \\n 2                [endearing]         0.5000                0.5  None \\n ------------------------------------------------------------ \\n Review: \\n a mesmerizing film that certainly keeps your attention... Ben Daniels is \\nfascinating (and courageous) to watch. \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n370\\n Labeled Sentiment: positive \\n     SENTIMENT STATS:                                   \\n  Predicted Sentiment Polarity Score Subjectivity Score \\n 0            positive            0.4               0.71 \\n  DETAILED ASSESSMENT STATS:                                         \\n                   Key Terms Polarity Score Subjectivity Score  Type \\n 0              [mesmerizing]       0.300000           0.700000  None \\n 1                [certainly]       0.214286           0.571429  None \\n 2              [fascinating]       0.700000           0.850000  None \\n ------------------------------------------------------------ \\n Review: \\n This movie is perfect for all the romantics in the world. John Ritter has \\nnever been better and has the best line in the movie! \"Sam\" hits close to \\nhome, is lovely to look at and so much fun to play along with. Ben Gazzara \\nwas an excellent cast and easy to fall in love with. I\\'m sure I\\'ve met \\nArthur in my travels somewhere. All around, an excellent choice to pick up \\nany evening.!:-) \\n Labeled Sentiment: positive \\n     SENTIMENT STATS:                                   \\n  Predicted Sentiment Polarity Score Subjectivity Score \\n 0            positive           0.66               0.73 \\n   DETAILED ASSESSMENT STATS:                                         \\n                    Key Terms Polarity Score Subjectivity Score  Type \\n 0                   [perfect]       1.000000           1.000000  None \\n 1                    [better]       0.500000           0.500000  None \\n 2                   [best, !]       1.000000           0.300000  None \\n 3                    [lovely]       0.500000           0.750000  None \\n 4                 [much, fun]       0.300000           0.200000  None \\n 5                 [excellent]       1.000000           1.000000  None \\n 6                      [easy]       0.433333           0.833333  None \\n 7                      [love]       0.500000           0.600000  None \\n 8                      [sure]       0.500000           0.888889  None \\n 9              [excellent, !]       1.000000           1.000000  None \\n 10                      [:-)]       0.500000           1.000000  mood \\n ------------------------------------------------------------ \\n Review: \\n I don\\'t care if some people voted this movie to be bad. If you want the \\nTruth this is a Very Good Movie! It has every thing a movie should have. \\nYou really should Get this one. \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n371\\n Labeled Sentiment: positive \\n     SENTIMENT STATS:                                   \\n  Predicted Sentiment Polarity Score Subjectivity Score \\n 0            positive           0.17               0.55 \\n  DETAILED ASSESSMENT STATS:                                         \\n                   Key Terms Polarity Score Subjectivity Score  Type \\n 0                      [bad]           -0.7           0.666667  None \\n 1            [very, good, !]            1.0           0.780000  None \\n 2                   [really]            0.2           0.200000  None \\n ------------------------------------------------------------ \\n Review: \\n Worst horror film ever but funniest film ever rolled in one you have got \\nto see this film it is so cheap it is unbeliaveble but you have to see it \\nreally!!!! P.s watch the carrot \\n Labeled Sentiment: positive \\n     SENTIMENT STATS:                                   \\n  Predicted Sentiment Polarity Score Subjectivity Score \\n 0            negative          -0.04               0.63 \\n  DETAILED ASSESSMENT STATS:                                         \\n                   Key Terms Polarity Score Subjectivity Score  Type \\n 0                    [worst]      -1.000000                1.0  None \\n 1                    [cheap]       0.400000                0.7  None \\n 2       [really, !, !, !, !]       0.488281                0.2   None \\n ------------------------------------------------------------ \\n The preceding analysis shows the sentiment, polarity, and subjectivity scores for \\neach sampled review. Besides this, we also see key terms and emotions and their polarity \\nscores, which mainly contributed to the overall sentiment of each review. You can see \\nthat even exclamations and emoticons are also given importance and weightage when \\ncomputing sentiment and polarity. The following snippet depicts the mood and modality \\nfor the sampled test movie reviews: \\n In [304]: for review, review_sentiment in sample_data: \\n     ...:     print \\'Review:\\' \\n     ...:     print review \\n     ...:     print \\'Labeled Sentiment:\\', review_sentiment  \\n     ...:     print \\'Mood:\\', mood(review) \\n     ...:     mod_score = modality(review) \\n     ...:     print \\'Modality Score:\\', round(mod_score, 2) \\n     ...:     print \\'Certainty:\\', \\'Strong\\' if mod_score > 0.5 \\\\ \\n     ...:                                else \\'Medium\\' if mod_score > 0.35 \\\\ \\n     ...:                                                     else \\'Low\\' \\n     ...:     print \\'-\\'*60   \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n372\\n Review: \\n Worst movie, (with the best reviews given it) I\\'ve ever seen. Over the top \\ndialog, acting, and direction. more slasher flick than thriller.With all the \\ngreat reviews this movie got I\\'m appalled that it turned out so silly. shame \\non you martin scorsese \\n Labeled Sentiment: negative \\n Mood: indicative \\n Modality Score: 0.75 \\n Certainty: Strong \\n ------------------------------------------------------------ \\n Review: \\n I hope this group of film-makers never re-unites. \\n Labeled Sentiment: negative \\n Mood: subjunctive \\n Modality Score: -0.25 \\n Certainty:  Low \\n ------------------------------------------------------------ \\n Review: \\n no comment - stupid movie, acting average or worse... screenplay - no sense \\nat all... SKIP IT! \\n Labeled Sentiment: negative \\n Mood: indicative \\n Modality Score: 0.75 \\n Certainty: Strong \\n ------------------------------------------------------------ \\n Review: \\n Add this little gem to your list of holiday regulars. It is<br /><br \\n/>sweet, funny, and endearing \\n Labeled Sentiment: positive \\n Mood: imperative \\n Modality Score: 1.0 \\n Certainty: Strong \\n ------------------------------------------------------------ \\n Review: \\n a mesmerizing film that certainly keeps your attention... Ben Daniels is \\nfascinating (and courageous) to watch. \\n Labeled Sentiment: positive \\n Mood: indicative \\n Modality Score: 0.75 \\n Certainty: Strong \\n ------------------------------------------------------------ \\n Review: \\n This movie is perfect for all the romantics in the world. John Ritter has \\nnever been better and has the best line in the movie! \"Sam\" hits close to \\nhome, is lovely to look at and so much fun to play along with. Ben Gazzara \\nwas an excellent cast and easy to fall in love with. I\\'m sure I\\'ve met \\nArthur in my travels somewhere. All around, an excellent choice to pick up \\nany evening.!:-) \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n373\\n Labeled Sentiment: positive \\n Mood: indicative \\n Modality Score: 0.58 \\n Certainty:  Strong \\n ------------------------------------------------------------ \\n Review: \\n I don\\'t care if some people voted this movie to be bad. If you want the \\nTruth this is a Very Good Movie! It has every thing a movie should have. You \\nreally should Get this one. \\n Labeled Sentiment: positive \\n Mood: conditional \\n Modality Score: 0.28 \\n Certainty:  Low \\n ------------------------------------------------------------ \\n Review: \\n Worst horror film ever but funniest film ever rolled in one you have got \\nto see this film it is so cheap it is unbeliaveble but you have to see it \\nreally!!!! P.s watch the carrot \\n Labeled Sentiment: positive \\n Mood: indicative \\n Modality Score: 0.75 \\n Certainty:  Strong \\n ------------------------------------------------------------ \\n The preceding output depicts the mood, modality score, and the certainty factor \\nexpressed by each review. It is interesting to see phrases like  \"Add this little gem\\xe2\\x80\\xa6\" \\nare correctly associated with the right mood, which is an  imperative , and  \"I hope \\nthis\\xe2\\x80\\xa6\" is correctly associated with  subjunctive mood. The other reviews have more of an \\n indicative  disposition, which is quite obvious since it expresses the beliefs of the review \\nwho wrote the movie review. Certainty is lower in cases of reviews that use words like \\n \"hope\" ,  \"if\" , and higher in case of strongly opinionated reviews. \\n Finally, we will evaluate the  sentiment prediction performance of this model on our \\nentire test review dataset as we have done before for our other models. The following \\nsnippet achieves the same: \\n # predict sentiment for test movie reviews dataset \\n pattern_predictions =  [analyze_sentiment_pattern_lexicon(review, \\nthreshold=0.1) \\n                       for review in test_reviews]    \\n # get model performance statistics \\n In [307]: print \\'Performance metrics:\\' \\n     ...: display_evaluation_metrics(true_labels=test_sentiments, \\n     ...:                            predicted_labels=pattern_predictions, \\n     ...:                            positive_class=\\'positive\\')   \\n     ...: print \\'\\\\nConfusion Matrix:\\'                            \\n     ...: display_confusion_matrix(true_labels=test_sentiments, \\n     ...:                          predicted_labels=pattern_predictions, \\n     ...:                          classes=[\\'positive\\', \\'negative\\']) \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n374\\n     ...: print \\'\\\\nClassification report:\\'                          \\n     ...: display_classification_report(true_labels=test_sentiments, \\n     ...:                                predicted_labels=pattern_\\npredictions, \\n     ...:                               classes=[\\'positive\\', \\'negative\\']) \\n Performance metrics: \\n Accuracy: 0.77 \\n Precision: 0.76 \\n Recall: 0.79 \\n F1 Score: 0.77 \\n Confusion Matrix: \\n                 Predicted:          \\n                   positive negative \\n Actual: positive       5958     1552 \\n        negative       1924     5566 \\n Classification report: \\n             precision    recall  f1-score   support \\n   positive       0.76      0.79      0.77      7510 \\n   negative       0.78      0.74      0.76      7490 \\n avg / total       0.77      0.77      0.77     15000 \\n This model gives a better and more balanced  performance   toward predicting the \\nsentiment of both positive and negative classes. We have an average sentiment prediction \\naccuracy of 77 percent and an average F1-score of 77 percent for this model. Although \\nthe number of correct positive predictions has dropped from our previous model to \\n5958/7510 reviews, the number of correct predictions for negative reviews has increased \\nsignificantly to 5566/7490 reviews. \\n     Comparing Model  Performances   \\n We have built a supervised classification model and three unsupervised lexicon-based \\nmodels to predict sentiment for movie reviews. For each model, we looked at its detailed \\nanalysis and statistics for calculating sentiment. We also evaluated each model on \\nstandard metrics like precision, recall, accuracy, and F1-score. In this section, we will \\nbriefly look at how each model\\xe2\\x80\\x99s performance compares against the other models. \\nFigure\\xc2\\xa0 7-3  shows the model performance metrics and a visualization comparing the \\nmetrics across all the models. \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n375\\n From the visualization and the table in Figure\\xc2\\xa0 7-3 , it is clear that the supervised \\nmodel using SVM gives us the best results, which are expected because it was trained on \\n35,000 training movie reviews. Pattern lexicon performs the best among the unsupervised \\ntechniques for our test movie reviews. Does this mean these models will always perform \\nthe best? Absolutely not. It depends on the data you are analyzing. Remember to consider \\nvarious models and also to evaluate all the metrics when evaluating any model, and not \\njust one or two. Some of the models in the chart have really high recall but low precision, \\nwhich indicates these models have a tendency to make more wrong predictions or \\nfalse positives. You can re-use these benchmarks and evaluate more sentiment analysis \\nmodels as you experiment with different features, lexicons, and techniques.      \\n Figure 7-3.  Comparison of sentiment analysis model  performances \\n \\nCHAPTER 7 \\xe2\\x96\\xa0 SEMANTIC AND SENTIMENT ANALYSIS\\n376\\n Summary \\n In this final chapter, we have covered a variety of topics focused on semantic and \\nsentiment analysis of textual data. We revisited several of our concepts from Chapter \\n  1 with regard to language semantics. We looked at the WordNet corpus in detail and \\nexplored the concept of synsets with practical examples. We also analyzed various lexical \\nsemantic relations from Chapter   1 here, using synsets and real-world examples. We \\nlooked at relationships including entailments, homonyms and homographs, synonyms \\nand antonyms, hyponyms and hypernyms, and holonyms and meronyms. Semantic \\nrelations and similarity computation techniques were also discussed in detail, with \\nexamples that leveraged common hypernyms among various synsets. Some popular \\ntechniques widely used in semantic and information extraction were discussed, including \\nword sense disambiguation and named entity recognition, with examples. Besides \\nsemantic relations, we also revisited concepts related to semantic representations, \\nnamely propositional logic and first order logic. We leveraged the use of theorem provers \\nand evaluated actual propositions and logical expressions computationally. \\n Next, we introduced the concept of sentiment analysis and opinion mining and saw \\nhow it is used in various domains like social media, surveys, and feedback data. We took \\na practical example of analyzing sentiment on actual movie reviews from IMDb and built \\nseveral models that included supervised machine learning and unsupervised lexicon-\\nbased models. We looked at each technique and its results in detail and compared the \\nperformance across all our models. \\n This brings us to the end of this book. I hope the various concepts and techniques \\ndiscussed here will be helpful to and that you can use the knowledge and techniques \\nfrom this book when you tackle challenging problems in the world of text analytics and \\nnatural language processing. You may have seen by now that there is a lot of unexplored \\nterritory out there in the world of analyzing unstructured text data. I wish you the very \\nbest and would like to leave you with the parting thought from Occam\\xe2\\x80\\x99s razor:  Sometimes \\nthe simplest solution is the best solution . \\n377\\n\\xc2\\xa9 Dipanjan Sarkar 2016 \\nD. Sarkar, Text Analytics with Python, DOI 10.1007/978-1-4842-2388-8\\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  A \\n Adjective phrase (ADJP) , 13 \\n Advanced word vectorization models , \\n187\\xe2\\x80\\x93188 \\n Adverb phrase (ADVP) , 13 \\n Affi  nity propagation (AP) \\n description , 308 \\n exemplars , 308 \\n feature matrix , 309 \\n K-means clustering, movie data , \\n310\\xe2\\x80\\x93313  \\n message-passing steps , 308\\xe2\\x80\\x93309 \\n number of movies, clusters , 309 \\n AFINN lexicon , 353\\xe2\\x80\\x93354 \\n Th e American National Corpus (ANC) , 40 \\n Anonymous functions , 86\\xe2\\x80\\x9387 \\n Antonyms , 27, 324\\xe2\\x80\\x93325 \\n Application programming interfaces \\n(APIs) , 52 \\n Artifi cial intelligence (AI) , 57 \\n Automated document summarization \\n abstraction-based techniques , 251 \\n defi nition , 220 \\n elephants , 251 \\n extraction-based techniques , 251 \\n gensim, normalization , 252 \\n LSA , 253\\xe2\\x80\\x93255 \\n mathematical and statistical models , \\n250 \\n product description , 261\\xe2\\x80\\x93263 \\n Python , 252 \\n summary_ratio , 252 \\n TextRank , 256\\xe2\\x80\\x93258, 260\\xe2\\x80\\x93261 \\n  Automated text classifi cation \\n binary classifi cation , 172 \\n description , 170 \\n learning methods , 171 \\n multi-class classifi cation , 172 \\n prediction process , 171 \\n reinforcement learning , 170 \\n semi-supervised learning , 170 \\n supervised learning , 170\\xe2\\x80\\x93171 \\n training process , 171\\xe2\\x80\\x93172 \\n unsupervised learning , 170 \\n Averaged word vectors , 188\\xe2\\x80\\x93190 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  B \\n Backus-Naur Form (BNF) , 94 \\n Bag of Words model , 179\\xe2\\x80\\x93182, 185\\xe2\\x80\\x93186 \\n BigramTagger , 140 \\n Bing Liu\\xe2\\x80\\x99s lexicon , 354 \\n Blueprint, text classifi cation , 172\\xe2\\x80\\x93174 \\n Th e British National Corpus (BNC) , 40    \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  C \\n Case conversion operations , 119 \\n Centroid-based clustering \\nmodels , 298 \\n Centrum Wiskunde and Informatica \\n(CWI) , 51 \\n Th e Child Language Data Exchange \\nSystem (CHILDES) , 39 \\n ChunkRule , 148 \\n Classifi cation algorithms \\n evaluation , 194 \\n multinomial na\\xc3\\xafve Bayes , 195\\xe2\\x80\\x93197  \\n supervised ML algorithms , 193 \\n SVM , 197\\xe2\\x80\\x93199 \\n training , 193 \\n tuning , 194 \\n types , 194 \\n Classifi erBasedPOSTagger class , 142 \\n Cleaning text , 115 \\n Index \\n\\xe2\\x96\\xa0 INDEX\\n378\\n Collocations , 226\\xe2\\x80\\x93230 \\n Common Language Runtime (CLR) , 59 \\n Comprehensions , 88\\xe2\\x80\\x9389 \\n  Conda package management , 64   \\n conll_tag_chunks() , 151 \\n  Constituency-based parsing , 158\\xe2\\x80\\x93159, \\n161\\xe2\\x80\\x93163, 165  \\n Continuous integration (CI) processes , 54 \\n Contractions , 118\\xe2\\x80\\x93119 \\n Th e Corpus of Contemporary American \\nEnglish (COCA) , 40 \\n Cosine distance and similarity , 283\\xe2\\x80\\x93285, \\n287, 289 \\n CPython , 59 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  D \\n Database management systems (DBMS) , \\n1 \\n Deep learning , 319 \\n Density-based clustering models , 298 \\n Dependency-based parsing \\n code , 154\\xe2\\x80\\x93155 \\n Language Syntax and Structure , 153 \\n nltk , 155  \\n rule-based dependency , 157 \\n sample sentence , 156\\xe2\\x80\\x93157 \\n scaling , 158 \\n spacy\\xe2\\x80\\x99s output , 154 \\n textual tokens , 154 \\n tree/graph , 153 \\n DependencyGrammar class , 157 \\n Dictionaries , 75\\xe2\\x80\\x9376 \\n Distribution-based clustering models , 298 \\n Document clustering \\n AP (see  Affi  nity propagation (AP) ) \\n BIRCH and CLARANS , 298 \\n centroid-based clustering models , 298 \\n defi nition , 296 \\n density-based clustering \\nmodels , 298 \\n distribution-based clustering \\nmodels , 298 \\n hierarchical clustering models , 297 \\n IMDb , 299 \\n K-meansclustering (see  K-means \\nclustering ) \\n movie data , 299\\xe2\\x80\\x93300 \\n normalization and feature \\nextraction , 300\\xe2\\x80\\x93301 \\n scikit-learn , 296 \\n Ward\\xe2\\x80\\x99s hierarchicalclustering \\n(see  Ward\\xe2\\x80\\x99s agglomerative \\nhierarchical clustering ) \\n Document similarity \\n build_feature_matrix() , 285 \\n corpus of , 286 \\n cosine distance and similarity , 287, \\n289 \\n HB-distance , 289\\xe2\\x80\\x93291 \\n mathematical computations , 285 \\n Okapi BM25 , 292\\xe2\\x80\\x93296 \\n TF-IDF features , 286 \\n toy_corpus index , 286 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  E \\n Entailments , 323 \\n Euclidean distance , 277\\xe2\\x80\\x93278 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  F \\n Feature-extraction techniques \\n advanced word vectorization models , \\n187\\xe2\\x80\\x93188  \\n averaged word vectors , 188\\xe2\\x80\\x93190 \\n Bag of Words model , 179\\xe2\\x80\\x93181 \\n defi nition , 177 \\n implementations, modules , 179 \\n TF-IDFmodel (see  Term Frequency-\\nInverse Document Frequency \\n(TF-IDF) model ) \\n TF-IDF weighted averaged word \\nvectors , 190\\xe2\\x80\\x93193 \\n Vector Space Model , 178 \\n First order logic (FOL) , 33, 338\\xe2\\x80\\x93341 \\n Flow of code , 78 \\n FOL . See  First order logic (FOL)  \\n Functions , 84\\xe2\\x80\\x9385 \\n functools module , 91 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  G \\n Gaussian mixture models (GMM) , 298 \\n Generators , 90\\xe2\\x80\\x9391 \\n gensim library , 105 \\n Global Interpreter Lock (GIL) , 58 \\n Grammar \\n classifi cation , 15 \\n constituency , 20\\xe2\\x80\\x9321 \\n conjunctions , 22 \\n coordinating conjunction , 23 \\n\\xe2\\x96\\xa0 INDEX\\n379\\n lexical category , 19 \\n model , 19 \\n noun phrases , 19 \\n phrase structure rules , 19 \\n prepositional phrases , 21 \\n recursive properties , 21 \\n rules and conventions , 22 \\n syntax trees , 20 \\n verb phrases , 20 \\n course of time , 15 \\n dependencies , 15\\xe2\\x80\\x9318 \\n models , 15 \\n rules , 15 \\n syntax and structure, language , 15 \\n Graphical user interfaces (GUIs) , 56 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  H \\n Hamming distance , 274\\xe2\\x80\\x93275 \\n Handling exceptions , 82\\xe2\\x80\\x9383 \\n Hellinger-Bhattacharya distance \\n(HB-distance) , 289\\xe2\\x80\\x93291 \\n Hierarchical clustering models , 297, 313 \\n Higher order logic (HOL) , 37 \\n High-level language (HLL) , 52 \\n Holonyms , 327\\xe2\\x80\\x93328 \\n Homographs , 324 \\n Homonyms , 324 \\n  Human-computer interaction (HCI) , 46   \\n Hypernyms , 325\\xe2\\x80\\x93327 \\n Hyperparameter tuning , 173, 194 \\n Hyponyms , 325\\xe2\\x80\\x93327 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  I \\n IMDb . See  Internet Movie Database \\n(IMDb) \\n Indexing , 97\\xe2\\x80\\x9399 \\n Information retrieval (IR) , 266 \\n Integrated development environments \\n(IDEs) , 61 \\n Internet Movie Database (IMDb) movie \\nreviews , 299, 307, 316\\xe2\\x80\\x93317 \\n datasets , 347\\xe2\\x80\\x93348 \\n feature-extraction , 345 \\n getting and formatting, data , 343 \\n lexicons (see  Lexicons ) \\n model performance metrics and \\nvisualization , 346\\xe2\\x80\\x93347, 374\\xe2\\x80\\x93375  \\n positive and negative , 343 \\n setting up dependencies , 343 \\n supervisedML (see  Supervised \\nmachine learning technique ) \\n text normalization , 343\\xe2\\x80\\x93345 \\n Iterators , 87\\xe2\\x80\\x9388 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  J \\n JAVA_HOME environment variable , 134 \\n  Java Runtime Environment (JRE) , 134   \\n Java Virtual Machine (JVM) , 59 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  K \\n Keyphrase extraction \\n collocations , 226\\xe2\\x80\\x93230 \\n defi nition , 219, 225 \\n text analytics , 226 \\n weighted tag\\xe2\\x80\\x93based phrase \\nextraction , 230\\xe2\\x80\\x93233 \\n K-means clustering \\n analysis data , 306\\xe2\\x80\\x93307 \\n data structure , 303 \\n defi nition , 301 \\n functions , 303 \\n IMDb movie data , 307 \\n iterative procedure , 301 \\n movie data , 302 \\n multidimensional \\nscaling (MDS) , 304\\xe2\\x80\\x93306 \\n Kullback-Leibler divergence , 268 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  L \\n Lancaster stemmer , 130 \\n Language semantics \\n antonyms , 27 \\n capitonyms , 27 \\n defi nition , 25 \\n FOL \\n collection of well-defi ned formal \\nsystems , 33 \\n components , 34 \\n HOL , 37 \\n natural language statements , 37 \\n quantifi ers and variables , 33, 35  \\n universal generalization , 36 \\n heterographs , 26 \\n heteronyms , 26 \\n homographs , 26 \\n homonyms , 26 \\n homophones , 26 \\n\\xe2\\x96\\xa0 INDEX\\n380\\n hypernyms , 27 \\n hyponyms , 27 \\n lemma , 25 \\n lexical , 25 \\n linguistic , 25 \\n networks and models , 28\\xe2\\x80\\x9329 \\n PL (see  Propositional logic (PL) ) \\n polysemes , 26 \\n representation , 29 \\n synonyms , 27 \\n syntax and rules , 25 \\n wordforms , 25 \\n Language syntax and structure \\n clauses , 11 \\n categories , 14 \\n declarative , 14 \\n exclamations , 14 \\n imperative , 14 \\n independent sentences , 14 \\n interrogative , 14 \\n relationship , 14 \\n relative , 14 \\n collection of words , 10 \\n constituent units , 10 \\n English , 10 \\n grammar (see  Grammar ) \\n hierarchical tree , 11 \\n phrases , 12\\xe2\\x80\\x9313 \\n rules, conventions and principles , 10 \\n sentence , 10 \\n word order typology , 23\\xe2\\x80\\x9324   \\n Latent dirichlet\\xc2\\xa0allocation (LDA) \\n algorithm , 243 \\n black box , 243 \\n end-to-end framework , 242 \\n gensim , 243 \\n get_topics_terms_weights() \\nfunction , 244 \\n LdaModel class , 244 \\n parameters , 242 \\n plate notation , 241\\xe2\\x80\\x93242 \\n print_topics_udf() function , 244 \\n Latent semantic analysis \\n(LSA) , 253\\xe2\\x80\\x93255 \\n Latent semantic indexing (LSI) \\n description , 235 \\n dictionary , 235\\xe2\\x80\\x93236 \\n framework , 236 \\n function, thresholds , 237 \\n gensim and toy corpus , 235 \\n low_rank_svd() function , 238 \\n matrix computations , 241 \\n parameters , 237\\xe2\\x80\\x93238 \\n terms and weights , 239\\xe2\\x80\\x93240 \\n TF-IDF feature matrix , 238  \\n TF-IDF\\xe2\\x80\\x93weighted model , 236 \\n thresholds , 240\\xe2\\x80\\x93241 \\n Lemmatization \\n nltk package , 131 \\n normalizing , 132  \\n root word , 131 \\n speech , 132  \\n wordnet corpus , 132 \\n Levenshtein edit distance , 278\\xe2\\x80\\x93283 \\n Lexical Functional Grammar \\n(LFG) , 158 \\n Lexical semantics , 25 \\n Lexical similarity , 271 \\n Lexicons \\n AFINN , 353\\xe2\\x80\\x93354 \\n Bing Liu , 354 \\n description , 352 \\n MPQA subjectivity , 354\\xe2\\x80\\x93355 \\n pattern (see  Pattern lexicon ) \\n SentiWordNet , 356\\xe2\\x80\\x93361 \\n VADER , 361\\xe2\\x80\\x93366 \\n Linguistics \\n defi nition , 8 \\n discourse analysis , 9 \\n lexicon , 9 \\n morphology , 9 \\n phonetics , 8 \\n Phonology , 8 \\n pragmatics , 9 \\n semantics , 9 \\n semiotics , 9  \\n stylistics , 9 \\n syntax , 8 \\n term , 8 \\n Lists , 73\\xe2\\x80\\x9374 \\n Looping constructs , 80, 82 \\n LSA . See  Latent semantic analysis (LSA) \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  M \\n Machine learning (ML) algorithms , 107 \\n Manhattan distance , 275\\xe2\\x80\\x93277 \\n max(candidates, key=WORD_COUNTS.\\nget) function , 126 \\n MaxentClassifi er , 142 \\n Meronyms , 327\\xe2\\x80\\x93328 \\nLanguage semantics (cont.)\\n\\xe2\\x96\\xa0 INDEX\\n381\\n  Multi-class text classifi cation system \\n confusion matrix and SVM , 211 \\n feature-extraction techniques , 206\\xe2\\x80\\x93207  \\n metrics, prediction performance , \\n207\\xe2\\x80\\x93208  \\n misclassifi ed documents , 212\\xe2\\x80\\x93214 \\n multinomial na\\xc3\\xafve Bayes and SVM , \\n209\\xe2\\x80\\x93210  \\n normalization , 206 \\n scikit-learn , 208 \\n training and testing datasets , 204\\xe2\\x80\\x93206 \\n Multinomial na\\xc3\\xafve Bayes , 195\\xe2\\x80\\x93197 \\n Multi-Perspective Question Answering \\n(MPQA) subjectivity lexicon , \\n354\\xe2\\x80\\x93355 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  N \\n Named entity recognition , 332\\xe2\\x80\\x93335 \\n Natural language \\n acquisition \\n and cognitive learning , 5\\xe2\\x80\\x936 \\n and usage , 5  \\n analysis, data , 1  \\n communication , 2  \\n database , 1 \\n DBMS , 1 \\n direction, fi t representation , 5 \\n human languages , 2 \\n linguistics (see  Linguistics ) \\n NLP , 1 \\n NLP (see  Natural language \\nprocessing (NLP) ) \\n origins of language , 2 \\n philosophy , 2\\xe2\\x80\\x933 \\n processing , 2 \\n semantics (see  Language semantics ) \\n sensors , 1 \\n SQL Server , 1 \\n syntax and structure , 11 \\n phrases , 13 \\n techniques and algorithms , 2 \\n textcorpora (see  Text corpora/text \\ncorpus ) \\n triangle of reference model , 4 \\n usage , 7\\xe2\\x80\\x938 \\n Natural language processing (NLP) , 1, 51, \\n107, 319 \\n contextual recognition and \\nresolution , 48 \\n defi nition , 46 \\n HCI , 46 \\n machine translation , 46 \\n QAS , 47 \\n speech recognition , 47 \\n text analytics , 49\\xe2\\x80\\x9350 \\n text categorization , 49 \\n text summarization , 48   \\n Th e Natural Language Toolkit \\n(NLTK) , 40, 105  \\n NGramTagChunker , 151 \\n Non-negative matrix factorization \\n(NNMF) , 245\\xe2\\x80\\x93246 \\n Normalization \\n contractions , 174\\xe2\\x80\\x93176 \\n corpus, text documents , 177  \\n lemmatization , 176 \\n stopwords , 177 \\n symbols and characters , 176  \\n techniques , 174 \\n Noun phrase (NP) , 12 \\n Numeric types , 70\\xe2\\x80\\x9372 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  O \\n Object-oriented programming (OOP) , 51 \\n Okapi BM25 ranking , 292\\xe2\\x80\\x93296 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  P \\n Parts of speech (POS) tagging , 38, 132, \\n135, 137 \\n Pattern lexicon \\n description , 366 \\n mood and modality, sampled test \\nmovie reviews , 371\\xe2\\x80\\x93373 \\n mood and modality, text \\ndocuments , 366\\xe2\\x80\\x93367 \\n sentiment prediction \\nperformance , 373\\xe2\\x80\\x93374 \\n sentiment statistics , 368\\xe2\\x80\\x93371 \\n Phrases \\n adjective phrase (ADJP) , 13 \\n adverb phrase (ADVP) , 13 \\n annotated , 13 \\n categories , 12 \\n noun phrase (NP) , 12 \\n prepositional phrase (PP) , 13 \\n principle , 12 \\n verb phrase (VP) , 13 \\n Pip package management , 63 \\n PL . See  Propositional logic (PL) \\n\\xe2\\x96\\xa0 INDEX\\n382\\n Polarity analysis , 342 \\n Polysemous , 321 \\n Popular corpora \\n ANC , 40 \\n BNC , 40 \\n Brown Corpus , 39 \\n CHILDES , 39 \\n COCA , 40 \\n Collins Corpus , 39 \\n Google N-gram Corpus , 40 \\n KWIC , 39 \\n LOB Corpus , 39 \\n Penn Treebank , 39 \\n reuters , 40 \\n Web, chat, email, tweets , 40 \\n WordNet , 39 \\n Porter stemmer , 131 \\n POS taggers \\n bigram models , 141 \\n building , 138\\xe2\\x80\\x93140, 142 \\n classifi cation-based approach , 142 \\n ContextTagger class , 140 \\n input tokens , 141 \\n MaxentClassifi er , 142 \\n NaiveBayesClassifi er , 142 \\n nltk , 138 \\n pattern module , 138 \\n trigram models , 141 \\n Prepositional phrase (PP) , 13 \\n ProjectiveDependencyParser , 157 \\n Propositional logic (PL) , 336\\xe2\\x80\\x93337 \\n atomic units , 30 \\n complex units , 30 \\n connectors , 30 \\n constructive dilemma , 33 \\n declarative , 29 \\n Disjunctive Syllogism , 32 \\n Hypothetical Syllogism , 32 \\n Modus Ponens , 32 \\n Modus Tollens , 32 \\n operators with symbols and \\nprecedence , 30 \\n sentential logic/statement logic , 29 \\n truth values , 31 \\n PunktSentenceTokenizer , 111 \\n PyEnchant , 128 \\n Python \\n ABC language , 51 \\n advantages and benefi ts , 52 \\n built-in methods , 99 \\n classes , 91\\xe2\\x80\\x9393 \\n code , 51 \\n conditional code fl ow , 79 \\n database programming , 57 \\n data types , 69\\xe2\\x80\\x9370 \\n dictionaries , 75\\xe2\\x80\\x9376 \\n disadvantages , 58 \\n environment , 62\\xe2\\x80\\x9364 \\n formatting , 100\\xe2\\x80\\x93101 \\n hands-on approach , 51  \\n identity , 69 \\n implementations and \\nversions , 59\\xe2\\x80\\x9360 \\n lists , 73\\xe2\\x80\\x9374 \\n machine learning , 57 \\n manipulations and operations , 100  \\n numeric types , 70\\xe2\\x80\\x9372 \\n OS , 61 \\n principles , 52 \\n programming language , 51 \\n programming paradigms , 52 \\n Scientifi c computing , 57 \\n scripting , 56 \\n strings , 72\\xe2\\x80\\x9373 \\n structure , 66\\xe2\\x80\\x9368  \\n syntax , 66\\xe2\\x80\\x9368 \\n systems programming , 56 \\n text analytics , 57 \\n text data , 51 \\n type , 69 \\n value , 69 \\n versions , 59\\xe2\\x80\\x9360 \\n virtual environment , 64\\xe2\\x80\\x9366 \\n web development , 56 \\n Python 2.7.x , 58, 60 \\n Python 2.x , 60 \\n Python 3.0 , 60 \\n Python 3.x , 60, 94 \\n Python Package Index (PyPI) , 55 \\n Python Reserved Words , 67\\xe2\\x80\\x9368 \\n Python standard library (PSL) , 56 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  Q \\n Question Answering Systems (QAS) , 47 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  R \\n range() , 60 \\n Recursive functions , 85\\xe2\\x80\\x9386 \\n RegexpStemmer , 130 \\n RegexpTokenizer class , 112, 114 \\n  Regular expressions (Regexes) , 101\\xe2\\x80\\x93104   \\n  Repeating characters , 121\\xe2\\x80\\x93123   \\n\\xe2\\x96\\xa0 INDEX\\n383\\n Rich internet applications (RIA) , 56 \\n  Robust ecosystem , 53 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  S \\n SciPy libraries , 57 \\n Semantic analysis , 271 \\n FOL , 338\\xe2\\x80\\x93341 \\n frameworks , 336 \\n messages , 336 \\n named entity recognition , 332\\xe2\\x80\\x93335 \\n natural language , 320 \\n parts of speech (POS), chunking, and \\ngrammars , 320 \\n PL , 336\\xe2\\x80\\x93337 \\n WordNet (see  WordNet ) \\n word sense disambiguation , 330\\xe2\\x80\\x93331 \\n Sentence tokenization \\n delimiters , 108 \\n German text , 110 \\n Gutenberg corpus , 109 \\n nltk interfaces , 112 \\n nltk.sent_tokenize function , 109 \\n pre-trained German language , 111 \\n pre-trained tokenizer , 111 \\n PunktSentenceTokenizer class , 111 \\n snippet , 112 \\n text corpora , 110 \\n text samples , 109 \\n Sentiment analysis \\n description , 320, 342 \\n IMDb moviereviews (see  Internet \\nMovie Database (IMDb) movie \\nreviews ) \\n polarity analysis , 342 \\n techniques , 342 \\n textual data , 342 \\n SentiWordNet , 356\\xe2\\x80\\x93361 \\n Sets , 74\\xe2\\x80\\x9375 \\n Shallow parsing \\n chunking process , 147 \\n code snippet , 143\\xe2\\x80\\x93147 \\n conll2000 corpus , 152 \\n conlltags2tree() function , 152 \\n Evaluating Classifi cation \\nModels , 149 \\n expression-based patterns , 148 \\n generic functions , 144 \\n IOB format , 150 \\n noun phrases , 147 \\n parse() function , 151 \\n parser performance , 152 \\n POS tags , 143, 147, 151 \\n sentence tree , 143 \\n snippet , 144 \\n tagger_classes parameter , 151 \\n tokens/sequences , 148 \\n treebank corpus , 153 \\n treebank training data , 149  \\n visual representation , 146 \\n Singular Value Decomposition (SVD) \\n description , 221 \\n extraction-based techniques , 251 \\n low rank matrix approximation , 222  \\n LSA , 253 \\n LSI , 235, 238, 240 \\n NNMF , 245 \\n Slicing , 97\\xe2\\x80\\x9399 \\n SnowballStemmer , 130 \\n Special characters removal , 116\\xe2\\x80\\x93117 \\n Speech recognition system , 47 \\n Spelling correction \\n candidate words , 123 \\n case_of function , 127 \\n code , 124\\xe2\\x80\\x93125 \\n director of research , 123 \\n English language , 124 \\n preceding function , 127 \\n replacements , 126 \\n vocabulary dictionary , 128 \\n StemmerI interface , 129 \\n Stemming \\n affi  xes , 128 \\n code , 129\\xe2\\x80\\x93130 \\n infl ections , 129 \\n snippet , 130 \\n Snowball Project , 130 \\n user-defi ned rules , 130 \\n Stopwords , 120 \\n Strings indexing syntax , 72\\xe2\\x80\\x9373, 98  \\n literals , 94\\xe2\\x80\\x9396 \\n operations and methods , 96 \\n Supervised machine learning technique \\n confusion matrix , 352 \\n normalization and \\nfeature-extraction , 349 \\n performance metrics , 352 \\n positive and negative emotions , 351  \\n predictions , 349\\xe2\\x80\\x93351 \\n support vector machine (SVM) , 349 \\n test dataset reviews , 351\\xe2\\x80\\x93352 \\n text classifi cation , 348 \\n Support vector machines (SVM) , 197\\xe2\\x80\\x93199, \\n209\\xe2\\x80\\x93211 \\n\\xe2\\x96\\xa0 INDEX\\n384\\n SVD . See  Singular Value Decomposition \\n(SVD) \\n SVM . See  Support vector machines (SVM) \\n Synonyms , 324\\xe2\\x80\\x93325 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  T \\n Term frequency-inverse document \\nfrequency (TF-IDF) model \\n Bag of Words feature vectors , 182 \\n CORPUS , 183\\xe2\\x80\\x93184 \\n CountVectorizer , 186 \\n defi nition , 181\\xe2\\x80\\x93182 \\n diagonal matrix , 185 \\n Euclidean norm , 182 \\n mathematical equations , 183 \\n matrix multiplication , 185 \\n tfi df feature vectors , 183 \\n tfi df weights , 185 \\n Tfi dfTransformer class , 183 \\n Tfi dfTransformer , 185\\xe2\\x80\\x93186 \\n Tfi dfVectorizer , 186 \\n Text analytics , 49\\xe2\\x80\\x9350, 104\\xe2\\x80\\x93105 \\n textblob , 105 \\n Text classifi cation \\n applications and uses , 214 \\n automated (see  Automated text \\nclassifi cation ) \\n blueprint , 172\\xe2\\x80\\x93174 \\n conceptual representation , 169 \\n defi nition , 168 \\n documents , 167 \\n feature-extraction (see  Feature-\\nextraction techniques ) \\n inherent properties , 167\\xe2\\x80\\x93168 \\n learning , 167 \\n machine learning (ML) , 167 \\n normalization , 174\\xe2\\x80\\x93177 \\n prediction performance, metrics \\n accuracy , 202 \\n confusion matrix , 201\\xe2\\x80\\x93202 \\n emails, spam and ham , 200\\xe2\\x80\\x93201 \\n F1 score , 204 \\n precision , 203 \\n recall , 203 \\n products , 169 \\n types , 169 \\n Text corpora/text corpus \\n access \\n Brown Corpus , 41, 43 \\n NLTK , 40\\xe2\\x80\\x9341 \\n Reuters Corpus , 43\\xe2\\x80\\x9344 \\n WordNet , 44, 46 \\n annotation and utilities , 38 \\n collection of texts/data , 37 \\n monolingual , 37 \\n multilingual , 37 \\n origins , 38  \\n popular , 39\\xe2\\x80\\x9340 \\n Text normalization , 115, 220, 223\\xe2\\x80\\x93224 \\n Text pre-processing techniques , 107 \\n  TextRank , 256\\xe2\\x80\\x93258, 260\\xe2\\x80\\x93261   \\n Text semantics , 319 \\n Text similarity \\n Bag of Characters vectorization , 272  \\n character vectorization , 272 \\n cosine distance and similarity , \\n283\\xe2\\x80\\x93285  \\n description , 265 \\n distance metrics , 273 \\n Euclidean distance , 277\\xe2\\x80\\x93278 \\n feature-extraction , 267, 270 \\n Hamming distance , 274\\xe2\\x80\\x93275 \\n information retrieval (IR) , 266  \\n Levenshtein edit \\ndistance , 278\\xe2\\x80\\x93283 \\n Manhattan distance , 275\\xe2\\x80\\x93277 \\n normalization , 268\\xe2\\x80\\x93269 \\n similarity measures , 267 \\n terms and computing , 272\\xe2\\x80\\x93273  \\n text data , 265 \\n unsupervised machine learning \\nalgorithms , 268 \\n vector representations , 274 \\n Text summarization \\n description , 217\\xe2\\x80\\x93218 \\n documents , 220 \\n feature extraction , 221, 224\\xe2\\x80\\x93225 \\n feature matrix , 221 \\n information extraction \\n automated \\ndocumentsummarization \\n(see  Automated document \\nsummarization ) \\n information overload , 218 \\n Internet , 218 \\n Keyphraseextraction (see \\n Keyphrase extraction ) \\n production of books , 218 \\n techniques , 219 \\n topicmodeling (see  Topic \\nmodeling ) \\n\\xe2\\x96\\xa0 INDEX\\n385\\n information overload , 218 \\n normalization , 220, 223\\xe2\\x80\\x93224 \\n social media , 217 \\n SVD , 221\\xe2\\x80\\x93223 \\n Text syntax \\n Graphviz , 134 \\n installation , 133\\xe2\\x80\\x93134 \\n libraries , 133\\xe2\\x80\\x93134 \\n machine learning concepts , 134 \\n nltk , 133 \\n processing and normalization , 132 \\n TF-IDF weighted averaged word vectors , \\n190\\xe2\\x80\\x93193 \\n tokenize_text function , 120 \\n Tokenizing text , 116 \\n Topic modeling \\n defi nition , 219 \\n frameworks and algorithms , 234 \\n gensim and scikit-learn , 234 \\n LDA (see  Latent Dirichlet\\xc2\\xa0allocation \\n(LDA) ) \\n LSI (see  Latent semantic indexing \\n(LSI) ) \\n NNMF , 245\\xe2\\x80\\x93246 \\n product reviews , 246\\xe2\\x80\\x93250 \\n treebank corpus , 147 \\n treebank data , 148 \\n  TreebankWordTokenizer , 113   \\n  TrigramTagger , 140   \\n Tuples , 76\\xe2\\x80\\x9377 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  U \\n Unicode characters , 95 \\n UnigramTagger , 140 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  V \\n VADERlexicon . See  Valence Aware \\nDictionary and sEntiment \\nReasoner (VADER) lexicon \\n Valence Aware Dictionary and sEntiment \\nReasoner (VADER) lexicon , \\n361\\xe2\\x80\\x93366 \\n Vector Space Model , 178 \\n Verb phrase (VP) , 13 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  W, X, Y \\n Ward\\xe2\\x80\\x99s agglomerative hierarchical \\nclustering \\n cosine similarity , 315 \\n defi ntion , 313 \\n dendrogram , 314 \\n distance metric , 314 \\n IMDb movie data , 316\\xe2\\x80\\x93317 \\n linkage criterion , 314, 315 \\n Ward\\xe2\\x80\\x99s minimum variance \\nmethod , 315 \\n Weighted tag\\xe2\\x80\\x93based phrase \\nextraction , 230\\xe2\\x80\\x93233 \\n WordNet \\n defi nition , 321 \\n entailments , 323 \\n holonyms and meronyms , 327\\xe2\\x80\\x93328  \\n homonyms and homographs , 324  \\n hyponyms and hypernyms , 325\\xe2\\x80\\x93327 \\n lexical semantic relations , 323  \\n semantic relationships and \\nsimilarity , 328\\xe2\\x80\\x93330 \\n synonyms and antonyms , 324\\xe2\\x80\\x93325 \\n synsets , 321, 323 \\n web application interface , 321 \\n WordNetLemmatizer class , 132 \\n Word order typology , 23\\xe2\\x80\\x9324 \\n Words \\n Adverbs , 11 \\n annotated, POS tags , 12  \\n correction , 121 \\n meaning , 11 \\n morphemes , 11 \\n N(oun) , 11 \\n parts of speech , 11 \\n plural nouns , 11 \\n pronouns , 12 \\n PRON tag , 12 \\n sense disambiguation , 330\\xe2\\x80\\x93331 \\n singular nouns , 11 \\n singular proper nouns , 11 \\n verbs , 11 \\n Word tokenization \\n interfaces , 112\\xe2\\x80\\x93113 \\n lemmatization , 112 \\n nltk.word_tokenize function , 113 \\n patterns , 113 \\n regular expressions , 114  \\n snippet , 113 \\n stemming , 112 \\n \\n \\n \\n \\n \\n \\n \\n \\n\\xc2\\x84  Z \\n Zen of Python , 54 \\n\\x0c'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_name = '/home/ngoni97/Documents/Python Programming/Natural Language Processing/Text_Analytics_with_Python.pdf'\n",
    "get_document(path_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a5fe48a-a328-459c-9bec-a1e6ba4c1e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(filename):\n",
    "    import os, pathlib, pymupdf\n",
    "    Text = \"\"\n",
    "    parent_directory = '/home/ngoni97/Documents/Documents/File Manager with ML/train_test_text_files'\n",
    "    path = os.path.join(parent_directory, os.path.basename(filename))\n",
    "    text_file_path = pathlib.Path(path.strip('.pdf') + '.txt')\n",
    "    with pymupdf.open(filename) as file: #open a document\n",
    "        for page in file: # iterate the document pages\n",
    "            text = page.get_text()\n",
    "            Text += text\n",
    "    with open(text_file_path, 'wb') as text_file: # create a text output\n",
    "        text_file.write(Text.encode('utf8')) # get plain text (is in UTF-8), write text of page\n",
    "        text_file.write(bytes((12,))) # write page delimiter (from feed 0x0C)\n",
    "    with open(text_file_path, 'rb') as file:\n",
    "        File = file.read()\n",
    "        return File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f48c46da-5b24-4f2b-8c12-06afa8a93f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Representation \\nLearning for \\nNatural Language \\nProcessing \\nZhiyuan Liu \\xc2\\xb7 Yankai Lin \\xc2\\xb7 Maosong Sun\\nRepresentation Learning for Natural Language\\nProcessing\\nZhiyuan Liu\\n\\xe2\\x80\\xa2 Yankai Lin\\n\\xe2\\x80\\xa2 Maosong Sun\\nRepresentation Learning\\nfor Natural Language\\nProcessing\\n123\\nZhiyuan Liu\\nTsinghua University\\nBeijing, China\\nMaosong Sun\\nDepartment of Computer\\nScience and Technology\\nTsinghua University\\nBeijing, China\\nYankai Lin\\nPattern Recognition Center\\nTencent Wechat\\nBeijing, China\\nISBN 978-981-15-5572-5\\nISBN 978-981-15-5573-2\\n(eBook)\\nhttps://doi.org/10.1007/978-981-15-5573-2\\n\\xc2\\xa9 The Editor(s) (if applicable) and The Author(s) 2020, corrected publication 2023. This book is an open\\naccess publication.\\nOpen Access This book is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing, adap-\\ntation, distribution and reproduction in any medium or format, as long as you give appropriate credit to\\nthe original author(s) and the source, provide a link to the Creative Commons license and indicate if\\nchanges were made.\\nThe images or other third party material in this book are included in the book\\xe2\\x80\\x99s Creative Commons\\nlicense, unless indicated otherwise in a credit line to the material. If material is not included in the book\\xe2\\x80\\x99s\\nCreative Commons license and your intended use is not permitted by statutory regulation or exceeds the\\npermitted use, you will need to obtain permission directly from the copyright holder.\\nThe use of general descriptive names, registered names, trademarks, service marks, etc. in this publi-\\ncation does not imply, even in the absence of a speci\\xef\\xac\\x81c statement, that such names are exempt from the\\nrelevant protective laws and regulations and therefore free for general use.\\nThe publisher, the authors and the editors are safe to assume that the advice and information in this\\nbook are believed to be true and accurate at the date of publication. Neither the publisher nor the\\nauthors or the editors give a warranty, express or implied, with respect to the material contained herein or\\nfor any errors or omissions that may have been made. The publisher remains neutral with regard to\\njurisdictional claims in published maps and institutional af\\xef\\xac\\x81liations.\\nThis Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd.\\nThe registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721,\\nSingapore\\nPreface\\nIn traditional Natural Language Processing (NLP) systems, language entries such as\\nwords and phrases are taken as distinct symbols. Various classic ideas and methods,\\nsuch as n-gram and bag-of-words models, were proposed and have been widely\\nused until now in many industrial applications. All these methods take words as the\\nminimum units for semantic representation, which are either used to further esti-\\nmate the conditional probabilities of next words given previous words (e.g., n-\\ngram) or used to represent semantic meanings of text (e.g., bag-of-words models).\\nEven when people \\xef\\xac\\x81nd it is necessary to model word meanings, they either man-\\nually build some linguistic knowledge bases such as WordNet or use context words\\nto represent the meaning of a target word (i.e., distributional representation). All\\nthese semantic representation methods are still based on symbols!\\nWith the development of NLP techniques for years, it is realized that many\\nissues in NLP are caused by the symbol-based semantic representation. First, the\\nsymbol-based representation always suffers from the data sparsity problem. Take\\nstatistical NLP methods such as n-gram with large-scale corpora, for example, due\\nto the intrinsic power-law distribution of words, the performance will decay dra-\\nmatically for those few-shot words, even many smoothing methods have been\\ndeveloped to calibrate the estimated probabilities about them. Moreover, there are\\nmultiple-grained entries in natural languages from words, phrases, sentences to\\ndocuments, it is dif\\xef\\xac\\x81cult to \\xef\\xac\\x81nd a uni\\xef\\xac\\x81ed symbol set to represent the semantic\\nmeanings for all of them simultaneously. Meanwhile, in many NLP tasks, it is\\nrequired to measure semantic relatedness between these language entries at different\\nlevels.\\nFor\\nexample,\\nwe\\nhave\\nto\\nmeasure\\nsemantic\\nrelatedness\\nbetween\\nwords/phrases and documents in Information Retrieval. Due to the absence of a\\nuni\\xef\\xac\\x81ed scheme for semantic representation, there used to be distinct approaches\\nproposed and explored for different tasks in NLP, and it sometimes makes NLP\\ndoes not look like a compatible community.\\nAs an alternative approach to symbol-based representation, distributed repre-\\nsentation was originally proposed by Geoffrey E. Hinton in a technique report in\\n1984. The report was then included in the well-known two-volume book Parallel\\nDistributed Processing (PDP) that introduced neural networks to model human\\nv\\ncognition and intelligence. According to this report, distributed representation is\\ninspired by the neural computation scheme of humans and other animals, and the\\nessential idea is as follows:\\nEach entity is represented by a pattern of activity distributed over many computing ele-\\nments, and each computing element is involved in representing many different entities.\\nIt means that each entity is represented by multiple neurons, and each neuron\\ninvolves in the representation of many concepts. This also indicates the meaning of\\ndistributed in distributed representation. As opposed to distributed representation,\\npeople used to assume one neuron only represents a speci\\xef\\xac\\x81c concept or object, e.g.,\\nthere exists a single neuron that will only be activated when recognizing a person or\\nobject, such as his/her grandmother, well known as the grandmother-cell hypothesis\\nor local representation. We can see the straightforward connection between the\\ngrandmother-cell hypothesis and symbol-based representation.\\nIt was about 20 years after distributed representation was proposed, neural\\nprobabilistic language model was proposed to model natural languages by Yoshua\\nBengio in 2003, in which words are represented as low-dimensional and real-valued\\nvectors based on the idea of distributed representation. However, it was until 2013\\nthat a simpler and more ef\\xef\\xac\\x81cient framework word2vec was proposed to learn word\\ndistributed representations from large-scale corpora, we come to the popularity of\\ndistributed representation and neural network techniques in NLP. The performance\\nof almost all NLP tasks has been signi\\xef\\xac\\x81cantly improved with the support of the\\ndistributed representation scheme and the deep learning methods.\\nThis book aims to review and present the recent advances of distributed repre-\\nsentation learning for NLP, including why representation learning can improve\\nNLP, how representation learning takes part in various important topics of NLP,\\nand what challenges are still not well addressed by distributed representation.\\nBook Organization\\nThis book is organized into 11 chapters with 3 parts. The \\xef\\xac\\x81rst part of the book depicts\\nkey components in NLP and how representation learning works for them. In this\\npart, Chap. 1 \\xef\\xac\\x81rst introduces the basics of representation learning and why it is\\nimportant for NLP. Then we give a comprehensive review of representation learning\\ntechniques on multiple-grained entries in NLP, including word representation\\n(Chap. 2), phrase representation as known as compositional semantics (Chap. 3),\\nsentence representation (Chap. 4), and document representation (Chap. 5).\\nThe second part presents representation learning for those components closely\\nrelated to NLP. These components include sememe knowledge that describes the\\ncommonsense knowledge of words as human concepts, world knowledge (also\\nknown as knowledge graphs) that organizes relational facts between entities in the\\nreal world, various network data such as social networks, document networks, and\\nvi\\nPreface\\ncross-modal data that connects natural languages to other modalities such as visual\\ndata. A deep understanding of natural languages requires these complex compo-\\nnents as a rich context. Therefore, we provide an extensive introduction to these\\ncomponents, i.e., sememe knowledge representation (Chap. 6), world knowledge\\nrepresentation (Chap. 7), network representation (Chap. 8), and cross-modal rep-\\nresentation (Chap. 9).\\nIn the third part, we will further provide some widely used open resource\\ntools on representation learning techniques (Chap. 10) and \\xef\\xac\\x81nally outlook the\\nremaining challenges and future research directions of representation learning for\\nNLP (Chap. 11).\\nAlthough the book is about representation learning for NLP, those theories and\\nalgorithms can be also applied in other related domains, such as machine learning,\\nsocial network analysis, semantic web, information retrieval, data mining, and\\ncomputational biology.\\nNote that, some parts of this book are based on our previous published or\\npre-printed papers, including [1, 11] in Chap. 2, [32] in Chap. 3, [10, 5, 29] in\\nChap. 4, [12, 7] in Chap. 5, [17, 14, 24, 30, 6, 16, 2, 15] in Chap. 6, [9, 8, 13, 21,\\n22, 23, 3, 4, 31] in Chap. 7, and [25, 19, 18, 20, 26, 27, 33, 28, 34] in Chap. 8.\\nBook Cover\\nThe book cover shows an oracle bone divided into three parts, corresponding to\\nthree revolutionized stages of cognition and representation in human history.\\nThe left part shows oracle scripts, the earliest known form of Chinese writing\\ncharacters used on oracle bones in the late 1200 BC. It is used to represent the\\nemergence of human languages, especially writing systems. We consider this as the\\n\\xef\\xac\\x81rst representation revolution for human beings about the world.\\nThe upper right part shows the digitalized representation of information and\\nsignals. After the invention of electronic computers in the 1940s, big data can be\\nef\\xef\\xac\\x81ciently represented and processed in computer programs. This can be regarded\\nas the second representation revolution for human beings about the world.\\nThe bottom right part shows the distributed representation in arti\\xef\\xac\\x81cial neural\\nnetworks originally proposed in the 1980s. As the representation basis of deep\\nlearning, it has extensively revolutionized many \\xef\\xac\\x81elds in arti\\xef\\xac\\x81cial intelligence,\\nincluding natural language processing, computer vision, and speech recognition\\never since the 2010s. We consider this as the third representation revolution about\\nthe world. This book focuses on the theory, methods, and applications of distributed\\nrepresentation learning in natural language processing.\\nPreface\\nvii\\nPrerequisites\\nThis book is designed for advanced undergraduate and graduate students, post-\\ndoctoral fellows, researchers, lecturers, and industrial engineers, as well as anyone\\ninterested in representation learning and NLP. We expect the readers to have some\\nprior knowledge in Probability, Linear Algebra, and Machine Learning. We rec-\\nommend the readers who are speci\\xef\\xac\\x81cally interested in NLP to read the \\xef\\xac\\x81rst part\\n(Chaps. 1\\xe2\\x80\\x935) which should be read sequentially. The second and third parts can be\\nread in selected order according to readers\\xe2\\x80\\x99 interests.\\nContact Information\\nWe welcome any feedback, corrections, and suggestions on the book, which may\\nbe sent to liuzy@tsinghua.edu.cn. The readers can also \\xef\\xac\\x81nd updates about the book\\nfrom the personal homepage http://nlp.csai.tsinghua.edu.cn/*lzy/.\\nBeijing, China\\nZhiyuan Liu\\nYankai Lin\\nMarch 2020\\nMaosong Sun\\nReferences\\n1. Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huanbo Luan. Joint learning of\\ncharacter and word embeddings. In Proceedings of IJCAI, 2015.\\n2. Yihong Gu, Jun Yan, Hao Zhu, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Fen Lin, and Leyu\\nLin. Language modeling with sparse product of sememe experts. In Proceedings of EMNLP,\\npages 4642\\xe2\\x80\\x934651, 2018.\\n3. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge\\nfor knowledge graph completion. arXiv preprint arXiv:1611.04125, 2016.\\n4. Xu Han, Zhiyuan Liu, and Maosong Sun. Neural knowledge acquisition via mutual attention\\nbetween knowledge graph and text. In Proceedings of AAAI, pages 4832\\xe2\\x80\\x934839, 2018.\\n5. Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun.\\nFewRel: A large-scale supervised few-shot relation classi\\xef\\xac\\x81cation dataset with state-of-the-art\\nevaluation. In Proceedings of EMNLP, 2018.\\n6. Huiming Jin, Hao Zhu, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Fen Lin, and Leyu Lin.\\nIncorporating chinese characters of words for lexical sememe prediction. In Proceedings of\\nACL, 2018.\\n7. Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. Denoising distantly supervised\\nopen-domain question answering. In Proceedings of ACL, 2018.\\n8. Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, and Song Liu. Modeling\\nrelation paths for representation learning of knowledge bases. In Proceedings of EMNLP,\\n2015.\\nviii\\nPreface\\n9. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and\\nrelation embeddings for knowledge graph completion. In Proceedings of AAAI, 2015.\\n10. Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Neural relation\\nextraction with selective attention over instances. In Proceedings of ACL, 2016.\\n11. Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Topical word embeddings. In\\nProceedings of AAAI, 2015.\\n12. Zhenghao Liu, Chenyan Xiong, Maosong Sun, and Zhiyuan Liu. Entity-duet neural ranking:\\nUnderstanding the role of knowledge graph semantics in neural information retrieval. In\\nProceedings of ACL, 2018.\\n13. Zhiyuan Liu, Maosong Sun, Yankai Lin, and Ruobing Xie. Knowledge representation\\nlearning: a review. JCRD, 53(2):247\\xe2\\x80\\x93261, 2016.\\n14. Yilin Niu, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Improved word representation\\nlearning with sememes. In Proceedings of ACL, 2017.\\n15. Fanchao Qi, Junjie Huang, Chenghao Yang, Zhiyuan Liu, Xiao Chen, Qun Liu, and Maosong\\nSun. Modeling semantic compositionality with sememe knowledge. In Proceedings of ACL,\\n2019.\\n16. Fanchao Qi, Yankai Lin, Maosong Sun, Hao Zhu, Ruobing Xie, and Zhiyuan Liu.\\nCross-lingual lexical sememe prediction. In Proceedings of EMNLP, 2018.\\n17. Maosong Sun and Xinxiong Chen. Embedding for words and word senses based on human\\nannotated knowledge base: A case study on hownet. Journal of Chinese Information\\nProcessing, 30:1\\xe2\\x80\\x936, 2016.\\n18. Cunchao Tu, Hao Wang, Xiangkai Zeng, Zhiyuan Liu, and Maosong Sun. Community-\\nenhanced\\nnetwork\\nrepresentation\\nlearning\\nfor\\nnetwork\\nanalysis.\\narXiv\\npreprint\\narXiv:1611.06645, 2016.\\n19. Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, and Maosong Sun. Max-margin deepwalk:\\nDiscriminative learning of network representation. In Proceedings of IJCAI, 2016.\\n20. Cunchao Tu, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Transnet: translation-based\\nnetwork representation learning for social relation extraction. In Proceedings of IJCAI, 2017.\\n21. Ruobing\\nXie,\\nZhiyuan\\nLiu,\\nTat-seng\\nChua,\\nHuanbo\\nLuan,\\nand\\nMaosong\\nSun.\\nImage-embodied knowledge representation learning. In Proceedings of IJCAI, 2016.\\n22. Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. Representation learning\\nof knowledge graphs with entity descriptions. In Proceedings of AAAI, 2016.\\n23. Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Representation learning of knowledge graphs\\nwith hierarchical types. In Proceedings of IJCAI, 2016.\\n24. Ruobing Xie, Xingchi Yuan, Zhiyuan Liu, and Maosong Sun. Lexical sememe prediction via\\nword embeddings and matrix factorization. In Proceedings of IJCAI, 2017.\\n25. Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network rep-\\nresentation learning with rich text information. In Proceedings of IJCAI, 2015.\\n26. Cheng Yang, Maosong Sun, Zhiyuan Liu, and Cunchao Tu. Fast network embedding\\nenhancement via high order proximity approximation. In Proceedings of IJCAI, 2017.\\n27. Cheng Yang, Maosong Sun, Wayne Xin Zhao, Zhiyuan Liu, and Edward Y Chang. A neural\\nnetwork approach to jointly modeling social networks and mobile trajectories. ACM\\nTransactions on Information Systems (TOIS), 35(4):36, 2017.\\n28. Cheng Yang, Jian Tang, Maosong Sun, Ganqu Cui, and Liu Zhiyuan. Multi-scale information\\ndiffusion prediction with reinforced recurrent networks. In Proceedings of IJCAI, 2019.\\n29. Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin\\nHuang, Jie Zhou, and Maosong Sun. DocRED: A large-scale document-level relation\\nextraction dataset. In Proceedings of ACL, 2019.\\n30. Xiangkai Zeng, Cheng Yang, Cunchao Tu, Zhiyuan Liu, and Maosong Sun. Chinese liwc\\nlexicon expansion via hierarchical classi\\xef\\xac\\x81cation of word embeddings with sememe attention.\\nIn Proceedings of AAAI, 2018.\\n31. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie:\\nEnhanced language representation with informative entities. In Proceedings of ACL, 2019.\\nPreface\\nix\\n32. Yu Zhao, Zhiyuan Liu, and Maosong Sun. Phrase type sensitive tensor indexing model for\\nsemantic composition. In Proceedings of AAAI, 2015.\\n33. Jie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong\\nSun. GEAR: Graph-based evidence aggregating and reasoning for fact veri\\xef\\xac\\x81cation. In\\nProceedings of ACL 2019, 2019.\\nThe original version of the Book Frontmatter has been revised with the addition of\\nreference citations in the Preface. The corrected version of Frontmatter can be found\\nat https://doi.org/10.1007/978-981-15-5573-2_12.\\nx\\nPreface\\nAcknowledgements\\nThe authors are very grateful to the contributions of our students and research\\ncollaborators, who have prepared initial drafts of some chapters or have given us\\ncomments, suggestions, and corrections. We list main contributors for preparing\\ninitial drafts of each chapter as follows,\\n\\xe2\\x80\\xa2 Chapter 1: Tianyu Gao, Zhiyuan Liu.\\n\\xe2\\x80\\xa2 Chapter 2: Lei Xu, Yankai Lin.\\n\\xe2\\x80\\xa2 Chapter 3: Yankai Lin, Yang Liu.\\n\\xe2\\x80\\xa2 Chapter 4: Yankai Lin, Zhengyan Zhang, Cunchao Tu, Hongyin Luo.\\n\\xe2\\x80\\xa2 Chapter 5: Jiawei Wu, Yankai Lin, Zhenghao Liu, Haozhe Ji.\\n\\xe2\\x80\\xa2 Chapter 6: Fanchao Qi, Chenghao Yang.\\n\\xe2\\x80\\xa2 Chapter 7: Ruobing Xie, Xu Han.\\n\\xe2\\x80\\xa2 Chapter 8: Cheng Yang, Jie Zhou, Zhengyan Zhang.\\n\\xe2\\x80\\xa2 Chapter 9: Ji Xin, Yuan Yao, Deming Ye, Hao Zhu.\\n\\xe2\\x80\\xa2 Chapter 10: Xu Han, Zhengyan Zhang, Cheng Yang.\\n\\xe2\\x80\\xa2 Chapter 11: Cheng Yang, Zhiyuan Liu.\\nFor the whole book, we thank Chaojun Xiao and Zhengyan Zhang for drawing\\nmodel \\xef\\xac\\x81gures, thank Chaojun Xiao for unifying the styles of \\xef\\xac\\x81gures and tables in\\nthe book, thank Shengding Hu for making the notation table and unifying the\\nnotations across chapters, thank Jingcheng Yuzhi and Chaojun Xiao for organizing\\nthe format of reference, thank Jingcheng Yuzhi, Jiaju Du, Haozhe Ji, Sicong\\nOuyang, and Ayana for the \\xef\\xac\\x81rst-round proofreading, and thank Weize Chen, Ganqu\\nCui, Bowen Dong, Tianyu Gao, Xu Han, Zhenghao Liu, Fanchao Qi, Guangxuan\\nXiao, Cheng Yang, Yuan Yao, Shi Yu, Yuan Zang, Zhengyan Zhang, Haoxi Zhong\\nand Jie Zhou for the second-round proofreading. We also thank Cuncun Zhao for\\ndesigning the book cover.\\nIn this book, there is a speci\\xef\\xac\\x81c chapter talking about sememe knowledge rep-\\nresentation. Many works in this chapter are carried out by our research group. These\\nworks have received great encouragement from the inventor of HowNet,\\nMr. Zhendong Dong, who died at 82 on February 28, 2019. HowNet is the great\\nxi\\nlinguistic and commonsense knowledge base composed by Mr. Dong for about 30\\nyears. At the end of his life, he and his son Mr. Qiang Dong decided to collaborate\\nwith us and released the open-source version of HowNet, OpenHowNet. As a\\npioneer of machine translation in China, Mr. Zhendong Dong devoted his whole\\nlife to natural language processing. He will be missed by all of us forever.\\nWe thank our colleagues and friends, Yang Liu and Juanzi Li at Tsinghua\\nUniversity, and Peng Li at Tencent Wechat, who offered close and frequent dis-\\ncussions which substantially improved this book. We also want to express our\\nspecial thanks to Prof. Bo Zhang. His insights to deep learning and representation\\nlearning, and sincere encouragements to our research of representation learning on\\nNLP, have greatly stimulated us to move forward with more con\\xef\\xac\\x81dence and\\npassion.\\nWe proposed the plan of this book in 2015 after discussing it with the Springer\\nSenior Editor, Dr. Celine Lanlan Chang. As the \\xef\\xac\\x81rst of the time of preparing a\\ntechnical book, we were not expecting it took so long to \\xef\\xac\\x81nish this book. We thank\\nCeline for providing insightful comments and incredible patience to the preparation\\nof this book. We are also grateful to Springer\\xe2\\x80\\x99s Assistant Editor, Jane Li, for\\noffering invaluable help during manuscript preparation.\\nFinally, we give our appreciations to our organizations, Department of Computer\\nScience and Technology at Tsinghua University, Institute for Arti\\xef\\xac\\x81cial Intelligence\\nat Tsinghua University, Beijing Academy of Arti\\xef\\xac\\x81cial Intelligence (BAAI), Chinese\\nInformation Processing Society of China, and Tencent Wechat, who have provided\\noutstanding environment, supports, and facilities for preparing this book.\\nThis book is supported by the Natural Science Foundation of China (NSFC) and\\nthe German Research Foundation (DFG) in Project Crossmodal Learning, NSFC\\n61621136008/DFG TRR-169.\\nxii\\nAcknowledgements\\nContents\\n1\\nRepresentation Learning and NLP . . . . . . . . . . . . . . . . . . . . . . . . .\\n1\\n1.1\\nMotivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n1\\n1.2\\nWhy Representation Learning Is Important for NLP . . . . . . . . .\\n3\\n1.3\\nBasic Ideas of Representation Learning . . . . . . . . . . . . . . . . . .\\n3\\n1.4\\nDevelopment of Representation Learning for NLP . . . . . . . . . .\\n4\\n1.5\\nLearning Approaches to Representation Learning for NLP . . . .\\n7\\n1.6\\nApplications of Representation Learning for NLP . . . . . . . . . . .\\n8\\n1.7\\nThe Organization of This Book . . . . . . . . . . . . . . . . . . . . . . . .\\n9\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n10\\n2\\nWord Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n2.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n13\\n2.2\\nOne-Hot Word Representation . . . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n2.3\\nDistributed Word Representation . . . . . . . . . . . . . . . . . . . . . . .\\n14\\n2.3.1\\nBrown Cluster . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n15\\n2.3.2\\nLatent Semantic Analysis . . . . . . . . . . . . . . . . . . . . . .\\n17\\n2.3.3\\nWord2vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n18\\n2.3.4\\nGloVe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n21\\n2.4\\nContextualized Word Representation . . . . . . . . . . . . . . . . . . . .\\n22\\n2.5\\nExtensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n23\\n2.5.1\\nWord Representation Theories. . . . . . . . . . . . . . . . . . .\\n24\\n2.5.2\\nMulti-prototype Word Representation . . . . . . . . . . . . .\\n25\\n2.5.3\\nMultisource Word Representation . . . . . . . . . . . . . . . .\\n26\\n2.5.4\\nMultilingual Word Representation . . . . . . . . . . . . . . . .\\n30\\n2.5.5\\nTask-Speci\\xef\\xac\\x81c Word Representation . . . . . . . . . . . . . . .\\n32\\n2.5.6\\nTime-Speci\\xef\\xac\\x81c Word Representation . . . . . . . . . . . . . . .\\n33\\n2.6\\nEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n33\\n2.6.1\\nWord Similarity/Relatedness . . . . . . . . . . . . . . . . . . . .\\n34\\n2.6.2\\nWord Analogy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n36\\nxiii\\n2.7\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n36\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n38\\n3\\nCompositional Semantics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n3.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n43\\n3.2\\nSemantic Space. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n3.2.1\\nVector Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n3.2.2\\nMatrix-Vector Space . . . . . . . . . . . . . . . . . . . . . . . . . .\\n45\\n3.3\\nBinary Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n46\\n3.3.1\\nAdditive Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n47\\n3.3.2\\nMultiplicative Model . . . . . . . . . . . . . . . . . . . . . . . . .\\n49\\n3.4\\nN-Ary Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n51\\n3.4.1\\nRecurrent Neural Network . . . . . . . . . . . . . . . . . . . . .\\n52\\n3.4.2\\nRecursive Neural Network . . . . . . . . . . . . . . . . . . . . .\\n53\\n3.4.3\\nConvolutional Neural Network . . . . . . . . . . . . . . . . . .\\n55\\n3.5\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n55\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n56\\n4\\nSentence Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n59\\n4.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n59\\n4.2\\nOne-Hot Sentence Representation . . . . . . . . . . . . . . . . . . . . . .\\n60\\n4.3\\nProbabilistic Language Model . . . . . . . . . . . . . . . . . . . . . . . . .\\n61\\n4.4\\nNeural Language Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n62\\n4.4.1\\nFeedforward Neural Network Language Model . . . . . .\\n62\\n4.4.2\\nConvolutional Neural Network Language Model . . . . .\\n63\\n4.4.3\\nRecurrent Neural Network Language Model . . . . . . . .\\n63\\n4.4.4\\nTransformer Language Model . . . . . . . . . . . . . . . . . . .\\n64\\n4.4.5\\nExtensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n69\\n4.5\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n72\\n4.5.1\\nText Classi\\xef\\xac\\x81cation . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n73\\n4.5.2\\nRelation Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n74\\n4.6\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n84\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n85\\n5\\nRETRACTED CHAPTER: Document Representation . . . . . . . . . .\\n91\\n5.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n91\\n5.2\\nOne-Hot Document Representation . . . . . . . . . . . . . . . . . . . . .\\n92\\n5.3\\nTopic Model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n93\\n5.3.1\\nLatent Dirichlet Allocation . . . . . . . . . . . . . . . . . . . . .\\n94\\n5.3.2\\nExtensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n97\\n5.4\\nDistributed Document Representation . . . . . . . . . . . . . . . . . . . .\\n100\\n5.4.1\\nParagraph Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n100\\n5.4.2\\nNeural Document Representation. . . . . . . . . . . . . . . . .\\n102\\nxiv\\nContents\\n5.5\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n107\\n5.5.1\\nNeural Information Retrieval . . . . . . . . . . . . . . . . . . . .\\n107\\n5.5.2\\nQuestion Answering . . . . . . . . . . . . . . . . . . . . . . . . . .\\n110\\n5.6\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n119\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n120\\n6\\nSememe Knowledge Representation . . . . . . . . . . . . . . . . . . . . . . . .\\n125\\n6.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n125\\n6.1.1\\nLinguistic Knowledge Graphs . . . . . . . . . . . . . . . . . . .\\n126\\n6.2\\nSememe Knowledge Representation . . . . . . . . . . . . . . . . . . . . .\\n128\\n6.2.1\\nSimple Sememe Aggregation Model . . . . . . . . . . . . . .\\n129\\n6.2.2\\nSememe Attention over Context Model . . . . . . . . . . . .\\n129\\n6.2.3\\nSememe Attention over Target Model . . . . . . . . . . . . .\\n131\\n6.3\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n132\\n6.3.1\\nSememe-Guided Word Representation . . . . . . . . . . . . .\\n132\\n6.3.2\\nSememe-Guided Semantic Compositionality\\nModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n134\\n6.3.3\\nSememe-Guided Language Modeling . . . . . . . . . . . . .\\n139\\n6.3.4\\nSememe Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n142\\n6.3.5\\nOther Sememe-Guided Applications . . . . . . . . . . . . . .\\n153\\n6.4\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n157\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n158\\n7\\nWorld Knowledge Representation . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163\\n7.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n163\\n7.1.1\\nWorld Knowledge Graphs. . . . . . . . . . . . . . . . . . . . . .\\n164\\n7.2\\nKnowledge Graph Representation . . . . . . . . . . . . . . . . . . . . . .\\n166\\n7.2.1\\nNotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n7.2.2\\nTransE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n168\\n7.2.3\\nExtensions of TransE . . . . . . . . . . . . . . . . . . . . . . . . .\\n172\\n7.2.4\\nOther Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n182\\n7.3\\nMultisource Knowledge Graph Representation . . . . . . . . . . . . .\\n189\\n7.3.1\\nKnowledge Graph Representation with Texts . . . . . . . .\\n190\\n7.3.2\\nKnowledge Graph Representation with Types . . . . . . .\\n192\\n7.3.3\\nKnowledge Graph Representation with Images. . . . . . .\\n194\\n7.3.4\\nKnowledge Graph Representation with Logic Rules . . .\\n195\\n7.4\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n196\\n7.4.1\\nKnowledge Graph Completion . . . . . . . . . . . . . . . . . .\\n197\\n7.4.2\\nKnowledge-Guided Entity Typing . . . . . . . . . . . . . . . .\\n199\\n7.4.3\\nKnowledge-Guided Information Retrieval . . . . . . . . . .\\n201\\n7.4.4\\nKnowledge-Guided Language Models . . . . . . . . . . . . .\\n205\\n7.4.5\\nOther Knowledge-Guided Applications . . . . . . . . . . . .\\n208\\n7.5\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n210\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n211\\nContents\\nxv\\n8\\nNetwork Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n217\\n8.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n217\\n8.2\\nNetwork Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n219\\n8.2.1\\nSpectral Clustering Based Methods . . . . . . . . . . . . . . .\\n219\\n8.2.2\\nDeepWalk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n223\\n8.2.3\\nMatrix Factorization Based Methods . . . . . . . . . . . . . .\\n230\\n8.2.4\\nStructural Deep Network Methods . . . . . . . . . . . . . . . .\\n232\\n8.2.5\\nExtensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n234\\n8.2.6\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n247\\n8.3\\nGraph Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n252\\n8.3.1\\nMotivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n253\\n8.3.2\\nGraph Convolutional Networks . . . . . . . . . . . . . . . . . .\\n254\\n8.3.3\\nGraph Attention Networks . . . . . . . . . . . . . . . . . . . . .\\n259\\n8.3.4\\nGraph Recurrent Networks . . . . . . . . . . . . . . . . . . . . .\\n260\\n8.3.5\\nExtensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n262\\n8.3.6\\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n266\\n8.4\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n275\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n277\\n9\\nCross-Modal Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n285\\n9.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n285\\n9.2\\nCross-Modal Representation . . . . . . . . . . . . . . . . . . . . . . . . . .\\n286\\n9.2.1\\nVisual Word2vec . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n286\\n9.2.2\\nCross-Modal Representation for Zero-Shot\\nRecognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n288\\n9.2.3\\nCross-Modal Representation for Cross-Media\\nRetrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n292\\n9.3\\nImage Captioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n294\\n9.3.1\\nRetrieval Models for Image Captioning . . . . . . . . . . . .\\n294\\n9.3.2\\nGeneration Models for Image Captioning. . . . . . . . . . .\\n295\\n9.3.3\\nNeural Models for Image Captioning . . . . . . . . . . . . . .\\n296\\n9.4\\nVisual Relationship Detection . . . . . . . . . . . . . . . . . . . . . . . . .\\n301\\n9.4.1\\nVisual Relationship Detection with Language Priors . . .\\n301\\n9.4.2\\nVisual Translation Embedding Network . . . . . . . . . . . .\\n303\\n9.4.3\\nScene Graph Generation . . . . . . . . . . . . . . . . . . . . . . .\\n303\\n9.5\\nVisual Question Answering . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n307\\n9.5.1\\nVQA and VQA Datasets. . . . . . . . . . . . . . . . . . . . . . .\\n307\\n9.5.2\\nVQA Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n308\\n9.6\\nSummary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n311\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n314\\nxvi\\nContents\\n10\\nResources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n319\\n10.1\\nOpen-Source Frameworks for Deep Learning . . . . . . . . . . . . . .\\n319\\n10.1.1\\nCaffe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n319\\n10.1.2\\nTheano . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n320\\n10.1.3\\nTensorFlow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n321\\n10.1.4\\nTorch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n321\\n10.1.5\\nPyTorch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n322\\n10.1.6\\nKeras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n323\\n10.1.7\\nMXNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n323\\n10.2\\nOpen Resources for Word Representation . . . . . . . . . . . . . . . .\\n324\\n10.2.1\\nWord2Vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n324\\n10.2.2\\nGloVe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n324\\n10.3\\nOpen Resources for Knowledge Graph Representation . . . . . . .\\n325\\n10.3.1\\nOpenKE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n325\\n10.3.2\\nScikit-Kge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n326\\n10.4\\nOpen Resources for Network Representation . . . . . . . . . . . . . .\\n326\\n10.4.1\\nOpenNE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n326\\n10.4.2\\nGEM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n326\\n10.4.3\\nGraphVite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n327\\n10.4.4\\nCogDL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n327\\n10.5\\nOpen Resources for Relation Extraction . . . . . . . . . . . . . . . . . .\\n327\\n10.5.1\\nOpenNRE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n327\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n328\\n11\\nOutlook . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n329\\n11.1\\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n329\\n11.2\\nUsing More Unsupervised Data . . . . . . . . . . . . . . . . . . . . . . . .\\n330\\n11.3\\nUtilizing Fewer Labeled Data . . . . . . . . . . . . . . . . . . . . . . . . .\\n330\\n11.4\\nEmploying Deeper Neural Architectures . . . . . . . . . . . . . . . . . .\\n331\\n11.5\\nImproving Model Interpretability . . . . . . . . . . . . . . . . . . . . . . .\\n332\\n11.6\\nFusing the Advances from Other Areas . . . . . . . . . . . . . . . . . .\\n333\\nReferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\n333\\nCorrection to: Representation Learning for Natural Language\\nProcessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\\nC1\\nContents\\nxvii\\nAcronyms\\nACNN\\nAnisotropic Convolutional Neural Network\\nAI\\nArti\\xef\\xac\\x81cial Intelligence\\nAUC\\nArea Under the Receiver Operating Characteristic Curve\\nBERT\\nBidirectional Encoder Representations from Transformers\\nBFS\\nBreadth-First Search\\nBiDAF\\nBi-Directional Attention Flow\\nBRNN\\nBidirectional Recurrent Neural Network\\nCBOW\\nContinuous Bag-of-Words\\nccDCLM\\nContext-to-Context Document-Context Language Model\\nCIDEr\\nConsensus-based Image Description Evaluation\\nCLN\\nColumn Network\\nCLSP\\nCross-Lingual Lexical Sememe Prediction\\nCNN\\nConvolutional Neural Network\\nCNRL\\nCommunity-enhanced Network Representation Learning\\nCOCO-QA\\nCommon Objects in COntext Question Answering\\nConSE\\nConvex Combination of Semantic Embeddings\\nConv-KNRM\\nConvolutional Kernel-based Neural Ranking Model\\nCSP\\nCharacter-enhanced Sememe Prediction\\nCWE\\nCharacter-based Word Embeddings\\nDCNN\\nDiffusion-Convolutional Neural Network\\nDeViSE\\nDeep Visual-Semantic Embedding Model\\nDFS\\nDepth-First Search\\nDGCN\\nDual Graph Convolutional Network\\nDGE\\nDirected Graph Embedding\\nDKRL\\nDescription-embodied Knowledge Graph Representation Learning\\nDRMM\\nDeep Relevance Matching Model\\nDSSM\\nDeep Structured Semantic Model\\nECC\\nEdge-Conditioned Convolution\\nERNIE\\nEnhanced Language Representation Model with Informative\\nEntities\\nxix\\nFM-IQA\\nFreestyle Multilingual Image Question Answering\\nGAAN\\nGated Attention Network\\nGAT\\nGraph Attention Networks\\nGCN\\nGraph Convolutional Network\\nGCNN\\nGeodesic Convolutional Neural Network\\nGEAR\\nGraph-based Evidence Aggregating and Reasoning\\nGENQA\\nGenerative Question Answering Model\\nGGNN\\nGated Graph Neural Network\\nGloVe\\nGlobal Vectors for Word Representation\\nGNN\\nGraph Neural Networks\\nGRN\\nGraph Recurrent Network\\nGRU\\nGated Recurrent Unit\\nHAN\\nHeterogeneous Graph Attention Network\\nHMM\\nHidden Markov Model\\nHOPE\\nHigh-Order Proximity preserved Embeddings\\nIDF\\nInverse Document Frequency\\nIE\\nInformation Extraction\\nIKRL\\nImage-embodied Knowledge Graph Representation Learning\\nIR\\nInformation Retrieval\\nKALM\\nKnowledge-Augmented Language Model\\nKB\\nKnowledge Base\\nKBC\\nKnowledge Base Completion\\nKG\\nKnowledge Graph\\nKL\\nKullback-Leibler\\nKNET\\nKnowledge-guided Attention Neural Entity Typing\\nK-NRM\\nKernel-based Neural Ranking Model\\nKR\\nKnowledge Representation\\nLBSN\\nLocation-Based Social Network\\nLDA\\nLatent Dirichlet Allocation\\nLIWC\\nLinguistic Inquiry and Word Count\\nLLE\\nLocally Linear Embedding\\nLM\\nLanguage Model\\nLSA\\nLatent Semantic Analysis\\nLSHM\\nLatent Space Heterogeneous Model\\nLSTM\\nLong Short-Term Memory\\nMAP\\nMean Average Precision\\nMETEOR\\nMetric for Evaluation of Translation with Explicit ORdering\\nMMD\\nMaximum Mean Discrepancy\\nMMDW\\nMax-Margin DeepWalk\\nM-NMF\\nModularized Nonnegative Matrix Factorization\\nmovMF\\nmixture of von Mises-Fisher distributions\\nMRF\\nMarkov Random Field\\nMSLE\\nMean-Square Log-Transformed Error\\nMST\\nMinimum Spanning Tree\\nMV-RNN\\nMatrix-Vector Recursive Neural Network\\nxx\\nAcronyms\\nNEU\\nNetwork Embedding Update\\nNKLM\\nNeural Knowledge Language Model\\nNLI\\nNatural Language Inference\\nNLP\\nNatural Language Processing\\nNRE\\nNeural Relation Extraction\\nOOKB\\nOut-of-Knowledge-Base\\nPCNN\\nPiece-wise Convolution Neural Network\\npLSI\\nProbabilistic Latent Semantic Indexing\\nPMI\\nPoint-wise Mutual Information\\nPOS\\nPart-of-Speech\\nPPMI\\nPositive Point-wise Mutual Information\\nPTE\\nPredictive Text Embedding\\nPV-DBOW\\nParagraph Vector with Distributed Bag-of-Words\\nPV-DM\\nParagraph Vector with Distributed Memory\\nQA\\nQuestion Answering\\nRBF\\nRestricted Boltzmann Machine\\nRC\\nRelation Classi\\xef\\xac\\x81cation\\nR-CNN\\nRegion-based Convolutional Neural Network\\nRDF\\nResource Description Framework\\nRE\\nRelation Extraction\\nRMSE\\nRoot Mean Squared Error\\nRNN\\nRecurrent Neural Network\\nRNTN\\nRecursive Neural Tensor Network\\nRPN\\nRegion Proposal Network\\nSAC\\nSememe Attention over Context Model\\nSAT\\nSememe Attention over Target Model\\nSC\\nSemantic Compositionality\\nSCAS\\nSemantic Compositionality with Aggregated Sememe\\nSCMSA\\nSemantic Compositionality with Mutual Sememe Attention\\nSDLM\\nSememe-Driven Language Model\\nSDNE\\nStructural Deep Network Embeddings\\nSE-WRL\\nSememe-Encoded Word Representation Learning\\nSGD\\nStochastic Gradient Descent\\nSGNS\\nSkip-gram with Negative Sampling Model\\nS-LSTM\\nSentence Long Short-Term Memory\\nSPASE\\nSememe Prediction with Aggregated Sememe Embeddings\\nSPCSE\\nSememe Prediction with Character and Sememe Embeddings\\nSPICE\\nSemantic Propositional Image Caption Evaluation\\nSPSE\\nSememe Prediction with Sememe Embeddings\\nSPWCF\\nSememe Prediction with Word-to-Character Filtering\\nSPWE\\nSememe Prediction with Word Embeddings\\nSSA\\nSimple Sememe Aggregation Model\\nSSWE\\nSentiment-Speci\\xef\\xac\\x81c Word Embeddings\\nSVD\\nSingular Value Decomposition\\nSVM\\nSupport Vector Machine\\nAcronyms\\nxxi\\nTADW\\nText-associated DeepWalk\\nTF\\nTerm Frequency\\nTF-IDF\\nTerm Frequency\\xe2\\x80\\x93Inverse Document Frequency\\nTKRL\\nType-embodied Knowledge Graph Representation Learning\\nTSP\\nTraveling Salesman Problem\\nTWE\\nTopical Word Embeddings\\nVQA\\nVisual Question Answering\\nVSM\\nVector Space Model\\nWRL\\nWord Representation Learning\\nWSD\\nWord Sense Disambiguation\\nYAGO\\nYet Another Great Ontology\\nxxii\\nAcronyms\\nSymbols and Notations\\nTokyo\\nWord example\\nConvolution operator\\n,\\nDe\\xef\\xac\\x81ned as\\n\\x01\\nElement-wise multiplication (Hadamard product)\\n)\\nInduces\\n/\\nProportional to\\nP\\nSummation operator\\nmin\\nMinimize\\nmax\\nMaximize/max pooling\\narg mink\\nThe parameter that minimizes a function\\narg maxk\\nThe parameter that maximizes a function\\nsim\\nSimilarity\\nexp\\nExponential function\\nAtt\\nAttention function\\nAvg\\nAverage function\\nF1-Score\\nF1 score\\nPMI\\nPair-wise mutual information\\nReLU\\nReLU activation function\\nSigmoid\\nSigmoid function\\nSoftmax\\nSoftmax function\\nV\\nVocabulary set\\nw;w\\nWord; word embedding vector\\nE\\nWord embedding matrix\\nR\\nRelation set\\nr;r\\nRelation; relation embedding vector\\na\\nAnswer to question\\nq\\nQuery\\nW;M;U\\nWeight matrix\\nb;d\\nbias vector\\nMi;:;M:;j\\nMatrix\\xe2\\x80\\x99s ith row; matrix\\xe2\\x80\\x99s jth column\\nxxiii\\naT;MT\\nTranspose of a vector or a matrix\\ntr\\xc3\\xb0\\x03\\xc3\\x9e\\nTrace of a matrix\\nM\\n!\\nTensor\\na; b; k\\nHyperparameters\\ng; f ; /; U; d\\nFunctions\\nNa\\nNumber of occurrences of a\\nd\\xc3\\xb0\\x03; \\x03\\xc3\\x9e\\nDistance function\\ns\\xc3\\xb0\\x03; \\x03\\xc3\\x9e\\nSimilarity function\\nP \\x03\\xc3\\xb0 \\xc3\\x9e; p \\x03\\xc3\\xb0 \\xc3\\x9e\\nProbability\\nI\\xc3\\xb0\\x03; \\x03\\xc3\\x9e\\nMutual information\\nH\\xc3\\xb0\\x03\\xc3\\x9e\\nEntropy\\nO \\x03\\xc3\\xb0 \\xc3\\x9e\\nTime complexity\\nDKL \\x03jj\\x03\\n\\xc3\\xb0\\n\\xc3\\x9e\\nKL divergence\\nL\\nLoss function\\nO\\nObjective function\\nE\\nEnergy function\\na\\nAttention score\\n/\\x04\\nOptimal value of a variable/function /\\nj \\x03 j\\nVector length; set size\\n\\x03k ka\\na -norm\\n1\\xc3\\xb0\\x03\\xc3\\x9e\\nIndicator function\\nh\\nParameter in a neural network\\nE\\nExpectation of a random variable\\n\\x03 \\xc3\\xbe ;\\x03\\x05\\nPositive sample; negative sample\\nc\\nMargin\\nl\\nMean of normal distribution\\nl\\nMean vector of Gaussian distribution\\nr\\nStandard error of normal distribution\\nR\\nCovariance matrix of Gaussian distribution\\nIn\\nn-dimensional identity matrix\\nN\\nNormal distribution\\nxxiv\\nSymbols and Notations\\nChapter 1\\nRepresentation Learning and NLP\\nAbstract Natural languages are typical unstructured information. Conventional\\nNatural Language Processing (NLP) heavily relies on feature engineering, which\\nrequires careful design and considerable expertise. Representation learning aims to\\nlearnrepresentationsofrawdataasusefulinformationforfurtherclassi\\xef\\xac\\x81cationorpre-\\ndiction. This chapter presents a brief introduction to representation learning, includ-\\ning its motivation and basic idea, and also reviews its history and recent advances in\\nboth machine learning and NLP.\\n1.1\\nMotivation\\nMachine learning addresses the problem of automatically learning computer pro-\\ngrams from data. A typical machine learning system consists of three components [5]:\\nMachine Learning = Representation + Objective + Optimization.\\n(1.1)\\nThat is, to build an effective machine learning system, we \\xef\\xac\\x81rst transform useful\\ninformation on raw data into internal representations such as feature vectors. Then by\\ndesigning appropriate objective functions, we can employ optimization algorithms\\nto \\xef\\xac\\x81nd the optimal parameter settings for the system.\\nData representation determines how much useful information can be extracted\\nfrom raw data for further classi\\xef\\xac\\x81cation or prediction. If there is more useful infor-\\nmation transformed from raw data to feature representations, the performance of\\nclassi\\xef\\xac\\x81cation or prediction will tend to be better. Hence, data representation is a\\ncrucial component to support effective machine learning.\\nConventional machine learning systems adopt careful feature engineering as\\npreprocessing to build feature representations from raw data. Feature engineering\\nneeds careful design and considerable expertise, and a speci\\xef\\xac\\x81c task usually requires\\ncustomized feature engineering algorithms, which makes feature engineering labor\\nintensive, time consuming, and in\\xef\\xac\\x82exible.\\nRepresentation learning aims to learn informative representations of objects from\\nraw data automatically. The learned representations can be further fed as input to\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_1\\n1\\n2\\n1\\nRepresentation Learning and NLP\\nmachine learning systems for prediction or classi\\xef\\xac\\x81cation. In this way, machine learn-\\ning algorithms will be more \\xef\\xac\\x82exible and desirable while handling large-scale and\\nnoisy unstructured data, such as speech, images, videos, time series, and texts.\\nDeep learning [9] is a typical approach for representation learning, which has\\nrecently achieved great success in speech recognition, computer vision, and natural\\nlanguage processing. Deep learning has two distinguishing features:\\n\\xe2\\x80\\xa2 Distributed Representation. Deep learning algorithms typically represent each\\nobject with a low-dimensional real-valued dense vector, which is named as dis-\\ntributed representation. As compared to one-hot representation in conventional\\nrepresentation schemes (such as bag-of-words models), distributed representation\\nis able to represent data in a more compact and smoothing way, as shown in Fig. 1.1,\\nand hence is more robust to address the sparsity issue in large-scale data.\\n\\xe2\\x80\\xa2 Deep Architecture. Deep learning algorithms usually learn a hierarchical deep\\narchitecture to represent objects, known as multilayer neural networks. The deep\\narchitecture is able to extract abstractive features of objects from raw data, which\\nis regarded as an important reason for the great success of deep learning for speech\\nrecognition and computer vision.\\nCurrently, the improvements caused by deep learning for NLP may still not be\\nso signi\\xef\\xac\\x81cant as compared to speech and vision. However, deep learning for NLP\\nhas been able to signi\\xef\\xac\\x81cantly reduce the work of feature engineering in NLP in the\\nmeantime of performance improvement. Hence, many researchers are devoting to\\ndeveloping ef\\xef\\xac\\x81cient algorithms on representation learning (especially deep learning)\\nfor NLP.\\nIn this chapter, we will \\xef\\xac\\x81rst discuss why representation learning is important for\\nNLP and introduce the basic ideas of representation learning. Afterward, we will\\nApple Inc.\\nSteve Jobs\\nTim Cook\\niPhone\\nCEO\\nfounder\\nproduct\\nSteve Jobs\\nTim Cook\\niPhone\\nApple Inc.\\nEntities\\nEmbeddings\\nFig. 1.1 Distributed representation of words and entities in human languages\\n1.1 Motivation\\n3\\nbrie\\xef\\xac\\x82y review the development history of representation learning for NLP, introduce\\ntypical approaches of contemporary representation learning, and summarize existing\\nand potential applications of representation learning. Finally, we will introduce the\\ngeneral organization of this book.\\n1.2\\nWhy Representation Learning Is Important for NLP\\nNLPaimstobuildlinguistic-speci\\xef\\xac\\x81cprogramsformachinestounderstandlanguages.\\nNatural language texts are typical unstructured data, with multiple granularities, mul-\\ntiple tasks, and multiple domains, which make NLP challenging to achieve satisfac-\\ntory performance.\\nMultiple Granularities. NLP concerns about multiple levels of language entries,\\nincluding but not limited to characters, words, phrases, sentences, paragraphs, and\\ndocuments. Representation learning can help to represent the semantics of these\\nlanguage entries in a uni\\xef\\xac\\x81ed semantic space, and build complex semantic relations\\namong these language entries.\\nMultiple Tasks. There are various NLP tasks based on the same input. For exam-\\nple, given a sentence, we can perform multiple tasks such as word segmentation,\\npart-of-speech tagging, named entity recognition, relation extraction, and machine\\ntranslation. In this case, it will be more ef\\xef\\xac\\x81cient and robust to build a uni\\xef\\xac\\x81ed repre-\\nsentation space of inputs for multiple tasks.\\nMultiple Domains. Natural language texts may be generated from multiple\\ndomains, including but not limited to news articles, scienti\\xef\\xac\\x81c articles, literary works,\\nand online user-generated content such as product reviews. Moreover, we can also\\nregard texts in different languages as multiple domains. Conventional NLP systems\\nhave to design speci\\xef\\xac\\x81c feature extraction algorithms for each domain according to its\\ncharacteristics. In contrast, representation learning enables us to build representations\\nautomatically from large-scale domain data.\\nIn summary, as shown in Fig. 1.2, representation learning can facilitate knowledge\\ntransfer across multiple language entries, multiple NLP tasks, and multiple appli-\\ncation domains, and signi\\xef\\xac\\x81cantly improve the effectiveness and robustness of NLP\\nperformance.\\n1.3\\nBasic Ideas of Representation Learning\\nIn this book, we focus on the distributed representation scheme (i.e., embedding),\\nand talk about recent advances of representation learning methods for multiple lan-\\nguage entries, including words, phrases, sentences, and documents, and their closely\\nrelated objects including sememe-based linguistic knowledge, entity-based world\\nknowledge, networks, and cross-modal entries.\\n4\\n1\\nRepresentation Learning and NLP\\nNLP Tasks\\nLanguage Entries\\nKnowledge\\nNetwork\\nDocument\\nSentence\\nPhrase\\nWord\\nNLP Applications\\nSemantic Analysis\\nSyntactic Analysis\\nLexical Analysis\\nFig. 1.2 Distributed representation can provide uni\\xef\\xac\\x81ed semantic space for multi-grained language\\nentries and for multiple NLP tasks\\nBy distributed representation learning, all objects that we are interested in are\\nprojected into a uni\\xef\\xac\\x81ed low-dimensional semantic space. As demonstrated in Fig. 1.1,\\nthe geometric distance between two objects in the semantic space indicates their\\nsemantic relatedness; the semantic meaning of an object is related to which objects\\nare close to it. In other words, it is the relative closeness with other objects that reveals\\nan object\\xe2\\x80\\x99s meaning rather than the absolute position.\\n1.4\\nDevelopment of Representation Learning for NLP\\nIn this section, we introduce the development of representation learning for NLP,\\nalso shown in Fig. 1.3. To study representation schemes in NLP, words would be a\\ngood start, since they are the minimum units in natural languages. The easiest way\\nto represent a word in a computer-readable way (e.g., using a vector) is one-hot\\nvector, which has the dimension of the vocabulary size and assigns 1 to the word\\xe2\\x80\\x99s\\ncorresponding position and 0 to others. It is apparent that one-hot vectors hardly\\ncontain any semantic information about words except simply distinguishing them\\nfrom each other.\\nOne of the earliest ideas of word representation learning can date back to n-gram\\nmodels [15]. It is easy to understand: when we want to predict the next word in a\\nsequence, we usually look at some previous words (and in the case of n-gram, they\\nare the previous n \\xe2\\x88\\x921 words). And if going through a large-scale corpus, we can\\ncount and get a good probability estimation of each word under the condition of all\\ncombinations of n \\xe2\\x88\\x921 previous words. These probabilities are useful for predicting\\nwords in sequences, and also form vector representations for words since they re\\xef\\xac\\x82ect\\nthe meanings of words.\\nThe idea of n-gram models is coherent with the distributional hypothesis: lin-\\nguistic items with similar distributions have similar meanings [7]. In another phrase,\\n\\xe2\\x80\\x9ca word is characterized by the company it keeps\\xe2\\x80\\x9d [6]. It became the fundamental\\nidea of many NLP models, from word2vec to BERT.\\n1.4 Development of Representation Learning for NLP\\n5\\n1948\\nN-gram Model\\n1954\\nDistributional Hypothesis\\nBag-of-words\\nDistributed Representation\\n1986\\nNeural Probabilistic\\nLanguage Model\\n2003\\n2013\\nWord2vec\\n2018\\nPre-trained Language Model\\nPredicts the next item in \\na sequence based on \\nits previous n-1 items.\\nA word is characterized by the \\ncompany it keeps.\\nRepresents a sentence or a \\ndocument as the bag of its words.\\nRepresents items by a pattern of \\nactivation distributed over elements.\\nLearns a distributed representation \\nof words for language modeling.\\nA simple and e cient distributed \\nword representation used in many \\nNLP models.\\nContextual word representation, \\npipeline, larger corpora and \\ndeeper neural architectures\\nFig. 1.3 The timeline for the development of representation learning in NLP. With the growing\\ncomputing power and large-scale text data, distributed representation trained with neural networks\\nand large corpora has become the mainstream\\nAnother example of the distributional hypothesis is Bag-Of-Words (BOW) mod-\\nels [7]. BOW models regard a document as a bag of its words, disregarding the orders\\nof these words in the document. In this way, the document can be represented as a\\nvocabulary-size vector, in which each word that has appeared in the document cor-\\nresponds to a unique and nonzero dimension. Then a score can be further computed\\nfor each word (e.g., the numbers of occurrences) to indicate the weights of these\\nwords in the document. Though very simple, BOW models work great in applica-\\ntions like spam \\xef\\xac\\x81ltering, text classi\\xef\\xac\\x81cation, and information retrieval, proving that\\nthe distributions of words can serve as a good representation for text.\\nIn the above cases, each value in the representation clearly matches one entry\\n(e.g., word scores in BOW models). This one-to-one correspondence between con-\\ncepts and representation elements is called local representation or symbol-based\\nrepresentation, which is natural and simple.\\nIn distributed representation, on the other hand, each entity (or attribute) is\\nrepresented by a pattern of activation distributed over multiple elements, and each\\ncomputing element is involved in representing multiple entities [11]. Distributed rep-\\nresentation has been proved to be more ef\\xef\\xac\\x81cient because it usually has low dimen-\\nsions that can prevent the sparsity issue. Useful hidden properties can be learned from\\nlarge-scale data and emerged in distributed representation. The idea of distributed\\nrepresentation was originally inspired by the neural computation scheme of humans\\nand other animals. It comes from neural networks (activations of neurons), and with\\nthe great success of deep learning, distributed representation has become the most\\ncommonly used approach for representation learning.\\nOne of the pioneer practices of distributed representation in NLP is Neural Prob-\\nabilistic Language Model (NPLM) [1]. A language model is to predict the joint\\nprobability of sequences of words (n-gram models are simple language models).\\nNPLM \\xef\\xac\\x81rst assigns a distributed vector for each word, then uses a neural network\\nto predict the next word. By going through the training corpora, NPLM successfully\\nlearns how to model the joint probability of sentences, while brings word embed-\\ndings (i.e., low-dimensional word vectors) as learned parameters in NPLM. Though\\n6\\n1\\nRepresentation Learning and NLP\\nWord Embedding Model\\nPre-training Objective\\nWord Embedding\\nTarget Task Model\\nTarget Task Objective\\nWord Embedding\\nPre-trained Language Model\\nPre-training Objective\\nWord Embedding\\nTarget Task Objective\\nTarget Task Model\\nWord Embedding\\nWord Embedding\\nPre-trained Language Model\\nFig. 1.4 This \\xef\\xac\\x81gure shows how word embeddings and pre-trained language models work in NLP\\npipelines. They both learn distributed representations for language entries (e.g., words) through\\npretraining objectives and transfer them to target tasks. Furthermore, pre-trained language models\\ncan also transfer model parameters\\nit is hard to tell what each element of a word embedding actually means, the vectors\\nindeed encode semantic meanings about the words, veri\\xef\\xac\\x81ed by the performance of\\nNPLM.\\nInspired by NPLM, there came many methods that embed words into distributed\\nrepresentations and use the language modeling objective to optimize them as model\\nparameters. Famous examples include word2vec [12], GloVe [13], and fastText [3].\\nThough differing in detail, these methods are all very ef\\xef\\xac\\x81cient to train, utilize large-\\nscale corpora, and have been widely adopted as word embeddings in many NLP\\nmodels. Word embeddings in the NLP pipeline map discrete words into informative\\nlow-dimensional vectors, and help to shine a light on neural networks in comput-\\ning and understanding languages. It makes representation learning a critical part of\\nnatural language processing.\\nThe research on representation learning in NLP took a big leap when ELMo\\n[14] and BERT [4] came out. Besides using larger corpora, more parameters, and\\nmore computing resources as compared to word2vec, they also take complicated\\ncontext in text into consideration. It means that instead of assigning each word\\nwith a \\xef\\xac\\x81xed vector, ELMo and BERT use multilayer neural networks to calculate\\ndynamic representations for the words based on their context, which is especially\\nuseful for the words with multiple meanings. Moreover, BERT starts a new fashion\\n(though not originated from it) of the pretrained \\xef\\xac\\x81ne-tuning pipeline. Previously,\\nword embeddings are simply adopted as input representation. But after BERT, it\\nbecomes a common practice to keep using the same neural network structure such as\\nBERT in both pretraining and \\xef\\xac\\x81ne-tuning, which is taking the parameters of BERT\\nfor initialization and \\xef\\xac\\x81ne-tuning the model on downstream tasks (Fig. 1.4).\\nThough not a big theoretical breakthrough, BERT-like models (also known as\\nPre-trained Language Models (PLM), for they are pretrained through language\\nmodeling objective on large corpora) have attracted wide attention in the NLP and\\nmachine learning community, for they have been so successful and achieved state-\\nof-the-art on almost every NLP benchmarks. These models show what large-scale\\ndata and computing power can lead to, and new research works on the topic of Pre-\\nTrained language Models (PLMs) emerge rapidly. Probing experiments demonstrate\\nthat PLMs implicitly encode a variety of linguistic knowledge and patterns inside\\n1.4 Development of Representation Learning for NLP\\n7\\ntheir multilayer network parameters [8, 10]. All these signi\\xef\\xac\\x81cant performances and\\ninteresting analyses suggest that there are still a lot of open problems to explore in\\nPLMs, as the future of representation learning for NLP.\\nBased on the distributional hypothesis, representation learning for NLP has\\nevolved from symbol-based representation to distributed representation. Starting\\nfrom word2vec, word embeddings trained from large corpora have shown signi\\xef\\xac\\x81cant\\npower in most NLP tasks. Recently, emerged PLMs (like BERT) take complicated\\ncontext into word representation and start a new trend of the pretraining \\xef\\xac\\x81ne-tuning\\npipeline, bringing NLP to a new level. What will be the next big change in repre-\\nsentation learning for NLP? We hope the contents of this book can give you some\\ninspiration.\\n1.5\\nLearning Approaches to Representation Learning for\\nNLP\\nPeople have developed various effective and ef\\xef\\xac\\x81cient approaches to learn semantic\\nrepresentations for NLP. Here we list some typical approaches.\\nStatistical Features: As introduced before, semantic representations for NLP in\\nthe early stage often come from statistics, instead of emerging from the optimization\\nprocess. For example, in n-gram or bag-of-words models, elements in the representa-\\ntion are usually frequencies or numbers of occurrences of the corresponding entries\\ncounted in large-scale corpora.\\nHand-craft Features: In certain NLP tasks, syntactic and semantic features are\\nuseful for solving the problem. For example, types of words and entities, semantic\\nroles and parse trees, etc. These linguistic features may be provided with the tasks\\nor can be extracted by speci\\xef\\xac\\x81c NLP systems. In a long period before the wide use\\nof distributed representation, researchers used to devote lots of effort into designing\\nuseful features and combining them as the inputs for NLP models.\\nSupervised Learning: Distributed representations emerge from the optimization\\nprocess of neural networks under supervised learning. In the hidden layers of neu-\\nral networks, the different activation patterns of neurons represent different entities\\nor attributes. With a training objective (usually a loss function for the target task)\\nand supervised signals (usually the gold-standard labels for training instances of the\\ntarget tasks), the networks can learn better parameters via optimization (e.g., gra-\\ndient descent). With proper training, the hidden states will become informative and\\ngeneralized as good semantic representations of natural languages.\\nFor example, to train a neural network for a sentiment classi\\xef\\xac\\x81cation task, the loss\\nfunction is usually set as the cross-entropy of the model predictions with respect to\\nthe gold-standard sentiment labels as supervision. While optimizing the objective,\\nthe loss gets smaller, and the model performance gets better. In the meantime, the\\nhidden states of the model gradually form good sentence representations by encoding\\nthe necessary information for sentiment classi\\xef\\xac\\x81cation inside the continuous hidden\\nspace.\\n8\\n1\\nRepresentation Learning and NLP\\nSelf-supervised Learning: In some cases, we simply want to get good represen-\\ntations for certain elements, so that these representations can be transferred to other\\ntasks. For example, in most neural NLP models, words in sentences are \\xef\\xac\\x81rst mapped\\nto their corresponding word embeddings (maybe from word2vec or GloVe) before\\nsent to the networks. However, there are no human-annotated \\xe2\\x80\\x9clabels\\xe2\\x80\\x9d for learning\\nword embeddings. To acquire the training objective necessary for neural networks,\\nwe need to generate \\xe2\\x80\\x9clabels\\xe2\\x80\\x9d intrinsically from existing data. This is called self-\\nsupervised learning (one way for unsupervised learning).\\nFor example, language modeling is a typical \\xe2\\x80\\x9cself-supervised\\xe2\\x80\\x9d objective, for it\\ndoes not require any human annotations. Based on the distributional hypothesis,\\nusing the language modeling objective can lead to hidden representations that encode\\nthe semantics of words. You may have heard of a famous equation: w(king) \\xe2\\x88\\x92\\nw(man) + w(woman) = w(queen), which demonstrates the analogical properties\\nthat the word embeddings have possessed through self-supervised learning.\\nWe can see another angle of self-supervised learning in autoencoders. It is also a\\nway to learn representations for a set of data. Typical autoencoders have a reduction\\n(encoding) phase and a reconstruction (decoding) phase. In the reduction phase, an\\nitem from the data is encoded into a low-dimensional representation, and in the\\nreconstruction phase, the model tries to reconstruct the item from the intermediate\\nrepresentation. Here, the training objective is the reconstruction loss, derived from\\nthe data itself. During the training process, meaningful information is encoded and\\nkept in the latent representation, while noise signals are discarded.\\nSelf-supervised learning has made a great success in NLP, for the plain text itself\\ncontains abundant knowledge and patterns about languages, and self-supervised\\nlearning can fully utilize the existing large-scale corpora. Nowadays, it is still the\\nmost exciting research area of representation learning for natural languages, and\\nresearchers continue to put their efforts into this direction.\\nBesides, many other machine learning approaches have also been explored in\\nrepresentation learning for NLP, such as adversarial training, contrastive learning,\\nfew-shot learning, meta-learning, continual learning, reinforcement learning, et al.\\nHow to develop more effective and ef\\xef\\xac\\x81cient approaches of representation learning\\nfor NLP and to better take advantage of large-scale and complicated corpora and\\ncomputing power, is still an important research topic.\\n1.6\\nApplications of Representation Learning for NLP\\nIn general, there are two kinds of applications of representation learning for NLP. In\\none case, the semantic representation is trained in a pretraining task (or designed by\\nhuman experts) and is transferred to the model for the target task. Word embedding\\nis an example of the application. It is trained by using language modeling objective\\nand is taken as inputs for other down-stream NLP models. In this book, we will\\n1.6 Applications of Representation Learning for NLP\\n9\\nalso introduce sememe knowledge representation and world knowledge representa-\\ntion, which can also be integrated into some NLP systems as additional knowledge\\naugmentation to enhance their performance in certain aspects.\\nIn other cases, the semantic representation lies within the hidden states of the neu-\\nral model and directly aims for better performance of target tasks as an end-to-end\\nfashion. For example, many NLP tasks want to semantically compose sentence or\\ndocument representation: tasks like sentiment classi\\xef\\xac\\x81cation, natural language infer-\\nence, and relation extraction require sentence representation and the tasks like ques-\\ntion answering need document representation. As shown in the latter part of the\\nbook, many representation learning methods have been developed for sentences and\\ndocuments and bene\\xef\\xac\\x81t these NLP tasks.\\n1.7\\nThe Organization of This Book\\nWe start the book from word representation. By giving a thorough introduction to\\nword representation, we hope the readers can grasp the basic ideas for representa-\\ntion learning for NLP. Based on that, we further talk about how to compositionally\\nacquire the representation for higher level language components, from sentences to\\ndocuments.\\nAs shown in Fig. 1.5, representation learning will be able to incorporate various\\ntypes of structural knowledge to support a deep understanding of natural languages,\\nnamed as knowledge-guided NLP. Hence, we next introduce two forms of knowledge\\nrepresentation that are closely related to NLP. On the one hand, sememe represen-\\ntation tries to encode linguistic and commonsense knowledge in natural languages.\\nSememe is de\\xef\\xac\\x81ned as the minimum indivisible unit of semantic meaning [2]. With\\nthe help of sememe representation learning, we can get more interpretable and more\\nrobust NLP models. On the other hand, world knowledge representation studies how\\nto encode world facts into continuous semantic space. It can not only help with\\nknowledge graph tasks but also bene\\xef\\xac\\x81t knowledge-guided NLP applications.\\nBesides, the network is also a natural way to represent objects and their relation-\\nships. In the network representation section, we study how to embed vertices and\\nedges in a network and how these elements interact with each other. Through the\\napplications, we further show how network representations can help NLP tasks.\\nAnother interesting topic related to NLP is the cross-modal representation, which\\nstudies how to model uni\\xef\\xac\\x81ed semantic representations across different modalities\\n(e.g., text, audios, images, videos, etc.). Through this section, we review several\\ncross-modal problems along with representative models.\\nAt the end of the book, we introduce some useful resources to the readers, includ-\\ning deep learning frameworks and open-source codes. We also share some views\\nabout the next big topics in representation learning for NLP. We hope that the\\nresources and the outlook can help our readers have a better understanding of the\\ncontent of the book, and inspire our readers about how representation learning in\\nNLP would further develop.\\n10\\n1\\nRepresentation Learning and NLP\\nDeep Network\\nOpen Data\\nSymbol\\nEmbedding\\nKnowledge\\nGuidance\\nKnowledge\\nExtraction\\nLearning\\nUnder-\\nstanding\\nGNN\\nKRL\\nDeep Learning\\nKnowledge Graph\\nFig. 1.5 The architecture of knowledge-guided NLP\\nReferences\\n1. Yoshua Bengio, R\\xc3\\xa9jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\\nlanguage model. Journal of Machine Learning Research, 3(Feb):1137\\xe2\\x80\\x931155, 2003.\\n2. Leonard Bloom\\xef\\xac\\x81eld. A set of postulates for the science of language. Language, 2(3):153\\xe2\\x80\\x93164,\\n1926.\\n3. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors\\nwith subword information. Transactions of the Association for Computational Linguistics,\\n5:135\\xe2\\x80\\x93146, 2017.\\n4. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019.\\n5. Pedro Domingos. A few useful things to know about machine learning. Communications of\\nthe ACM, 55(10):78\\xe2\\x80\\x9387, 2012.\\n6. John R Firth. A synopsis of linguistic theory, 1930\\xe2\\x80\\x931955. 1957.\\n7. Zellig S Harris. Distributional structure. Word, 10(2\\xe2\\x80\\x933):146\\xe2\\x80\\x93162, 1954.\\n8. John Hewitt and Christopher D. Manning. A structural probe for \\xef\\xac\\x81nding syntax in word repre-\\nsentations. In Proceedings of NAACL-HLT, 2019.\\n9. Goodfellow Ian, Yoshua Bengio, and Aaron Courville. Deep learning. Book in preparation for\\nMIT Press, 2016.\\n10. Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. Lin-\\nguistic knowledge and transferability of contextual representations. In Proceedings of NAACL-\\nHLT, 2019.\\nReferences\\n11\\n11. James L McClelland, David E Rumelhart, PDP Research Group, et al. Parallel distributed\\nprocessing. Explorations in the Microstructure of Cognition, 2:216\\xe2\\x80\\x93271, 1986.\\n12. T Mikolov and J Dean. Distributed representations of words and phrases and their composi-\\ntionality. Proceedings of NeurIPS, 2013.\\n13. Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\\nrepresentation. In Proceedings of EMNLP, 2014.\\n14. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\\nand Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of NAACL-\\nHLT, pages 2227\\xe2\\x80\\x932237, 2018.\\n15. Claude E Shannon. A mathematical theory of communication. Bell system technical journal,\\n27(3):379\\xe2\\x80\\x93423, 1948.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 2\\nWord Representation\\nAbstract Word representation, aiming to represent a word with a vector, plays\\nan essential role in NLP. In this chapter, we \\xef\\xac\\x81rst introduce several typical word\\nrepresentation learning methods, including one-hot representation and distributed\\nrepresentation. After that, we present two widely used evaluation tasks for measuring\\nthe quality of word embeddings. Finally, we introduce the recent extensions for word\\nrepresentation learning models.\\n2.1\\nIntroduction\\nWords are usually considered as the smallest meaningful units of speech or writing in\\nhuman languages. High-level structures in a language, such as phrases and sentences,\\nare further composed of words. For human beings, to understand a language, it is\\ncrucial to understand the meanings of words. Therefore, it is essential to accurately\\nrepresent words, which could help models better understand, categorize, or generate\\ntext in NLP tasks.\\nA word can be naturally represented as a sequence of several characters. However,\\nit is very inef\\xef\\xac\\x81cient and ineffective only to use raw character sequences to represent\\nwords. First, the variable lengths of words make it hard to be processed and used in\\nmachine learning methods. Moreover, it is very sparse, because only a tiny proportion\\nof arrangements are meaningful. For example, English words are usually character\\nsequences which are composed of 1\\xe2\\x80\\x9320 characters in the English alphabet, but most\\nof these character sequences such as \\xe2\\x80\\x9caaaaa\\xe2\\x80\\x9d are meaningless.\\nOne-hot representation is another natural approach to represent words, which\\nassignsauniqueindextoeachword.Itisalsonotgoodenoughtorepresentwordswith\\none-hot representation. First, one-hot representation could not capture the seman-\\ntic relatedness among words. Second, one-hot representation is a high-dimensional\\nsparse representation, which is very inef\\xef\\xac\\x81cient. Third, it is very in\\xef\\xac\\x82exible for one-hot\\nrepresentation to deal with new words, which requires assigning new indexes for new\\nwords and would change the dimensions of the representation. The change may lead\\nto some problems for existing NLP systems.\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_2\\n13\\n14\\n2\\nWord Representation\\nRecently, distributed word representation approaches are proposed to address the\\nproblem of one-hot word representation. The distributional hypothesis [23, 30] that\\nlinguistic objects with similar distributions have similar meanings is the basis for dis-\\ntributed word representation learning. Based on the distributional hypothesis, various\\nword representation models, such as CBOW and Skip-gram, have been proposed and\\napplied in different areas.\\nIn the remaining part of this chapter, we start with one-hot word representation.\\nFurther, we introduce distributed word representation models, including Brown Clus-\\nter, Latent Semantic Analysis, Word2vec, and GloVe in detail. Then we introduce\\ntwo typical evaluation tasks for word representation. Finally, we discuss various\\nextensions of word representation models.\\n2.2\\nOne-Hot Word Representation\\nIn this section, we will introduce one-hot word representation in details. Given a\\n\\xef\\xac\\x81xed set of vocabulary V = {w1, w2, . . . , w|V |}, one very intuitive way to represent\\na word w is to encode it with a |V |-dimensional vector w, where each dimension of\\nw is either 0 or 1. Only one dimension in w can be 1 while all the other dimensions\\nare 0. Formally, each dimension of w can be represented as\\nwi =\\n\\x02\\n1 if w = wi\\n0 otherwise.\\n(2.1)\\nOne-hot word representation, in essence, maps each word to an index of the\\nvocabulary, which can be very ef\\xef\\xac\\x81cient for storage and computation. However, it\\ndoesnotcontainrichsemanticandsyntacticinformationofwords.Therefore,one-hot\\nrepresentation cannot capture the relatedness among words. The difference between\\ncat and dog is as much as the difference between cat and bed in one-hot word\\nrepresentation. Besides, one-hot word representation embeds each word into a |V |-\\ndimensional vector, which can only work for a \\xef\\xac\\x81xed vocabulary. Therefore, it is\\nin\\xef\\xac\\x82exible to deal with new words in a real-world scenario.\\n2.3\\nDistributed Word Representation\\nRecently, distributed word representation approaches are proposed to address the\\nproblem of one-hot word representation. The distributional hypothesis [23, 30] that\\nlinguistic objects with similar distributions have similar meanings is the basis for\\nsemantic word representation learning.\\nBased on the distributional hypothesis, Brown Cluster [9] groups words into hier-\\narchical clusters where words in the same cluster have similar meanings. The cluster\\n2.3 Distributed Word Representation\\n15\\nlabel can roughly represent the similarity between words, but it cannot precisely\\ncompare words in the same group. To address this issue, distributed word represen-\\ntation1 aims to embed each word into a continuous real-valued vector. It is a dense\\nrepresentation, and \\xe2\\x80\\x9cdense\\xe2\\x80\\x9d means that one concept is represented by more than one\\ndimension of the vector, and each dimension of the vector is involved in representing\\nmultiple concepts. Due to its continuous characteristic, distributed word represen-\\ntation can be easily applied in deep neural models for NLP tasks. Distributed word\\nrepresentation approaches such as Word2vec and GloVe usually learn word vectors\\nfrom a large corpus based on the distributional hypothesis. In this section, we will\\nintroduce several distributed word representation approaches in detail.\\n2.3.1\\nBrown Cluster\\nBrown Cluster classi\\xef\\xac\\x81es words into several clusters that have similar semantic mean-\\nings. Detailedly, Brown Cluster learns a binary tree from a large-scale corpus, in\\nwhich the leaves of the tree indicate the words and the internal nodes of the tree\\nindicate word hierarchical clusters. This is a hard clustering method since each word\\nbelongs to exactly one group.\\nThe idea of Brown Cluster to cluster the words comes from the n-gram language\\nmodel. A language model evaluates the probability of a sentence. For example,\\nthe sentence have a nice day should have a higher probability than a random\\nsequence of words. Using a k-gram language model, the probability of a sentence\\ns = {w1, w2, w3, . . . , wn} can be represented as\\nP(s) =\\nn\\x03\\ni=1\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92k).\\n(2.2)\\nIt is easy to estimate P(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92k) from a large corpus, but the model has |V |k \\xe2\\x88\\x921\\nindependent parameters which is a huge number for computers in the 1990s. Even if k\\nis 2, the number of parameters is considerable. Moreover, the estimation is inaccurate\\nfor rare words. To address these problems, [9] proposes to group words into clusters\\nand train a cluster-level n-gram language model rather than a word-level model. By\\nassigning a cluster to each word, the probability can be written as\\nP(s) =\\nn\\x03\\ni=1\\nP(ci|ci\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92k)P(wi|ci),\\n(2.3)\\n1We emphasize that distributed representation and distributional representation are two completely\\ndifferent aspects of representations. A word representation method may belong to both categories.\\nDistributed representation indicates that the representation is a real-valued vector, while distri-\\nbutional representation indicates that the meaning of a word is learned under the distributional\\nhypothesis.\\n16\\n2\\nWord Representation\\nwhere ci is the corresponding cluster of wi. In cluster-level language model, there\\nare only |Ck| \\xe2\\x88\\x921 + |V | \\xe2\\x88\\x92|C| independent parameters, where C is the cluster set\\nwhich is usually much smaller than the vocabulary |V |.\\nThe quality of the cluster affects the performance of the language model. Given a\\ntraining text s, for a 2-gram language model, the quality of a mapping \\xcf\\x80 from words\\nto clusters is de\\xef\\xac\\x81ned as\\nQ(\\xcf\\x80) = 1\\nn log P(s)\\n(2.4)\\n= 1\\nn\\nn\\n\\x04\\ni=1\\n\\x05\\nlog P(ci|ci\\xe2\\x88\\x921) + log P(wi|ci)\\n\\x06\\n.\\n(2.5)\\nLet Nw be the number of times word w appears in corpus s, Nw1w2 be the number\\nof times bigram w1w2 appears, and N\\xcf\\x80(w) be the number of times a cluster appears.\\nThen the quality function Q(\\xcf\\x80) can be rewritten in a statistical way as follows:\\nQ(\\xcf\\x80) = 1\\nn\\nn\\n\\x04\\ni=1\\n\\x05\\nlog P(ci|ci\\xe2\\x88\\x921) + log P(wi|ci)\\n\\x06\\n(2.6)\\n=\\n\\x04\\nw1,w2\\nNw1w2\\nn\\nlog P(\\xcf\\x80(w2)|\\xcf\\x80(w1))P(w2|\\xcf\\x80(w2))\\n(2.7)\\n=\\n\\x04\\nw1,w2\\nNw1w2\\nn\\nlog N\\xcf\\x80(w1)\\xcf\\x80(w2)\\nN\\xcf\\x80(w1)\\nNw2\\nN\\xcf\\x80(w2)\\n(2.8)\\n=\\n\\x04\\nw1,w2\\nNw1w2\\nn\\nlog N\\xcf\\x80(w1)\\xcf\\x80(w2)n\\nN\\xcf\\x80(w1)N\\xcf\\x80(w2)\\n+\\n\\x04\\nw1,w2\\nNw1w2\\nn\\nlog Nw2\\nn\\n(2.9)\\n=\\n\\x04\\nc1,c2\\nNc1c2\\nn\\nlog Nc1c2n\\nNc1 Nc2\\n+\\n\\x04\\nw2\\nNw2\\nn\\nlog Nw2\\nn .\\n(2.10)\\nSince P(w) = Nw\\nn , P(c) = Nc\\nn , and P(c1c2) =\\nNc1c2\\nn , the quality function can be\\nrewritten as\\nQ(\\xcf\\x80) =\\n\\x04\\nc1,c2\\nP(c1c2) log\\nP(c1c2)\\nP(c1)P(c2) +\\n\\x04\\nw\\nP(w) log P(w)\\n(2.11)\\n= I (C) \\xe2\\x88\\x92H(V ),\\n(2.12)\\nwhere I (C) is the mutual information between clusters and H(V ) is the entropy of\\nthe word distribution, which is a constant value. Therefore, to optimize Q(\\xcf\\x80) equals\\nto optimize the mutual information.\\nThere is no practical method to obtain optimum partitions. Nevertheless, Brown\\nCluster uses a greedy strategy to obtain a suboptimal result. Initially, it assigns a\\ndistinct class for each word. Then it merges two classes with the least average mutual\\ninformation. After |V | \\xe2\\x88\\x92|C| mergences, the partition is generated. Keeping the |C|\\n2.3 Distributed Word Representation\\n17\\nTable 2.1 Some clusters of Brown Cluster\\nCluster #1\\nFriday\\nMonday\\nThursday\\nWednesday\\nTuesday\\nSaturday\\nCluster #2\\nJune\\nMarch\\nJuly\\nApril\\nJanuary\\nDecember\\nCluster #3\\nWater\\nGas\\nCoal\\nLiquid\\nAcid\\nSand\\nCluster #4\\nGreat\\nBig\\nVast\\nSudden\\nMere\\nSheer\\nCluster #5\\nMan\\nWoman\\nBoy\\nGirl\\nLawyer\\nDoctor\\nCluster #6\\nAmerican\\nIndian\\nEuropean\\nJapanese\\nGerman\\nAfrican\\nclusters, we can continuously perform |C| \\xe2\\x88\\x921 mergences to get a binary tree. With\\ncertain care in implementation, the complexity of this algorithm is O(|V |3).\\nWe show some clusters in Table 2.1. From the table, we can \\xef\\xac\\x81nd that each cluster\\nrelates to a sense in the natural language. The words in the same cluster tend to\\nexpress similar meanings or could be used exchangeably.\\n2.3.2\\nLatent Semantic Analysis\\nLatent Semantic Analysis (LSA) is a family of strategies derived from vector space\\nmodels, which could capture word semantics much better. LSA aims to explore latent\\nfactors for words and documents by matrix factorization to improve the estimation\\nof word similarities. Reference [14] applies Singular Value Decomposition (SVD)\\non the word-document matrix and exploits uncorrelated factors for both words and\\ndocuments. The SVD of word-document matrix M yields three matrices E, \\xce\\xa3 and\\nD such that\\nM = E\\xce\\xa3D\\xe2\\x8a\\xa4,\\n(2.13)\\nwhere \\xce\\xa3 is the diagonal matrix of singular values of M, each row vector wi in\\nmatrix E corresponds to word wi, and each row vector di in matrix D corresponds\\nto document di. Then the similarity between two words could be\\nsim(wi, w j) = Mi,:M\\xe2\\x8a\\xa4\\nj,: = wi\\xce\\xa32w j.\\n(2.14)\\nHere, the number of singular values k included in \\xce\\xa3 is a hyperparameter that\\nneeds to be tuned. With a reasonable amount of the largest singular values used, LSA\\ncould capture much useful information in the word-document matrix and provide a\\nsmoothing effect that prevents large variance.\\nWith a relatively small k, once the matrices E, \\xce\\xa3 and D are computed, measuring\\nword similarity could be very ef\\xef\\xac\\x81cient because there are often fewer nonzero dimen-\\nsions in word vectors. However, the computation of E and D can be costly because\\nfull SVD on a n \\xc3\\x97 m matrix takes O(min{m2n, mn2}) time, while the parallelization\\nof SVD is not trivial.\\n18\\n2\\nWord Representation\\nAnother algorithm for LSA is Random Indexing [34, 55]. It overcomes the dif\\xef\\xac\\x81-\\nculty of SVD-based LSA, by avoiding costly preprocessing of a huge word-document\\nmatrix. In random indexing, each document is assigned with a randomly generated\\nhigh-dimensional sparse ternary vector (called index vector). Then for each word\\nin the document, the index vector is added to the word\\xe2\\x80\\x99s vector. The index vectors\\nare supposed to be orthogonal or nearly orthogonal. This algorithm is simple and\\nscalable, which is easy to parallelize and implemented incrementally. Moreover, its\\nperformance is comparable with the SVD-based LSA, according to [55].\\n2.3.3\\nWord2vec\\nGoogle\\xe2\\x80\\x99s word2vec2 toolkit was released in 2013. It can ef\\xef\\xac\\x81ciently learn word vectors\\nfrom a large corpus. The toolkit has two models, including Continuous Bag-Of-\\nWords (CBOW) and Skip-gram. Based on the assumption that the meaning of a\\nword can be learned from its context, CBOW optimizes the embeddings so that they\\ncan predict a target word given its context words. Skip-gram, on the contrary, learns\\nthe embeddings that can predict the context words given a target word. In this section,\\nwe will introduce these two models in detail.\\n2.3.3.1\\nContinuous Bag-of-Words\\nCBOW predicts the center word given a window of context. Figure2.1 shows the\\nidea of CBOW with a window of 5 words.\\nFormally, CBOW predicts wi according to its contexts as\\nP(wi|w j(| j\\xe2\\x88\\x92i|\\xe2\\x89\\xa4l, j\\xcc\\xb8=i)) = Softmax\\n\\xe2\\x8e\\x9b\\n\\xe2\\x8e\\x9dM\\n\\xe2\\x8e\\x9b\\n\\xe2\\x8e\\x9d\\n\\x04\\n| j\\xe2\\x88\\x92i|\\xe2\\x89\\xa4l, j\\xcc\\xb8=i\\nw j\\n\\xe2\\x8e\\x9e\\n\\xe2\\x8e\\xa0\\n\\xe2\\x8e\\x9e\\n\\xe2\\x8e\\xa0,\\n(2.15)\\nwhere P(wi|w j(| j\\xe2\\x88\\x92i|\\xe2\\x89\\xa4l, j\\xcc\\xb8=i))istheprobabilityofwordwi givenitscontexts,l isthesize\\nof training contexts, M is the weight matrix in R|V |\\xc3\\x97m, V indicates the vocabulary,\\nand m is the dimension of the word vector.\\nThe CBOW model is optimized by minimizing the sum of negative log probabil-\\nities:\\nL = \\xe2\\x88\\x92\\n\\x04\\ni\\nlog P(wi|w j(| j\\xe2\\x88\\x92i|\\xe2\\x89\\xa4l, j\\xcc\\xb8=i)).\\n(2.16)\\nHere, the window size l is a hyperparameter to be tuned. A larger window size\\nmay lead to a higher accuracy as well as the more expense of the training time.\\n2https://code.google.com/archive/p/word2vec/.\\n2.3 Distributed Word Representation\\n19\\nClassifier\\nWord Matrix\\nAverage/Concatenate\\nwi\\nwi-1\\nwi+1\\nwi+2\\nW\\nW\\nW\\nwi-2\\nW\\nFig. 2.1 The architecture of CBOW model\\n2.3.3.2\\nSkip-Gram\\nOn the contrary to CBOW, Skip-gram predicts the context given the center word.\\nFigure2.2 shows the model.\\nFormally, given a word wi, Skip-gram predicts its context as\\nP(w j|wi) = softmax(Mwi)\\n\\x05\\n| j \\xe2\\x88\\x92i| \\xe2\\x89\\xa4l, j \\xcc\\xb8= i\\n\\x06\\n,\\n(2.17)\\nwhere P(w j|wi) is the probability of context word w j given wi, and M is the weight\\nmatrix. The loss function is de\\xef\\xac\\x81ned similar to CBOW as\\nL = \\xe2\\x88\\x92\\n\\x04\\ni\\n\\x04\\nj(| j\\xe2\\x88\\x92i|\\xe2\\x89\\xa4l, j\\xcc\\xb8=i)\\nP(w j|wi).\\n(2.18)\\nWord Matrix\\nClassifier\\nwi-2\\nwi-1\\nwi+1\\nwi+2\\nwi\\nW\\nFig. 2.2 The architecture of skip-gram model\\n20\\n2\\nWord Representation\\n2.3.3.3\\nHierarchical Softmax and Negative Sampling\\nTo train CBOW or Skip-gram directly is very time consuming. The most time-\\nconsuming part is the softmax layer. The conventional softmax layer needs to obtain\\nthe scores of all words even though only one word is used in computing the loss\\nfunction. An intuitive idea to improve ef\\xef\\xac\\x81ciency is to get a reasonable but much\\nfaster approximation of that word. Here, we will introduce two typical approxima-\\ntion methods which are included in the toolkit, including hierarchical softmax and\\nnegative sampling. We explain these two methods using CBOW as an example.\\nThe idea of hierarchical softmax is to build hierarchical classes for all words\\nand to estimate the probability of a word by estimating the conditional probability\\nof its corresponding hierarchical class. Figure2.3 gives an example. Each internal\\nnode of the tree indicates a hierarchical class and has a feature vector, while each\\nleaf node of the tree indicates a word. In this example, the probability of word\\nthe is p0 \\xc3\\x97 p01 while the probability of cat is p0 \\xc3\\x97 p00 \\xc3\\x97 p001. The conditional\\nprobability is computed by the feature vector of each node and the context vector.\\nFor example,\\np0 =\\nexp(w0 \\xc2\\xb7 wc)\\nexp(w0 \\xc2\\xb7 wc) + exp(w1 \\xc2\\xb7 wc),\\n(2.19)\\np1 = 1 \\xe2\\x88\\x92p0,\\n(2.20)\\nwhere wc is the context vector, w0 and w1 are the feature vectors.\\nHierarchical softmax generates the hierarchical classes according to the word\\nfrequency, i.e., a Huffman tree. By the approximation, it can compute the probability\\nof each word much faster, and the complexity of calculating the probability of each\\nword is O(log |V |).\\nNegative sampling is more straightforward. To calculate the probability of a word,\\nnegative sampling directly samples k words as negative samples according to the\\nword frequency. Then, it computes a softmax over the k + 1 words to approximate\\nthe probability of the target word.\\nFig. 2.3 An illustration of\\nhierarchical softmax\\n1\\ndog\\ncat\\nthe\\nis\\n1\\n1\\n2.3 Distributed Word Representation\\n21\\nTable 2.2 Co-occurrence probabilities and the ratio of probabilities for target words ice and\\nsteam with context word solid, gas, water, and fashion\\nProbability and ratio\\nk = solid\\nk = gas\\nk = water\\nk = f ashion\\nP(k|ice)\\n1.9e \\xe2\\x88\\x924\\n6.6e \\xe2\\x88\\x925\\n3e \\xe2\\x88\\x923\\n1.7e \\xe2\\x88\\x925\\nP(k|steam)\\n2.2e \\xe2\\x88\\x925\\n7.8e \\xe2\\x88\\x924\\n2.2e \\xe2\\x88\\x923\\n1.8e \\xe2\\x88\\x925\\nP(k|ice)/P(k|steam)\\n8.9\\n8.5e \\xe2\\x88\\x922\\n1.36\\n0.96\\n2.3.4\\nGloVe\\nMethods like Skip-gram and CBOW are shallow window-based methods. These\\nmethods scan a context window across the entire corpus, which fails to take advantage\\nof some global information. Global Vectors for Word Representation (GloVe), on the\\ncontrary, can capture corpus statistics directly.\\nAs shown in Table 2.2, the meaning of a word can be learned from the co-\\noccurrence matrix. The ratio of co-occurrence probabilities can be especially useful.\\nIn the example, the meaning of ice and water can be examined by studying the\\nratio of their co-occurrence probabilities with various probe words. For words related\\nto ice but not steam, for example, solid, the ratio P(solid|ice)/P(solid|\\nsteam) will be large. Similarly, gas is related to steam but not ice, so\\nP(gas|ice)/P(gas|steam) will be small. For words that are relevant or irrel-\\nevant to both words, the ratio is close to 1.\\nBased on this idea, GloVe models\\nF(wi, w j, \\xcb\\x9cwk) = Pik\\nPjk\\n,\\n(2.21)\\nwhere \\xcb\\x9cw \\xe2\\x88\\x88Rd are separate context word vectors, and Pi j is the probability of word\\nj to be in the context of word i, formally\\nPi j = Ni j\\nNi\\n,\\n(2.22)\\nwhere Ni j is the number of occurrences of word j in the context of word i, and\\nNi = \\x0b\\nk Nik is the number of times any word appears in the context of word j.\\nF(\\xc2\\xb7) is supposed to encode the information presented in the ratio Pik/Pjk in the\\nword vector space. To keep the inherently linear structure, F should only depend on\\nthe difference of two target words\\nF(wi \\xe2\\x88\\x92w j, \\xcb\\x9cwk) = Pik\\nPjk\\n.\\n(2.23)\\nThe arguments of F are vectors while the right side of the equation is a scalar, to\\navoid F obfuscating the linear structure, a dot product is used:\\n22\\n2\\nWord Representation\\nF\\n\\x05\\n(wi \\xe2\\x88\\x92w j)\\xe2\\x8a\\xa4\\xcb\\x9cwk\\n\\x06\\n= Pik\\nPjk\\n.\\n(2.24)\\nThe model keeps the invariance under relabeling the target word and context word.\\nIt requires F to be a homomorphism between the groups (R, +) and (R>0, \\xc3\\x97). The\\nsolution is F = exp. Then\\nw\\xe2\\x8a\\xa4\\ni \\xcb\\x9cwk = log Nik \\xe2\\x88\\x92log Ni.\\n(2.25)\\nTo keep exchange symmetry, log Ni is eliminated by adding biases bi and \\xcb\\x9cbk. The\\nmodel becomes\\nw\\xe2\\x8a\\xa4\\ni \\xcb\\x9cwk + bi + \\xcb\\x9cbk = log Nik,\\n(2.26)\\nwhich is signi\\xef\\xac\\x81cantly simpler than Eq.(2.21).\\nThe loss function is de\\xef\\xac\\x81ned as\\nL =\\n|V |\\n\\x04\\ni, j=1\\nf (Ni j)(w\\xe2\\x8a\\xa4\\ni \\xcb\\x9cw j + bi + \\xcb\\x9cb j \\xe2\\x88\\x92log Ni j),\\n(2.27)\\nwhere f (\\xc2\\xb7) is a weighting function:\\nf (x) =\\n\\x02\\n(x/xmax)\\xce\\xb1\\nif x < xmax,\\n1\\notherwise.\\n(2.28)\\n2.4\\nContextualized Word Representation\\nIn natural language, the meaning of an individual word usually relates to its context\\nin a sentence. For example,\\n\\xe2\\x80\\xa2 The central bank has slashed its forecast for economic\\ngrowth this year from 4.1 to 2.6%.\\n\\xe2\\x80\\xa2 More recently, on a blazing summer day, he took me back\\nto one of the den sites, in a slumping bank above the\\nSouth Saskatchewan River.\\nInthesetwosentences,althoughtheword bankisalwaysthesame,theirmeanings\\nare different. However, most of the traditional word embeddings (CBOW, Skip-gram,\\nGloVe, etc.) cannot well understand the different nuances of the meanings of words\\nwith the different surrounding texts. The reason is that these models only learn a\\nunique representation for each word, and therefore it is impossible for these models\\nto capture how the meanings of words change based on their surrounding contexts.\\nTo address this issue, [48] proposes ELMo, which uses a deep, bidirectional LSTM\\nmodel to build word representations. ELMo could represent each word depending\\n2.4 Contextualized Word Representation\\n23\\non the entire context in which it is used. More speci\\xef\\xac\\x81cally, rather than having a look-\\nup table of word embedding matrix, ELMo converts words into low-dimensional\\nvectors on-the-\\xef\\xac\\x82y by feeding the word and its surrounding text into a deep neural\\nnetwork. ELMo utilizes a bidirectional language model to conduct word representa-\\ntion. Formally, given a sequence of N words, (w1, w2, . . . , wN), a forward language\\nmodel (LM, the details of language model are in Sect. 4) models the probability of\\nthe sequence by predicting the probability of each word tk according to the historical\\ncontext:\\nP(w1, w2, . . . , wN) =\\nN\\n\\x03\\nk=1\\nP(wk | w1, w2, . . . , wk\\xe2\\x88\\x921).\\n(2.29)\\nThe forward LM in ELMo is a multilayer LSTM, and the jth layer of the LSTM-\\nbased forward LM will generate the context-dependent word representation \\xe2\\x88\\x92\\xe2\\x86\\x92\\nh LM\\nk, j\\nfor the word wk. The backward LM is similar to the forward LM. The only difference\\nis that it reverses the input word sequence to (wN, wN\\xe2\\x88\\x921, . . . , w1) and predicts each\\nword according to the future context:\\nP(w1, w2, . . . , wN) =\\nN\\n\\x03\\nk=1\\nP(wk | wk+1, wk+2, . . . , wN).\\n(2.30)\\nAs the same as the forward LM, the jth backward LM layer generates the repre-\\nsentations \\xe2\\x86\\x90\\xe2\\x88\\x92\\nh LM\\nk, j for the word wk.\\nELMogeneratesatask-speci\\xef\\xac\\x81cwordrepresentation,whichcombinesalllayerrep-\\nresentations of the bidirectional LM. Formally, it computes a task-speci\\xef\\xac\\x81c weighting\\nof all bidirectional LM layers:\\nwk = \\xce\\xb1task\\nL\\n\\x04\\nj=0\\nstask\\nj\\nhLM\\nk, j ,\\n(2.31)\\nwhere stask are softmax-normalized weights and \\xce\\xb1task is the weight of the entire word\\nvector for the task.\\n2.5\\nExtensions\\nBesides those very popular toolkits, such as word2vec and GloVe, various works\\nare focusing on different aspects of word representation, contributing to numerous\\nextensions. These extensions usually focus on the following directions.\\n24\\n2\\nWord Representation\\n2.5.1\\nWord Representation Theories\\nWith the success of word representation, researchers begin to explore the theories of\\nword representation. Some works attempt to give more theoretical analysis to prove\\nthe reasonability of existing tricks on word representation learning [39, 45], while\\nsome works try to discuss the new learning methods [26, 61].\\nReasonability. Word2vec and other similar tools are empirical methods of word\\nrepresentationlearning.Manytricksareproposedin[43]tolearntherepresentationof\\nwords from a large corpus ef\\xef\\xac\\x81ciently, for example, negative sampling. Considering\\nthe effectiveness of these methods, a more theoretical analysis should be done to\\nprove the reasonability of these tricks. Reference [39] gives some theoretical analysis\\nof these tricks. They formalize the Skip-gram model with negative sampling as\\nan implicit matrix factorization process. The Skip-gram model generates a word\\nembedding matrix E and a context matrix C. The size of the word embedding matrix\\nE is |V | \\xc3\\x97 m. Each row of context matrix C is a context word\\xe2\\x80\\x99s m-dimensional vector.\\nThe training process of Skip-gram is an implicit factorization of M = EC\\xe2\\x8a\\xa4. C is not\\nexplicitly considered in word2vec. This work further analyzes that the matrix M is\\nMi j = wi \\xc2\\xb7 c j = PMI(wi, c j) \\xe2\\x88\\x92log k,\\n(2.32)\\nwhere k is the number of negative samples, PMI(w, c) is the point-wise mutual\\ninformation\\nPMI(w, c) = log\\nP(w, c)\\nP(w)P(c).\\n(2.33)\\nThe shifted PMI matrix can directly be used to compare the similarity of words.\\nAnother intuitive idea is to factorize the shifted PMI matrix directly. Reference [39]\\nevaluates the performance of using the SVD matrix factorization method on the\\nimplicit matrix M. Matrix factorization achieves signi\\xef\\xac\\x81cantly better objective value\\nwhen the embedding size is smaller than 500 dimensions and the number of negative\\nsamples is 1. With more negative samples and higher embedding dimensions, Skip-\\ngram with negative sampling gets better objective value. This is because when the\\nnumber of zeros increases in M, and SVD prefers to factorize a matrix with mini-\\nmum values. With 1,000 dimensional embeddings and different numbers of negative\\nsamples in {1, 5, 15}, SVD achieves slightly better performance on word analogy and\\nword similarity. In contrast, Skip-gram with negative sampling achieves 2% better\\nperformance on syntactical analogy.\\nInterpretability. Most existing distributional word representation methods could\\ngenerate a dense real-valued vector for each word. However, the word embeddings\\nobtained by these models are hard to be interpreted. Reference [26] introduces non-\\nnegative and sparsity embeddings, where the models are interpretable and each\\ndimension indicates a unique concept. This method factorizes the corpus statistics\\nmatrix X \\xe2\\x88\\x88R|V |\\xc3\\x97|D| into a word embedding matrix E \\xe2\\x88\\x88R|V |\\xc3\\x97m and a document\\nstatistics matrix D \\xe2\\x88\\x88Rm\\xc3\\x97|D|. Its training objective is\\n2.5 Extensions\\n25\\narg min\\nE,D\\n1\\n2\\n|V |\\n\\x04\\ni=1\\n\\xe2\\x88\\xa5Xi,: \\xe2\\x88\\x92Ei,:D\\xe2\\x88\\xa52 + \\xce\\xbb\\xe2\\x88\\xa5Ei,:\\xe2\\x88\\xa51,\\ns.t. Di,:D\\xe2\\x8a\\xa4\\ni,: \\xe2\\x89\\xa41, \\xe2\\x88\\x801 \\xe2\\x89\\xa4i \\xe2\\x89\\xa4m,\\nEi, j \\xe2\\x89\\xa50, 1 \\xe2\\x89\\xa4i \\xe2\\x89\\xa4|V |, 1 \\xe2\\x89\\xa4j \\xe2\\x89\\xa4m.\\n(2.34)\\nBy iteratively optimizing E and D via gradient descent, this model can learn non-\\nnegative and sparse embeddings for words. Since the embeddings are sparse and\\nnonnegative, words with the highest scores in each dimension show high similarity,\\nwhich can be viewed as a concept of this dimension. To further improve the embed-\\ndings, this work also proposes phrasal-level constraints into the loss function. With\\nnew constraints, it could achieve both interpretability and compositionality.\\n2.5.2\\nMulti-prototype Word Representation\\nUsing only one single vector to represent a word is problematic due to the ambiguity\\nof words. A single vector cannot represent multiple meanings of a word well because\\nit may lead to semantic confusion among the different senses of this word.\\nThe multi-prototype vector space model [51] is proposed to better represent dif-\\nferent meanings of a word. In multi-prototype vector space model, a mixture of\\nvon Mises-Fisher distributions (movMF) clustering method with \\xef\\xac\\x81rst-order unigram\\ncontexts [5] is used to cluster different meanings of a word. Formally, it assigns a\\ndifferent word representation wi(x) to the same word x in each different cluster i.\\nWhen the multi-prototype embedding is used, the similarity between two words x, y\\nis computed straightforwardly. If contexts of words are not available, the similarity\\nbetween two words is de\\xef\\xac\\x81ned as\\nAvgSim(x, y) = 1\\nK 2\\nK\\n\\x04\\ni=1\\nK\\n\\x04\\nj=1\\ns(wi(x), w j(y)),\\n(2.35)\\nMaxSim(x, y) =\\nmax\\n1\\xe2\\x89\\xa4i, j\\xe2\\x89\\xa4K s(wi(x), w j(y)),\\n(2.36)\\nwhere K is a hyperparameter indicating the number of the clusters and s(\\xc2\\xb7) is a simi-\\nlarity function of two vectors such as cosine similarity. When contexts are available,\\nthe similarity can be computed more precisely as:\\nAvgSimC(x, y) = 1\\nK 2\\nK\\n\\x04\\ni=1\\nK\\n\\x04\\nj=1\\nsc,x,isc,y, js(wi(x), w j(y)),\\n(2.37)\\nMaxSimC(x, y) = s( \\xcb\\x86w(x), \\xcb\\x86w(y)),\\n(2.38)\\n26\\n2\\nWord Representation\\nwheresc,x,i = s(wi(c), wi(x))isthelikelihoodofcontextc belongingtoclusteri,and\\n\\xcb\\x86w(x) = warg max1\\xe2\\x89\\xa4i\\xe2\\x89\\xa4K sc,x,i(x) is the maximum likelihood cluster for x in context c. With\\nmulti-prototype embeddings, the accuracy on the word similarity task is signi\\xef\\xac\\x81cantly\\nimproved, but the performance is still sensitive to the number of clusters.\\nAlthough the multi-prototype embedding method can effectively cluster different\\nmeaningsofawordviaitscontexts,theclusteringisof\\xef\\xac\\x82ine,andthenumberofclusters\\nis \\xef\\xac\\x81xed and needs to be prede\\xef\\xac\\x81ned. It is dif\\xef\\xac\\x81cult for a model to select an appropriate\\namount of meanings for different words, to adapt to new senses, new words, or new\\ndata,andtoalignthesenseswithprototypes.Toaddresstheseproblems,[12]proposes\\na uni\\xef\\xac\\x81ed model for word sense representation and word sense disambiguation. This\\nmodel uses available knowledge bases such as WordNet [46] to determine the senses\\nof a word. Each word and each sense had a single vector and are trained jointly. This\\nmodel can learn representations of both words and senses, and two simple methods\\nare proposed to do disambiguation using the word and sense vectors.\\n2.5.3\\nMultisource Word Representation\\nThereis muchinformationabout words that canbeleveragedtoimprovethequalityof\\nword representations. We will introduce other kinds of word representation learning\\nmethods utilizing multisource information.\\n2.5.3.1\\nWord Representation with Internal Information\\nThere is much information locating inside words, which can be utilized to improve\\nthe quality of word representations further.\\nUsing Character Information. Many languages such as Chinese and Japanese\\nhave thousands of characters, and the words in these languages are composed of\\nseveral characters. Characters in these languages have richer semantic information\\ncomparing with other languages containing only dozens of characters. Hence, the\\nmeaning of a word can not only be learned from its contexts but also the composition\\nof characters. Driven by this intuitive idea, [13] proposes a joint learning model for\\nCharacter and Word Embeddings (CWE). In CWE, a word is a composition of a\\nword embedding and its character embeddings. Formally,\\nx = w + 1\\n|w|\\n\\x04\\ni\\nci,\\n(2.39)\\nwhere x is the representation of a word, which is the composition of a word vector w\\nand several character vectors ci, and |w| is the number of characters in the word. Note\\nthat this model can be integrated with various models such as Skip-gram, CBOW,\\nand GloVe.\\n2.5 Extensions\\n27\\nFurther, position-based and cluster-based methods are proposed to address this\\nissue that characters are highly ambiguous. In position-based approach, each char-\\nacter is assigned three vectors which appear in begin, middle and end of a word\\nrespectively. Since the meaning of a character varies when it appears in the different\\npositions of a word, this method can signi\\xef\\xac\\x81cantly resolve the ambiguity problem.\\nHowever, characters that appear in the same position may also have different mean-\\nings. In the cluster-based method, a character is assigned K different vectors for its\\ndifferent meanings, in which a word\\xe2\\x80\\x99s context is used to determine which vector to\\nbe used.\\nBy introducing character embeddings, the representation of low-frequency words\\ncan be signi\\xef\\xac\\x81cantly improved. Besides, this method can deal with new words while\\nother methods fail. Experiments show that the joint learning method can achieve bet-\\nter performance on both word similarity and word analogy tasks. By disambiguating\\ncharacters using the position-based and cluster-based method, it can further improve\\nthe performance.\\nUsing Morphology Information. Many languages such as English have rich\\nmorphology information and plenty of rare words. Most word representation models\\nassign a distinct vector to each word ignoring the rich morphology information. This\\nis a limitation because the af\\xef\\xac\\x81xes of a word can help infer the meaning of a word\\nand the morphology information of word is essential especially when facing rare\\ncontexts.\\nTo address this issue, [8] proposes to represent a word as a bag of morphology n-\\ngrams.ThismodelsubstituteswordvectorsinSkip-gramwiththesumofmorphology\\nn-gram vectors. When creating the dictionary of n-grams, they select all n-grams with\\na length greater or equal than 3 and smaller or equal than 6. To distinguish pre\\xef\\xac\\x81xes and\\nsuf\\xef\\xac\\x81xes with other af\\xef\\xac\\x81xes, they also add special characters to indicate the beginning\\nand the end of a word. This model is ef\\xef\\xac\\x81cient and straightforward, which achieves\\ngood performance on word similarity and word analogy tasks especially when the\\ntraining set is small.\\nReference [41] further uses a bidirectional LSTM to generate word representation\\nby composing morphologies. This model does not use a look-up table to assign a\\ndistinct vector to each word like what those independent word embedding methods\\nare doing. Hence, this model not only signi\\xef\\xac\\x81cantly reduces the number of parameters\\nbut also addresses some disadvantages of independent word embeddings. Moreover,\\nthe embeddings of words in this model could affect each other.\\n2.5.3.2\\nWord Representation with External Knowledge\\nBesides internal information of words, there is much external knowledge that could\\nhelp us learn the word representations.\\nUsing Knowledge Base. Some languages have rich internal information, whereas\\npeople have also annotated lots of knowledge bases which can be used in word\\nrepresentation learning to constrain embeddings. Reference [62] introduces relation\\nconstraints into the CBOW model. With these constraints, the embeddings can not\\n28\\n2\\nWord Representation\\nonly predict its contexts, but also predict words with relations. The objective is to\\nmaximize the sum of log probability of all relations as\\nO = 1\\nN\\nN\\n\\x04\\ni=1\\n\\x04\\nw\\xe2\\x88\\x88Rwi\\nlog P(w|wi),\\n(2.40)\\nwhere Rwi indicates a set of words which have relation with wi. Then the joint\\nobjective is de\\xef\\xac\\x81ned as\\nO = 1\\nN\\nN\\n\\x04\\ni=1\\nlog P(wi|w j(| j\\xe2\\x88\\x92i|<l, j\\xcc\\xb8=i)) + \\xce\\xb2\\nN\\nN\\n\\x04\\ni=1\\n\\x04\\nw\\xe2\\x88\\x88Rwi\\nlog p(w|wi),\\n(2.41)\\nwhere \\xce\\xb2 is a hyperparameter. The external information helps to train a better word\\nrepresentation, which shows signi\\xef\\xac\\x81cant improvements on word similarity bench-\\nmarks.\\nMoreover, Retro\\xef\\xac\\x81tting [19] introduces a post-processing step which can introduce\\nknowledge bases into word representation learning. It is more modular than other\\napproacheswhichconsiderknowledgebaseduringtraining.Letthewordembeddings\\nlearned by existing word representation approaches be E. Retro\\xef\\xac\\x81tting attempts to \\xef\\xac\\x81nd\\nanother embedding space \\xcb\\x86E, which is close to E but considers the relations in the\\nknowledge base. Formally,\\nL =\\n\\x04\\ni\\n\\x05\\n\\xce\\xb1i\\xe2\\x88\\xa5wi \\xe2\\x88\\x92\\xcb\\x86wi\\xe2\\x88\\xa52 +\\n\\x04\\n(i, j)\\xe2\\x88\\x88R\\n\\xce\\xb2i j\\xe2\\x88\\xa5wi \\xe2\\x88\\x92w j\\xe2\\x88\\xa52\\n\\x06\\n,\\n(2.42)\\nwhere \\xce\\xb1 and \\xce\\xb2 are hyperparameters indicating the strength of the associations, and\\nR is a set of relations in the knowledge base. The adapted embeddings \\xcb\\x86E can be\\noptimized by several iterations of the following online updates:\\n\\xcb\\x86wi =\\n\\x0b\\n{ j|(i, j)\\xe2\\x88\\x88R} \\xce\\xb2i j \\xcb\\x86w j + \\xce\\xb1iwi\\n\\x0b\\n{ j|(i, j)\\xe2\\x88\\x88R} \\xce\\xb2i j + \\xce\\xb1i\\n,\\n(2.43)\\nwhere \\xce\\xb1 is usually set to 1 and \\xce\\xb2i j is deg(i)\\xe2\\x88\\x921 (deg(\\xc2\\xb7) is a node\\xe2\\x80\\x99s degree in a knowledge\\ngraph). With knowledge bases such as the paraphrase database [27], WordNet [46]\\nand FrameNet [3], this model can achieve consistent improvement on word similarity\\ntasks. But it also may signi\\xef\\xac\\x81cantly reduce the performance on the analogy of syntactic\\nrelations. Since this module is a post-processing of word embeddings, it is compatible\\nwith various distributed representation models.\\nIn addition to the aforementioned synonym-based knowledge bases, there are also\\nsememe-based knowledge bases, in which the sememe is de\\xef\\xac\\x81ned as the minimum\\nsemantic unit of word meanings. HowNet [16] is one of such knowledge bases,\\nwhich annotates each Chinese word with one or more relevant sememes. General\\n2.5 Extensions\\n29\\nknowledge injecting methods could not apply to HowNet. As a result, [47] proposes\\na speci\\xef\\xac\\x81c model to introduce HowNet into word representation learning.\\nBases on Skip-gram model, [47] introduces sense and sememe embeddings to\\nrepresent target word wi. More speci\\xef\\xac\\x81cally, this model leverages context words,\\nwhich are represented with original word embeddings, as attention over multiple\\nsenses of target word wi to obtain its new embeddings.\\nwi =\\n|S(wi )|\\n\\x04\\nk=1\\nAtt(s(wi)\\nk\\n)s(wi)\\nk\\n,\\n(2.44)\\nwhere s(wi)\\nk\\ndenotes the kth sense embedding of wi and S(wi) is the sense set of wi.\\nThe attention term is as follows:\\nAtt(s(wi)\\nk\\n) =\\nexp(w\\xe2\\x80\\xb2\\nc \\xc2\\xb7 \\xcb\\x86s(wi)\\nk\\n)\\n\\x0b|S(wi )|\\nn=1 exp(w\\xe2\\x80\\xb2c \\xc2\\xb7 \\xcb\\x86s(wi)\\nn\\n)\\n,\\n(2.45)\\nwhere \\xcb\\x86s(wi)\\nk\\nstands for the average of sememe embeddings x, \\xcb\\x86s(wi)\\nk\\n= Avg(x(sk)) and\\nw\\xe2\\x80\\xb2\\nc is the average of context word embeddings, w\\xe2\\x80\\xb2\\nc = Avg(w j)(| j \\xe2\\x88\\x92i| \\xe2\\x89\\xa4l, j \\xcc\\xb8= i).\\nThis model shows a substantial advance in both word similarity and analogy\\ntasks. Moreover, the introduction of sense embeddings can also be used in word\\nsense disambiguation.\\nConsidering Document Information. Word embedding methods like Skip-gram\\nsimply consider the context information within a window to learn word represen-\\ntation. However, the information in the whole document could help our word rep-\\nresentation learning. Topical Word Embeddings (TWE) [42] introduces topic infor-\\nmation generated by Latent Dirichlet Allocation (LDA) to help distinguish different\\nmeanings of a word. The model is de\\xef\\xac\\x81ned to maximize the following average log\\nprobability:\\nO = 1\\nN\\nN\\n\\x04\\ni=1\\n\\x04\\n\\xe2\\x88\\x92k\\xe2\\x89\\xa4c\\xe2\\x89\\xa4k,c\\xcc\\xb8=0\\n(log P(wi+c|wi) + log P(wi+c|zi)) ,\\n(2.46)\\nwhere wi is the word embedding and zi is the topic embedding of wi. Each word wi\\nis assigned a unique topic, and each topic has a topic embedding. The topical word\\nembedding model shows advantages of contextual word similarity and document\\nclassi\\xef\\xac\\x81cation tasks.\\nHowever, TWE simply combines the LDA with word embeddings and lacks statis-\\ntical foundations. The LDA topic model needs numerous documents to learn seman-\\ntically coherent topics. Reference [40] further proposes the TopicVec model, which\\nencodes words and topics in the same semantic space. TopicVec outperforms TWE\\nand other word embedding methods on text classi\\xef\\xac\\x81cation datasets. It can learn coher-\\nent topics on only one document which is not possible for other topic models.\\n30\\n2\\nWord Representation\\n2.5.3.3\\nWord Representation with Hierarchical Structure\\nHuman knowledge is in a hierarchical structure. Recently, many works also introduce\\na hierarchical structure of texts into word representation learning.\\nDependency-based Word Representation. Continuous word embeddings are\\ncombinations of semantic and syntactic information. However, existing word repre-\\nsentation models depend solely on linear contexts and show more semantic infor-\\nmation than syntactic information. To make the embeddings show more syntactic\\ninformation, the dependency-based word embedding [38] uses the dependency-based\\ncontext. The dependency-based embeddings are less topical and exhibit more func-\\ntional similarity than the original Skip-gram embeddings. It takes the information of\\ndependency parsing tree into consideration when learning word representations. The\\ncontexts of a target word w are the modi\\xef\\xac\\x81ers of this word, i.e., (m1,r1), . . . , (mk,rk),\\nwhere ri is the type of the dependency relation between the head node and the mod-\\ni\\xef\\xac\\x81er. When training, the model optimizes the probability of dependency-based con-\\ntexts rather than neighboring contexts. This model gains some improvements on\\nword similarity benchmarks compared with Skip-gram. Experiments also show that\\nwords with syntactic similarity are more similar in the vector space.\\nSemantic Hierarchies. Because of the linear substructure of the vector space,\\nit is proven that word embeddings can make simple analogies. For example, the\\ndifference between Japan and Tokyo is similar to the difference between China\\nand Beijing. But it has trouble identifying hypernym-hyponym relations since\\nthese relationships are complicated and do not necessarily have linear substructure.\\nTo address this issue, [25] tries to identify hypernym-hyponym relationships using\\nword embeddings. The basic idea is to learn a linear projection rather than simply\\nuse the embedding offset to represent the relationship. The model optimizes the\\nprojection as\\nM\\xe2\\x88\\x97= arg min\\nM\\n1\\nN\\n\\x04\\n(i, j)\\n\\xe2\\x88\\xa5Mxi \\xe2\\x88\\x92y j\\xe2\\x88\\xa52,\\n(2.47)\\nwhere xi and y j are hypernym and hyponym embeddings.\\nTo further increase the capability of the model, they propose to \\xef\\xac\\x81rst cluster word\\npairs into several groups and learn a linear projection for each group. The linear\\nprojection can help identify various hypernym-hyponym relations.\\n2.5.4\\nMultilingual Word Representation\\nThere are thousands of languages in the world. In word level, how to represent words\\nfrom different languages in a uni\\xef\\xac\\x81ed vector space is an interesting problem. The\\nbilingual word embedding model [64] uses machine translation word alignments as\\nconstraining translational evidence and embeds words of two languages into a single\\nvector space. The basic idea is (1) to initialize each word according to its aligned\\n2.5 Extensions\\n31\\nwords in another language and (2) to constrain the distance between two languages\\nduring the training using translation pairs.\\nWhen learning bilingual word embeddings, it \\xef\\xac\\x81rstly trains source word embed-\\ndings. Then they use aligned sentence pairs to count the co-occurrence of source and\\ntarget words. The target word embeddings can be initialized as\\nEt\\xe2\\x88\\x92init =\\nS\\n\\x04\\ns=1\\nNts + 1\\nNt + S Es,\\n(2.48)\\nwhere Es and Et\\xe2\\x88\\x92init are the trained embeddings of the source word and the initial\\nembedding of the target word, respectively. Nts is the number of target words being\\naligned with source word. S is all the possible alignments of word t. So Nt + S\\nnormalizes the weights as a distribution. During the training, they jointly optimize\\nthe word embedding objective as well as the bilingual constraint. The constraint is\\nde\\xef\\xac\\x81ned as\\nLcn\\xe2\\x86\\x92en = \\xe2\\x88\\xa5Een \\xe2\\x88\\x92Nen\\xe2\\x86\\x92cnEcn\\xe2\\x88\\xa52,\\n(2.49)\\nwhere Nen\\xe2\\x86\\x92cn is the normalized align counts.\\nWhen given a lexicon of bilingual word pairs, [44] proposes a simple model that\\ncan learn bilingual word embeddings in a uni\\xef\\xac\\x81ed space. Based on the distributional\\ngeometric similarities of word vectors of two languages, this model learns a linear\\ntransformation matrix T that transforms the vector space of source language to that\\nof the target language. The training loss is\\nL = \\xe2\\x88\\xa5TEs \\xe2\\x88\\x92Et\\xe2\\x88\\xa52,\\n(2.50)\\nwhere Et is the word vector matrix of aligned words in target language.\\nHowever, this model performs badly when the seed lexicon is small. To tackle\\nthis limitation, some works introduce the idea of bootstrapping into bilingual word\\nrepresentation learning. Let\\xe2\\x80\\x99s take [63] for example. In this work, in addition to\\nmonolingual word embedding learning and bilingual word embedding alignment\\nbased on seed lexicon, a new matching mechanism is introduced. The main idea of\\nmatching is to \\xef\\xac\\x81nd the most probably matched source (target) word for each target\\n(source) word and make their embeddings closer. Next, we explain the target-to-\\nsource matching process formally, and the source-to-target side is similar.\\nThe target-to-source matching loss function is de\\xef\\xac\\x81ned as\\nLT 2S = \\xe2\\x88\\x92log P\\n\\x05\\nC(T )|E(S)\\x06\\n= \\xe2\\x88\\x92log\\n\\x04\\nm\\nP\\n\\x05\\nC(T ), m|E(S)\\x06\\n,\\n(2.51)\\nwhere C(T ) denotes the target corpus and m is a latent variable specifying the matched\\nsource word for each target word. On independency assumption, it has\\n32\\n2\\nWord Representation\\nP\\n\\x05\\nC(T ), m|E(S)\\x06\\n=\\n\\x03\\nw(T )\\ni\\n\\xe2\\x88\\x88C(T )\\nP\\n\\x0c\\nw(T )\\ni\\n, m|E(S)\\r\\n=\\n|V (T )|\\n\\x03\\ni=1\\nP\\n\\x0c\\nw(T)\\ni\\n|w(S)\\nmi\\n\\rNw(T )\\ni\\n, (2.52)\\nwhere Nw(T )\\ni\\nis the number of w(T )\\ni\\noccurrences in the target corpus. By training using\\nViterbi EM algorithm, this method can improve bilingual word embeddings on its\\nown and address the limitation of a small seed lexicon.\\n2.5.5\\nTask-Speci\\xef\\xac\\x81c Word Representation\\nIn recent years, word representation learning has achieved great success and played\\na crucial role in NLP tasks. People \\xef\\xac\\x81nd that word representation learning of the\\ngeneral \\xef\\xac\\x81eld is still a limitation in a speci\\xef\\xac\\x81c task and begin to explore the learning\\nof task-speci\\xef\\xac\\x81c word representation. In this section, we will take sentiment analysis\\nas an example.\\nWord Representation for Sentiment Analysis. Most word representation meth-\\nods capture syntactic and semantic information while ignoring sentiment of text. This\\nis problematic because words with similar syntactic polarity but opposite sentiment\\npolarity obtain closed word vectors. Reference [58] proposes to learn Sentiment-\\nSpeci\\xef\\xac\\x81c Word Embeddings (SSWE) by integrating the sentiment information. An\\nintuitive idea is to jointly optimize the sentiment classi\\xef\\xac\\x81cation model using word\\nembeddings as its feature and SSWE minimizes the cross-entropy loss to achieve\\nthis goal. To better combine the unsupervised word embedding method and the super-\\nviseddiscriminativemodel,theyfurtherusethewordsinawindowratherthanawhole\\nsentence to classify sentiment polarity. They propose the following ranking-based\\nloss:\\nLr(t) = max(0, 1 \\xe2\\x88\\x921s(t) f r\\n0 (t) + 1s(t) f r\\n1 (t)),\\n(2.53)\\nwhere f r\\n0 , f r\\n1 are the predicted positive and negative scores. 1s(t) is an indicator\\nfunction:\\n1s(t) =\\n\\x02\\n1\\nif t is positive,\\n\\xe2\\x88\\x921 if t is negative.\\n(2.54)\\nThis loss function only punishes the model when the model gives an incorrect\\nresult.\\nTo get massive training data, they use distant-supervision technology to gener-\\nate sentiment labels for a document. The increase of labeled data can improve the\\nsentiment information in word embeddings. On sentiment classi\\xef\\xac\\x81cation tasks, senti-\\nment embeddings outperform other strong baselines including SVM and other word\\nembedding methods. SSWE also shows strong polarity consistency, where the clos-\\nest words of a word are more likely to have the same sentiment polarity compared\\n2.5 Extensions\\n33\\nwith existing word representation models. This sentiment speci\\xef\\xac\\x81c word embedding\\nmethod provides us a general way to learn task-speci\\xef\\xac\\x81c word embeddings, which is\\nto design a joint loss function and to generate massive labeled data automatically.\\n2.5.6\\nTime-Speci\\xef\\xac\\x81c Word Representation\\nThe meaning of a word changes during the time. Analyzing the changing meaning\\nof a word is an exciting topic in both linguistic and NLP research. With the rise\\nof word embedding methods, some works [29, 35] use embeddings to analyze the\\nchange of words\\xe2\\x80\\x99 meanings. They separate corpus into bins with respect to years\\nto train time-speci\\xef\\xac\\x81c word embeddings and compare embeddings of different time\\nseries to analyze the change of word semantics. This method is intuitive but has some\\nproblems. Dividing corpus into bins causes the data sparsity issue. The objective of\\nword embedding methods is nonconvex so that different random initialization leads\\nto different results, which makes comparing word embeddings dif\\xef\\xac\\x81cult. Embeddings\\nof a word in different years are in different semantic spaces and cannot be compared\\ndirectly. Most work indirectly compares the meanings of a word in a different time\\nby the changes of a word\\xe2\\x80\\x99s closest words in the semantic space.\\nTo address these issues, [4] proposes a dynamic Skip-gram model which connects\\nseveral Bayesian Skip-gram models [6] using Kalman \\xef\\xac\\x81lters [33]. In this model, the\\nembeddings of words in different periods could affect each other. For example, a word\\nthat appears in the 1990s\\xe2\\x80\\x99 document can affect the embeddings of that word in the\\n1980s and 2000s. Moreover, it also trains the embedding in different periods by the\\nwholecorpustoreducethesparsityissue.Thismodelalsoputsalltheembeddingsinto\\nthe same semantic space, which is a signi\\xef\\xac\\x81cant improvement against other methods\\nand makes word embeddings in different periods comparable. Therefore, the change\\nof word embeddings in this model is continuous and smooth. Experimental results\\nshow that the cosine distance between two words changes much more smoothly in\\nthis model than those models which simply divide the corpus into bins.\\n2.6\\nEvaluation\\nIn recent years, various methods to embed words into a vector space have been\\nproposed. Hence, it is essential to evaluate different methods. There are two gen-\\neral evaluations of word embeddings, including word similarity and word analogy.\\nThey both aim to check if the word distribution is reasonable. These two evaluations\\nsometimes give different results. For example, CBOW achieves better performance\\non word similarity, whereas Skip-gram outperforms CBOW on word analogy. There-\\nfore, which method to choose depends on the high-level application. Task-speci\\xef\\xac\\x81c\\nword embedding methods are usually designed for speci\\xef\\xac\\x81c high-level tasks and\\n34\\n2\\nWord Representation\\nachieve signi\\xef\\xac\\x81cant improvement on these tasks compared with baselines such as\\nCBOW and Skip-gram. However, they only marginally outperform baselines on two\\ngeneral evaluations.\\n2.6.1\\nWord Similarity/Relatedness\\nThe dynamics of words are very complex and subtle. There is no static, \\xef\\xac\\x81nite set of\\nrelations that can describe all interactions between two words. It is also not trivial for\\ndownstream tasks to leverage different kinds of word relations. A more practical way\\nis to assign a score to a pair of words to represent to what extent they are related. This\\nmeasurement is called word similarity. When talking about the term word similarity,\\nthe precise meaning may vary a lot in different situations. There are several kinds of\\nsimilarity that may be referred to in various literature.\\nMorphological similarity. Many languages including English de\\xef\\xac\\x81ne morphol-\\nogy. The same morpheme can have multiple surface forms according to the syntac-\\ntical function. For example, the word active is an adjective and activeness\\nis its noun version. The word activate is a verb and activation is its noun\\nversion. The morphology is an important dimension when considering the meaning\\nand usage of words. It de\\xef\\xac\\x81nes some relations between words from a syntactical view.\\nSome relations are used in the Syntactic Word Relationship test set [43], including\\nadjectives to adverbs, past tense, and so on. However, in many higher level applica-\\ntions and tasks, the words are often morphologically normalized by the base form\\n(this process is also known as lemmatization). One widely used technique is the\\nPorter stemming algorithm [49]. This algorithm converts active, activeness,\\nactivate, and activation to the same root format activ. By removing mor-\\nphological features, the semantic meaning of words is more emphasized.\\nSemantic Similarity. Two words are semantically similar if they can express\\nthe same concept, or sense, like article and document. One word may have\\ndifferent senses, and each of its synonyms is associated with one or more of its senses.\\nWordNet [46] is a lexical database that organizes the words as groups according to the\\nsenses. Each group of words is called a synset, which contains all synonymous words\\nsharing the same speci\\xef\\xac\\x81c sense. The words within the same synset are considered\\nsemantically similar. Words from two synsets that are linked by some certain relation\\n(such as hyponym) are also considered semantically similar to some degree, like\\nbank(river) and bank\\n\\x05\\nbank(river)is the hyponym of bank\\n\\x06\\n.\\nSemantic relatedness. Most modern literature that considers word similarity\\nrefers to the semantic relatedness of words. Semantic relatedness is more general\\nthan semantic similarity. Words that are not semantically similar could still be related\\nin many ways such as meronymy (car and wheel) or antonymy (hot and cold).\\nSemantic relatedness often yields co-occurrence, but they are not equivalent. The\\nsyntactic structure could also yield co-occurrence. Reference [10] argues that distri-\\nbutional similarity is not an adequate proxy for semantic relatedness.\\n2.6 Evaluation\\n35\\nTable 2.3 Datasets for evaluating word similarity/relatedness\\nDataset\\nSimilarity Type\\nRG-65 [52]\\nWord Similarity\\nWordSim-353 [22]\\nMixed\\nWordSim-353 REL [1]\\nWord Relatedness\\nWordSim-353 SIM [1]\\nWord Similarity\\nMTurk-287 [50]\\nWord Relatedness\\nSimLex-999 [31]\\nWord Similarity\\nToevaluatethewordrepresentationsystemintrinsically,themostpopularapproach\\nis to collect a set of word pairs and compute the correlation between human judg-\\nment and system output. So far, many datasets are collected and made public. Some\\ndatasets focus on the word similarity, such as RG-65 [52] and SimLex-999 [31].\\nOther datasets concern word relatedness, such as MTurk [50]. WordSim-353 [22] is\\na very popular dataset for word representation evaluation, but its annotation guideline\\ndoes not differentiate similarity and relatedness very clearly. Reference [1] conducts\\nanother round of annotation based on WordSim-353 and generates two subsets, one\\nfor similarity and the other for relatedness. Some information about these datasets is\\nsummarized in Table 2.3.\\nTo evaluate the similarity of two distributed word vectors, researchers usually\\nselect cosine similarity as an evaluation metric. The cosine similarity of word w and\\nword v is de\\xef\\xac\\x81ned as\\nsim(w, v) =\\nw \\xc2\\xb7 v\\n\\xe2\\x88\\xa5w\\xe2\\x88\\xa5\\xe2\\x88\\xa5v\\xe2\\x88\\xa5.\\n(2.55)\\nWhen evaluating a word representation approach, the similarity of each word pair\\nis computed in advance using cosine similarity. After that, Spearman\\xe2\\x80\\x99s correlation\\ncoef\\xef\\xac\\x81cient \\xcf\\x81 is then used to evaluate the similarity between human annotator and\\nword representation model as\\n\\xcf\\x81 = 1 \\xe2\\x88\\x926 \\x0b d2\\ni\\nn3 \\xe2\\x88\\x92n ,\\n(2.56)\\nwhere a higher Spearman\\xe2\\x80\\x99s correlation coef\\xef\\xac\\x81cient indicates they are more similar.\\nReference [10] describes a series of methods based on WordNet to evaluate the\\nsimilarity of a pair of words. After the comparison between the traditional WordNet-\\nbased methods and distributed word representations, [1] addresses that relatedness\\nandsimilarityaretwodifferentconcerns.TheypointoutthatWordNet-basedmethods\\nperform better on similarity than on relatedness, while distributed word representa-\\ntion shows similar performance on both. A series of distributed word representations\\nare compared on a wide variety of datasets in [56]. The state-of-the-art on both\\nsimilarity and relatedness is achieved by distributed representation, without a doubt.\\n36\\n2\\nWord Representation\\nThis evaluation method is simple and straightforward. However, as stated in [20],\\nthere are several problems with this evaluation. Since the datasets are small (less than\\n1,000 word pairs in each dataset), one system may yield many different scores on\\ndifferent partitions. Testing on the whole dataset makes it easier to over\\xef\\xac\\x81t and hard\\nto compute the statistical signi\\xef\\xac\\x81cance. Moreover, the performance of a system on\\nthese datasets may not be very correlated to its performance on downstream tasks.\\nThe word similarity measurement can come in an alternative format, the TOEFL\\nsynonyms test. In this test, a cue word is given, and the test is required to choose one\\nfrom four words that are the synonym of the cue word. The exciting part of this task is\\nthat the performance of a system could be compared with human beings. Reference\\n[37] evaluates the system with the TOEFL synonyms test to address the knowledge\\ninquiring and representing of LSA. The reported score is 64.4%, which is very close\\nto the average rating of the human test-takers. On this test set with 80 queries, [54]\\nreported a score of 72.0%. Reference [24] extends the original dataset with the help\\nof WordNet and generates a new dataset3 (named WordNet-based synonymy test)\\ncontaining thousands of queries.\\n2.6.2\\nWord Analogy\\nBesides word similarity, the word analogy task is an alternative way to measure\\nhow well representations capture semantic meanings of words. This task gives three\\nwords w1, w2, and w3, then it requires the system to predict a word w4 such that\\nthe relation between w1 and w2 is the same as that between w3 and w4. This task is\\nused since [43, 45] to exploit the structural relationships among words. Here, the\\nword relations could be divided into two categories, including semantic relations\\nand syntactic relations. This is a relatively novel method for word representation\\nevaluation but quickly becomes a standard evaluation metric since the dataset is\\nreleased. Unlike the TOEFL synonyms test, most words in this dataset are frequent\\nacrossallkindsofthecorpus,butthefourthwordischosenfromthewholevocabulary\\ninstead of four options. This test favors distributed word representations because it\\nemphasizes the structure of word space.\\nThe comparison between different models on the word analogy task measured by\\naccuracy could be found in [7, 56, 57, 61].\\n2.7\\nSummary\\nIn this chapter, we \\xef\\xac\\x81rst introduce word representation methods, including one-hot\\nrepresentation and various distributed representation methods. These classical meth-\\nods are the important foundation of various NLP models, and meanwhile present the\\n3http://www.cs.cmu.edu/~dayne/wbst-nanews.tar.gz.\\n2.7 Summary\\n37\\nmajor concepts and mechanisms of word representation learning for the reader. Next,\\nconsidering classical word representation methods often suffer from the word poly-\\nsemy, we further introduce the effective contextualized word representation methods\\nELMo, to show the approach to capture complex word features across different\\nlinguistic contexts. As word representation methods are widely utilized in various\\ndownstream tasks, we then overview numerous extensions toward some representa-\\ntive directions and discuss how to adapt word representations for speci\\xef\\xac\\x81c scenarios.\\nFinally, we introduce several evaluation tasks of word representation, including word\\nsimilarity and word analogy, which are the basic experimental settings for research-\\ning word representation methods.\\nIn the past decade, learning methods and applications of word representation\\nhave been studied in depth. Here we recommend some surveys and books on word\\nrepresentative learning for reading:\\n\\xe2\\x80\\xa2 Erk. Vector Space Models of Word Meaning and Phrase Meaning: A Survey [18].\\n\\xe2\\x80\\xa2 Lai et al. How to Generate a Good Word Embedding [36].\\n\\xe2\\x80\\xa2 Camacho et al. From Word to Sense Embeddings: A Survey on Vector Represen-\\ntations of Meaning [11].\\n\\xe2\\x80\\xa2 Ruder et al. A Survey of Cross-lingual Word Embedding Models [53].\\n\\xe2\\x80\\xa2 Bakarov. A Survey of Word Embeddings Evaluation Methods [2].\\nIn the future, toward more effective word representation learning, some directions\\nare requiring further efforts:\\n(1) Utilizing More Knowledge. Current word representation learning models focus\\non representing words based on plain textual corpora. In fact, besides rich\\nsemantic information in text, there are also various kinds of word-related infor-\\nmation hidden in heterogeneous knowledge in the real world, such as visual\\nknowledge, factual knowledge, and commonsense knowledge. Some prelimi-\\nnary explorations have attempted [59, 60] to utilize heterogeneous knowledge\\nfor learning better word representations, and these explorations indicate that\\nutilizing more knowledge is a promising direction toward enhancing word rep-\\nresentations. There remain open problems for further explorations.\\n(2) Considering More Contexts. As shown in this chapter, those word representa-\\ntion learning methods considering contexts can achieve more expressive word\\nembeddings, which can grasp richer semantic information and further bene-\\n\\xef\\xac\\x81t downstream NLP tasks than classical distributed methods. Context-aware\\nword representations have been systematically veri\\xef\\xac\\x81ed for their effectiveness\\nin existing works [32, 48], and adopting those context-aware word representa-\\ntions has also become a necessary and mainstream operation for various NLP\\ntasks. After BERT [15] has been proposed, language models pretrained on large-\\nscale corpora have entered the public vision and their \\xef\\xac\\x81ne-tuning models have\\nalso achieved the state-of-the-art performance on speci\\xef\\xac\\x81c NLP tasks. These new\\nexplorations based on large-scale textual corpora and pretrained \\xef\\xac\\x81ne-tuning lan-\\nguage representation architectures indicate a promising direction to consider\\nmore contexts with more powerful representation architectures, and we will dis-\\ncuss them more in the next chapter.\\n38\\n2\\nWord Representation\\n(3) Orienting Finer Granularity. Polysemy is a widespread phenomenon for\\nwords. Hence, it is essential and meaningful to consider the \\xef\\xac\\x81ner granulated\\nsemantic information than the words themselves. As some linguistic knowledge\\nbases have been developed, such as synonym-based knowledge bases Word-\\nNet [21] and sememe-based knowledge bases HowNet [17], we thus have ways\\nto study the atomic semantics of words. The current work on word representa-\\ntions learning is coarse-grained, and mainly focuses on shallow semantics of the\\nwords themselves in text, and ignores the rich semantic information inside the\\nwords, which is also an important resource for achieving better word embed-\\ndings. Reference [28] explores to inject \\xef\\xac\\x81ner granulated atomic semantics of\\nwords into word representations and performs much better language understand-\\ning. Although these explorations are still preliminary, orienting \\xef\\xac\\x81ner granularity\\nof word representations is important. In the next chapter, we will also introduce\\nmore details in this part.\\nIn the past decade, learning methods and applications of distributed representation\\nhave been studied in depth. Because of its ef\\xef\\xac\\x81ciency and effectiveness, lots of task-\\nspeci\\xef\\xac\\x81c models have been proposed for various tasks. Word representation learning\\nhas become a popular and important topic in NLP. However, word representation\\nlearning is still challenging due to its ambiguity, data sparsity, and interpretability. In\\nrecent years, word representation learning has been no longer studied in isolation, but\\nexplored together with sentence or document representation learning using pretrained\\nlanguage models. Readers are recommended to refer to the following chapters to\\nfurther learn the integration of word representations in other scenarios.\\nReferences\\n1. Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pa\\xc2\\xb8sca, and Aitor Soroa.\\nA study on similarity and relatedness using distributional and wordnet-based approaches. In\\nProceedings of HLT-NAACL, 2009.\\n2. Amir\\nBakarov.\\nA\\nsurvey\\nof\\nword\\nembeddings\\nevaluation\\nmethods.\\narXiv\\npreprint arXiv:1801.09536, 2018.\\n3. Collin F Baker, Charles J Fillmore, and John B Lowe. The berkeley framenet project. In\\nProceedings of ACL, 1998.\\n4. Robert Bamler and Stephan Mandt. Dynamic word embeddings via skip-gram \\xef\\xac\\x81ltering. arXiv\\npreprint arXiv:1702.08359, 2017.\\n5. Arindam Banerjee, Inderjit S Dhillon, Joydeep Ghosh, and Suvrit Sra. Clustering on the unit\\nhypersphere using von mises-\\xef\\xac\\x81sher distributions. Journal of Machine Learning Research, 2005.\\n6. Oren Barkan. Bayesian neural word embedding. In Proceedings of AAAI, 2017.\\n7. Marco Baroni, Georgiana Dinu, and Germ\\xc3\\xa1n Kruszewski. Dont count, predict a systematic\\ncomparison of context-counting vs. context-predicting semantic vectors. In Proceedings of\\nACL, 2014.\\n8. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors\\nwith subword information. Transactions of the Association for Computational Linguistics,\\n5:135\\xe2\\x80\\x93146, 2017.\\n9. Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai.\\nClass-based n-gram models of natural language. Computational linguistics, 18(4):467\\xe2\\x80\\x93479,\\n1992.\\nReferences\\n39\\n10. AlexanderBudanitskyandGraeme Hirst.Evaluatingwordnet-basedmeasuresoflexical seman-\\ntic relatedness. Computational Linguistics, 32(1):13\\xe2\\x80\\x9347, 2006.\\n11. Jose Camacho-Collados and Mohammad Taher Pilehvar. From word to sense embeddings:\\nA survey on vector representations of meaning. Journal of Arti\\xef\\xac\\x81cial Intelligence Research,\\n63:743\\xe2\\x80\\x93788, 2018.\\n12. Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. A uni\\xef\\xac\\x81ed model for word sense representation\\nand disambiguation. In Proceedings of EMNLP, 2014.\\n13. Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huanbo Luan. Joint learning of\\ncharacter and word embeddings. In Proceedings of IJCAI, 2015.\\n14. Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A.\\nHarshman. Indexing by latent semantic analysis. Japan Analytical & Scienti\\xef\\xac\\x81c Instruments\\nShow, 41(6):391\\xe2\\x80\\x93407, 1990.\\n15. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019.\\n16. Zhendong Dong and Qiang Dong. Hownet-a hybrid language and knowledge resource. In\\nProceedings of NLP-KE, 2003.\\n17. Zhendong Dong and Qiang Dong. HowNet and the Computation of Meaning (With CD-Rom).\\nWorld Scienti\\xef\\xac\\x81c, 2006.\\n18. Katrin Erk. Vector space models of word meaning and phrase meaning: A survey. Language\\nand Linguistics Compass, 6(10):635\\xe2\\x80\\x93653, 2012.\\n19. Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A\\nSmith. Retro\\xef\\xac\\x81tting word vectors to semantic lexicons. In Proceedings of NAACL-HLT, 2015.\\n20. ManaalFaruqui,YuliaTsvetkov,PushpendreRastogi,andChrisDyer.Problemswithevaluation\\nof word embeddings using word similarity tasks. In Proceedings of RepEval, 2016.\\n21. Christiane Fellbaum. Wordnet. The encyclopedia of applied linguistics, 2012.\\n22. Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman,\\nand Eytan Ruppin. Placing search in context: The concept revisited. In Proceedings of WWW,\\n2001.\\n23. John R Firth. A synopsis of linguistic theory, 1930\\xe2\\x80\\x931955. 1957.\\n24. Dayne Freitag, Matthias Blume, John Byrnes, Edmond Chow, Sadik Kapadia, Richard Rohwer,\\nand Zhiqiang Wang. New experiments in distributional representations of synonymy. In Pro-\\nceedings of CoNLL, 2005.\\n25. Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, Haifeng Wang, and Ting Liu. Learning semantic\\nhierarchies via word embeddings. In Proceedings of ACL, 2014.\\n26. Alona Fyshe, Leila Wehbe, Partha Pratim Talukdar, Brian Murphy, and Tom M Mitchell. A\\ncompositional and interpretable semantic space. In Proceedings of HLT-NAACL, 2015.\\n27. Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. Ppdb: The paraphrase\\ndatabase. In Proceedings of HLT-NAACL, 2013.\\n28. Yihong Gu, Jun Yan, Hao Zhu, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Fen Lin, and Leyu\\nLin. Language modeling with sparse product of sememe experts. In Proceedings of EMNLP,\\npages 4642\\xe2\\x80\\x934651, 2018.\\n29. William L Hamilton, Jure Leskovec, and Dan Jurafsky. Diachronic word embeddings reveal\\nstatistical laws of semantic change. In Proceedings of ACL, 2016.\\n30. Zellig S Harris. Distributional structure. Word, 10(2\\xe2\\x80\\x933):146\\xe2\\x80\\x93162, 1954.\\n31. Felix Hill, Roi Reichart, and Anna Korhonen. Simlex-999: Evaluating semantic models with\\n(genuine) similarity estimation. Computational Linguistics, 2015.\\n32. Jeremy Howard and Sebastian Ruder. Universal language model \\xef\\xac\\x81ne-tuning for text classi\\xef\\xac\\x81-\\ncation. In Proceedings of ACL, pages 328\\xe2\\x80\\x93339, 2018.\\n33. Rudolph Emil Kalman et al. A new approach to linear \\xef\\xac\\x81ltering and prediction problems. Journal\\nof Basic Engineering, 82(1):35\\xe2\\x80\\x9345, 1960.\\n34. Pentti Kanerva, Jan Kristofersson, and Anders Holst. Random indexing of text samples for\\nlatent semantic analysis. In Proceedings of CogSci, 2000.\\n35. Yoon Kim, Yi-I Chiu, Kentaro Hanaki, Darshan Hegde, and Slav Petrov. Temporal analysis of\\nlanguage through neural language models. In Proceedings of the ACL Workshop, 2014.\\n40\\n2\\nWord Representation\\n36. Siwei Lai, Kang Liu, Shizhu He, and Jun Zhao. How to generate a good word embedding.\\nIEEE Intelligent Systems, 31(6):5\\xe2\\x80\\x9314, 2016.\\n37. Thomas K Landauer and Susan T Dumais. A solution to plato\\xe2\\x80\\x99s problem: The latent seman-\\ntic analysis theory of acquisition, induction, and representation of knowledge. Psychological\\nreview, 104(2):211, 1997.\\n38. Omer Levy and Yoav Goldberg. Dependency-based word embeddings. In Proceedings of ACL,\\n2014.\\n39. Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In\\nProceedings of NeurIPS, 2014.\\n40. Shaohua Li, Tat-Seng Chua, Jun Zhu, and Chunyan Miao. Generative topic embedding: a\\ncontinuous representation of documents. In Proceedings of ACL, 2016.\\n41. Wang Ling, Chris Dyer, Alan W Black, Isabel Trancoso, Ram\\xc3\\xb3n Fermandez, Silvio Amir, Luis\\nMarujo, and Tiago Lu\\xc3\\xads. Finding function in form: Compositional character models for open\\nvocabulary word representation. In Proceedings of EMNLP, 2015.\\n42. Yang Liu, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Topical word embeddings. In\\nProceedings of AAAI, 2015.\\n43. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\\xef\\xac\\x81cient estimation of word\\nrepresentations in vector space. In Proceedings of ICLR, 2013.\\n44. Tomas Mikolov, Quoc V Le, and Ilya Sutskever. Exploiting similarities among languages for\\nmachine translation. arXiv preprint arXiv:1309.4168, 2013.\\n45. Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space\\nword representations. In Proceedings of HLT-NAACL, 2013.\\n46. George A Miller. Wordnet: a lexical database for english. Communications of the ACM,\\n38(11):39\\xe2\\x80\\x9341, 1995.\\n47. Yilin Niu, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Improved word representation learn-\\ning with sememes. In Proceedings of ACL, 2017.\\n48. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\\nand Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of NAACL-\\nHLT, pages 2227\\xe2\\x80\\x932237, 2018.\\n49. Martin F Porter. An algorithm for suf\\xef\\xac\\x81x stripping. Program, 14(3):130\\xe2\\x80\\x93137, 1980.\\n50. Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich, and Shaul Markovitch. A word at a\\ntime: computing word relatedness using temporal semantic analysis. In Proceedings of WWW,\\n2011.\\n51. Joseph Reisinger and Raymond J Mooney. Multi-prototype vector-space models of word mean-\\ning. In Proceedings of HLT-NAACL, 2010.\\n52. Herbert Rubenstein and John B Goodenough. Contextual correlates of synonymy. Communi-\\ncations of the ACM, 8(10):627\\xe2\\x80\\x93633, 1965.\\n53. Sebastian Ruder, Ivan Vuli\\xc2\\xb4c, and Anders S\\xc3\\xb8gaard. A survey of cross-lingual word embedding\\nmodels. Journal of Arti\\xef\\xac\\x81cial Intelligence Research, 65:569\\xe2\\x80\\x93631, 2019.\\n54. Magnus Sahlgren. Vector-based semantic analysis: Representing word meanings based on\\nrandom labels. In Proceedings of Workshop on SKAC, 2001.\\n55. Magnus Sahlgren. An introduction to random indexing. In Proceedings of TKE, 2005.\\n56. Tobias Schnabel, Igor Labutov, David Mimno, and Thorsten Joachims. Evaluation methods for\\nunsupervised word embeddings. In Proceedings of EMNLP, 2015.\\n57. Fei Sun, Jiafeng Guo, Yanyan Lan, Jun Xu, and Xueqi Cheng. Learning word representations\\nby jointly modeling syntagmatic and paradigmatic relations. In Proceedings of ACL, 2015.\\n58. Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. Learning sentiment-\\nspeci\\xef\\xac\\x81c word embedding for twitter sentiment classi\\xef\\xac\\x81cation. In Proceedings of ACL, 2014.\\n59. KristinaToutanova,DanqiChen,PatrickPantel,HoifungPoon,PallaviChoudhury,andMichael\\nGamon. Representing text for joint embedding of text and knowledge bases. In Proceedings of\\nthe EMNLP, pages 1499\\xe2\\x80\\x931509, 2015.\\n60. Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph and text jointly\\nembedding. In Proceedings of EMNLP, pages 1591\\xe2\\x80\\x931601, 2014.\\nReferences\\n41\\n61. DaniYogatama,ManaalFaruqui,ChrisDyer,andNoahASmith.Learningwordrepresentations\\nwith hierarchical sparse coding. In Proceedings of ICML, 2015.\\n62. Mo Yu and Mark Dredze. Improving lexical embeddings with semantic knowledge. In Pro-\\nceedings of ACL, 2014.\\n63. Meng Zhang, Haoruo Peng, Yang Liu, Huan-Bo Luan, and Maosong Sun. Bilingual lexicon\\ninduction from non-parallel data with minimal supervision. In Proceedings of AAAI, 2017.\\n64. Will Y Zou, Richard Socher, Daniel M Cer, and Christopher D Manning. Bilingual word\\nembeddings for phrase-based machine translation. In Proceedings of EMNLP, 2013.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 3\\nCompositional Semantics\\nAbstract Many important applications in NLP \\xef\\xac\\x81elds rely on understanding more\\ncomplex language units such as phrases, sentences, and documents beyond words.\\nTherefore, compositional semantics has remained a core task in NLP. In this chapter,\\nwe \\xef\\xac\\x81rst introduce various models for binary semantic composition, including additive\\nmodels and multiplicative models. After that, we present various typical models for\\nN-ary semantic composition including recurrent neural network, recursive neural\\nnetwork, and convolutional neural network.\\n3.1\\nIntroduction\\nFrom the previous chapter, following the distributed hypothesis, one could project\\nthe semantic meaning of a word into a low-dimensional real-valued vector according\\nto its context information, which is named as word vectors. Here comes a further\\nproblem: how to compress a higher semantic unit into a vector or other kinds of\\nmathematical representations like a matrix or a tensor. In other words, using repre-\\nsentation learning to model a semantic composition function remains an unsolved\\nbut surging research topic recently.\\nCompositionality enables natural languages to construct complex semantic mean-\\nings from the combinations of simpler semantic elements. This property is often\\ncaptured with the following principle: the semantic meaning of a whole is a function\\nof the semantic meanings of its several parts. Therefore, the semantic meanings of\\ncomplex structures will depend on how their semantic elements combine.\\nHere we express the composition of two semantic units, which are denoted as\\nu and v, respectively, and the most intuitive way to de\\xef\\xac\\x81ne the joint representation\\ncould be formulated as follows:\\np = f (u, v),\\n(3.1)\\nwhere p corresponds to the representation of the joint semantic unit (u, v). It should\\nbe noted that here u and v could denote words, phrases, sentences, paragraphs, or\\neven higher level semantic units.\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_3\\n43\\n44\\n3\\nCompositional Semantics\\nHowever, given the representations of two semantic constituents, it is not enough\\nto derive their joint embeddings with the lack of syntactic information. For instance,\\nalthough the phrase machine learning and learning machine have the\\nsame vocabulary, they contain different meanings: machine learning refers to\\na research \\xef\\xac\\x81eld in arti\\xef\\xac\\x81cial intelligence while learning machine means some\\nspeci\\xef\\xac\\x81c learning algorithms. This phenomenon stresses the importance of syntactic\\nand order information in a compositional sentence. Reference [12] takes the role of\\nsyntactic and order information into consideration and suggests a further re\\xef\\xac\\x81nement\\nof the above principle: the meaning of a whole is a function of the meaning of its\\nseveral parts and the way they are syntactically combined. Therefore, the composition\\nfunction in Eq. (3.1) is rede\\xef\\xac\\x81ned to combine the syntactic relationship rule R between\\nthe semantic units u and v:\\np = f (u, v, R),\\n(3.2)\\nwhere R denotes the syntactic relationship rule between two constituent semantic\\nunits.\\nUnfortunately, even this formulation may not be fully adequate. Therefore, [7]\\nclaims that the meaning of a whole is greater than the meanings of its several\\nparts. It implies that people may suffer from the problem of constructing com-\\nplex meanings rather than simply understanding the meanings of several parts and\\ntheir syntactic relations. In real language composition, in different contexts, the\\nsame sentence could have different meanings, which means that some sentences\\nare hard to understand without any background information. For example, the\\nsentence Tom and Jerry is one of the most popular comedies\\nin that style. needs two main backgrounds: Firstly, Tom and Jerry is\\na special noun phrase or knowledge entity which indicates a cartoon comedy, rather\\nthantwoordinarypeople.Theotherpriorknowledgeshouldbethat style,which\\nneeds further explanation in the previous sentences. Hence, a full understanding of\\nthe compositional semantics needs to take existing knowledge into account. Here,\\nthe argument K is added into the composition function, incorporating knowledge\\ninformation as a prior in the compositional process:\\np = f (u, v, R, K ),\\n(3.3)\\nwhere K represents the background knowledge.\\nReference [4] claims that we should ask for the meaning of a word in isolation but\\nonly in the context of a statement. That is, the meaning of a whole is constructed from\\nits parts, and the meanings of the parts are meanwhile derived from the whole. More-\\nover, compositionality is a matter of degree rather than a binary notion. Linguistic\\nstructures range from fully compositional (e.g., black hair), to partly compositional\\nsyntactically \\xef\\xac\\x81xed expressions, (e.g., take advantage), in which the constituents can\\nstill be assigned separate meanings, and non-compositional idioms (e.g., kick the\\nbucket) or multi-word expressions (e.g., by and large), whose meaning cannot be\\ndistributed across their constituents [11].\\n3.1 Introduction\\n45\\nFrom the above three equations formulating composition function, it could be\\nconcluded that composition could be viewed as a speci\\xef\\xac\\x81c binary operation but\\nbeyond this. The syntactic message could help to indicate a particular approach while\\nbackground knowledge helps to explain some obscure words or speci\\xef\\xac\\x81c context-\\ndependent entities such as pronouns. Beyond binary compositional operations, one\\ncould build the sentence-level composition by applying binary composition oper-\\nations recursively. In this chapter, we will \\xef\\xac\\x81rst explain some sorts of basic binary\\ncomposition functions in both the semantic vector space and matrix-vector space.\\nAfter, we will climb up to more complex composition scenarios and introduce several\\napproaches to model sentence-level composition.\\n3.2\\nSemantic Space\\n3.2.1\\nVector Space\\nIn general, the central task in semantic representation is projecting words from an\\nabstract semantic space to a mathematical low-dimensional space. As introduced in\\nthe previous chapters, to make the transformation reasonable, the purpose is to main-\\ntain the word similarity in this new projected space. In other words, the more similar\\nthe words are, the closer their vectors should be. For instance, we hope the word\\nvectors w(book) and w(magazine) are close while the word vectors w(apple) and\\nw(computer) are far away. In this chapter, we will introduce several widely used\\ntypical semantic vector space including one-hot representation, distributed represen-\\ntation, and distributional representation.\\n3.2.2\\nMatrix-Vector Space\\nDespite the wide use of semantic vector spaces, an alternative semantic space is\\nproposed to be a more powerful and general compositional semantic framework.\\nDifferent from conventional vector spaces, matrix-vector semantic space utilizes a\\nmatrix to represent the word meaning rather than a skinny vector. The motivation\\nbehind this is when modeling the semantic meaning under a speci\\xef\\xac\\x81c context, one is\\nwondering not only what is the meaning of each word, but also the holistic meaning\\nof the whole sentence. Thus, we concern about the semantic transformation between\\nadjacent words inside each sentence. However, the semantic vector space could not\\ncharacterize the semantic transformation of one word on the others explicitly.\\nDriven by the idea of modeling semantic transformation, some researchers have\\nproposed to use a matrix to represent the transformation operation of one word on the\\nothers. Different from those vector space models, it could incorporate some structural\\ninformation like the word order and syntax composition.\\n46\\n3\\nCompositional Semantics\\n3.3\\nBinary Composition\\nThe goal is to construct vector representations for phrases, sentences, paragraphs,\\nand documents. Without loss of generality, we assume that each constituent of a\\nphrase (sentence, paragraph, or document) is embedded into a vector which will\\nbe subsequently combined in some way to generate a representation vector for the\\nphrase (sentence, paragraph, or document).1\\nIn this section, we focus on binary composition. We will take phrases consisting\\nof a head and a modi\\xef\\xac\\x81er or complement as an example. If we cannot model the binary\\ncomposition (or phrase representation), there is little hope that we can construct more\\ncomplex compositional representations for sentences or even documents. Therefore,\\ngiven a phrase such as \\xe2\\x80\\x9cmachine learning\\xe2\\x80\\x9d and the vectors u and v representing the\\nconstituents \\xe2\\x80\\x9cmachine\\xe2\\x80\\x9d and \\xe2\\x80\\x9clearning\\xe2\\x80\\x9d, respectively, we aim to produce a represen-\\ntation vector p of the whole phrase. Let the hypothetical vectors for machine and\\nlearning be [0, 3, 1, 5, 2] and [1, 4, 2, 2, 0], respectively. This simpli\\xef\\xac\\x81ed seman-\\ntic space will serve to illustrate examples of the composition functions which we\\nconsider in this section.\\nThe fundamental problem of semantic composition modeling in representing a\\ntwo-word phrase is designing a primitive composition function as a binary operator.\\nBased on this function, one could apply it on a word sequence recursively and derive\\nsentence-level composition. Here a word sequence could be any level of the seman-\\ntic units, such as a phrase, a sentence, a paragraph, a knowledge entity, or even a\\ndocument.\\nFrom the previous section, one of the basic formulae is to formulate semantic\\ncomposition f in the following equation:\\np = f (u, v, R, K ),\\n(3.4)\\nwhere u, v denote the representations of the constituent parts in this semantic unit,\\np denotes the joint representation, R indicates the relationship while K indicates\\nthe necessary background knowledge. The expression de\\xef\\xac\\x81nes a wide class of com-\\nposition functions. For easier discussion, we give some appropriate constraints to\\nnarrow the space of our considering function. First, we will ignore the background\\nknowledge K to explore what can be achieved without any utilization of background\\nor world knowledge. Second, for the consideration of the syntactic relation R, we\\ncan proceed by investigating only one relation at a time. And then we can remove\\nany explicit dependence on R which allows us to explore any possible distinct com-\\nposition function for various syntactic relations. That is, we simplify the formula\\np = f (u, v) by simply ignoring the background knowledge and relationship.\\n1Note that, the problem of combining semantic vectors of small units to make a representation for a\\nmulti-word sequence is different from the problem of incorporating information about multi-word\\ncontexts into a distributional representation for a single target word.\\n3.3 Binary Composition\\n47\\nIn recent years, modeling the binary composition function is a well-studied but\\nstill challenging problem. There are mainly two perspectives toward this question,\\nincluding the additive model and the multiplicative model.\\n3.3.1\\nAdditive Model\\nThe additive model has a constraint in which it assumes that p, u, and v lie in the\\nsame semantic space. This essentially means that all syntactic types have the same\\ndimension. One of the simplest ways is to directly use the sum to represent the joint\\nrepresentation:\\np = u + v.\\n(3.5)\\nAccording to Eq. (3.5), the sum of the two vectors representing machine and\\nlearning would be w(machine) + w(learning) = [1, 7, 3, 7, 2]. It assumes that\\nthe composition of different constituents is a symmetric function of them; in other\\nwords, it does not consider the order of constituents. Although having lots of draw-\\nbacks such as lack of the ability to model word orders and absence from background\\nsyntactic or knowledge information, this approach still provides a relatively strong\\nbaseline [9].\\nTo overcome the word order issue, one easy variant is applying a weighted sum\\ninstead of uniform weights. This is to say, the composition has the following form:\\np = \\xce\\xb1u + \\xce\\xb2v,\\n(3.6)\\nwhere \\xce\\xb1 and \\xce\\xb2 correspond to different weights for two vectors. Under this setting, two\\nsequences (u, v) and (v, u) have different representations, which is consistent with\\nreal language phenomena. For example, \\xe2\\x80\\x9cmachine learning\\xe2\\x80\\x9d and \\xe2\\x80\\x9clearning machine\\xe2\\x80\\x9d\\nhave different meanings which requires different representations. In this setting, we\\ncould give greater emphasis to heads than other constituents. As an example, if we\\nset \\xce\\xb1 to 0.3 and \\xce\\xb2 to 0.7, the 0.3 \\xc3\\x97 w(machine) = [0, 0.9, 0.3, 1.5, 0.6] and 0.7 \\xc3\\x97\\nw(learning) = [0.7, 2.8, 1.4, 1.4, 0],and\\xe2\\x80\\x9cmachinelearning\\xe2\\x80\\x9disrepresentedbytheir\\naddition 0.3 \\xc3\\x97 w(machine) + 0.7 \\xc3\\x97 w(learning) = [0.7, 3.6, 1.7, 2.9, 0.6].\\nHowever, this model could not consider prior knowledge and syntax information.\\nTo incorporate prior information into the additive model, one method combines\\nnearest neighborhood semantics into composition, deriving\\np = u + v +\\nK\\n\\x02\\ni=1\\nni,\\n(3.7)\\nwhere n1, n2, . . . , nK denote all semantic neighbors of v. Therefore, this method\\ncould ensemble all synonyms of the component as a smoothing factor into com-\\nposition function, which reduces the variance of language. For example, if in\\n48\\n3\\nCompositional Semantics\\nthe composition of \\xe2\\x80\\x9cmachine\\xe2\\x80\\x9d and \\xe2\\x80\\x9clearning\\xe2\\x80\\x9d, the chosen neighbor is \\xe2\\x80\\x9coptimiz-\\ning\\xe2\\x80\\x9d, with w(optimizing) = [1, 5, 3, 2, 1], then this leads to the situation that\\nthe representation of \\xe2\\x80\\x9cmachine learning\\xe2\\x80\\x9d becomes w(machine) + w(learning) +\\nw(optimizing) = [2, 12, 6, 9, 3].\\nSince the joint representations of one additive model still lie in the same semantic\\nspace with their original component vectors, it is natural to conduct cosine similarity\\nto measure their semantic relationships. Thus, under a naive additive model, we have\\nthe following similarity equation:\\ns(p, w) =\\np \\xc2\\xb7 w\\n\\xe2\\x88\\xa5p\\xe2\\x88\\xa5\\xc2\\xb7 \\xe2\\x88\\xa5w\\xe2\\x88\\xa5=\\n(u + v)w\\n\\xe2\\x88\\xa5u + v\\xe2\\x88\\xa5\\xe2\\x88\\xa5w\\xe2\\x88\\xa5\\n(3.8)\\n=\\n\\xe2\\x88\\xa5u\\xe2\\x88\\xa5\\n\\xe2\\x88\\xa5u + v\\xe2\\x88\\xa5s(u, w) +\\n\\xe2\\x88\\xa5v\\xe2\\x88\\xa5\\n\\xe2\\x88\\xa5u + v\\xe2\\x88\\xa5s(v, w),\\n(3.9)\\nwhere w denotes any other word in the vocabulary and s indicates the similarity\\nfunction. From derivation ahead, it could be concluded that this composition function\\ncomposes both magnitude and directions of two component vectors. In other words, if\\nonevectordominatesthemagnitude,itwillalsodominatethesimilarity.Furthermore,\\nwe have\\n\\xe2\\x88\\xa5p\\xe2\\x88\\xa5= \\xe2\\x88\\xa5u + v\\xe2\\x88\\xa5\\xe2\\x89\\xa4\\xe2\\x88\\xa5u\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5v\\xe2\\x88\\xa5.\\n(3.10)\\nThis lemma suggests that the semantic unit with a deeper-rooted parsing tree could\\ndetermine the joint representation when combining with a shallow unit. Because the\\ndeeper the semantic unit is, the larger the magnitude it has.\\nMoreover, incorporating geometry insight, we can observe that the additive model\\nbuilds a more solid understanding of semantic composition. Supposing that our com-\\nponent vectors are u and v, the additive model aims to project them to x and y, where\\nx follows the direction of u while y is orthogonal to u. The following \\xef\\xac\\x81gure could\\nclearly illustrate this issue (Fig.3.1).\\nFig. 3.1 An illustration of\\nthe additive model\\nA\\nB\\nC\\nD\\ny\\nu\\nx\\nv\\n3.3 Binary Composition\\n49\\nFrom the \\xef\\xac\\x81gure, the vector x and the vector y could be represented as\\nx = u \\xc2\\xb7 v\\nu \\xc2\\xb7 u \\xc2\\xb7 u,\\ny = v \\xe2\\x88\\x92x = v \\xe2\\x88\\x92u \\xc2\\xb7 v\\nu \\xc2\\xb7 u \\xc2\\xb7 u.\\n(3.11)\\nThen, using the linear combination of these two new vectors x, y yields a new\\nadditive model:\\np = \\xce\\xb1x + \\xce\\xb2y\\n(3.12)\\n= \\xce\\xb1 u \\xc2\\xb7 v\\nu \\xc2\\xb7 u \\xc2\\xb7 u + \\xce\\xb2\\n\\x03\\nv \\xe2\\x88\\x92u \\xc2\\xb7 v\\nu \\xc2\\xb7 u \\xc2\\xb7 u\\n\\x04\\n(3.13)\\n= (\\xce\\xb1 \\xe2\\x88\\x92\\xce\\xb2) \\xc2\\xb7 u \\xc2\\xb7 v\\nu \\xc2\\xb7 u \\xc2\\xb7 u + \\xce\\xb2v.\\n(3.14)\\nFurthermore, using cosine similarity measurement, the relationship could be writ-\\nten as follows:\\ns(p, w) = |\\xce\\xb1 \\xe2\\x88\\x92\\xce\\xb2|\\n|\\xce\\xb1|\\ns(u, w) + |\\xce\\xb2|\\n|\\xce\\xb1|s(v, w).\\n(3.15)\\nFrom similarity measurement derivation, it is indicated that with this projection\\nmethod, the composition similarity could be viewed as a linear combination of the\\nsimilarities of two components, which means that combining semantic units with\\ndifferent semantic depths, the deeper one will not dominate the representation.\\n3.3.2\\nMultiplicative Model\\nThough the additive model achieves great success in semantic composition, the sim-\\npli\\xef\\xac\\x81cation it adopted may be too restrictive because it assumes all words, phrases,\\nsentences, and documents are substantially similar enough to be represented in a uni-\\n\\xef\\xac\\x81ed semantic space. Different from the additive model which regards composition as\\na simple linear transformation, the multiplicative model aims to make higher order\\ninteraction. Among all models from this perspective, the most intuitive approach\\ntried to apply the pair-wise product as a composition function approximation. In this\\nmethod, the composition function is shown as the following:\\np = u \\xe2\\x8a\\x99v,\\n(3.16)\\nwhere, pi = ui \\xc2\\xb7 vi, which implies each dimension of the output only depends on\\nthe corresponding dimension of two input vectors. However, similar to the simplest\\nadditive model, this model is also suffering from the lack of the ability to model word\\norder, and the absence from background syntactic or knowledge information.\\n50\\n3\\nCompositional Semantics\\nIn the additive model, we have p = \\xce\\xb1u + \\xce\\xb2v to alleviate the word order issue.\\nNote that here \\xce\\xb1 and \\xce\\xb2 are two scalars, which could be easily changed to two matrices.\\nTherefore, the composition function could be represented as\\np = W\\xce\\xb1 \\xc2\\xb7 u + W\\xce\\xb2 \\xc2\\xb7 v,\\n(3.17)\\nwhere W\\xce\\xb1 and W\\xce\\xb2 are matrices which determine the importance of u and v to p.\\nWith this expression, the composition could be more expressive and \\xef\\xac\\x82exible although\\nmuch harder to train.\\nGeneralizing multiplicative model ahead, another approach is to utilize tensors as\\nmultiplicative descriptors and the composition function could be viewed as\\np = \\xe2\\x88\\x92\\xe2\\x86\\x92\\nW \\xc2\\xb7 uv,\\n(3.18)\\nwhere \\xe2\\x88\\x92\\xe2\\x86\\x92\\nW denotes a 3-order tensor, i.e., the formula above could be written as\\npk = \\x05\\ni, j Wi jk \\xc2\\xb7 ui \\xc2\\xb7 v j. Hence, this model makes that each element of p could be\\nin\\xef\\xac\\x82uenced by all elements of both u and v, with a relationship of linear combination\\nby assigning each (i, j) a unique weight.\\nStarting from this simple but general baseline, some researchers proposed to\\nmake the function not symmetric to consider word order in the sequence. Paying\\nmore attention to the \\xef\\xac\\x81rst element, the composition function could be\\np = \\xe2\\x88\\x92\\xe2\\x86\\x92\\nW \\xc2\\xb7 uuv,\\n(3.19)\\nwhere \\xe2\\x88\\x92\\xe2\\x86\\x92\\nW denotes a 4-order tensor. This method could be understood as replacing\\nlinear transformation of u and v to a quadratic in u asymmetrically. So this is a variant\\nof the tensor multiplicative compositional model.\\nDifferent from expanding a simple multiplicative model to complex ones, other\\nkinds of approaches are proposed to reduce the parameter space. With the reduction\\nof parameter size, people could make compositions much more ef\\xef\\xac\\x81cient rather than\\nhave an O(n3) time complexity in the tensor-based model. Thus, some compression\\ntechniques could be applied in the original tensor model. One representative instance\\nis the circular convolution model, which could be shown as\\np = u \\xe2\\x8a\\x9bv,\\n(3.20)\\nwhere \\xe2\\x8a\\x9brepresents the circular convolution operation with the following de\\xef\\xac\\x81nition:\\npi =\\n\\x02\\nj\\nu j \\xc2\\xb7 vi\\xe2\\x88\\x92j.\\n(3.21)\\nIf we assign each pair with unique weights, the composition function will be\\npi =\\n\\x02\\nj\\nWi j \\xc2\\xb7 u j \\xc2\\xb7 vi\\xe2\\x88\\x92j.\\n(3.22)\\n3.3 Binary Composition\\n51\\nNote that the circular convolution model could be viewed as a special instance of\\na tensor-based composition model. If we write the circular convolution in the tensor\\nform, we have Wi jk = 0, where k \\xcc\\xb8= i + j. Thus, the parameter number could be\\nreduced from n3 to n2, while maintaining the interactions between each pair of\\ndimensions in the input vectors.\\nBoth in the additive and multiplicative models, the basic condition is all compo-\\nnents lie in the same semantic space as the output. Nevertheless, different modeling\\ntypes of words in different semantic spaces could bring us a different perspective.\\nFor instance, given (u, v), the multiplicative model could be reformulated as\\np = W \\xc2\\xb7 (u \\xc2\\xb7 v) = U \\xc2\\xb7 v.\\n(3.23)\\nThis implies that each left unit could be treated as an operation on the repre-\\nsentation of the right one. In other words, each remaining unit could be formulated\\nas a transformation matrix, while the right one should be represented as a seman-\\ntic vector. This argument could be meaningful, especially for some kinds of phrase\\ncompositions. Reference [2] argues that for ADJ-NOUN phrases, the joint semantic\\ninformation could be viewed as the conjunction of the semantic meanings of two\\ncomponents. Given a phrase red car, its semantic meaning is the conjunction of\\nall red things and all different kinds of cars. Thus, red could be formulated as an\\noperator on the vector of car, deriving the new semantic vector, which expressed\\nthe meaning of red car. These observations lead to another genre of semantic\\ncompositional modeling: semantic matrix-composition space.\\n3.4\\nN-Ary Composition\\nIn real-world NLP tasks, the input is usually a sequence of multiple words rather than\\njust a pair of words. Therefore, besides designing a suitable binary compositional\\noperator, the order to apply binary operations is also important. In this section, we\\nwill introduce three mainstream strategies in N-ary composition by taking language\\nmodeling as an example.\\nTo illustrate the language modeling task more clearly, the composition problem\\nto model a sentence or even a document could be formulated as\\nGiven a sentence/document consisting of a word sequence {w0, w1, w2, . . . , wn},\\nwe aim to design following functions to obtain the joint semantic representation of\\nthe whole sentence/document:\\n1. A semantic representation method like semantic vector space or compositional\\nmatrix space.\\n2. A binary compositional operation function f (u, v) like we introduced in the pre-\\nvious sections. Here the input u and v denote the representations of two constitute\\nsemantic units, while the output is also the representation in the same space.\\n52\\n3\\nCompositional Semantics\\n3. A sequential order to apply the binary function in step 2. To describe in detail,\\nwe could use a bracket to identify the order to apply the composition function.\\nFor instance, we could use ((w1, w2), w3) to represent the sequential order from\\nbeginning to end.\\nIn this section, we will introduce several systematic strategies to model sentence\\nsemantics by describing the solutions for the three problems above. We will classify\\nthe methods by word-level order: sequential order, recursive order (following parsing\\ntrees), and convolution order.\\n3.4.1\\nRecurrent Neural Network\\nTo design orders to apply binary compositional functions, the most intuitive method\\nis utilizing sequentiality. Namely, the sequence order should be sn = (sn\\xe2\\x88\\x921, wn),\\nwhere sn\\xe2\\x88\\x921 is the order of the \\xef\\xac\\x81rst n \\xe2\\x88\\x921 words. Motivated by this thought, the neural\\nnetwork model used is the Recurrent Neural Network (RNN).\\nAn RNN applies the composition function sequentially and derives the represen-\\ntations of hidden semantic units. Based on these hidden semantic units, we could\\nuse them on some speci\\xef\\xac\\x81c NLP tasks like sentiment analysis or text classi\\xef\\xac\\x81cation.\\nAlso, note that the basic RNN only utilizes the sequential information from head to\\ntail of a sentence/document. To improve its representation ability, the RNN could\\nbe enhanced as bi-directional RNN by considering sequential and reverse-sequential\\ninformation.\\nAfter deciding sequential order to model sentence-level semantics, the next ques-\\ntion is determining the binary composition functions. In detail, supposing that ht\\ndenotes the representation of the \\xef\\xac\\x81rst t words and wt represents the tth word, the\\ngeneral composition could be formulated as\\nht = f (ht\\xe2\\x88\\x921, xt),\\n(3.24)\\nwhere f is a well-designed binary composition function.\\nFrom the de\\xef\\xac\\x81nition of the RNN, the composition function could be formulated as\\nfollows:\\nht = tanh(W1ht\\xe2\\x88\\x921 + W2wt),\\n(3.25)\\nwhere W1 and W2 are two weighted matrices.\\nWe could see that here we use a matrix-weighted summation to represent binary\\nsemantic composition:\\np = W\\xce\\xb1u + W\\xce\\xb2v.\\n(3.26)\\nLSTM. Since the raw RNN only utilizes the simple tangent function, it is hard\\nto obtain the long-term dependency of a long sentence/document. Reference [5]\\nreinvents Long Short-Term Memory (LSTM) networks to strengthen the ability to\\n3.4 N-Ary Composition\\n53\\nmodel long-term semantic dependency in RNN. In detail, the composition function of\\nthe LSTM allows information from previous layers to \\xef\\xac\\x82ow directly to their following\\nlayers. The composition function could be de\\xef\\xac\\x81ned as\\nft = Sigmoid(Wh\\nf ht\\xe2\\x88\\x921 + Wx\\nf xt + b f ),\\n(3.27)\\nit = Sigmoid(Wh\\ni ht\\xe2\\x88\\x921 + Wx\\ni xt + bi),\\n(3.28)\\not = Sigmoid(Wh\\noht\\xe2\\x88\\x921 + Wx\\noxt + bo),\\n(3.29)\\n\\xcb\\x86ct = tanh(Wh\\ncht\\xe2\\x88\\x921 + Wx\\ncxt + bc),\\n(3.30)\\nct = ft \\xe2\\x8a\\x99ct\\xe2\\x88\\x921 + it \\xe2\\x8a\\x99\\xcb\\x86ct,\\n(3.31)\\nht = ot \\xe2\\x8a\\x99ct.\\n(3.32)\\nVariants of LSTM. To simplify LSTM and obtain more ef\\xef\\xac\\x81cient algorithms,\\n[3] proposes to utilize a simple but comparable RNN architecture, named Gated\\nRecurrent Unit (GRU). Compared with LSTM, GRU has fewer parameters, which\\nbring higher ef\\xef\\xac\\x81ciency. The composition function is showed as\\nzt = Sigmoid(Wh\\nz ht\\xe2\\x88\\x921 + Wx\\nz xt + bz),\\n(3.33)\\nrt = Sigmoid(Wh\\nr ht\\xe2\\x88\\x921 + Wx\\nr xt + br),\\n(3.34)\\n\\xcb\\x86ht = tanh(Wh(rt \\xe2\\x8a\\x99ht\\xe2\\x88\\x921) + Wx\\nhxt + bh),\\n(3.35)\\nht = (1 \\xe2\\x88\\x92zt) \\xe2\\x8a\\x99ht\\xe2\\x88\\x921 + zt \\xe2\\x8a\\x99\\xcb\\x86ht.\\n(3.36)\\n3.4.2\\nRecursive Neural Network\\nBesides the recurrent neural network, another strategy to apply binary compositional\\nfunction follows a parsing tree instead of sequential word order. Based on this philos-\\nophy, [15] proposes a recursive neural network to model different levels of semantic\\nunits. In this subsection, we will introduce some algorithms following the recursive\\nparsing tree with different binary compositional functions.\\nSince all the recursive neural networks are binary trees, the basic problem we need\\nto consider is how to derive the representation of the father component on the tree\\ngiven its two children semantic components. Reference [15] proposes a recursive\\nmatrix-vector model (MV-RNN) which captures constituent parsing tree structure\\ninformation by assigning a matrix-vector representation for each constituent. The\\nvector captures the meaning of the constituent itself, and the matrix represents how\\nit modi\\xef\\xac\\x81es the meaning of the word it combines with. Suppose we have two children\\ncomponents a, b and their father component p, the composition can be formulated\\nas follows:\\n54\\n3\\nCompositional Semantics\\np = fvec(a, b) = g\\n\\x06\\nW1\\n\\x07Ba\\nAb\\n\\x08\\t\\n,\\n(3.37)\\nP = fmatrix(a, b) = W2\\n\\x07A\\nB\\n\\x08\\n,\\n(3.38)\\nwhere a, b, p are the embedding vectors for each component and A, B, P are the\\nmatrices, W1 is a matrix that maps the transformed words into another semantic\\nspace, the element-wise function g is an activation function, and W2 is a matrix that\\nmaps the two matrices into one combined matrix P with the same dimension. The\\nwhole process is illustrated in Fig. 3.2. And then MV-RNN selects the highest node\\nof the path in the parse tree between the two target entities to represent the input\\nsentence.\\nIn fact, the composition operation used in the above recursive network is similar\\nto an RNN unit introduced in the previous subsection. And the RNN unit here can\\nbe replaced by LSTM units or GRU units. Reference [16] proposes two types of\\ntree-structured LSTMs including the Child-Sum Tree-LSTM and the N-ary Tree-\\nLSTM to capture constituent or dependency parsing tree structure information. For\\nthe Child-Sum Tree-LSTM, given a tree, let C(t) denote the children set of the node\\nt. Its transition equations are de\\xef\\xac\\x81ned as follows:\\nvery\\nf(Ba,Ab)=\\nBa=\\nAb=\\nbeautiful\\ngirl\\n(a,A)\\n(b,B)\\n(c,C)\\n...\\n...\\n...\\nFig. 3.2 The architecture of the matrix-vector recursive encoder\\n3.4 N-Ary Composition\\n55\\n\\xcb\\x86ht =\\n\\x02\\nk\\xe2\\x88\\x88C(t)\\nhk,\\n(3.39)\\nit = Sigmoid(W(i)wt + Ui \\xcb\\x86ht + b(i)),\\n(3.40)\\nftk = Sigmoid(W( f )wt + U f \\xcb\\x86hk + b( f )) (k \\xe2\\x88\\x88C(t)),\\n(3.41)\\not = Sigmoid(W(o)wt + Uo \\xcb\\x86ht + b(o)),\\n(3.42)\\nut = tanh(W(u)wt + Uu \\xcb\\x86ht + b(u)),\\n(3.43)\\nct = it \\xe2\\x8a\\x99ut +\\n\\x02\\nk\\xe2\\x88\\x88C(t)\\nftk \\xe2\\x8a\\x99ct\\xe2\\x88\\x921,\\n(3.44)\\nht = ot \\xe2\\x8a\\x99tanh(ct).\\n(3.45)\\nThe N-ary Tree-LSTM has similar transition equations as the Child-Sum Tree-\\nLSTM. The only difference is that it limits the tree structures to have at most N\\nbranches.\\n3.4.3\\nConvolutional Neural Network\\nReference [6] proposes to embed an input sentence using a Convolutional Neural\\nNetwork (CNN) which extracts local features by a convolution layer and combines\\nall local features via a max-pooling operation to obtain a \\xef\\xac\\x81xed-sized vector for the\\ninput sentence.\\nFormally, the convolution operation is de\\xef\\xac\\x81ned as a matrix multiplication between\\na sequence of vectors, a convolution matrix W, and a bias vector b with a sliding\\nwindow. Let us de\\xef\\xac\\x81ne the vector qi as the concatenation of the subsequence of input\\nrepresentations in the ith window, we have\\nh j = max\\ni\\n[ f (Wqi + b)] j,\\n(3.46)\\nwhere f indicates a nonlinear function such as sigmoid or tangent function, and h\\nindicates the \\xef\\xac\\x81nal representation of the sentence.\\n3.5\\nSummary\\nIn this chapter, we \\xef\\xac\\x81rst introduce the semantic space for compositional semantics.\\nAfterwards, we take phrase representation as an example to introduce various models\\nfor binary semantic composition, including additive models and multiplicative mod-\\nels. Finally, we introduce typical models for N-ary semantic composition including\\nrecurrent neural network, recursive neural network, and convolutional neural net-\\nwork. Compositional semantics allows languages to construct complex meanings\\nfrom the combinations of simpler elements, and its binary semantic composition\\n56\\n3\\nCompositional Semantics\\nand N-ary semantic composition is the foundation of multiple NLP tasks including\\nsentence representation, document representation, relational path representation, etc.\\nWe will give a detailed introduction to these scenarios in the following chapters.\\nFor further understanding of compositional semantics, there are also some rec-\\nommended surveys and books:\\n\\xe2\\x80\\xa2 Pelletier et al., The principle of semantic compositionality [13].\\n\\xe2\\x80\\xa2 Jeff et al., Composition in distributional models of semantics [10].\\nFor better modeling compositional semantics, some directions require further\\nefforts in the future:\\n(1) Neurobiology-inspired Compositional Semantics. What is the neurobiology\\nfor dealing with compositional semantics in human language? Recently, [14]\\n\\xef\\xac\\x81nds that the human combinatory system is related to rapidly peaking activity\\nin the left anterior temporal lobe and later engagement of the medial prefrontal\\ncortex. The analysis of how language builds meaning and lays out directions\\nin neurobiological research may bring some instructive reference for modeling\\ncompositional semantics in representation learning. It is valuable to design novel\\ncompositional forms inspired by recent neurobiological advances.\\n(2) Combination of Symbolic and Distributed Representation. Human language\\nis inherently a discrete symbolic representation of knowledge. However, we\\nrepresent the semantics of discrete symbols with distributed/distributional rep-\\nresentations when dealing with natural language in deep learning. Recently, there\\nare some approaches such as neural module networks [1] and neural symbolic\\nmachine [8] attempting to consider discrete symbols in neural networks. How\\nto take advantage of these symbolic neural models to represent the composition\\nof semantics is an open problem to be explored.\\nReferences\\n1. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In\\nProceedings of CVPR, pages 39\\xe2\\x80\\x9348, 2016.\\n2. MarcoBaroniandRobertoZamparelli.Nounsarevectors,adjectivesarematrices:Representing\\nadjective-noun constructions in semantic space. In Proceedings of EMNLP, 2010.\\n3. Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Gated feedback\\nrecurrent neural networks. In Proceedings of ICML, 2015.\\n4. Gottlob Frege. Die grundlagen derarithmetik. Eine logisch mathematische Untersuchung u\\xe2\\x80\\x99ber\\nden Begrijfder Zahl. Breslau: Koebner, 1884.\\n5. Sepp Hochreiter and J\\xc3\\xbcrgen Schmidhuber. Long short-term memory. Neural Computation,\\n9(8):1735\\xe2\\x80\\x931780, 1997.\\n6. Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network\\nfor modelling sentences. In Proceedings of ACL, 2014.\\n7. George Lakoff. Linguistic gestalts. In Proceedings of ILGISA, 1977.\\n8. Chen Liang, Jonathan Berant, Quoc Le, Kenneth Forbus, and Ni Lao. Neural symbolic\\nmachines: Learning semantic parsers on freebase with weak supervision. In Proceedings of\\nACL, pages 23\\xe2\\x80\\x9333, 2017.\\nReferences\\n57\\n9. JeffMitchell andMirella Lapata.Vector-basedmodelsofsemantic composition.In Proceedings\\nof ACL, 2008.\\n10. Jeff Mitchell and Mirella Lapata. Composition in distributional models of semantics. Cognitive\\nscience, 34(8):1388\\xe2\\x80\\x931429, 2010.\\n11. Geoffrey Nunberg, Ivan A Sag, and Thomas Wasow. Idioms. Language, pages 491\\xe2\\x80\\x93538, 1994.\\n12. Barbara Partee. Lexical semantics and compositionality. An Invitation to Cognitive Science:\\nLanguage, 1:311\\xe2\\x80\\x93360, 1995.\\n13. Francis Jeffry Pelletier. The principle of semantic compositionality. Topoi, 13(1):11\\xe2\\x80\\x9324, 1994.\\n14. Liina Pylkk\\xc3\\xa4nen. The neural basis of combinatory syntax and semantics. Science,\\n366(6461):62\\xe2\\x80\\x9366, 2019.\\n15. Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. Semantic compo-\\nsitionality through recursive matrix-vector spaces. In Proceedings of EMNLP, 2012.\\n16. Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representa-\\ntions from tree-structured long short-term memory networks. In Proceedings of ACL, 2015.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 4\\nSentence Representation\\nAbstract Sentence is an important linguistic unit of natural language. Sentence Rep-\\nresentation has remained as a core task in natural language processing, because many\\nimportant applications in related \\xef\\xac\\x81elds lie on understanding sentences, for example,\\nsummarization, machine translation, sentiment analysis, and dialogue system. Sen-\\ntence representation aims to encode the semantic information into a real-valued rep-\\nresentation vector, which will be utilized in further sentence classi\\xef\\xac\\x81cation or match-\\ning tasks. With large-scale text data available on the Internet and recent advances\\non deep neural networks, researchers tend to employ neural networks (e.g., con-\\nvolutional neural networks and recurrent neural networks) to learn low-dimensional\\nsentence representations and achieve great progress on relevant tasks. In this chapter,\\nwe \\xef\\xac\\x81rst introduce the one-hot representation for sentences and the n-gram sentence\\nrepresentation (i.e., probabilistic language model). Then we extensively introduce\\nneural-based models for sentence modeling, including feedforward neural network,\\nconvolutional neural network, recurrent neural network, and the latest Transformer,\\nand pre-trained language models. Finally, we introduce several typical applications\\nof sentence representations.\\n4.1\\nIntroduction\\nNatural language sentences consist of words or phrases, follow grammatical rules,\\nand convey complete semantic information. Compared with words and phrases, sen-\\ntences have more complex structures, including both sequential and hierarchical\\nstructures, which are essential for understanding sentences. In NLP, how to rep-\\nresent sentences is critical for related applications, such as sentence classi\\xef\\xac\\x81cation,\\nsentiment analysis, sentence matching, and so on.\\nBefore deep learning took off, sentences were usually represented as one-hot vec-\\ntors or TF-IDF vectors, following the assumption of bag-of-words. In this case, a\\nsentence is represented as a vocabulary-sized vector, in which each element repre-\\nsents the importance of a speci\\xef\\xac\\x81c word (either term frequency or TF-IDF) to the\\nsentence. However, this method confronts two issues. Firstly, the dimension of such\\nrepresentation vectors is usually up to thousands or millions. Thus, they usually face\\nsparsity problem and bring in computational ef\\xef\\xac\\x81ciency problem. Secondly, such a\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_4\\n59\\n60\\n4\\nSentence Representation\\nrepresentation method follows the bag-of-words assumption and ignores the sequen-\\ntial and structural information, which can be crucial for understanding the semantic\\nmeanings of sentences.\\nInspired by recent advances of deep learning models in computer vision and\\nspeech, researchers proposed to model sentences with deep neural networks, such as\\nconvolutional neural network, recurrent neural network, and so on. Compared with\\nconventional word frequency-based sentence representations, deep neural networks\\ncan capture the internal structures of sentences, e.g., sequential and dependency\\ninformation, through convolutional or recurrent operations. Thus, neural network-\\nbased sentence representations have achieved great success in sentence modeling\\nand NLP tasks.\\n4.2\\nOne-Hot Sentence Representation\\nOne-hot representation is the most simple and straightforward method for word rep-\\nresentation tasks. This method represents each word with a \\xef\\xac\\x81xed length binary vector.\\nSpeci\\xef\\xac\\x81cally, for a vocabulary V = {w1, w2, . . . , w|V |}, the one-hot representation of\\nword w is w = [0, . . . , 0, 1, 0, . . . , 0]. Based on the one-hot word representation and\\nthe vocabulary, it can be extended to represent a sentence s = {w1, w2, . . . , wl} as\\ns =\\nl\\x02\\nk=1\\nwi,\\n(4.1)\\nwhere l indicates the length of the sentence s. The sentence representation s is the\\nsum of the one-hot representations of n words within the sentence, i.e., each element\\nin s represents the Term Frequency (TF) of the corresponding word.\\nMoreover, researchers usually take the importance of different words into consid-\\neration, rather than treat all the words equally. For example, the function words such\\nas \\xe2\\x80\\x9ca\\xe2\\x80\\x9d, \\xe2\\x80\\x9can\\xe2\\x80\\x9d, and \\xe2\\x80\\x9cthe\\xe2\\x80\\x9d usually appear in different sentences, and reserve little mean-\\nings. Therefore, the Inverse Document Frequency (IDF) is employed to measure the\\nimportance of wi in V as follows:\\nidfwi = log |D|\\ndfwi\\n,\\n(4.2)\\nwhere |D| is the number of all documents in the corpus D and dfwi represents the\\nDocument Frequency (DF) of wi.\\nWith the importance of each word, the sentences are represented more precisely\\nas follows:\\n\\xcb\\x86s = s \\xe2\\x8a\\x97idf,\\n(4.3)\\nwhere \\xe2\\x8a\\x97is the element-wise product.\\nHere, \\xcb\\x86s is the TF-IDF representation of the sentence s.\\n4.3 Probabilistic Language Model\\n61\\n4.3\\nProbabilistic Language Model\\nOne-hot sentence representation usually neglects the structure information in a sen-\\ntence. To address this issue, researchers propose probabilistic language model, which\\ntreats n-grams rather than words as the basic components. An n-gram means a subse-\\nquence of words in a context window of length n, and probabilistic language model\\nde\\xef\\xac\\x81nes the probability of a sentence s = [w1, w2, . . . , wl] as\\nP(s) =\\nl\\x03\\ni=1\\nP(wi|wi\\xe2\\x88\\x921\\n1\\n).\\n(4.4)\\nActually, model indicated in Eq.(4.4) is not practicable due to its enormous param-\\neter space. In practice, we simplify the model and set an n-sized context window,\\nassuming that the probability of word wi only depends on [wi\\xe2\\x88\\x92n+1 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 wi\\xe2\\x88\\x921]. More\\nspeci\\xef\\xac\\x81cally, an n-gram language model predicts word wi in the sentence s based\\non its previous n \\xe2\\x88\\x921 words. Therefore, the simpli\\xef\\xac\\x81ed probability of a sentence is\\nformalized as\\nP(s) =\\nl\\x03\\ni=1\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n+1),\\n(4.5)\\nwhere the probability of selecting the word wi can be calculated from n-gram model\\nfrequency counts:\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n+1) = P(wi\\ni\\xe2\\x88\\x92n+1)\\nP(wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n+1)\\n.\\n(4.6)\\nTypically, the conditional probabilities in n-gram language models are not cal-\\nculated directly from the frequency counts, since it suffers severe problems when\\nconfronted with any n-grams that have not explicitly been seen before. Therefore,\\nresearchers proposed several types of smoothing approaches, which assign some of\\nthe total probability mass to unseen words or n-grams, such as \\xe2\\x80\\x9cadd-one\\xe2\\x80\\x9d smoothing,\\nGood-Turing discounting, or back-off models.\\nn-gram model is a typical probabilistic language model for predicting the next\\nword in an n-gram sequence, which follows the Markov assumption that the proba-\\nbility of the target word only relies on the previous n \\xe2\\x88\\x921 words. The idea is employed\\nby most of current sentence modeling methods. n-gram language model is used as\\nan approximation of the true underlying language model. This assumption is crucial\\nbecause it massively simpli\\xef\\xac\\x81es the problem of learning the parameters of language\\nmodels from data. Recent works on word representation learning [3, 40, 43] are\\nmainly based on the n-gram language model.\\n62\\n4\\nSentence Representation\\n4.4\\nNeural Language Model\\nAlthough smoothing approaches could alleviate the sparse problem in the probabilis-\\ntic language model, it still performs poorly for those unseen or uncommon words and\\nn-grams. Moreover, since probabilistic language models are constructed on larger\\nand larger texts, the number of unique words (the vocabulary) increases and the\\nnumber of possible sequences of words increases exponentially with the size of the\\nvocabulary, causing a data sparsity problem. Thus statistics are needed to estimate\\nprobabilities accurately.\\nTo address this issue, researchers propose neural language models which use\\ncontinuousrepresentationsorembeddingsofwordsandneuralnetworkstomaketheir\\npredictions, in which embeddings in the continuous space help to alleviate the curse\\nof dimensionality in language modeling, and neural networks avoid this problem by\\nrepresenting words in a distributed way, as nonlinear combinations of weights in a\\nneural net [2]. An alternate description is that a neural network approximates the\\nlanguage function. The neural net architecture might be feedforward or recurrent,\\nand while the former is simpler, the latter is more common.\\nSimilar to probabilistic language models, neural language models are constructed\\nand trained as probabilistic classi\\xef\\xac\\x81ers that learn to predict a probability distribution:\\nP(s) =\\nl\\x03\\ni=1\\nP(wi|wi\\xe2\\x88\\x921\\n1\\n),\\n(4.7)\\nwhere the conditional probability of the selecting word wi can be calculated by\\nvarious kinds of neural networks such as feedforward neural networks, recurrent\\nneural networks, and so on. In the following sections, we will introduce these neural\\nlanguage models in detail.\\n4.4.1\\nFeedforward Neural Network Language Model\\nThe goal of neural network language model is to estimate the conditional probabil-\\nity P(wi|w1, . . . , wi\\xe2\\x88\\x921). However, the feedforward neural network (FNN) lacks an\\neffective way to represent the long-term historical context. Therefore, it adopts the\\nidea of n-gram language models to approximate the conditional probability, which\\nassumes that each word in a word sequence more statistically depends on those\\nwords closer to it, and only n \\xe2\\x88\\x921 context words are used to calculate the conditional\\nprobability, i.e., P(wi|wi\\xe2\\x88\\x921\\n1\\n) \\xe2\\x89\\x88P(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n+1).\\nTheoverallarchitectureoftheFNNlanguagemodelisproposedby[3].Toevaluate\\nthe conditional probability of the word wi, it \\xef\\xac\\x81rst projects its n \\xe2\\x88\\x921 context-related\\nwords to their word vector representations x = [wi\\xe2\\x88\\x92n+1, . . . , wi\\xe2\\x88\\x921], and then feeds\\nthem into an FNN, which can be generally represented as\\n4.4 Neural Language Model\\n63\\ny = M f (Wx + b) + d,\\n(4.8)\\nwhere W is a weighted matrix to transform word vectors to hidden representations,\\nM is a weighted matrix for the connections between the hidden layer and the output\\nlayer, and b, d are bias vectors. And then the conditional probability of the word wi\\ncan be calculated as\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) =\\nexp(ywi)\\n\\x04\\nj exp(y j).\\n(4.9)\\n4.4.2\\nConvolutional Neural Network Language Model\\nThe Convolutional Neural Network (CNN) is the family of neural network models\\nthat features a type of layer known as the convolutional layer. This layer can extract\\nfeatures by a learnable \\xef\\xac\\x81lter (or kernel) at the different positions of an input. Pham\\net al. [47] propose the CNN language model to enhance the FNN language model.\\nThe proposed CNN network is produced by injecting a convolutional layer after the\\nword input representation x = [wi\\xe2\\x88\\x92n, . . . , wi\\xe2\\x88\\x921]. Formally, the convolutional layer\\ninvolves a sliding window of the input vectors centered on each word vector using a\\nparameter matrix Wc, which can be generally represented as\\ny = M\\n\\x05\\nmax(Wcx)\\n\\x06\\n,\\n(4.10)\\nwhere max(\\xc2\\xb7) indicates a max-pooling layer. The architecture of CNN is shown in\\nFig.4.1.\\nMoreover, [12] also introduces a convolutional neural network for language mod-\\neling with a novel gating mechanism.\\n4.4.3\\nRecurrent Neural Network Language Model\\nTo address the lack of ability for modeling long-term dependency in the FNN lan-\\nguage model, [41] proposes a Recurrent Neural Network (RNN) language model\\nwhich applies RNN in language modeling. RNNs are fundamentally different from\\nFNNs in the sense that they operate on not only an input space but also an internal\\nstate space, and the internal state space enables the representation of sequentially\\nextended dependencies. Therefore, the RNN language model can deal with those\\nsentences of arbitrary length. At every time step, its input is the vector of its previous\\nword instead of the concatenation of vectors of its n previous words, and the infor-\\nmation of all other previous words can be taken into account by its internal state.\\nFormally, the RNN language model can be de\\xef\\xac\\x81ned as\\n64\\n4\\nSentence Representation\\nFig. 4.1 The architecture of\\nCNN\\nWc *\\nNon-linear \\nLayer\\nMax\\nPooling\\nConvolution\\nLayer\\nInput\\nRepresentation\\ntanh\\nhi = f (W1hi\\xe2\\x88\\x921 + W2wi + b),\\n(4.11)\\ny = Mhi\\xe2\\x88\\x921 + d,\\n(4.12)\\nwhere W1, W2, M are weighted matrices and b, d are bias vectors. Here, the RNN\\nunit can also be implemented by LSTM or GRU. The architecture of RNN is shown\\nin Fig.4.2.\\nRecently, researchers make some comparisons among neural network language\\nmodels with different architectures on both small and large corpora. The experimental\\nresults show that, generally, the RNN language model outperforms the CNN language\\nmodel.\\n4.4.4\\nTransformer Language Model\\nIn 2018, Google proposed a pre-trained language model (PLM), called BERT, which\\nachieved state-of-the-art results on a variety of NLP tasks. At that time, it was very\\nbig news. Since then, all the NLP researchers began to consider how PLMs can\\nbene\\xef\\xac\\x81t their research tasks.\\n4.4 Neural Language Model\\n65\\ntanh\\nRNN\\nUnit\\nRNN\\nUnit\\nRNN\\nUnit\\nRNN\\nUnit\\n\\xcf\\x83\\n\\xcf\\x83\\n\\xcf\\x83\\ntanh\\nGRU Cell\\n\\xcf\\x83\\n\\xcf\\x83\\ntanh\\n1-\\nxt\\nht\\nh0\\nh1\\nhn\\nx0\\nx1\\nxn\\nht-1\\nct-1\\nxt\\nht\\nct\\nxt\\nht\\nht-1\\nLSTM Cell\\nFig. 4.2 The architecture of RNN\\nIn this section, we will \\xef\\xac\\x81rst introduce the Transformer architecture and then talk\\nabout BERT and other PLMs in detail.\\n4.4.4.1\\nTransformer\\nTransformer [65] is a nonrecurrent encoder-decoder architecture with a series of\\nattention-based blocks. For the encoder, there are 6 layers and each layer is composed\\nof a multi-head attention sublayer and a position-wise feedforward sublayer. And\\nthere is a residual connection between sublayers. The architecture of the Transformer\\nis as shown in Fig.4.3.\\nThere are several attention heads in the multi-head attention sublayer. A head\\nrepresents a scaled dot-product attention structure, which takes the query matrix Q,\\nthe key matrix K, and the value matrix V as the inputs, and the output is computed\\nby\\nAttention(Q, K, V) = Softmax\\n\\x07QKT\\n\\xe2\\x88\\x9adk\\n\\x08\\nV,\\n(4.13)\\nwhere dk is the dimension of query matrix.\\nThe multi-head attention sublayer linearly projects the input hidden states H\\nseveral times into the query matrix, the key matrix, and the value matrix for h heads.\\nThe dimensions of the query, key, and value vectors are dk, dk, and dv, respectively.\\n66\\n4\\nSentence Representation\\nInputs\\nOutputs\\n(shifted right)\\nN \\xc3\\x97\\nFeed\\nForward\\nAdd & Norm\\nMulti-Head\\nAttention\\nAdd & Norm\\nMasked\\nMulti-Head\\nAttention\\nAdd & Norm\\nFeed\\nForward\\nAdd & Norm\\nMulti-Head\\nAttention\\nAdd & Norm\\nN \\xc3\\x97\\nPositional\\nEncoding\\nPositional\\nEncoding\\nLinear\\nSoftmax\\nOutput\\nProbabilities\\nFig. 4.3 The architecture of Transformer\\nThe multi-head attention sublayer could be formulated as\\nMultihead(H) = [head1, head2, . . . , headh]WO,\\n(4.14)\\nwhere headi = Attention(HWQ\\ni , HWK\\ni , HWV\\ni ), and WQ\\ni , WK\\ni and WV\\ni are linear\\nprojections. WO is also a linear projection for the output. Here, the fully connected\\nposition-wise feedforward sublayer contains two linear transformations with ReLU\\nactivation:\\n4.4 Neural Language Model\\n67\\nFFN(x) = W2 max(0, W1x + b1) + b2.\\n(4.15)\\nTransformer is better than RNNs for modeling the long-term dependency, where\\nall tokens will be equally considered during the attention operation. The Transformer\\nwas proposed to solve the problem of machine translation. Since Transformer has a\\nvery powerful ability to model sequential data, it becomes the most popular backbone\\nof NLP applications.\\n4.4.4.2\\nTransformer-Based PLM\\nNeural models can learn large amounts of language knowledge from language mod-\\neling. Since the language knowledge covers the demands of many downstream\\nNLP tasks and provides powerful representations of words and sentences, some\\nresearchers found that knowledge can be transferred to other NLP tasks easily. The\\ntransferred models are called Pre-trained Language Models (PLMs).\\nLanguage modeling is the most basic and most important NLP task. It contains a\\nvariety of knowledge for language understanding, such as linguistic knowledge and\\nfactual knowledge. For example, the model needs to decide whether it should add\\nan article before a noun. This requires linguistic knowledge about articles. Another\\nexample is the question of what is the following word after \\xe2\\x80\\x9cTrump is the president\\nof\\xe2\\x80\\x9d. The answer is \\xe2\\x80\\x9cAmerica\\xe2\\x80\\x9d, which requires factual knowledge. Since language\\nmodeling is very complex, the models can learn a lot from this task.\\nOn the other hand, language modeling only requires plain text without any human\\nannotation. With this feature, the models can learn complex NLP abilities from a very\\nlarge-scale corpus. Since deep learning needs large amounts of data and language\\nmodeling can make full use of all texts in the world, PLMs signi\\xef\\xac\\x81cantly bene\\xef\\xac\\x81t the\\ndevelopment of NLP research.\\nInspired by the success of the Transformer, GPT [50] and BERT [14] begin to\\nadopt the Transformer as the backbone of the pre-trained language models. GPT and\\nBERT are the most representative Transformer-based pre-trained language models\\n(PLMs). Since they achieved state-of-the-art performance on various NLP tasks,\\nnearly all PLMs after them are based on the Transformer. In this subsection, we will\\ntalk about GPT and BERT in more detail.\\nGPT is the \\xef\\xac\\x81rst work to pretrain a PLM based on the Transformer. The train-\\ning procedure of GPT [50] contains two classic stages: generative pretraining and\\ndiscriminative \\xef\\xac\\x81ne-tuning.\\nIn the pretraining stage, the input of the model is a large-scale unlabeled corpus\\ndenoted as U = {u1, u2, . . . , un}. The pretraining stage aims to optimize a language\\nmodel. The learning objective over the corpus is to maximize a conditional likelihood\\nin a \\xef\\xac\\x81xed-size window:\\nL1(U ) =\\n\\x02\\ni\\nlog P(ui|ui\\xe2\\x88\\x92k, . . . , ui\\xe2\\x88\\x921; \\xce\\x98),\\n(4.16)\\n68\\n4\\nSentence Representation\\nwhere k represents the size of the window, the conditional likelihood P is modeled\\nby a neural network with parameters \\xce\\x98.\\nFor a supervised dataset \\xcf\\x87, the input is a sequence of words s = (w1, w2, .., wl)\\nand the output is a label y. The pretraining stage provides an advantageous start\\npoint of parameters that can be used to initialize subsequent supervised tasks. At\\nthis occasion, the objective is a discriminative task that maximizes the conditional\\npossibility distribution:\\nL2(\\xcf\\x87) =\\n\\x02\\n(s,y)\\nlog P(y|w1, . . . , wl),\\n(4.17)\\nwhere P(y|w1, . . . , wl) is modeled by a K-layer Transformer. After the input tokens\\npass through the pretrained GPT, a hidden vector of the \\xef\\xac\\x81nal layer hK\\nl will be pro-\\nduced. To obtain the output distribution, a linear transformation layer is added, which\\nhas the same size as the number of labels:\\nP(y|w1, . . . , wm) = Softmax(WyhK\\nl ).\\n(4.18)\\nThe \\xef\\xac\\x81nal training objective is combined with a language modeling L1 for better\\ngeneralization:\\nL (\\xcf\\x87) = L2(\\xcf\\x87) + \\xce\\xbb \\xe2\\x88\\x97L1(\\xcf\\x87),\\n(4.19)\\nwhere \\xce\\xbb is a weight hyperparameter.\\nBERT [14] is a milestone work in the \\xef\\xac\\x81eld of PLM. BERT achieved signi\\xef\\xac\\x81cant\\nempirical results on 17 different NLP tasks, including SQuAD (outperform human\\nbeing), GLUE (7.7% point absolute improvement), MultiNLI (4.6% point absolute\\nimprovement), etc. Compared to GPT, BERT uses a bidirectional deep Transformer\\nas the model backbone. As illustrated in Fig.4.4, BERT contains pretraining and\\n\\xef\\xac\\x81ne-tuning stages.\\nIn the pretraining stage, two objectives are designed: Masked Language Model\\n(MLM) and Next Sentence Prediction (NSP). (1) For MLM, tokens are randomly\\nPre-training\\nUnlabeled Sentence A and B Pair\\nMasked Sentence A\\nMasked Sentence B\\n[CLS]\\nTok1\\nTokN\\n[SEP]\\n\\xe2\\x80\\xa6\\nTok1\\nTokM\\n\\xe2\\x80\\xa6\\nE[CLS]\\nE1\\nEN\\nE[SEP]\\nE\\xe2\\x80\\x991\\nE\\xe2\\x80\\x99M\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nE[CLS]\\nE1\\nEN\\nE[SEP]\\nE\\xe2\\x80\\x991\\nE\\xe2\\x80\\x99M\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nNSP\\nMask LM\\nMask LM\\nBERT\\nFine-Tuning\\nMNLI\\nNER\\nQuestion Answer Pair\\nQuestion\\nParagraph\\n[CLS]\\nTok1\\nTokN\\n[SEP]\\n\\xe2\\x80\\xa6\\nTok1\\nTokM\\n\\xe2\\x80\\xa6\\nE[CLS]\\nE1\\nEN\\nE[SEP]\\nE\\xe2\\x80\\x991\\nE\\xe2\\x80\\x99M\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nE[CLS]\\nE1\\nEN\\nE[SEP]\\nE\\xe2\\x80\\x991\\nE\\xe2\\x80\\x99M\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nBERT\\nSQuAD\\nStart/End Span\\nFig. 4.4 The pretraining and \\xef\\xac\\x81ne-tuning stages for BERT\\n4.4 Neural Language Model\\n69\\nmasked with a special token [MASK]. The training objective is to predict the masked\\ntokens based on the contexts. Compared with the standard unidirectional conditional\\nlanguage model, which can only be trained in one direction, MLM aims to train a deep\\nbidirectional representation model. This task is inspired by Cloze [64]. (2) The objec-\\ntive of NSP is to capture relationships between sentences for some sentence-based\\ndownstream tasks such as natural language inference (NLI) and question answering\\n(QA). In this task, a binary classi\\xef\\xac\\x81er is trained to predict whether the sentence is\\nthe next sentence for the current. This task effectively captures the deep relationship\\nbetween sentences, exploring semantic information from a different level.\\nAfter pretraining, BERT can capture various language knowledge for downstream\\nsupervised tasks. By modifying inputs and outputs, BERT can be \\xef\\xac\\x81ne-tuned for any\\nNLP tasks, which contain the applications with the input of single text or text pairs.\\nThe input consists of sentence A and sentence B, which can represent (1) sentence\\npairs in paraphrase, (2) hypothesis-premise pairs in entailment, (3) question-passage\\npairs in QA, and (4) text-\\xe2\\x88\\x85for text classi\\xef\\xac\\x81cation task or sequence tagging. For the\\noutput, BERT can produce the token-level representation for each token, which is\\nused to sequence tagging task or question answering. Besides, the special token\\n[CLS] in BERT is fed into the classi\\xef\\xac\\x81cation layer for sequence classi\\xef\\xac\\x81cation.\\n4.4.4.3\\nPLM Family\\nPre-trained language models have rapid progress after BERT. We summarize sev-\\neral important directions of PLMs and show some representative models and their\\nrelationship in Fig.4.5.\\nHere is a brief introduction of the PLMs after BERT. Firstly, there are some\\nvariants of BERT for better general language representation, such as RoBERTa [38]\\nand XLNet [70]. These models mainly focus on the improvement of pretraining tasks.\\nSecondly, some people work on pretrained generation models, such as MASS [57]\\nand UniLM [15]. These models achieve promising results on the generation tasks\\ninstead of the Natural Language Understanding (NLU) tasks used by BERT. Thirdly,\\nthesentencepairformatofBERTinspiredworksonthecross-lingualandcross-modal\\n\\xef\\xac\\x81elds. XLM [8], ViLBERT [39], and VideoBERT [59] are the important works in this\\ndirection. Lastly, there are some works [46, 81] that explore to incorporate external\\nknowledge into PLMs since some low-frequency knowledge cannot be ef\\xef\\xac\\x81ciently\\nlearned by PLMs.\\n4.4.5\\nExtensions\\n4.4.5.1\\nImportance Sampling\\nInspired by the contrastive divergence model, [4] proposes to adopt importance sam-\\npling to accelerate the training of neural language models. They \\xef\\xac\\x81rst normalize the\\n70\\n4\\nSentence Representation\\nFig. 4.5 The Pre-trained language model family\\noutputs of neural network language model and view neural network language models\\nas a special case of energy-based probability models as following:\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) =\\nexp(\\xe2\\x88\\x92ywi)\\n\\x04\\nj exp(\\xe2\\x88\\x92y j).\\n(4.20)\\nThe key idea of importance sampling is to approximate the mean of log-likelihood\\ngradient of the loss function of neural network language model by sampling several\\nimportant words instead of calculating the explicit gradient. Here, the log-likelihood\\ngradient of the loss function of neural network language model can be generally\\nrepresented as\\n\\xe2\\x88\\x82P(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n)\\n\\xe2\\x88\\x82\\xce\\xb8\\n= \\xe2\\x88\\x92\\xe2\\x88\\x82ywi\\n\\xe2\\x88\\x82\\xce\\xb8 +\\n|V |\\n\\x02\\nj=1\\nP(w j|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n)\\xe2\\x88\\x82y j\\n\\xe2\\x88\\x82\\xce\\xb8\\n= \\xe2\\x88\\x92\\xe2\\x88\\x82yi\\n\\xe2\\x88\\x82\\xce\\xb8 + Ewk\\xe2\\x88\\xbcP\\n\\t\\xe2\\x88\\x82yk\\n\\xe2\\x88\\x82\\xce\\xb8\\n\\n,\\n(4.21)\\nwhere \\xce\\xb8 indicates all parameters of the neural network language model. Here, the\\nlog-likelihood gradient of the loss function consists of two parts including positive\\ngradient for target word wi and negative gradient for all words w j, i.e., Ewi\\xe2\\x88\\xbcP[ \\xe2\\x88\\x82y j\\n\\xe2\\x88\\x82\\xce\\xb8 ].\\nHere, the second part can be approximated by sampling important words following\\nthe probability distribution P:\\n4.4 Neural Language Model\\n71\\nEwk\\xe2\\x88\\xbcP\\n\\t\\xe2\\x88\\x82yk\\n\\xe2\\x88\\x82\\xce\\xb8\\n\\n\\xe2\\x89\\x88\\n\\x02\\nwk\\xe2\\x88\\x88V \\xe2\\x80\\xb2\\n1\\n|V \\xe2\\x80\\xb2|\\n\\xe2\\x88\\x82yk\\n\\xe2\\x88\\x82\\xce\\xb8 ,\\n(4.22)\\nwhere V \\xe2\\x80\\xb2 is the word set sampled under P.\\nHowever, since we cannot obtain probability distribution P in advance, it is impos-\\nsible to sample important words following the probability distribution P. Therefore,\\nimportance sampling adopts a Monte Carlo scheme which uses an existing proposal\\ndistribution Q to approximate P, and then we have\\nEwk\\xe2\\x88\\xbcP\\n\\t\\xe2\\x88\\x82yk\\n\\xe2\\x88\\x82\\xce\\xb8\\n\\n\\xe2\\x89\\x88\\n1\\n|V \\xe2\\x80\\xb2\\xe2\\x80\\xb2|\\n\\x02\\nwl\\xe2\\x88\\x88V \\xe2\\x80\\xb2\\xe2\\x80\\xb2\\n\\xe2\\x88\\x82yl\\n\\xe2\\x88\\x82\\xce\\xb8 P(wl|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n)/Q(wl),\\n(4.23)\\nwhere V \\xe2\\x80\\xb2\\xe2\\x80\\xb2 is the word set sampled under Q. Moreover, the sample size of impor-\\ntance sampling approach should be increased as training processes in order to avoid\\ndivergence, which aims to ensure its effective sample size S:\\nS =\\n(\\x04\\nwl\\xe2\\x88\\x88V \\xe2\\x80\\xb2\\xe2\\x80\\xb2 rl)2\\n\\x04\\nwl\\xe2\\x88\\x88V \\xe2\\x80\\xb2\\xe2\\x80\\xb2 r2\\nl\\n,\\n(4.24)\\nwhere rl is further de\\xef\\xac\\x81ned as\\nrl =\\nP(wl|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n)/Q(wl)\\n\\x04\\nw j\\xe2\\x88\\x88V \\xe2\\x80\\xb2\\xe2\\x80\\xb2 P(w j|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n)/Q(w j)\\n.\\n(4.25)\\n4.4.5.2\\nWord Classi\\xef\\xac\\x81cation\\nBesides important sampling, researchers [7, 22] also propose class-based language\\nmodel, which adopts word classi\\xef\\xac\\x81cation to improve the performance and speed of a\\nlanguage model. In class-based language model, all words are assigned to a unique\\nclass, and the conditional probability of a word given its context can be decomposed\\ninto the probability of the word\\xe2\\x80\\x99s class given its previous words and the probability\\nof the word given its class and history, which is formally de\\xef\\xac\\x81ned as\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) =\\n\\x02\\nc(wi)\\xe2\\x88\\x88C\\nP(wi|c(wi))P(c(wi)|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n),\\n(4.26)\\nwhere C indicates the set of all classes and c(wi) indicates the class of word wi.\\nMoreover, [44] proposes a hierarchical neural network language model, which\\nextends word classi\\xef\\xac\\x81cation to a hierarchical binary clustering of words in a language\\nmodel. Instead of simply assigning each word with a unique class, it \\xef\\xac\\x81rst builds\\na hierarchical binary tree of words according to the word similarity obtained from\\nWordNet.Next,itassignsauniquebitvectorc(wi) = [c1(wi), c2(wi), . . . , cl(wi)]for\\n72\\n4\\nSentence Representation\\neach word, which indicates the hierarchical classes of them. And then the conditional\\nprobability of each word can be de\\xef\\xac\\x81ned as\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) =\\nl\\x03\\nj=0\\nP(c j(wi)|c1(wi), c2(wi), . . . , c j\\xe2\\x88\\x921(wi), wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n).\\n(4.27)\\nThe hierarchical neural network language model can achieve O(k/ log k) speed\\nup as compared to a standard language model. However, the experimental results of\\n[44] show that although the hierarchical neural network language model achieves\\nan impressive speed up for modeling sentences, it has worse performance than the\\nstandard language model. The reason is perhaps that the introduction of hierarchical\\narchitecture or word classes imposes a negative in\\xef\\xac\\x82uence on the word classi\\xef\\xac\\x81cation\\nby neural network language models.\\n4.4.5.3\\nCaching\\nCaching is also one of the important extensions in language model. A type of cache-\\nbased language model assumes that each word in recent context is more likely to\\nappear again [58]. Hence, the conditional probability of a word can be calculated by\\nthe information from history and caching:\\nP(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) = \\xce\\xbbPs(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) + (1 \\xe2\\x88\\x92\\xce\\xbb)Pc(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n),\\n(4.28)\\nwhere Ps(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) indicates the conditional probability generated by standard lan-\\nguage and Pc(wi|wi\\xe2\\x88\\x921\\ni\\xe2\\x88\\x92n) indicates the conditional probability generated by caching,\\nand \\xce\\xbb is a constant.\\nAnother cache-based language model is also used to speed up the RNN language\\nmodeling [27]. The main idea of this approach is to store the outputs and states of\\nlanguage models for future predictions given the same contextual history.\\n4.5\\nApplications\\nIn this section, we will introduce two typical sentence-level NLP applications includ-\\ning text classi\\xef\\xac\\x81cation and relation extraction, as well as how to utilize sentence rep-\\nresentation for these applications.\\n4.5 Applications\\n73\\n4.5.1\\nText Classi\\xef\\xac\\x81cation\\nText classi\\xef\\xac\\x81cation is a typical NLP application and has lots of important real-world\\ntasks such as parsing and semantic analysis. Therefore, it has attracted the interest of\\nmany researchers. The conventional text classi\\xef\\xac\\x81cation models (e.g., the LDA [6] and\\ntree kernel [48] models) focus on capturing more contextual information and correct\\nword order by extracting more useful and distinct features, but still expose a few\\nissues (e.g., data sparseness) which has the signi\\xef\\xac\\x81cant impact on the classi\\xef\\xac\\x81cation\\naccuracy. Recently, with the development of deep learning in the various \\xef\\xac\\x81elds of\\narti\\xef\\xac\\x81cial intelligence, neural models have been introduced into the text classi\\xef\\xac\\x81cation\\n\\xef\\xac\\x81eld due to their abilities of text representation learning. In this section, we will\\nintroduce the two typical tasks of text classi\\xef\\xac\\x81cation, including sentence classi\\xef\\xac\\x81cation\\nand sentiment classi\\xef\\xac\\x81cation.\\n4.5.1.1\\nSentence Classi\\xef\\xac\\x81cation\\nSentence classi\\xef\\xac\\x81cation aims to assign a sentence an appropriate category, which is a\\nbasic task of the text classi\\xef\\xac\\x81cation application.\\nConsidering the effectiveness of the CNN models in capturing sentence semantic\\nmeanings, [31] \\xef\\xac\\x81rst proposes to utilize the CNN models trained on the top of pre-\\ntrained word embeddings to classify sentences, which achieved promising results on\\nseveral sentence classi\\xef\\xac\\x81cation datasets. Then, [30] introduces a dynamic CNN model\\nto model the semantic meanings of sentences. This model handles sentences of vary-\\ning lengths and uses dynamic max-pooling over linear sequences, which could help\\nthe model capture both short-range and long-range semantic relations in sentences.\\nFurthermore, [9] proposes a novel CNN-based model named as Very Deep CNN,\\nwhich operates directly at the character level. It shows that those deeper models have\\nbetter results on sentence classi\\xef\\xac\\x81cation and can capture the hierarchical information\\nfrom scattered characters to whole sentences. Yin and Sch\\xc3\\xbctze [74] also propose\\nMV-CNN, which utilizes multiple types of pretrained word embeddings and extracts\\nfeatures from multi-granular phrases with variable-sized convolutional layers. To\\naddress the drawbacks of MV-CNN such as model complexity and the requirement\\nfor the same dimension of embeddings, [80] proposes a novel model called MG-CNN\\nto capture multiple features from multiple sets of embeddings that are concatenated\\nat the penultimate layer. Zhang et al. [79] present RA-CNN to jointly exploit labels\\non documents and their constituent sentences, which can estimate the probability\\nthat a given sentence is informative and then scales the contribution of each sentence\\nto aggregate a document representation in proportion to the estimates.\\nThe RNN model which aims to capture the sequential information of sentences\\nis also widely used in sentence classi\\xef\\xac\\x81cation. Lai et al. [32] propose a neural net-\\nwork for text classi\\xef\\xac\\x81cation, which applies a recurrent structure to capture contextual\\ninformation. Moreover, [37] introduces a multitask learning framework based on the\\nRNN to jointly learn across multiple sentence classi\\xef\\xac\\x81cation tasks, which employs\\n74\\n4\\nSentence Representation\\nthree different mechanisms of sharing information to model sentences with both task-\\nspeci\\xef\\xac\\x81c and shared layers. Yang et al. [71] introduce word-level and sentence-level\\nattention mechanisms into an RNN-based model as well as a hierarchical structure\\nto capture the hierarchical information of documents for sentence classi\\xef\\xac\\x81cation.\\n4.5.1.2\\nSentiment Classi\\xef\\xac\\x81cation\\nSentiment classi\\xef\\xac\\x81cation is a special task of the sentence classi\\xef\\xac\\x81cation application,\\nwhose objective is to classify the sentimental polarities of opinions a piece of text\\ncontains, e.g., favorable or unfavorable, positive or negative. This task appeals the\\nNLP community since it has lots of potential downstream applications such as movie\\nreview suggestions.\\nSimilar to text classi\\xef\\xac\\x81cation, the sentence representation based on neural models\\nhas also been widely explored for sentiment classi\\xef\\xac\\x81cation. Glorot et al. [20] use a\\nstacked denoising autoencoder in sentiment classi\\xef\\xac\\x81cation for the \\xef\\xac\\x81rst time. Then,\\na series of recursive neural network models based on the recursive tree structure\\nof sentences are conducted to learn sentence representations for sentiment classi-\\n\\xef\\xac\\x81cation, including the recursive autoencoder (RAE) [55], matrix-vector recursive\\nneural network (MV-RNN) [54], and recursive neural tensor network (RNTN) [56].\\nBesides, [29] adopts a CNN to learn sentence representations and achieves promising\\nperformance in sentiment classi\\xef\\xac\\x81cation.\\nThe RNN models also bene\\xef\\xac\\x81t sentiment classi\\xef\\xac\\x81cation as they are able to capture\\nthe sequential information. Li et al. [35] and Tai et al. [62] investigate a tree-structured\\nLSTMmodelontextclassi\\xef\\xac\\x81cation.Therearealsosomehierarchicalmodelsproposed\\nto deal with document-level sentiment classi\\xef\\xac\\x81cation [5, 63], which generate seman-\\ntic representations at different levels (e.g., phrase, sentence, or document) within\\na document. Moreover, the attention mechanism is also introduced into sentiment\\nclassi\\xef\\xac\\x81cation, which aims to select important words from a sentence or important\\nsentences from a document [71].\\n4.5.2\\nRelation Extraction\\nTo enrich existing KGs, researchers have devoted many efforts to automatically \\xef\\xac\\x81nd-\\ning novel relational facts in text. Therefore, relation extraction (RE), which aims\\nat extracting relational facts according to semantic information in plain text, has\\nbecome a crucial NLP application. As RE is also an important downstream applica-\\ntion of sentence representation, we will, respectively, introduce the techniques and\\nextensions to show how to utilize sentence representation for different RE scenarios.\\nConsidering neural networks have become the backbone of the recent NLP research,\\nwe mainly focus on Neural RE (NRE) models in this section.\\n4.5 Applications\\n75\\nFig. 4.6 An example of sentence-level relation extraction\\n4.5.2.1\\nSentence-Level NRE\\nSentence-level NRE aims at predicting the semantic relations between the given\\nentity (or nominal) pair in a sentence. As shown in Fig.4.6, given the input sentence\\ns which consists of n words s = {w1, w2, . . . , wn} and its corresponding entity pair\\ne1 and e2 as input, sentence-level NRE wants to obtain the conditional probability\\nP(r|s, e1, e2) of relation r (r \\xe2\\x88\\x88R) via a neural network, which can be formalized\\nas\\nP(r|s, e1, e2) = P(r|s, e1, e2, \\xce\\xb8),\\n(4.29)\\nwhere \\xce\\xb8 is all parameters of the neural network and r is a relation in the relation set\\nR.\\nA basic form of sentence-level NRE consists of three components: (a) an input\\nencoder to give a representation for each input word, (b) a sentence encoder which\\ncomputes either a single vector or a sequence of vectors to represent the original\\nsentence, and (c) a relation classi\\xef\\xac\\x81er which calculates the conditional probability\\ndistribution of all relations.\\nInput Encoder. First, a sentence-level NRE system projects the discrete words\\nof the source sentence into a continuous vector space, and obtains the input repre-\\nsentation w = {w1, w2, . . . , wm} of the source sentence.\\n(1) Word Embeddings. Word embeddings aim to transform words into distributed\\nrepresentations to capture the syntactic and semantic meanings of the words.\\nIn the sentence s, every word wi is represented by a real-valued vector. Word\\nrepresentations are encoded by column vectors in an embedding matrix E \\xe2\\x88\\x88\\nRda\\xc3\\x97|V | where V is a \\xef\\xac\\x81xed-sized vocabulary. Although word embeddings are\\nthe most common way to represent input words, there are also efforts made to\\nutilize more complicated information of input sentences for RE.\\n(2) Position Embeddings. In RE, the words close to the target entities are usually\\ninformative to determine the relation between the entities. Therefore, position\\nembeddings are used to help models keep track of how close each word is to\\nthe head or tail entities. It is de\\xef\\xac\\x81ned as the combination of the relative distances\\nfrom the current word to the head or tail entities. For example, in the sentence\\nBill_Gates is the founder of Microsoft., the relative distance\\n76\\n4\\nSentence Representation\\nfrom the word founder to the head entity Bill_Gates is \\xe2\\x88\\x923 and the tail\\nentity Microsoft is 2. Besides word position embeddings, more linguistic\\nfeatures are also considered in addition to the word embeddings to enrich the\\nlinguistic features of the input sentence.\\n(3) Part-of-speech (POS) Tag Embeddings. POS tag embeddings are to represent the\\nlexical information of the target word in the sentence. Because word embeddings\\nare obtained from a large-scale general corpus, the general information they\\ncontain may not be in accordance with the meaning in a speci\\xef\\xac\\x81c sentence. Hence,\\nit is necessary to align each word with its linguistic information considering its\\nspeci\\xef\\xac\\x81c context, e.g., noun and verb. Formally, each word wi is encoded by the\\ncorresponding column vector in an embedding matrix Ep \\xe2\\x88\\x88Rd p\\xc3\\x97|V p|, where d p\\nis the dimension of embedding vector and V p indicates a \\xef\\xac\\x81xed-sized POS tag\\nvocabulary.\\n(4) WordNet Hypernym Embeddings. WordNet hypernym embeddings aim to take\\nadvantages of the prior knowledge of hypernym to help RE models. When\\ngiven the hypernym information of each word in WordNet (e.g., noun.food and\\nverb.motion), it is easier to build the connections between different but concep-\\ntually similar words. Formally, each word wi is encoded by the corresponding\\ncolumn vector in an embedding matrix Eh \\xe2\\x88\\x88Rdh\\xc3\\x97|V h|, where dh is the dimension\\nof embedding vector and V h indicates a \\xef\\xac\\x81xed-sized hypernym vocabulary.\\nFor each word, the NRE models often concatenate some of the above four feature\\nembeddings as their input embeddings. Therefore, the feature embeddings of all\\nwords are concatenated and denoted as a \\xef\\xac\\x81nal input sequence w = {w1, w2, . . . , wm},\\nwhere wi \\xe2\\x88\\x88Rd, d is the total dimension of all feature embeddings concatenated for\\neach word.\\nSentence Encoder. The sentence encoder is the core for sentence representation,\\nwhich encodes input representations into either a single vector or a sequence of\\nvectors x to represent sentences. We will introduce the different sentence encoders\\nin the following.\\n(1) Convolutional Neural Network Encoder. Zeng et al. [76] propose to encode\\ninput sentences using a CNN model, which extracts local features by a convolutional\\nlayer and combines all local features via a max-pooling operation to obtain a \\xef\\xac\\x81xed-\\nsized vector for the input sentence. Formally, a convolutional layer is de\\xef\\xac\\x81ned as an\\noperation on a vector sequence w:\\np = CNN(w),\\n(4.30)\\nwhere CNN indicates the convolution operation inside the convolutional layer.\\nAnd the ith element of the sentence vector x can be calculated as follows:\\n[x]i = f (max(pi)),\\n(4.31)\\nwhere f is a nonlinear function applied at the output, such as the hyperbolic tangent\\nfunction.\\n4.5 Applications\\n77\\nFurther, PCNN [75], which is a variation of CNN, adopts a piece-wise max-\\npooling operation. All hidden vectors {p1, p2, . . .} are divided into three segments\\nby the head and tail entities. The max-pooling operation is performed over the three\\nsegments separately, and the x is the concatenation of the pooling results over the\\nthree segments.\\n(2) Recurrent Neural Network Encoder. Zhang and Wang [78] propose to embed\\ninput sentences using an RNN model which can learn the temporal features. Formally,\\neach input word representation is put into recurrent layers step by step. For each step\\ni, the network takes the ith word representation vector wi and the output of the\\nprevious i \\xe2\\x88\\x921 steps hi\\xe2\\x88\\x921 as input:\\nhi = RNN(wi, hi\\xe2\\x88\\x921),\\n(4.32)\\nwhere RNN indicates the transform function inside the RNN cell, which can be the\\nLSTM units or the GRU units mentioned before.\\nThe conventional RNN models typically deal with text sequences from start to\\nend, and build the hidden state of each word only considering its preceding words.\\nIt has been veri\\xef\\xac\\x81ed that the hidden state considering its following words is more\\neffective. Hence, the bi-directional RNN (BRNN) [52] is adopted to learn hidden\\nstates using both preceding and following words.\\nSimilar to the previous CNN models in RE, the RNN model combines the output\\nvectors of the recurrent layer as local features, and then uses a max-pooling operation\\nto extract the global feature, which forms the representation of the whole input\\nsentence. The max-pooling layer could be formulated as\\n[x] j = max\\ni\\n[hi] j.\\n(4.33)\\nBesides max-pooling, word attention can also combine all local feature vectors\\ntogether. The attention mechanism [1] learns attention weights on each step. Sup-\\nposing H = [h1, h2, . . . , hm] is the matrix consisting of all output vectors produced\\nby the recurrent layer, the feature vector of the whole sentence x is formed by a\\nweighted sum of these output vectors:\\n\\xce\\xb1 = Softmax(s\\xe2\\x8a\\xa4tanh(H)),\\n(4.34)\\nx = H\\xce\\xb1\\xe2\\x8a\\xa4,\\n(4.35)\\nwhere s is a trainable query vector.\\nBesides,[42]proposesamodelthatcapturesinformationfrombothwordsequence\\nand tree-structured dependency by stacking bidirectional path-based LSTM-RNNs\\n(i.e., bottom-up and top-down). More speci\\xef\\xac\\x81cally, it focuses on the shortest path\\nbetween the two target entities in the dependency tree, and utilizes the stacked layers\\nto encode the shortest path for the whole sentence representation. In fact, some\\npreliminary work [69] has shown that these paths are useful in RE, and various\\n78\\n4\\nSentence Representation\\nrecursive neural models are also proposed for this. Next, we will introduce these\\nrecursive models in detail.\\n(3) Recursive Neural Network Encoder. The recursive encoder aims to extract\\nfeatures from the information of syntactic parsing trees, considering the syntactic\\ninformation is bene\\xef\\xac\\x81cial for extracting relations from sentences. Generally, these\\nencoders treat the tree structure inside syntactic parsing trees as a strategy of com-\\nposition as well as a direction to combine each word feature.\\nSocher et al. [54] propose a recursive matrix-vector model (MV-RNN) which\\ncaptures the structure information by assigning a matrix-vector representation for\\neach constituent of the constituents in parsing trees. The vector captures the meaning\\nof the constituent itself and the matrix represents how it modi\\xef\\xac\\x81es the meaning of the\\nword it combines with. Tai et al. [62] further propose two types of tree-structured\\nLSTMs including the Child-Sum Tree-LSTM and the N-ary Tree-LSTM to capture\\ntree structure information. For the Child-Sum Tree-LSTM, given a tree, let C(t)\\ndenote the set of children of node t. Its transition equations are de\\xef\\xac\\x81ned as follows:\\n\\xcb\\x86ht =\\n\\x02\\nk\\xe2\\x88\\x88C(t)\\nTLSTM(hk),\\n(4.36)\\nwhere TLSTM(\\xc2\\xb7) indicates a Tree-LSTM cell, which is simply modi\\xef\\xac\\x81ed from LSTM\\ncell. The N-ary Tree-LSTM has similar transition equations as the Child-Sum Tree-\\nLSTM. The only difference is that it limits the tree structures to have at most N\\nbranches.\\nRelation Classi\\xef\\xac\\x81er. When obtaining the representation x of the input sentence,\\nrelation classi\\xef\\xac\\x81er calculates the conditional probability P(r|x, e1, e2) via a softmax\\nlayer as follows:\\nP(r|x, e1, e2) = Softmax(Mx + b),\\n(4.37)\\nwhere M indicates the relation matrix and b is a bias vector.\\n4.5.2.2\\nBag-Level NRE\\nAlthough existing neural models have achieved great success for extracting novel\\nrelational facts, it always suffers the lack of training data. To address this issue,\\nresearchers proposed a distant supervision assumption to generate training data\\nvia aligning KGs and plain text automatically. The intuition of distant supervision\\nassumption is that all sentences that contain two entities will express their relations\\nin KGs. For example, (New York, city_of, United States) is a relational\\nfact in a KG, distant supervision assumption will regard all sentences that contain\\nthese two entities as positive instances for the relation city_of. It offers a natural\\nway of utilizing information from multiple sentences (bag-level) rather than a single\\nsentence (sentence-level) to decide if a relation holds between two entities.\\nTherefore, bag-level NRE aims to predict the semantic relations between an entity\\npair using all involved sentences. As shown in Fig.4.7, given the input sentence set\\n4.5 Applications\\n79\\nFig. 4.7 An example of bag-level relation extraction\\nS which consists of n sentences S = {s1, s2, . . . , sn} and its corresponding entity\\npair e1 and e2 as inputs, bag-level NRE wants to obtain the conditional probability\\nP(r|S, e1, e2) of relation r (r \\xe2\\x88\\x88R) via a neural network, which can be formalized\\nas\\nP(r|S, e1, e2) = P(r|S, e1, e2, \\xce\\xb8).\\n(4.38)\\nA basic form of bag-level NRE consists of four components: (a) an input encoder\\nsimilar to sentence-level NRE, (b) a sentence encoder similar to sentence-level NRE,\\n(c) a bag encoder which computes a vector representing all related sentences in a bag,\\nand (d) a relation classi\\xef\\xac\\x81er similar to sentence-level NRE which takes bag vectors\\nas input instead of sentence vectors. As the input encoder, sentence encoder, and\\nrelation classi\\xef\\xac\\x81er of bag-level NRE are similar to the ones of sentence-level NRE,\\nwe will thus mainly focus on introducing the bag encoder in detail.\\nBag Encoder. The bag encoder encodes all sentence vectors into a single vector\\nS. We will introduce the different bag encoders in the following:\\n(1) Random Encoder. It simply assumes that each sentence can express the relation\\nbetween two target entities and randomly select one sentence to represent the bag.\\nFormally, the bag representation is de\\xef\\xac\\x81ned as\\nS = si (i \\xe2\\x88\\x88{1, 2, . . . , n}),\\n(4.39)\\nwhere si indicates the sentence representation of si \\xe2\\x88\\x88S and i is a random index.\\n(2) Max Encoder. As introduced above, not all sentences containing two target\\nentities can express their relations. For example, the sentence New York City\\nis the premier gateway for legal immigration to the\\nUnited States does not express the relation city of. Hence, in [75], they\\nfollow the at-least-one assumption which assumes that at least one sentence that\\ncontains these two target entities can express their relations, and select the sentence\\n80\\n4\\nSentence Representation\\nwith the highest probability for the relation to represent the bag. Formally, bag rep-\\nresentation is de\\xef\\xac\\x81ned as\\nS = si (i = arg max\\ni\\nP(r|si, e1, e2)).\\n(4.40)\\n(3) Average Encoder. Both random encoder or max encoder use only one sentence\\nto represent the bag, which ignores the rich information of different sentences. To\\nexploit the information of all sentences, [36] believes that the representation S of\\nthe bag depends on all sentences\\xe2\\x80\\x99 representations. Each sentence representation si\\ncan give the relation information about two entities to a certain extent. The average\\nencoder assumes that all sentences contribute equally to the representation of the\\nbag. It means the embedding S of the bag is the average of all the sentence vectors:\\nS =\\n\\x02\\ni\\n1\\nn si.\\n(4.41)\\n(4) Attentive Encoder. Due to the wrong label issue brought by distant supervision\\nassumption inevitably, the performance of average encoder will be in\\xef\\xac\\x82uenced by\\nthose sentences that contain no relation information. To address this issue, [36]\\nfurther proposes to employ a selective attention to reduce those noisy sentences.\\nFormally, the bag representation is de\\xef\\xac\\x81ned as a weighted sum of sentence vectors:\\nS =\\n\\x02\\ni\\n\\xce\\xb1isi,\\n(4.42)\\nwhere \\xce\\xb1i is de\\xef\\xac\\x81ned as\\n\\xce\\xb1i =\\nexp(s\\xe2\\x8a\\xa4\\ni Ar)\\n\\x04\\nj exp(x\\xe2\\x8a\\xa4\\nj Ar),\\n(4.43)\\nwhere A is a diagonal matrix and r is the representation vector of relation r.\\nRelation Classi\\xef\\xac\\x81er. Similar to sentence-level NRE, when obtaining the bag rep-\\nresentation S, relation classi\\xef\\xac\\x81er also calculates the conditional probability P(r|S, e1,\\ne2) via a softmax layer as follows:\\nP(r|S, e1, e2) = Softmax(MS + b),\\n(4.44)\\nwhere M indicates the relation matrix and b is a bias vector.\\n4.5.2.3\\nExtensions\\nRecently, NRE systems have achieved signi\\xef\\xac\\x81cant improvements in both, the super-\\nvised and distantly supervised scenarios. However, there are still many challenges in\\nthe task of RE, and many researchers have been focusing on other aspects to improve\\n4.5 Applications\\n81\\nthe performance of NRE as well. In this section, we will introduce these extensions\\nin detail.\\nUtilization of External Information. Most existing NRE systems stated above\\nonly concentrate on the sentences which are extracted, regardless of the rich external\\ninformation such as KGs. This heterogeneous information could provide additional\\nknowledge from KG and is essential when extracting new relational facts.\\nHan et al. [24] propose a novel joint representation learning framework for knowl-\\nedge acquisition. The key idea is that the joint model learns knowledge and text\\nrepresentations within a uni\\xef\\xac\\x81ed semantic space via KG-text alignments. For the text\\npart, the sentence with two entities Mark Twain and Florida is regarded as\\nthe input for a CNN encoder, and the output of CNN is considered to be the latent\\nrelation PlaceOfBirth of this sentence. For the KG part, entity and relation rep-\\nresentations are learned via translation-based methods. The learned representations\\nof KG and text parts are aligned during training. Besides this preliminary attempt,\\nmany efforts have been devoted to this direction [25, 28, 51, 67, 68].\\nIncorporating Relational Paths. Although existing NRE systems have achieved\\npromising results, they still suffer a major problem: the models can only directly\\nlearn from those sentences which contain both two-target entities. However, those\\nsentences containing only one of the entities could also provide useful information\\nand help build inference chains. For example, if we know that \\xe2\\x80\\x9cA is the son of B\\xe2\\x80\\x9d\\nand \\xe2\\x80\\x9cB is the son of C\\xe2\\x80\\x9d, we can infer that A is the grandson of C.\\nTo utilize the information of both direct and indirect sentences, [77] introduces\\na path-based NRE model that incorporates textual relational paths. The model \\xef\\xac\\x81rst\\nemploys a CNN encoder to embed the semantic meanings of sentences. Then, the\\nmodel builds a relation path encoder, which measures the probability of relations\\ngiven an inference chain in the text. Finally, the model combines information from\\nboth direct sentences and relational paths, and then predicts the con\\xef\\xac\\x81dence of each\\nrelationship. This work is the \\xef\\xac\\x81rst effort to consider the knowledge of relation path\\nin text for NRE, and there are also several methods later to consider the reasoning\\npath of sentence semantic meanings for RE [11, 19].\\nDocument-level Relation Extraction. In fact, not all relational facts can be\\nextracted by sentence-level RE, i.e., a large number of relational facts are expressed\\nin multiple sentences. Taking Fig.4.9 as an example, multiple entities are mentioned\\nin the document and exhibit complex interactions. In order to identify the relational\\nfact (Riddarhuset, country, Sweden), one has to \\xef\\xac\\x81rst identify the fact that\\nRiddarhuset is located in Stockholm from Sentence 4, then identify the facts\\nStockholm is the capital of Sweden and Sweden is a country from Sentence 1.\\nWith the above facts, we can \\xef\\xac\\x81nally infer that the sovereign state of Riddarhuset\\nis Sweden. This process requires reading and reasoning over multiple sentences in\\na document, which is intuitively beyond the reach of sentence-level RE methods.\\nAccording to the statistics on a human-annotated corpus sampled from Wikipedia\\ndocuments [72], at least 40.7% relational facts can only be extracted from multi-\\nple sentences, which is not negligible. Swampillai and Stevenson [61] and Verga\\net al. [66] also report similar observations. Therefore, it is necessary to move RE\\n82\\n4\\nSentence Representation\\nFig. 4.8 An example of document-level relation extraction\\nforward from the sentence level to the document level. Figure4.8 is an example for\\ndocument-level RE.\\nFig. 4.9 An example from DocRED [72]\\n4.5 Applications\\n83\\nHowever, existing datasets for document-level RE either only have a small num-\\nber of manually annotated relations and entities [34], or exhibit noisy annotations\\nfrom distant supervision [45, 49], or serve speci\\xef\\xac\\x81c domains or approaches [33]. To\\naddress this issue, [72] constructs a large-scale, manually annotated, and general-\\npurpose document-level RE dataset, named as DocRED. DocRED is constructed\\nfrom Wikipedia and Wikidata, and has two key features. First, DocRED contains\\n132, 375 entities and 56, 354 relational facts annotated on 5, 053 Wikipedia docu-\\nments, which is the largest human-annotated document-level RE dataset now. Sec-\\nond, over 40% of the relational facts in DocRED can only be extracted from multiple\\nsentences. This makes DocRED require reading multiple sentences in a document\\nto recognize entities and inferring their relations by synthesizing all information of\\nthe document.\\nThe experimental results on DocRED show that the performance of existing\\nsentence-level RE methods declines signi\\xef\\xac\\x81cantly on DocRED, indicating the task\\ndocument-level RE is more challenging than sentence-level RE and remains an open\\nproblem. It also relates to the document representation which will be introduced in\\nthe next chapter.\\nFew-shot Relation Extraction.\\nAs we mentioned before, the performance of the conventional RE models [23,\\n76] heavily depend on time-consuming and labor-intensive annotated data, which\\nmake themselves hard to generalize well. Although adopting distant supervision\\nis a primary approach to alleviate this problem, the distantly supervised data also\\nexhibits a long-tail distribution, where most relations have very limited instances.\\nFurthermore, distant supervision suffers the wrong labeling problem, which makes\\nit harder to classify long-tail relations. Hence, it is necessary to study training RE\\nmodels with insuf\\xef\\xac\\x81cient training instances. Figure4.10 is an example for few-shot\\nRE.\\nFig. 4.10 An example of few-shot relation extraction\\n84\\n4\\nSentence Representation\\nTable 4.1 An example for a 3 way 2 shot scenario. Different colors indicate different entities,\\nunderline for head entity, and emphasize for tail entity\\nSupporting set\\n(A) capital_of\\n(1) London is the capital of the U.K\\n(2) Washington is the capital of the U.S.A\\n(B) member_of\\n(1) Newton served as the president of the Royal\\nSociety\\n(2) Leibniz was a member of the Prussian\\nAcademy of Sciences\\n(C) birth_name\\n(1) Samuel Langhorne Clemens, better known\\nby his pen name Mark Twain, was an American\\nwriter\\n(2) Alexei Maximovich Peshkov, primarily\\nknown as Maxim Gorky, was a Russian and\\nSoviet writer\\nTest instance\\n(A) or (B) or (C)\\nEuler was elected a foreign member of the\\nRoyal Swedish Academy of Sciences\\nFewRel [26] is a new large-scale supervised few-shot RE dataset, which requires\\nmodels capable of handling classi\\xef\\xac\\x81cation task with a handful of training instances,\\nas shown in Table4.1. Bene\\xef\\xac\\x81ting from the FewRel dataset, there are some efforts to\\nexploring few-shot RE [17, 53, 73] and achieve promising results. Yet, few-shot RE\\nstill remains a challenging problem for further research [18].\\n4.6\\nSummary\\nIn this chapter, we introduce sentence representation learning. Sentence representa-\\ntion encodes the semantic information of a sentence into a real-valued representation\\nvector, and can be utilized in further sentence classi\\xef\\xac\\x81cation or matching tasks. First,\\nwe introduce the one-hot representation for sentences and probabilistic language\\nmodels. Secondly, we extensively introduce several neural language models, includ-\\ning adopting the feedforward neural networks, the convolutional neural networks, the\\nrecurrent neural networks, and the Transformer for language models. These neural\\nmodels can learn rich linguistic and semantic knowledge from language modeling.\\nBene\\xef\\xac\\x81ting from this, the pre-trained language models trained with large-scale cor-\\npora have achieved state-of-the-art performance on various downstream NLP tasks\\nby transferring the learned semantic knowledge from general corpora to the target\\ntasks. Finally, we introduce several typical applications of sentence representation\\nincluding text classi\\xef\\xac\\x81cation and relation extraction.\\n4.6 Summary\\n85\\nFor further understanding of sentence representation learning and its applications,\\nthere are also some recommended surveys and books including\\n\\xe2\\x80\\xa2 Yoav, Neural network methods for natural language processing [21].\\n\\xe2\\x80\\xa2 Deng & Liu, Deep learning in natural language processing [13].\\nIn the future, for better sentence representation, some directions are requiring\\nfurther efforts:\\n(1) Exploring Advanced Architectures. The improvement of model architectures\\nis the key factor in the success of sentence representation. From the feedforward\\nneural networks to the Transformer, people are designing more suitable neural\\nmodels for sequential inputs. Based on the Transformer, some researchers are\\nworking on new NLP architectures. For instance, Transformer-XL [10] is pro-\\nposed to solve the problem of \\xef\\xac\\x81xed-length context in the Transformer. Since\\nthe Transformer is the state-of-the-art NLP architecture, current works mainly\\nadopt attention mechanisms. Beyond these works, is it possible to introduce\\nmore human cognitive mechanisms to neural models?\\n(2) Modeling Long Documents. The representation of long documents is an impor-\\ntant extension of sentence representation. There are some new challenges during\\nmodeling long documents, such as discourse analysis and co-reference resolu-\\ntion. Although some existing works already provide document-level NLP tasks\\n(e.g., DocRED [72]), the model performance on these tasks is still much lower\\nthan the human performance. We will also introduce the advances in document\\nrepresentation learning in the following chapter.\\n(3) Performing Ef\\xef\\xac\\x81cient Representation. Although the combination of Trans-\\nformer and large-scale data leads to very powerful sentence representation, these\\nrepresentation models require expensive computational cost, which limits the\\napplications in downstream tasks. Some existing works explore to use model\\ncompression techniques for more ef\\xef\\xac\\x81cient models. These techniques include\\nknowledge distillation [60], parameter pruning [16], etc. Beyond these works,\\nthere remain lots of unsolved problems for developing better representation mod-\\nels, which can ef\\xef\\xac\\x81ciently learn from large-scale data and provide effective vectors\\nin downstream tasks.\\nReferences\\n1. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. In Proceedings of ICLR, 2015.\\n2. Yoshua Bengio. Neural net language models. Scholarpedia, 3(1):3881, 2008.\\n3. Yoshua Bengio, R\\xc3\\xa9jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\\nlanguage model. Journal of Machine Learning Research, 3(Feb):1137\\xe2\\x80\\x931155, 2003.\\n4. Yoshua Bengio, Jean-S\\xc3\\xa9bastien Sen\\xc3\\xa9cal, et al. Quick training of probabilistic neural nets by\\nimportance sampling. In Proceedings of AISTATS, 2003.\\n86\\n4\\nSentence Representation\\n5. Parminder Bhatia, Yangfeng Ji, and Jacob Eisenstein. Better document-level sentiment analysis\\nfrom rst discourse parsing. In Proceedings of EMNLP, 2015.\\n6. David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of\\nMachine Learning Research, 3:993\\xe2\\x80\\x931022, 2003.\\n7. Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai.\\nClass-based n-gram models of natural language. Computational linguistics, 18(4):467\\xe2\\x80\\x93479,\\n1992.\\n8. Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In Pro-\\nceedings of NeurIPS, 2019.\\n9. Alexis Conneau, Holger Schwenk, Lo\\xc3\\xafc Barrault, and Yann Lecun. Very deep convolutional\\nnetworks for text classi\\xef\\xac\\x81cation. In Proceedings of EACL, volume 1, 2017.\\n10. ZihangDai,ZhilinYang,YimingYang,JaimeGCarbonell,QuocLe,andRuslanSalakhutdinov.\\nTransformer-xl: Attentive language models beyond a \\xef\\xac\\x81xed-length context. In Proceedings of\\nACL, page 2978\\xe2\\x80\\x932988, 2019.\\n11. Rajarshi Das, Arvind Neelakantan, David Belanger, and Andrew McCallum. Chains of reason-\\ning over entities, relations, and text using recurrent neural networks. In Proceedings of EACL,\\npages 132\\xe2\\x80\\x93141, 2017.\\n12. Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with\\ngated convolutional networks. In Proceedings of ICML, 2017.\\n13. Li Deng and Yang Liu. Deep learning in natural language processing. Springer, 2018.\\n14. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019.\\n15. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming\\nZhou, and Hsiao-Wuen Hon. Uni\\xef\\xac\\x81ed language model pre-training for natural language under-\\nstanding and generation. In Proceedings of NeurIPS, 2019.\\n16. Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth on demand with\\nstructured dropout. In Proceedings of ICLR, 2020.\\n17. Tianyu Gao, Xu Han, Zhiyuan Liu, and Maosong Sun. Hybrid attention-based prototypical\\nnetworks for noisy few-shot relation classi\\xef\\xac\\x81cation. In Proceedings of AAAI, pages 6407\\xe2\\x80\\x936414,\\n2019.\\n18. Tianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. FewRel 2.0:\\nTowards more challenging few-shot relation classi\\xef\\xac\\x81cation. In Proceedings of EMNLP-IJCNLP,\\npages 6251\\xe2\\x80\\x936256, 2019.\\n19. Michael Glass, Al\\xef\\xac\\x81o Gliozzo, Oktie Hassanzadeh, Nandana Mihindukulasooriya, and Gae-\\ntano Rossiello. Inducing implicit relations from text using distantly supervised deep nets. In\\nInternational Semantic Web Conference, pages 38\\xe2\\x80\\x9355. Springer, 2018.\\n20. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale senti-\\nment classi\\xef\\xac\\x81cation: A deep learning approach. In Proceedings of ICML, 2011.\\n21. Yoav Goldberg. Neural network methods for natural language processing. Synthesis Lectures\\non Human Language Technologies, 10(1):1\\xe2\\x80\\x93309, 2017.\\n22. Joshua Goodman. Classes for fast maximum entropy training. In Proceedings of ASSP, 2001.\\n23. Matthew R Gormley, Mo Yu, and Mark Dredze. Improved relation extraction with feature-rich\\ncompositional embedding models. In Proceedings of EMNLP, 2015.\\n24. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge\\nfor knowledge graph completion. arXiv preprint arXiv:1611.04125, 2016.\\n25. Xu Han, Zhiyuan Liu, and Maosong Sun. Neural knowledge acquisition via mutual attention\\nbetween knowledge graph and text. In Proceedings of AAAI, pages 4832\\xe2\\x80\\x934839, 2018.\\n26. Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun.\\nFewRel: A large-scale supervised few-shot relation classi\\xef\\xac\\x81cation dataset with state-of-the-art\\nevaluation. In Proceedings of EMNLP, 2018.\\n27. Zhiheng Huang, Geoffrey Zweig, and Benoit Dumoulin. Cache based recurrent neural network\\nlanguage model inference for \\xef\\xac\\x81rst pass speech recognition. In Proceedings of ICASSP, 2014.\\n28. Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao. Distant supervision for relation extraction\\nwith sentence-level attention and entity descriptions. In Proceedings of AAAI, pages 3060\\xe2\\x80\\x93\\n3066, 2017.\\nReferences\\n87\\n29. Rie Johnson and Tong Zhang. Effective use of word order for text categorization with convo-\\nlutional neural networks. In Proceedings of ACL-HLT, 2015.\\n30. Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. A convolutional neural network\\nfor modelling sentences. In Proceedings of ACL, 2014.\\n31. Yoon Kim. Convolutional neural networks for sentence classi\\xef\\xac\\x81cation. In Proceedings of\\nEMNLP, 2014.\\n32. Siwei Lai, Liheng Xu, Kang Liu, and Jun Zhao. Recurrent convolutional neural networks for\\ntext classi\\xef\\xac\\x81cation. In Proceedings of AAAI, 2015.\\n33. Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction\\nvia reading comprehension. In Proceedings of CoNLL, 2017.\\n34. Jiao Li, Yueping Sun, Robin J. Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman,\\nAllan Peter Davis, Carolyn J. Mattingly, Thomas C. Wiegers, and Zhiyong Lu. BioCreative V\\nCDR task corpus: a resource for chemical disease relation extraction. Database, pages 1\\xe2\\x80\\x9310,\\n2016.\\n35. Jiwei Li, Minh-Thang Luong, Dan Jurafsky, and Eduard Hovy. When are tree structures nec-\\nessary for deep learning of representations? In Proceedings of EMNLP, 2015.\\n36. Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Neural relation extrac-\\ntion with selective attention over instances. In Proceedings of ACL, 2016.\\n37. Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Recurrent neural network for text classi\\xef\\xac\\x81cation\\nwith multi-task learning. In Proceedings of IJCAI, 2016.\\n38. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\\napproach. arXiv preprint arXiv:1907.11692, 2019.\\n39. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visi-\\nolinguistic representations for vision-and-language tasks. In Proceedings of NeurIPS, 2019.\\n40. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\\xef\\xac\\x81cient estimation of word\\nrepresentations in vector space. In Proceedings of ICLR, 2013.\\n41. Tomas Mikolov, Martin Kara\\xef\\xac\\x81\\xc3\\xa1t, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recur-\\nrent neural network based language model. In Proceedings of InterSpeech, 2010.\\n42. Makoto Miwa and Mohit Bansal. End-to-end relation extraction using lstms on sequences and\\ntree structures. In Proceedings of ACL, 2016.\\n43. Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic\\nlanguage models. In Proceedings of ICML, 2012.\\n44. Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model.\\nIn Proceedings of Aistats, 2005.\\n45. Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen-tau Yih. Cross-\\nsentence n-ary relation extraction with graph LSTMs. Transactions of the Association for\\nComputational Linguistics, 5:101\\xe2\\x80\\x93115, 2017.\\n46. Matthew E Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh,\\nand Noah A Smith. Knowledge enhanced contextual word representations. In Proceedings of\\nEMNLP-IJCNLP, 2019.\\n47. Ngoc-Quan Pham, German Kruszewski, and Gemma Boleda. Convolutional neural network\\nlanguage models. In Proceedings of EMNLP, 2016.\\n48. Matt Post and Shane Bergsma. Explicit and implicit syntactic features for text classi\\xef\\xac\\x81cation.\\nIn Proceedings of ACL, 2013.\\n49. Chris Quirk and Hoifung Poon. Distant supervision for relation extraction beyond the sentence\\nboundary. In Proceedings of EACL, 2017.\\n50. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\\nunderstanding by generative pre-training. URL https://s3-us-west-2.amazonaws.com/openai-\\nassets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf, 2018.\\n51. Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M Marlin. Relation extraction\\nwith matrix factorization and universal schemas. In Proceedings of NAACL-HLT, pages 74\\xe2\\x80\\x9384,\\n2013.\\n88\\n4\\nSentence Representation\\n52. Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE Transac-\\ntions on Signal Processing, 45(11):2673\\xe2\\x80\\x932681, 1997.\\n53. Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. Matching the\\nBlanks: Distributional similarity for relation learning. In Proceedings of ACL, pages 2895\\xe2\\x80\\x93\\n2905, 2019.\\n54. Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. Semantic compo-\\nsitionality through recursive matrix-vector spaces. In Proceedings of EMNLP, 2012.\\n55. Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning.\\nSemi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings\\nof EMNLP, 2011.\\n56. Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y\\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over a senti-\\nment treebank. In Proceedings of EMNLP, 2013.\\n57. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mass: Masked sequence to\\nsequence pre-training for language generation. In Proceedings of ICML, 2019.\\n58. Daniel Soutner, Zden\\xcb\\x87ek Loose, Lud\\xcb\\x87ek M\\xc3\\xbcller, and Ale\\xc5\\xa1 Pra\\xc5\\xbe\\xc3\\xa1k. Neural network language\\nmodel with cache. In Proceedings of ICTSD, 2012.\\n59. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A\\njoint model for video and language representation learning. In Proceedings of ICCV, 2019.\\n60. Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge distillation for bert model\\ncompression. In Proceedings of EMNLP-IJCNLP, page 4314\\xe2\\x80\\x934323, 2019.\\n61. Kumutha Swampillai and Mark Stevenson. Inter-sentential relations in information extraction\\ncorpora. In Proceedings of LREC, 2010.\\n62. Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representa-\\ntions from tree-structured long short-term memory networks. In Proceedings of ACL, 2015.\\n63. Duyu Tang, Bing Qin, and Ting Liu. Document modeling with gated recurrent neural network\\nfor sentiment classi\\xef\\xac\\x81cation. In Proceedings of EMNLP, 2015.\\n64. Wilson L Taylor. \\xe2\\x80\\x9ccloze procedure\": A new tool for measuring readability. Journalism Bulletin,\\n30(4):415\\xe2\\x80\\x93433, 1953.\\n65. Ashish Vaswani, Noam Shazeer, Niki Parmar, Llion Jones, Jakob Uszkoreit, Aidan N Gomez,\\nand Lukasz Kaiser. Attention is all you need. In Proceedings of NeurIPS, 2017.\\n66. Patrick Verga, Emma Strubell, and Andrew McCallum. Simultaneously self-attending to all\\nmentions for full-abstract biological relation extraction. In Proceedings of NAACL-HLT, 2018.\\n67. Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph and text jointly\\nembedding. In Proceedings of EMNLP, pages 1591\\xe2\\x80\\x931601, 2014.\\n68. Zhigang Wang and Juan-Zi Li. Text-enhanced representation learning for knowledge graph. In\\nProceedings of IJCAI, pages 1293\\xe2\\x80\\x931299, 2016.\\n69. Kun Xu, Yansong Feng, Songfang Huang, and Dongyan Zhao. Semantic relation classi\\xef\\xac\\x81cation\\nvia convolutional neural networks with simple negative sampling. In Proceedings of EMNLP,\\n2015.\\n70. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V\\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. In Proceedings\\nof NeurIPS, 2019.\\n71. Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. Hierarchical\\nattention networks for document classi\\xef\\xac\\x81cation. In Proceedings of NAACL, 2016.\\n72. Yuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin, Zhenghao Liu, Zhiyuan Liu, Lixin Huang,\\nJie Zhou, and Maosong Sun. DocRED: A large-scale document-level relation extraction dataset.\\nIn Proceedings of ACL, 2019.\\n73. Zhi-Xiu Ye and Zhen-Hua Ling. Multi-level matching and aggregation network for few-shot\\nrelation classi\\xef\\xac\\x81cation. In Proceedings of ACL, pages 2872\\xe2\\x80\\x932881, 2019.\\n74. Wenpeng Yin and Hinrich Sch\\xc3\\xbctze. Multichannel variable-size convolution for sentence clas-\\nsi\\xef\\xac\\x81cation. In Proceedings of CoNLL, 2015.\\n75. Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. Distant supervision for relation extraction\\nvia piecewise convolutional neural networks. In Proceedings of EMNLP, 2015.\\nReferences\\n89\\n76. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation classi\\xef\\xac\\x81cation via\\nconvolutional deep neural network. In Proceedings of COLING, 2014.\\n77. Wenyuan Zeng, Yankai Lin, Zhiyuan Liu, and Maosong Sun. Incorporating relation paths in\\nneural relation extraction. In Proceedings of EMNLP, 2017.\\n78. Dongxu Zhang and Dong Wang. Relation classi\\xef\\xac\\x81cation via recurrent neural network. arXiv\\npreprint arXiv:1508.01006, 2015.\\n79. Ye Zhang, Iain Marshall, and Byron C Wallace. Rationale-augmented convolutional neural\\nnetworks for text classi\\xef\\xac\\x81cation. In Proceedings of EMNLP, 2016.\\n80. Ye Zhang, Stephen Roller, and Byron C Wallace. Mgnc-cnn: A simple approach to exploiting\\nmultiple word embeddings for sentence classi\\xef\\xac\\x81cation. In Proceedings of NAACL, 2016.\\n81. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie:\\nEnhanced language representation with informative entities. In Proceedings of ACL, 2019.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 5\\nRETRACTED CHAPTER: Document\\nRepresentation\\nThe authors have retracted this Chapter because signi\\xef\\xac\\x81cant portions of the text are\\nduplicated from [1] and [2]. The authors apologise to readers for this error. All authors\\nagree with this retraction.\\n1. Blei DM. Probabilistic Topic Models. Communications of the ACM, April 2012,\\nVol. 55 No. 4, Pages 77\\xe2\\x80\\x9384, 10.1145/2133806.2133826\\n2. Le Quoc and Tomas Mikolov. Distributed Representations of Sentences and Doc-\\numents. Proceedings of the 31st International Conference on Machine Learning,\\nPMLR 32(2):1188\\xe2\\x80\\x931196, 2014\\n\\xc2\\xa9 The Author(s) 2020, corrected publication 2023\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_5\\n91\\nChapter 6\\nSememe Knowledge Representation\\nAbstract Linguistic Knowledge Graphs (e.g., WordNet and HowNet) describe lin-\\nguistic knowledge in formal and structural language, which can be easily incorpo-\\nrated in modern natural language processing systems. In this chapter, we focus on\\nthe research about HowNet. We \\xef\\xac\\x81rst brie\\xef\\xac\\x82y introduce the background and basic\\nconcepts of HowNet and sememe. Next, we introduce the motivations of sememe\\nrepresentation learning and existing approaches. At the end of this chapter, we review\\nimportant applications of sememe representation.\\n6.1\\nIntroduction\\nIn the \\xef\\xac\\x81eld of Natural Language Processing (NLP), words are generally the smallest\\nobjects of study because they are considered as the smallest meaningful units that\\ncan stand by themselves of human languages. However, the meanings of words\\ncan be further divided into smaller parts. For example, the meaning of man can be\\nconsidered as the combination of the meanings of human, male and adult, and\\nthe meaning of boy is composed of the meanings of human, male, and child.\\nIn linguistics, the minimum indivisible units of meaning, i.e., semantic units, are\\nde\\xef\\xac\\x81ned as sememes [8]. And some linguists believe that meanings of all the words\\ncan be composed of a limited closed set of sememes.\\nHowever, sememes are implicit and as a result, it is hard to intuitively de\\xef\\xac\\x81ne the set\\nof sememes and determine which sememes a word can have at a glance. Therefore,\\nsome researchers spend tens of years sifting sememes from all kinds of dictionaries\\nand linguistic Knowledge Bases (KBs), and annotating words with these selected\\nsememes to construct sememe-based linguistic KB. WordNet and HowNet [17] are\\nthe two most famous ones of such KBs. In this section, we focus on the representation\\nof linguistic knowledge in HowNet.\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_6\\n125\\n126\\n6\\nSememe Knowledge Representation\\n6.1.1\\nLinguistic Knowledge Graphs\\n6.1.1.1\\nWordNet\\nWordNet is a large lexical database for the English language and could also be viewed\\nas a KG containing multi-relational data. It was \\xef\\xac\\x81rst started in 1985, and created under\\nthe direction of George Armitage Miller, a psychology professor in the Cognitive\\nScience Laboratory of Princeton University. Nowadays, WordNet is becoming the\\nmost popular lexicon dictionary in the world that could be available through the Web\\nfor free and is widely used in NLP applications such as text analysis, information\\nretrieval, and relation extraction. There is also a Global WordNet Association aiming\\nto provide a public and noncommercial platform for WordNets of all languages in\\nthe world.\\nBased on meanings, WordNet groups English nouns, verbs, adjectives, and\\nadverbs into synsets (i.e., sets of cognitive synonyms), which represent a distinct\\nconcept. Each synset possesses a brief description, and in most cases, there are even\\nsome short sentences functioning as examples illustrating the use of words in this\\nsynset. The conceptual-semantic and lexical relations link the synsets and words. The\\nmain relation among words is synonymy, which indicates that the words share similar\\nmeanings and could be replaced by others in some contexts, while the main relation\\namong synsets is hyperonymy/hyponymy (i.e., the ISA relation), which indicates\\nthe relationship between a more general synset and a more speci\\xef\\xac\\x81c synset. There are\\nalso hierarchical structures for verb synsets, and the antonymy is describing the rela-\\ntion between adjectives with opposite meanings. To sum up, all WordNets\\xe2\\x80\\x99 117, 000\\nsynsets are linked to each other by a small number of conceptual relations.\\n6.1.1.2\\nHowNet\\nHowNet was initially designed and constructed by Zhendong Dong and his son Qiang\\nDong in the 1990s. And it has been kept frequently updated since it was published\\nin 1999.\\nThe sememe set of HowNet is determined by extracting, analyzing, merging,\\nand \\xef\\xac\\x81ltering semantics of thousands of Chinese characters. And the sememe set can\\nalso be adjusted or expanded in the subsequent process of annotating words. Each\\nsememe in HowNet is represented by a word or phrase in Chinese and English such\\nas (human | {\\n}) and (ProperName | {\\n}).\\nHowNet also builds a taxonomy for the sememes. All the sememes of HowNet\\ncan be classi\\xef\\xac\\x81ed as one of the following types: Thing, Part, Attribute, Time, Space,\\nAttribute Value, and Event. In addition, to depict the semantics of words more pre-\\ncisely, HowNet incorporates relations between sememes, which are called \\xe2\\x80\\x9cdynamic\\nroles\\xe2\\x80\\x9d, into the sememe annotations of words.\\nConsidering the polysemy, HowNet differentiates diverse senses of each word\\nin the sememe annotations. And each sense is also expressed in both Chinese and\\n6.1 Introduction\\n127\\nword\\nsense\\nsememe\\napple\\ncomputer\\nbring\\nbring\\nSpecificBrand\\nSpecificBrand\\ncommunicate\\nPatternValue\\nPatternValue\\ntool\\nfruit\\nfruit\\nable\\nable\\ntree\\nreproduce\\napple(computer)\\napple(phone)\\napple(fruit)\\napple(tree)\\npatient\\npatient\\nagent\\nmodifier\\nCoEvent\\nCoEvent\\ninstrument\\nPatientProdect\\nscope\\nscope\\nmodifier\\nFig. 6.1 An example of word annotated with sememes in HowNet\\nTable 6.1 Statistics of HowNet\\nType\\nCount\\nSense\\n229,767\\nDistinct Chinese word\\n127,266\\nDistinct English word\\n104,025\\nSememe\\n2,187\\nEnglish. An example of sememe annotation for a word is illustrated in Fig.6.1.\\nWe can see from the \\xef\\xac\\x81gure that the word apple has four senses including\\napple(computer), apple(phone), apple(fruit), and apple(tree),\\nand each sense is the root node of a \\xe2\\x80\\x9csememe tree\\xe2\\x80\\x9d where each pair of father and son\\nsememe nodes is multi-relational. Additionally, HowNet annotates the POS tag for\\neach sense, and adds sentiment category as well as some usage examples for certain\\nsenses.\\nThe latest version of HowNet was published in January 2019 and the statistics are\\nshown in Table6.1.\\nSince HowNet was published, it has attracted wide attention. People use HowNet\\nand sememe in various NLP tasks including word similarity computation [40], word\\nsense disambiguation [70], question classi\\xef\\xac\\x81cation [62], and sentiment analysis [16,\\n20]. Among these researches, [40] is one of the most in\\xef\\xac\\x82uential works, in which the\\nsimilarity of given two words is computed by measuring the degree of resemblance\\nof their sememe trees.\\nRecent years also witnessed some works incorporating sememes into neural net-\\nwork models. Reference [49] proposes a novel word representation learning model\\nnamed SST that reforms Skip-gram [43] by adding contextual attention to senses of\\n128\\n6\\nSememe Knowledge Representation\\nthe target word, which are represented with combinations of corresponding sememes\\xe2\\x80\\x99\\nembeddings. Experimental results show that SST can not only improve the quality\\nof word embeddings but also learn satisfactory sense embeddings to do word sense\\ndisambiguation.\\nReference [23] incorporates sememes into the decoding phase of language mod-\\neling where sememes are predicted \\xef\\xac\\x81rst, and then senses and words are predicted in\\nsuccession. The proposed model shows enhancement in the perplexity of language\\nmodeling and the performance of the downstream task headline generation.\\nBesides, HowNet is also utilized in lexicon expansion [68], semantic rationality\\nevaluation [41], etc.\\nConsidering that human annotation is time-consuming and labor-intensive, some\\nworks attempt to employ machine learning methods to predict sememes for new\\nwords automatically. Reference [66] proposes the task \\xef\\xac\\x81rstly and presents two sim-\\nple but effective models: SPWE, which is based on collaborative \\xef\\xac\\x81ltering, and SPSE,\\nwhich is based on matrix factorization. Reference [30] further takes the internal infor-\\nmation of words into account when predicting sememes and achieves a considerable\\nboost of performance. And [38] takes advantage of de\\xef\\xac\\x81nitions of words to predict\\nsememes. As for [56], they propose the task of cross-lingual lexical sememe pre-\\ndiction and present a bilingual word representation learning and alignment-based\\nmodel, which demonstrates effectiveness in predicting sememes for cross-lingual\\nwords.\\n6.2\\nSememe Knowledge Representation\\nWordRepresentationLearning(WRL)isafundamentalandcriticalstepinmanyNLP\\ntasks such as language modeling [4] and neural machine translation [64]. There have\\nbeen a lot of researches for learning word representations, among which Word2vec\\n[43] achieves a nice balance between effectiveness and ef\\xef\\xac\\x81ciency. In Word2vec, each\\nword corresponds to one single embedding, ignoring the polysemy of most words.\\nTo address this issue, [29] introduces a multi-prototype model for WRL, conducting\\nunsupervised word sense induction and embeddings according to context clusters.\\nReference [13] further utilizes the synset information in WordNet to instruct word\\nsense representation learning.\\nThese previous studies demonstrate that word sense disambiguation is critical\\nfor WRL, and the sememe annotation of word senses in HowNet can provide nec-\\nessary semantic regularization for these tasks [63]. To explore its feasibility, we\\nintroduce the Sememe-Encoded Word Representation Learning (SE-WRL) model,\\nwhich detects word senses and learns representations simultaneously. More specif-\\nically, this framework regards each word sense as a combination of its sememes,\\nand iteratively performs word sense disambiguation according to their contexts and\\nlearns representations of sememes, senses, and words by extending Skip-gram in\\nWord2vec [43]. In this framework, an attention-based method is proposed to select\\nappropriate word senses according to contexts automatically. To take full advantage\\n6.2 Sememe Knowledge Representation\\n129\\nof sememes, we introduce three different learning and attention strategies SSA, SAC,\\nand SAT for SE-WRL, which will be described in the following paragraphs.\\n6.2.1\\nSimple Sememe Aggregation Model\\nThe Simple Sememe Aggregation model (SSA) is a straightforward idea based on\\nSkip-gram model. For each word, SSA considers all sememes in all senses of the\\nword together, and represents the target word using the average of all its sememe\\nembeddings. Formally, we have\\nw = 1\\nm\\n\\x02\\ns(w)\\ni\\n\\xe2\\x88\\x88S(w)\\n\\x02\\nx\\n(si )\\nj\\n\\xe2\\x88\\x88X(w)\\ni\\nx(si)\\nj ,\\n(6.1)\\nwhich means the word embedding of w is composed by the average of all its sememe\\nembeddings. Here, S(w) is the sense set of w and X(w)\\ni\\nis the sememe set of the ith\\nsense of w. m stands for the overall number of sememes belonging to w.\\nThis model follows the assumption that the semantic meaning of a word is com-\\nposed of the semantic units, i.e., sememes. As compared to the conventional Skip-\\ngram model, since sememes are shared by multiple words, this model can utilize\\nsememe information to encode latent semantic correlations between words. In this\\ncase, similar words that share the same sememes may \\xef\\xac\\x81nally obtain similar repre-\\nsentations.\\n6.2.2\\nSememe Attention over Context Model\\nThe SSA Model replaces the target word embedding with the aggregated sememe\\nembeddings to encode sememe information into word representation learning. How-\\never, each word in the SSA model still has only one single representation in different\\ncontexts, which cannot deal with the polysemy of most words. It is intuitive that we\\nshould construct distinct embeddings for a target word according to speci\\xef\\xac\\x81c contexts,\\nwith the favor of word sense annotation in HowNet.\\nTo address this issue, the Sememe Attention over Context model (SAC) is pro-\\nposed. SAC utilizes the attention scheme to automatically select appropriate senses\\nfor context words according to the target word. That is, SAC conducts word sense\\ndisambiguation for context words to learn better representations of target words. The\\nstructure of the SAC model is shown in Fig.6.2.\\nMore speci\\xef\\xac\\x81cally, SAC utilizes the original word embedding for target word w,\\nand uses sememe embeddings to represent context word wc instead of the original\\ncontext word embeddings. Suppose a word typically demonstrates some speci\\xef\\xac\\x81c\\nsenses in one sentence. Here, the target word embedding is employed as attention\\n130\\n6\\nSememe Knowledge Representation\\nsememe\\nsense\\ncontext\\nword\\ns1\\ns2\\ns3\\nAtt\\nAtt\\nAtt\\nwt-2\\nwt-1\\nwt+1\\nwt+2\\nwt\\nFig. 6.2 The architecture of SAC model\\nto select the most appropriate senses to make up the context word embeddings. The\\ncontext word embedding wc can be formalized as follows:\\nwc =\\n|S(wc)|\\n\\x02\\nj=1\\nAtt(s(wc)\\nj\\n)s(wc)\\nj\\n,\\n(6.2)\\nwhere s(wc)\\nj\\nstands for the jth sense embedding of wc, and Att(s(wc)\\nj\\n) represents the\\nattention score of the jth sense with respect to the target word w, de\\xef\\xac\\x81ned as follows:\\nAtt(s(wc)\\nj\\n) =\\nexp(w \\xc2\\xb7 \\xcb\\x86s(wc)\\nj\\n)\\n\\x03|S(wc)|\\nk=1\\nexp(w \\xc2\\xb7 \\xcb\\x86s(wc)\\nk\\n)\\n.\\n(6.3)\\nNote that, when calculating attention, the average of sememe embeddings is used\\nto represent each sense s(wc)\\nj\\n:\\n\\xcb\\x86s(wc)\\nj\\n=\\n1\\n|X(wc)\\nj\\n|\\n|X(wc)\\nj\\n|\\n\\x02\\nk=1\\nx\\n(s j)\\nk\\n.\\n(6.4)\\nThe attention strategy assumes that the more relevant a context word sense embed-\\nding is to the target word w, the more this sense should be considered when building\\ncontext word embeddings. With the favor of attention scheme, each context word\\n6.2 Sememe Knowledge Representation\\n131\\nsememe\\nsense\\ncontext\\nword\\ns1\\ns2\\ns3\\nAtt\\nAtt\\nAtt\\nwt-2\\nwt-1\\nwt+1\\nwt+2\\nwt\\ncontextual\\nembedding\\nFig. 6.3 The architecture of SAT model\\ncan be represented as a particular distribution over its sense. This can be regarded as\\nsoft WSD and it helps learn better word representations.\\n6.2.3\\nSememe Attention over Target Model\\nThe Sememe Attention over Context Model can \\xef\\xac\\x82exibly select appropriate senses\\nand sememes for context words according to the target word. The process can also\\nbe applied to select appropriate senses for the target word by taking context words\\nas attention. Hence, the Sememe Attention over Target model (SAT) is proposed,\\nwhich is shown in Fig.6.3.\\nDifferent from the SAC model, SAT learns the original word embeddings for\\ncontext words and sememe embeddings for target words. Then SAT applies context\\nwords to perform attention over multiple senses of the target word w to build the\\nembedding of w, formalized as follows:\\nw =\\n|S(w)|\\n\\x02\\nj=1\\nAtt(s(w)\\nj\\n)s(w)\\nj ,\\n(6.5)\\nand the context-based attention is de\\xef\\xac\\x81ned as follows:\\n132\\n6\\nSememe Knowledge Representation\\nAtt(s(w)\\nj\\n) =\\nexp(w\\xe2\\x80\\xb2\\nc \\xc2\\xb7 \\xcb\\x86s(w)\\nj )\\n\\x03|S(w)|\\nk=1 exp(w\\xe2\\x80\\xb2c \\xc2\\xb7 \\xcb\\x86s(w)\\nk )\\n,\\n(6.6)\\nwhere the average of sememe embeddings \\xcb\\x86s(w)\\nj\\nis also used to represent each sense\\ns(w)\\nj\\n. Here, w\\xe2\\x80\\xb2\\nc is the context embedding, consisting of a constrained window of word\\nembeddings in C(wi). We have\\nw\\xe2\\x80\\xb2\\nc =\\n1\\n2K \\xe2\\x80\\xb2\\nk=i+K \\xe2\\x80\\xb2\\n\\x02\\nk=i\\xe2\\x88\\x92K \\xe2\\x80\\xb2\\nwk,\\nk \\xcc\\xb8= i.\\n(6.7)\\nNote that, since in experiments, the sense selection of the target word is found to\\nbe only dependent on more limited context words for calculating attention, hence a\\nsmaller K \\xe2\\x80\\xb2 is selected as compared to K.\\nRecall that SAC only uses one target word as attention to select senses of context\\nwords whereas SAT uses several context words together as attention to select appro-\\npriate senses of target words. Hence SAT is expected to conduct more reliable WSD\\nand result in more accurate word representations, which is explored in experiments.\\n6.3\\nApplications\\nIn the previous section, we introduce HowNet and sememe representation. In fact,\\nlinguistic knowledge graphs such as HowNet contain rich information which could\\neffectivelyhelpdownstreamapplications.Therefore,inthissection,wewillintroduce\\nthe major applications of sememe representation, including sememe-based word\\nrepresentation, linguistic knowledge graph construction, and language modeling.\\n6.3.1\\nSememe-Guided Word Representation\\nSememe-Guided word representation is intended for improving word embeddings\\nfor sememe prediction by introducing the information of sememe-based linguistic\\nKBs of the source language. Qi et al. [56] present two methods of the sememe-guided\\nword representation.\\n6.3.1.1\\nRelation-Based Word Representation\\nA simple and intuitive method is to let words with similar sememe annotations tend\\nto have similar word embeddings, which is named as word relation-based approach.\\nTo begin with, a synonym list is constructed from sememe-based linguistic KBs,\\n6.3 Applications\\n133\\nwhere words sharing a certain number of sememes are regarded as synonyms. Next,\\nsynonyms are forced to have closer word embeddings.\\nFormally, let wi be the original word embedding of wi and \\xcb\\x86wi be its adjusted\\nword embedding. And let Syn(wi) denote the synonym set of word wi. Then the loss\\nfunction is de\\xef\\xac\\x81ned as\\nLsememe =\\n\\x02\\nwi\\xe2\\x88\\x88V\\n\\x04\\n\\xce\\xb1i\\xe2\\x88\\xa5wi \\xe2\\x88\\x92\\xcb\\x86wi\\xe2\\x88\\xa52 +\\n\\x02\\nw j\\xe2\\x88\\x88Syn(wi)\\n\\xce\\xb2i j\\xe2\\x88\\xa5\\xcb\\x86wi \\xe2\\x88\\x92\\xcb\\x86w j\\xe2\\x88\\xa52\\x05\\n,\\n(6.8)\\nwhere \\xce\\xb1 and \\xce\\xb2 control the relative strengths of the two terms. It should be noted\\nthat the idea of forcing similar words to have close word embeddings is similar\\nto the state-of-the-art retro\\xef\\xac\\x81tting approach [19]. However, the retro\\xef\\xac\\x81tting approach\\ncannot be applied here because sememe-based linguistic KBs such as HowNet cannot\\ndirectly provide its needed synonym list.\\n6.3.1.2\\nSememe Embedding-Based Word Representation\\nSimple and effective as the word relation-based approach is, it cannot make full\\nuse of the information of sememe-based linguistic KBs because it disregards the\\ncomplicated relations between sememes and words as well as relations between dif-\\nferent sememes. To address this limitation, the sememe embedding-based approach\\nis proposed, which learns both sememe and word embeddings jointly.\\nIn this approach, sememes are represented with distributed vectors as well and\\nplace them into the same semantic space as words. Similar to SPSE [66], which\\nlearns sememe embeddings by decomposing the word-sememe matrix and sememe-\\nsememe matrix, the method utilizes sememe embeddings as regularizers to learn\\nbetter word embeddings. Different from SPSE, the model described in [56] does not\\nuse pretrained word embeddings. Instead, it learns word embeddings and sememe\\nembeddings simultaneously. More speci\\xef\\xac\\x81cally, a word-sememe matrix M can be\\nextracted from HowNet, where Mi j = 1 indicates word wi is annotated with sememe\\nx j, otherwise Mi j = 0. Hence by factorizing M, the loss function can be de\\xef\\xac\\x81ned as\\nLsememe =\\n\\x02\\nwi\\xe2\\x88\\x88V,x j\\xe2\\x88\\x88X\\n(wi \\xc2\\xb7 x j + bs + b\\xe2\\x80\\xb2\\nj \\xe2\\x88\\x92Mi j)2,\\n(6.9)\\nwhere bi and b\\xe2\\x80\\xb2\\nj are the biases of wi and x j, and X denotes sememe set.\\nIn this approach, word and sememe embeddings are obtained in a uni\\xef\\xac\\x81ed seman-\\ntic space. The sememe embeddings bear all the information about the relationships\\nbetween words and sememes, and they inject the information into word embed-\\ndings. Therefore, the word embeddings are expected to be more suitable for sememe\\nprediction.\\n134\\n6\\nSememe Knowledge Representation\\n6.3.2\\nSememe-Guided Semantic Compositionality Modeling\\nSemantic Compositionality (SC) is de\\xef\\xac\\x81ned as the linguistic phenomenon that the\\nmeaning of a syntactically complex unit is a function of meanings of the complex\\nunit\\xe2\\x80\\x99s constituents and their combination rule [50]. Some linguists regard SC as the\\nfundamental truth of semantics [51]. In the \\xef\\xac\\x81eld of NLP, SC has proved effective in\\nmany tasks including language modeling [47], sentiment analysis [42, 61], syntactic\\nparsing [59], etc.\\nMost literature on SC pays attention to using vector-based distributional mod-\\nels of semantics to learn representations of Multiword Expressions (MWEs), i.e.,\\nembeddings of phrases or compounds. Reference [46] conducts a pioneering work\\nwhich introduces a general framework to formulate this task:\\np = f (w1, w2, R, K ),\\n(6.10)\\nwhere1 f is the compositionality function, p denotes the embedding of an MWE,\\nw1 and w2 represent the embeddings of the MWE\\xe2\\x80\\x99s two constituents, R stands for\\nthe combination rule, and K refers to the additional knowledge which is needed to\\nconstruct the semantics of the MWE.\\nMost of the proposed approaches ignore R and K , centering on reforming com-\\npositionality function f [3, 21, 60, 61]. Some try to integrate combination rule R\\ninto SC models [7, 35, 65, 71]. A few works consider external knowledge K. Ref-\\nerence [72] tries to incorporate task-speci\\xef\\xac\\x81c knowledge into an LSTM model for\\nsentence-level SC.\\nReference [55] proposes a novel sememe-based method to model semantic com-\\npositionality. They argue that sememes are bene\\xef\\xac\\x81cial to modeling SC. To verify this,\\nthey \\xef\\xac\\x81rst design a simple SC degree (SCD) measurement experiment and \\xef\\xac\\x81nd that\\nthe SCDs of MWEs computed by simple sememe-based formulae are highly corre-\\nlated with human judgment. This result shows that sememes can \\xef\\xac\\x81nely depict mean-\\nings of MWEs and their constituents, and capture the semantic relations between\\nthe two sides. Moreover, they propose two sememe-incorporated SC models for\\nlearning embeddings of MWEs, namely Semantic Compositionality with Aggre-\\ngated Sememe (SCAS) model and Semantic Compositionality with Mutual Sememe\\nAttention (SCMSA) model. When learning the embedding of an MWE, the SCAS\\nmodel concatenates the embeddings of the MWE\\xe2\\x80\\x99s constituents and their sememes,\\nwhile the SCMSA model considers the mutual attention between a constituent\\xe2\\x80\\x99s\\nsememes and the other constituent. Finally, they integrate the combination rule, i.e.,\\nR in Eq. (6.10), into the two models. Their models achieve signi\\xef\\xac\\x81cant performance\\nover the MWE similarity computation task and sememe prediction task compared\\nwith baseline methods.\\nIn this section, we focus on the work conducted by [55]. We will \\xef\\xac\\x81rst intro-\\nduce sememe-based SC Degree (SCD) computation formulae, and then expand their\\nSememe-incorporated SC models.\\n1This formula only applies to two-word MWEs but can be easily extended to longer MWEs.\\n6.3 Applications\\n135\\n6.3.2.1\\nSememe-Based SCD Computation Formulae\\nAlthough SC widely exists in MWEs, not every MWE is fully semantically com-\\npositional. In fact, different MWEs show different degrees of SC. Reference [55]\\nbelieves that sememes can be used to measure SCD conveniently.\\nTo this end, based on the assumption that all the sememes of a word accurately\\ndepict the word\\xe2\\x80\\x99s meaning, they intuitively design a set of SCD computation formu-\\nlae, which are consistent with the principle of SCD.\\nThe formulae are illustrated in Table6.2. They de\\xef\\xac\\x81ne four SCDs denoted by\\nnumbers 3, 2, 1, and 0, where larger numbers mean higher SCDs. Sp, Sw1, and Sw2\\nrepresent the sememe sets of an MWE, its \\xef\\xac\\x81rst and second constituent, respectively.\\nHere is a brief explanation for their SCD computation formulae:\\n(1) For SCD 3, the sememe set of an MWE is identical to the union of the two\\nconstituents\\xe2\\x80\\x99 sememe sets, which means the meaning of the MWE is exactly the\\nsame as the combination of the constituents\\xe2\\x80\\x99 meanings. Therefore, the MWE is fully\\nsemantically compositional and should have the highest SCD.\\n(2) For SCD 0, an MWE has totally different sememes from its constituents, which\\nmeans the MWE\\xe2\\x80\\x99s meaning cannot be derived from its constituents\\xe2\\x80\\x99 meanings. Hence\\nthe MWE is completely non-compositional, and its SCD should be the lowest.\\n(3) As for SCD 2, the sememe set of an MWE is a proper subset of the union of\\nits constituents\\xe2\\x80\\x99 sememe sets, which means the meanings of the constituents cover\\nthe MWE\\xe2\\x80\\x99s meaning but cannot precisely infer the MWE\\xe2\\x80\\x99s meaning.\\n(4) Finally, for SCD 1, an MWE shares some sememes with its constituents, but\\nboth the MWE itself and its constituents have some unique sememes.\\nThere is an example for each SCD in Table6.2, including a Chinese MWE, its\\ntwo constituents, and their sememes.2\\n6.3.2.2\\nEvaluating SCD Computation Formulae\\nTo evaluate their sememe-based SCD computation formulae, [55] constructs a\\nhuman-annotated SCD dataset. They ask several native speakers to label SCDs for\\n500 Chinese MWEs, where there are four degrees to choose. Before labeling an\\nMWE, they are shown the dictionary de\\xef\\xac\\x81nitions of both the MWE and its con-\\nstituents.\\nEach MWE is labeled by 3 annotators, and the average of the 3 SCDs given by\\nthem is the MWE\\xe2\\x80\\x99s \\xef\\xac\\x81nal SCD.\\nEventually, they obtain a dataset containing 500 Chinese MWEs together with\\ntheir human-annotated SCDs.\\nThen they evaluate the correlativity between SCDs of the MWEs in the dataset\\ncomputed by sememe-based rules and those given by humans. They \\xef\\xac\\x81nd Pearson\\xe2\\x80\\x99s\\ncorrelation coef\\xef\\xac\\x81cient is up to 0.75, and Spearman\\xe2\\x80\\x99s rank correlation coef\\xef\\xac\\x81cient is\\n2In Chinese, most MWEs are words consisting of more than two characters which are actually\\nsingle-morpheme words.\\n136\\n6\\nSememe Knowledge Representation\\nTable 6.2 Sememe-based semantic compositionality degree computation formulae and examples. Bold sememes of constituents are shared with the constituents\\xe2\\x80\\x99\\ncorresponding MWE\\n6.3 Applications\\n137\\n0.74. These results manifest remarkable capability of sememes to compute SCDs of\\nMWEs and provide a proof that sememes of a word can \\xef\\xac\\x81nely represent the word\\xe2\\x80\\x99s\\nmeaning.\\n6.3.2.3\\nSememe-Incorporated SC Models\\nIn this section, we \\xef\\xac\\x81rst introduce two basic sememe-incorporated SC models in detail,\\nnamely Semantic Compositionality with Aggregated Sememe (SCAS) and Semantic\\nCompositionality with Mutual Sememe Attention (SCMSA). SCAS model simply\\nconcatenates the embeddings of the MWE\\xe2\\x80\\x99s constituents and their sememes, while\\nthe SCMSA model takes account of the mutual attention between a constituent\\xe2\\x80\\x99s\\nsememes and the other constituent. Then we describe how to integrate combination\\nrules into the two basic models.\\nIncorporating Sememes Only. Following the notations in Eq. (6.10), for an\\nMWE p = {w1, w2}, its embedding can be represented as\\np = f (w1, w2, K ),\\n(6.11)\\nwhere p, w1, w2 \\xe2\\x88\\x88Rd, and d is the dimension of embeddings, K denotes the\\nsememe knowledge here, and we assume that we only know the sememes of w1\\nand w2, considering that MWEs are normally not in the sememe KBs. X indicates\\nthe set of all the sememes and Xw = {x1, ..., x|Xw|} \\xe2\\x8a\\x82X to signify the sememe set\\nof w. In addition, x \\xe2\\x88\\x88Rd denotes the embedding of sememe x.\\n(1) SCAS Model The \\xef\\xac\\x81rst model we introduce is the SCAS model, which is\\nillustrated in Fig.6.4. The idea of the SCAS model is straightforward, i.e., simply\\nMWE\\nConstituent\\nSememe\\nw\\xe2\\x80\\xb21\\nw1\\nw\\xe2\\x80\\xb22\\nw2\\nFig. 6.4 The architecture of SCAS model\\n138\\n6\\nSememe Knowledge Representation\\nconcatenating word embedding of a constituent and the aggregation of its sememes\\xe2\\x80\\x99\\nembeddings. Formally, we have\\nw\\xe2\\x80\\xb2\\n1 =\\n\\x02\\nxi\\xe2\\x88\\x88Xw1\\nxi,\\nw\\xe2\\x80\\xb2\\n2 =\\n\\x02\\nx j\\xe2\\x88\\x88Xw2\\nxj,\\n(6.12)\\nwhere w\\xe2\\x80\\xb2\\n1 and w\\xe2\\x80\\xb2\\n2 represent the aggregated sememe embeddings of w1 and w2, respec-\\ntively. Then p can be obtained by\\np = tanh(Wc[w1 + w2;w\\xe2\\x80\\xb2\\n1 + w\\xe2\\x80\\xb2\\n2] + bc),\\n(6.13)\\nwhere Wc \\xe2\\x88\\x88Rd\\xc3\\x972d is the composition matrix and bc \\xe2\\x88\\x88Rd is a bias vector.\\n(2) SCMSA Model\\nThe SCAS model simply uses the sum of all the sememes\\xe2\\x80\\x99 embeddings of a\\nconstituent as the external information. However, a constituent\\xe2\\x80\\x99s meaning may vary\\nwith the other constituent, and accordingly, the sememes of a constituent should have\\ndifferent weights when the constituent is combined with different constituents (there\\nis an example in the case study).\\nCorrespondingly, we introduce the SCMSA model (Fig.6.5), which adopts the\\nmutual attention mechanism to dynamically endow sememes with weights. Formally,\\nwe have\\ne1 = tanh(Waw1 + ba),\\na2,i =\\nexp (si \\xc2\\xb7 e1)\\n\\x03\\nx j\\xe2\\x88\\x88Xw2 exp (x j \\xc2\\xb7 e1),\\nw\\xe2\\x80\\xb2\\n2 =\\n\\x02\\nxi\\xe2\\x88\\x88Xw2\\na2,ixi,\\n(6.14)\\nwhere Wa \\xe2\\x88\\x88Rd\\xc3\\x97d is the weight matrix and ba \\xe2\\x88\\x88Rd is a bias vector. Similarly, w\\xe2\\x80\\xb2\\n1\\ncan be calculated. Then they still use Eq. (6.13) to obtain p.\\nIntegrating Combination Rules. Reference [55] further integrates combination\\nrules into their sememe-incorporated SC models. In other words,\\np = f (w1, w2, K, R).\\n(6.15)\\nWe can use totally different composition matrices for MWEs with different com-\\nbination rules:\\nWc = Wr\\nc,\\nr \\xe2\\x88\\x88Rs,\\n(6.16)\\nwhere Wr\\nc \\xe2\\x88\\x88Rd\\xc3\\x972d and Rs refers to combination rule set containing syntax rules of\\nMWEs, e.g., adjective-noun and noun-noun.\\nHowever, there are many different combination rules, and some rules have sparse\\ninstances which are not enough to train the corresponding composition matrices\\n6.3 Applications\\n139\\nMWE\\nConstituent\\nSememe\\nw\\xe2\\x80\\xb21\\nw1\\nw\\xe2\\x80\\xb22\\nw2\\na21\\na22\\na23\\na11\\na12\\nFig. 6.5 The architecture of SCMSA model\\nwith d \\xc3\\x97 2d parameters. In addition, we believe that the composition matrix should\\ncontain common compositionality information except the combination rule-speci\\xef\\xac\\x81c\\ncompositionality information. Hence, they let composition matrix Wc be the sum of\\na low-rank matrix containing combination rule information and a matrix containing\\ncommon compositionality information:\\nWc = Ur\\n1Ur\\n2 + Wc\\nc,\\n(6.17)\\nwhere Ur\\n1 \\xe2\\x88\\x88Rd\\xc3\\x97dr , Ur\\n2 \\xe2\\x88\\x88Rdr\\xc3\\x972d, and dr \\xe2\\x88\\x88N+ is a hyperparameter and may vary\\nwith the combination rule, and Wc\\nc \\xe2\\x88\\x88Rd\\xc3\\x972d.\\n6.3.3\\nSememe-Guided Language Modeling\\nLanguage Modeling (LM) aims to measure the probability of a word sequence,\\nre\\xef\\xac\\x82ecting its \\xef\\xac\\x82uency and likelihood as a feasible sentence in a human language.\\nLanguage Modeling is an essential component in a wide range of natural language\\n140\\n6\\nSememe Knowledge Representation\\n(a)\\n(b)\\nConventional Decoder\\nSememe-Driven Decoder\\nSememe\\nPredictor\\ncontext\\nvector\\nword\\ndistribution\\nsememe\\ndistribution\\nsense\\ndistribution\\nSense\\nPredictor\\nWord\\nPredictor\\nword\\ndistribution\\ncontext\\nvector\\nFig. 6.6 Decoders of a conventional LM, b sememe-driven LM\\nprocessing (NLP) tasks, such as machine translation [9, 10], speech recognition [34],\\ninformation retrieval [5, 24, 45, 54], and document summarization [2, 57].\\nA probabilistic language model calculates the conditional probability of the next\\nword given their contextual words, which are typically learned from large-scale text\\ncorpora. Taking the simplest language model, for example, n-gram estimates the\\nconditional probabilities according to maximum likelihood over text corpora [31].\\nRecent years have witnessed the advances of Recurrent Neural Networks (RNNs)\\nas the state-of-the-art approach for language modeling [44], in which the context is\\nrepresented as a low-dimensional hidden state to predict the next word (Fig.6.6).\\nThose conventional language models, including neural models, typically assume\\nwords as atomic symbols and model sequential patterns at the word level. However,\\nthis assumption does not necessarily hold to some extent. Consider the following\\nexample sentence for which people want to predict the next word in the blank,\\nThe U.S. trade deficit last year is initially estimated to be 40 billion\\n.\\nPeople may \\xef\\xac\\x81rst realize a unit should be \\xef\\xac\\x81lled in, then realize it should be a\\ncurrency unit. Based on the country this sentence is talking about, the U.S.,\\none may con\\xef\\xac\\x81rm it should be an American currency unit and predict the\\nword dollars. Here, the unit, currency, and American, which are basic\\nsemantic units of the word dollars, are also the sememes of the word dollars.\\nHowever,thisprocesshasnotbeenexplicitlytakenintoconsiderationbyconventional\\nlanguage models. That is in most cases, words are atomic language units, words are\\nnot necessarily atomic semantic units for language modeling. Thus, explicit modeling\\nof sememes could improve both the performance and the interpretability of language\\nmodels. However, as far as we know, a few efforts have been devoted to exploring the\\neffectiveness of sememes in language models, especially neural language models.\\nIt is nontrivial for neural language models to incorporate discrete sememe knowl-\\nedge, as it is not compatible with continuous representations in neural models. In\\nthis part, Sememe-Driven Language Model (SDLM) is proposed to leverage lexi-\\ncal sememe knowledge. In order to predict the next word, SDLM utilizes a novel\\n6.3 Applications\\n141\\nsememe-sense-word generation process: (1) First, SDLM estimates sememes\\xe2\\x80\\x99 dis-\\ntribution according to the context. (2) Regarding these sememes as experts, SDLM\\nemploys a sparse product of expert method to select the most probable senses. (3)\\nFinally, SDLM calculates the distribution of words by marginalizing out the distri-\\nbution of senses.\\nSDLM is composed of three modules in series: Sememe Predictor, Sense Pre-\\ndictor, and Word Predictor (Fig.6.6). The Sememe Predictor \\xef\\xac\\x81rst takes the context\\nvector as input and assigns a weight to each sememe. Then each sememe is regarded\\nas an expert and makes predictions about the probability distribution over a set of\\nsenses in the Sense Predictor. Finally, the probability of each word is obtained in the\\nWord Predictor.\\nSememe Predictor. The Sememe Predictor takes the context vector g \\xe2\\x88\\x88RH1\\nas input and assigns a weight to each sememe. Assume that given the context\\nw1, w2, . . . , wt\\xe2\\x88\\x921, the events that word wt contains sememe xk (k \\xe2\\x88\\x88{1, 2, . . . , K})\\nare independent, since the sememe is the minimum semantic unit and there is no\\nsemantic overlap between any two different sememes. For simplicity, the superscript\\nt is ignored. The Sememe Predictor is designed as a linear decoder with the sig-\\nmoid activation function. Therefore, pk, the probability that the next word contains\\nsememe xk, is formulated as\\npk = P(xk|g) = Sigmoid(g \\xc2\\xb7 vk + bk),\\n(6.18)\\nwhere vk \\xe2\\x88\\x88RH1, bk \\xe2\\x88\\x88R are trainable parameters, and Sigmoid(\\xc2\\xb7) denotes the sig-\\nmoid activation function.\\nSense Predictor and Word Predictor. The architecture of the Sense Predictor is\\nmotivated by Product of Experts (PoE) [25]. Each sememe is regarded as an expert\\nthat only makes predictions on the senses connected with it. Let S(xk) denote the\\nset of senses that contain sememe xk, the kth expert. Different from conventional\\nneural language models, which directly use the inner product of the context vector\\ng \\xe2\\x88\\x88RH1 and the output embedding w \\xe2\\x88\\x88RH2 for word w to generate the score for\\neach word, Sense Predictor uses \\xcf\\x86(k)(g, w) to calculate the score given by expert\\nxk. And a bilinear function parameterized with a matrix Uk \\xe2\\x88\\x88RH1\\xc3\\x97H2 is chosen as a\\nstraight implementation of \\xcf\\x86(k)(\\xc2\\xb7, \\xc2\\xb7):\\n\\xcf\\x86(k)(g, w) = g\\xe2\\x8a\\xa4Ukw.\\n(6.19)\\nThe score of sense s provided by sememe expert xk can be written as \\xcf\\x86(k)(g, s).\\nTherefore, P(xk)(s|g), the probability of sense s given by expert xk, is formulated as\\nP(xk)(s|g) =\\nexp(qkCk,s\\xcf\\x86(k)(g, s))\\n\\x03\\ns\\xe2\\x80\\xb2\\xe2\\x88\\x88S(xk ) exp(qkCk,s\\xe2\\x80\\xb2\\xcf\\x86(k)(g, s\\xe2\\x80\\xb2)),\\n(6.20)\\nwhere Ck,s is a normalization constant because sense s is not connected to all experts\\n(the connections are sparse with approximately \\xce\\xbbN edges, \\xce\\xbb < 5). Here we can\\n142\\n6\\nSememe Knowledge Representation\\nchoose either Ck,s = 1/|X(s)| (left normalization) or Ck,s = 1/\\n\\x06\\n|X(s)||S(xk)| (sym-\\nmetric normalization).\\nIn the Sense Predictor, qk can be viewed as a gate which controls the magnitude of\\nthe term Ck,s\\xcf\\x86(k)(g, s), thus controlling the \\xef\\xac\\x82atness of the sense distribution provided\\nby sememe expert xk. Consider the extreme case when pk \\xe2\\x86\\x920, the prediction will\\nconverge to the discrete uniform distribution. Intuitively, it means that the sememe\\nexpert will refuse to provide any useful information when it is not likely to be related\\nto the next word.\\nFinally, the predictions can be summarized on sense s by taking the product of\\nthe probabilities given by relevant experts and then normalize the result; that is to\\nsay, P(s|g), the probability of sense s, satis\\xef\\xac\\x81es\\nP(s|g) \\xe2\\x88\\x9d\\n\\x07\\nxk\\xe2\\x88\\x88X(s)\\nP(xk)(s|g).\\n(6.21)\\nUsing Eqs.6.19 and 6.20, P(s|g) can be formulated as\\nP(s|g) =\\nexp(\\x03\\nxk\\xe2\\x88\\x88X(s) qkCk,sg\\xe2\\x8a\\xa4Uks)\\n\\x03\\ns\\xe2\\x80\\xb2 exp(\\x03\\nxk\\xe2\\x88\\x88X(s\\xe2\\x80\\xb2) qkCk,s\\xe2\\x80\\xb2g\\xe2\\x8a\\xa4Uks\\xe2\\x80\\xb2).\\n(6.22)\\nIt should be emphasized that all the supervision information provided by HowNet\\nis embodied in the connections between the sememe experts and the senses. If the\\nmodel wants to assign a high probability to sense s, it must assign a high probability to\\nsome of its relevant sememes. If the model wants to assign a low probability to sense\\ns, it can assign a low probability to its relevant sememes. Moreover, the prediction\\nmade by sememe expert xk has its own tendency because of its own \\xcf\\x86(k)(\\xc2\\xb7, \\xc2\\xb7). Besides,\\nthe sparsity of connections between experts and senses is also determined by HowNet\\nitself.\\nAs illustrated in Fig.6.7, in the Word Predictor, P(w|g), the probability of word\\nw is calculated by summing up probabilities of corresponding s given by the Sense\\nPredictor, that is\\nP(w|g) =\\n\\x02\\ns\\xe2\\x88\\x88S(w)\\nP(s|g).\\n(6.23)\\n6.3.4\\nSememe Prediction\\nThe manual construction of HowNet is actually time-consuming and labor-intensive,\\ne.g., HowNet has been built for more than 10 years by several linguistic experts.\\nHowever, as the development of communications and techniques, new words and\\nphrases are emerging, the semantic meanings of existing words are also dynamically\\nevolving. In this case, sustained manual annotation and updates are becoming much\\nmore overwhelmed. Moreover, due to the high complexity of sememe ontology and\\n6.3 Applications\\n143\\n0.9\\n0.1\\n0.2\\nLSTM\\nLSTM\\nLSTM\\nLSTM\\n0.1\\n0.3\\n0.2\\nP\\nword\\nP\\nsense\\nsememe\\nexperts\\ncontext\\nvector\\nFig. 6.7 The architecture of SDLM model\\nword meanings, it is also challenging to maintain annotation consistency among\\nexperts when they collaboratively annotate lexical sememes.\\nTo address the issues of in\\xef\\xac\\x82exibility and inconsistency of manual annotation, the\\nautomatic lexical sememe prediction task is proposed, which is expected to assist\\nexpert annotation and reduce manual workload. Note that for simplicity, most works\\nintroduced in this part do not consider the complicated hierarchies of word sememes,\\nand simply group all annotated sememes of each word as the sememe set for learning\\nand prediction.\\nThe basic idea of sememe prediction is that those words of similar semantic mean-\\nings may share overlapped sememes. Hence, the key challenge of sememe prediction\\nis how to represent semantic meanings of words and sememes to model the semantic\\nrelatedness between them. In this part, we will focus on introducing the sememe pre-\\ndiction word accomplished by Xie et al. [66]. In their work, they propose to model\\nthe semantics of words and sememes using distributed representation learning [26].\\nDistributed representation learning aims to encode objects into a low-dimensional\\nsemantic space, which has shown its impressive capability of modeling semantics of\\nhuman languages, e.g., word embeddings [43] have been widely studied and utilized\\nin various tasks of NLP.\\nAs shown in previous work [43], it is effective to measure word similarities using\\ncosine similarity or Euclidean distance of their word embeddings learned from a\\nlarge-scale text corpus. Hence, a straightforward method for sememe prediction is\\n144\\n6\\nSememe Knowledge Representation\\nthat, given an unlabeled word, we \\xef\\xac\\x81nd its most related words in HowNet according\\nto their word embeddings, and recommend the annotated sememes of these related\\nwords to the given word. The method is intrinsically similar to collaborative \\xef\\xac\\x81ltering\\n[58] in recommendation systems, capable of capturing semantic relatedness between\\nwords and sememes based on their annotation co-occurrences.\\nWord embeddings can also be learned with techniques of matrix factorization\\n[37]. Inspired by the successful practice of matrix factorization for personalized\\nrecommendation [36], a new model which factorizes the word-sememe matrix from\\nHowNet and obtains sememe embeddings is proposed. In this way, the relatedness of\\nwords and sememes can be measured directly using dot products of their embeddings,\\naccording to which we could recommend the most related sememes to an unlabeled\\nword.\\nThe two methods are named as Sememe Prediction with Word Embeddings\\n(SPWE) and with Sememe Embeddings (SPSE/SPASE), respectively.\\n6.3.4.1\\nSememe Prediction with Word Embeddings\\nGiven an unlabeled word, it is straightforward to recommend sememes according to\\nits most related words, assuming that similar words should have similar sememes.\\nThis idea is similar to collaborative \\xef\\xac\\x81ltering in the personalized recommendation, for\\nin the scenario of sememe prediction words can be regarded as users and sememes\\nas the items/products to be recommended. Inspired by this, Sememe Prediction\\nwith Word Embeddings (SPWE) model is proposed, which uses similarities of word\\nembeddings to judge user distances.\\nFormally, the score function P(x j|w) of sememes x j given a word w is de\\xef\\xac\\x81ned\\nas\\nP(x j|w) =\\n\\x02\\nwi\\xe2\\x88\\x88V\\ncos(w, wi)Mi jcri,\\n(6.24)\\nwhere cos(w, wi) is the cosine similarity between word embeddings of w and wi\\npretrained by GloVe. Mi j indicates the annotation of sememe x j on word wi, where\\nMi j = 1 indicates the word wi which has the sememe x j in HowNet and otherwise\\nhas not. Higher the score function P(x j|w) is, more possible the word w should be\\nrecommended with x j.\\nDiffering from classical collaborative \\xef\\xac\\x81ltering in recommendation systems, only\\nthe most similar words should be concentrated when predicting sememes for new\\nwords since irrelevant words have totally different sememes which may be noises\\nfor sememe prediction. To address this problem, a declined con\\xef\\xac\\x81dence factor cri is\\nassigned for each word wi, whereri is the descend rank of word similarity cos(w, wi),\\nand c \\xe2\\x88\\x88(0, 1) is a hyperparameter. In this way, only a few top words that are similar\\nto w have strong in\\xef\\xac\\x82uences on predicting sememes.\\nSPWE only uses word embeddings for word similarities and is simple and effec-\\ntive for sememe prediction. It is because, differing from the noisy and incomplete\\nuser-item matrix in most recommender systems, HowNet is carefully annotated by\\n6.3 Applications\\n145\\nhuman experts, and thus the word-sememe matrix is with high con\\xef\\xac\\x81dence. Therefore,\\nthe word-sememe matrix can be con\\xef\\xac\\x81dently applied to collaboratively recommend\\nreliable sememes based on similar words.\\n6.3.4.2\\nSememe Prediction with Sememe Embeddings\\nSememe Prediction with Word Embeddings model follows the assumption that the\\nsememes of a word can be predicted according to its related words\\xe2\\x80\\x99 sememes. How-\\never, simply considering sememes as discrete labels may inevitably neglect the latent\\nrelations between sememes. To take the latent relations of sememes into consider-\\nation, Sememe Prediction with Sememe Embeddings (SPSE) model is proposed,\\nwhich projects both words and sememes into the same semantic vector space, learn-\\ning sememe embeddings according to the co-occurrences of words and sememes in\\nHowNet.\\nSimilar to GloVe [53] which decomposes co-occurrence matrix of words to learn\\nword embeddings, sememe embeddings can be learned by factorizing word-sememe\\nmatrix and sememe-sememe matrix simultaneously. These two matrices are both\\nconstructed from HowNet. As for word embeddings, similar to SPWE, SPSE uses\\nword embeddings pretrained from a large-scale corpus and \\xef\\xac\\x81xes them during fac-\\ntorizing of the word-sememe matrix. With matrix factorization, both sememe and\\nword embeddings can be encoded into the same low-dimensional semantic space,\\nand then computed the cosine similarity between normalized embeddings of words\\nand sememes for sememe prediction.\\nMore speci\\xef\\xac\\x81cally, similar to M, a sememe-sememe matrix C can also be extracted,\\nwhere C jk is de\\xef\\xac\\x81ned as point-wise mutual information that C jk = PMI(x j, xk) to\\nindicate the correlations between two sememes x j and xk. Note that, by factorizing\\nC, two distinct embeddings for each sememe s will be obtained, denoted as x and \\xc2\\xafx,\\nrespectively. The loss function of learning sememe embeddings is de\\xef\\xac\\x81ned as follows:\\nL =\\n\\x02\\nwi\\xe2\\x88\\x88W,x j\\xe2\\x88\\x88X\\n\\x08\\nwi \\xc2\\xb7 (x j +Nx j) + bi + b\\xe2\\x80\\xb2\\nj \\xe2\\x88\\x92Mi j\\n\\t2 + \\xce\\xbb\\n\\x02\\nx j,xk\\xe2\\x88\\x88X\\n\\x08\\nx j \\xc2\\xb7 \\xc2\\xafxk \\xe2\\x88\\x92C jk\\n\\t2,\\n(6.25)\\nwhere bi and b\\xe2\\x80\\xb2\\nj denote the bias of wi and x j. These two parts correspond to the\\nlosses of factorizing matrices M and C, adjusted by the hyperparameter \\xce\\xbb. Since\\nthe sememe embeddings are shared by both factorizations, our SPSE model enables\\njointly encoding both words and sememes into a uni\\xef\\xac\\x81ed semantic space.\\nSince each word is typically annotated with 2\\xe2\\x80\\x935 sememes in HowNet, most ele-\\nments in the word-sememe matrix are zeros. If all zero elements and nonzero ele-\\nments are treated equally during factorization, the performance will be much worse.\\nTo address this issue, different factorization strategies are assigned for zero and\\nnonzero elements. For each zero element, the model chooses to factorize them with\\na small probability like 0.5%, and otherwise, the model chooses to ignore. While for\\nnonzero elements, the model always chooses to factorize them. With the help of this\\nstrategy, the model can pay more attention to those annotated word-sememe pairs.\\n146\\n6\\nSememe Knowledge Representation\\nIn SPSE, sememe embeddings are learned accompanying with word embeddings\\nvia matrix factorization into the uni\\xef\\xac\\x81ed low-dimensional semantic space. Matrix\\nfactorization has been veri\\xef\\xac\\x81ed as an effective approach in the personalized recom-\\nmendation, because it can accurately model relatedness between users and items, and\\nis highly robust to noises in user-item matrices. Using this model, we can \\xef\\xac\\x82exibly\\ncompute semantic relatedness of words and sememes, which provides us an effec-\\ntive tool to manipulate and manage sememes, including but not limited to sememe\\nprediction.\\n6.3.4.3\\nSememe Prediction with Aggregated Sememe Embeddings\\nInspired by the characteristics of sememes, we assume that the word embeddings are\\nsemantically composed of sememe embeddings. In the word-sememe joint space, we\\ncan simply implement semantic composition as additive operations that each word\\nembedding is expected to be the sum of its all sememes\\xe2\\x80\\x99 embeddings. Following this\\nassumption, Sememe Prediction with Aggregated Sememe Embeddings (SPASE)\\nmodel is proposed. SPASE is also based on matrix factorization, and is formally\\ndenoted as\\nwi =\\n\\x02\\nx j\\xe2\\x88\\x88Xwi\\nM\\xe2\\x80\\xb2\\ni jx j,\\n(6.26)\\nwhere Xwi is the sememe set of the word wi and M\\xe2\\x80\\xb2\\ni j represents the weight of\\nsememe x j for word wi, which only has value on nonzero elements of word-sememe\\nlabeled matrix M. To learn sememe embeddings, we attempt to decompose the word\\nembedding matrix V into M\\xe2\\x80\\xb2 and sememe embedding matrix X, with pretrained word\\nembeddings \\xef\\xac\\x81xed during training, which could also be written as V = M\\xe2\\x80\\xb2X.\\nThe contribution of SPASE is that it complies with the de\\xef\\xac\\x81nition of sememes\\nin HowNet that sememes are the semantic components of words. In SPASE, each\\nsememe can be regarded as a tiny semantic unit, and all words can be represented\\nby composing several semantic units, i.e., sememes, which make up an interesting\\nsemantic regularity. However, SPASE is dif\\xef\\xac\\x81cult to train because word embeddings\\nare \\xef\\xac\\x81xed, and the number of words is much larger than the number of sememes. In the\\ncase of modeling complex semantic compositions of sememes into words, the rep-\\nresentation capability of SPASE may be strongly constrained by limited parameters\\nof sememe embeddings and excessive simpli\\xef\\xac\\x81cation of additive assumption.\\n6.3.4.4\\nLexical Sememe Prediction with Internal Information\\nIn the previous section, we introduce the automatic lexical sememe prediction pro-\\nposed by Xie et al. [66]. These methods ignore the internal information within words\\n(e.g., the characters in Chinese words), which is also signi\\xef\\xac\\x81cant for word understand-\\ning, especially for words which are of low frequency or do not appear in the corpus\\n6.3 Applications\\n147\\nWors embedding\\n(iron)\\nExternal information\\nInternal information\\nHostOf\\nRelate To\\ndomain\\n(craftsman)\\n(ironsmith)\\nironsmith\\n(human)\\n(occupation)\\n(metal)\\n(industrial)\\nWors embedding\\nword\\nsense\\nsememe\\nFig. 6.8 Sememes of the word\\n(ironsmith) in HowNet, where occupation, human,\\nand industrial can be inferred by both external (contexts) and internal (characters) information,\\nwhile metal is well-captured only by the internal information within the character\\n(iron)\\nat all. In this section, we introduce the work of Jin et al. [30], which takes Chinese\\nas an example and explores methods of taking full advantage of both external and\\ninternal information of words for sememe prediction.\\nIn Chinese, words are composed of one or multiple characters, and most characters\\nhavecorrespondingsemanticmeanings. As shownby[67], morethan90%of Chinese\\ncharacters in modern Chinese corpora are morphemes. Chinese words can be divided\\ninto single-morpheme words and compound words, where compound words account\\nfor a dominant proportion. The meanings of compound words are closely related\\nto their internal characters as shown in Fig. 6.8. Taking a compound word\\n(ironsmith), for instance, it consists of two Chinese characters:\\n(iron)\\nand\\n(craftsman), and the semantic meaning of\\ncan be inferred from the\\ncombinationof its twocharacters (iron +craftsman \\xe2\\x86\\x92ironsmith). Evenfor\\nsome single-morpheme words, their semantic meanings may also be deduced from\\ntheir characters. For example, both characters of the single-morpheme word\\n(hover) represent the meaning of hover or linger. Therefore, it is intuitive to\\ntake the internal character information into consideration for sememe prediction.\\nReference [30] proposes a novel framework for Character-enhanced Sememe\\nPrediction (CSP), which leverages both internal character information and external\\ncontext for sememe prediction. CSP predicts the sememe candidates for a target word\\nfrom its word embedding and the corresponding character embeddings. Speci\\xef\\xac\\x81cally,\\nfollowing SPWE and SPSE as introduced by [66] to model external information,\\nSememe Prediction with Word-to-Character Filtering (SPWCF) and Sememe Pre-\\ndiction with Character and Sememe Embeddings (SPCSE) are proposed to model\\ninternal character information.\\nSememe Prediction with Word-to-Character Filtering. Inspired by collabora-\\ntive \\xef\\xac\\x81ltering [58], Jin et al. [30] propose to recommend sememes for an unlabeled\\nword according to its similar words based on internal information. And words are\\nconsidered as similar if they contain the same characters at the same positions.\\n148\\n6\\nSememe Knowledge Representation\\nFig. 6.9 An example of the\\nposition of characters in a\\nword\\nBegin\\nEnd\\nMiddle\\nIn Chinese, the meaning of a character may vary according to its position within\\na word [14]. Three positions within a word are considered: Begin, Middle, and\\nEnd. For example, as shown in Fig. 6.9, the character at the Begin position of the\\nword\\n(railway station) is\\n(fire), while\\n(vehicle) and\\n(station) are at the Middle and End position, respectively. The character\\nusually means station when it is at the End position, while it usually means\\nstand at the Begin position like in\\n(stand),\\n(standing\\nguard), and\\n(stand up).\\nFormally, for a word w = c1c2...c|w|, we de\\xef\\xac\\x81ne \\xcf\\x80B(w) = {c1}, \\xcf\\x80M(w) =\\n{c2, ..., c|w\\xe2\\x88\\x921|}, \\xcf\\x80E(w) = {c|w|}, and\\nPp(x j|c) \\xe2\\x88\\xbc\\n\\x03\\nwi\\xe2\\x88\\x88W\\xe2\\x88\\xa7c\\xe2\\x88\\x88\\xcf\\x80p(wi) Mi j\\n\\x03\\nwi\\xe2\\x88\\x88W\\xe2\\x88\\xa7c\\xe2\\x88\\x88\\xcf\\x80p(wi) |Xwi|,\\n(6.27)\\nthat represents the score of a sememe x j given a character c and a position p, where\\n\\xcf\\x80p may be \\xcf\\x80B, \\xcf\\x80M, or \\xcf\\x80E. M is the same matrix used in SPWE. Finally, the score\\nfunction P(x j|w) of sememe x j given a word w is de\\xef\\xac\\x81ned as\\nP(x j|w) \\xe2\\x88\\xbc\\n\\x02\\np\\xe2\\x88\\x88{B,M,E}\\n\\x02\\nc\\xe2\\x88\\x88\\xcf\\x80p(w)\\nPp(x j|c).\\n(6.28)\\nSPWCF is a simple and ef\\xef\\xac\\x81cient method. It performs well because compositional\\nsemantics are pervasive in Chinese compound words, which makes it straightforward\\nand effective to \\xef\\xac\\x81nd similar words according to common characters.\\nSememe Prediction with Character and Sememe Embeddings (SPCSE). The\\nmethod Sememe Prediction with Word-to-Character Filtering (SPWCF) can effec-\\ntively recommend the sememes that have strong correlations with characters. How-\\never, just like SPWE, it ignores the relations between sememes. Hence, inspired\\nby SPSE, Sememe Prediction with Character and Sememe Embeddings (SPCSE) is\\nproposed to take the relations between sememes into account. In SPCSE, the model\\ninstead learns the sememe embeddings based on internal character information, then\\ncomputes the semantic distance between sememes and words for prediction.\\nInspired by GloVe [53] and SPSE, matrix factorization is adopted in SPCSE, by\\ndecomposing the word-sememe matrix and the sememe-sememe matrix simultane-\\nously. Instead of using pretrained word embeddings in SPSE, pretrained character\\nembeddings are used in SPCSE. Since the ambiguity of characters is stronger than\\nthat of words, multiple embeddings are learned for each character [14]. The most rep-\\nresentative character and its embedding are selected to represent the word meaning.\\nBecause low-frequency characters are much rare than those low-frequency words,\\n6.3 Applications\\n149\\nFig. 6.10 An example of adopting multiple-prototype character embeddings. The numbers are the\\ncosine distances. The sememe\\n(metal) is the closest to one embedding of\\n(iron)\\nand even low-frequency words are usually composed of common characters, it is\\nfeasible to use pretrained character embeddings to represent rare words. During fac-\\ntorizing of the word-sememe matrix, the character embeddings are \\xef\\xac\\x81xed.\\nNe is set as the number of embeddings for each character, and each character c\\nhas Ne embeddings c1, ..., cNe. Given a word w and a sememe x, the embedding of\\na character of w closest to the sememe embedding by cosine distance is selected as\\nthe representation of the word w, as shown in Fig. 6.10. Speci\\xef\\xac\\x81cally, given a word\\nw = c1...c|w| and a sememe x j, we de\\xef\\xac\\x81ne\\nk\\xe2\\x88\\x97,r\\xe2\\x88\\x97= arg min\\nk,r\\n\\n1 \\xe2\\x88\\x92cos(cr\\nk, x\\xe2\\x80\\xb2\\nj + \\xc2\\xafx\\xe2\\x80\\xb2\\nj)\\n\\x0b\\n,\\n(6.29)\\nwhere k\\xe2\\x88\\x97and r\\xe2\\x88\\x97indicate the indices of the character and its embedding closest to\\nthe sememe x j in the semantic space. With the same word-sememe matrix M and\\nsememe-sememe correlation matrix C in SPSE, the sememe embeddings are learned\\nwith the loss function:\\nL =\\n\\x02\\nwi\\xe2\\x88\\x88W,x j\\xe2\\x88\\x88X\\n\\x08\\ncr\\xe2\\x88\\x97\\nk\\xe2\\x88\\x97\\xc2\\xb7\\n\\x08\\nx\\xe2\\x80\\xb2\\nj + \\xc2\\xafx\\xe2\\x80\\xb2\\nj\\n\\t\\n+ bc\\nk\\xe2\\x88\\x97+ b\\xe2\\x80\\xb2\\xe2\\x80\\xb2\\nj \\xe2\\x88\\x92Mi j\\n\\t2 + \\xce\\xbb\\xe2\\x80\\xb2 \\x02\\nx j,xq\\xe2\\x88\\x88X\\n\\x08\\nx\\xe2\\x80\\xb2\\nj \\xc2\\xb7 \\xc2\\xafx\\xe2\\x80\\xb2\\nq \\xe2\\x88\\x92C jq\\n\\t2 ,\\n(6.30)\\nwhere x\\xe2\\x80\\xb2\\nj and \\xc2\\xafx\\xe2\\x80\\xb2\\nj are the sememe embeddings for sememe x j, and cr\\xe2\\x88\\x97\\nk\\xe2\\x88\\x97is the embedding\\nof the character that is the closest to sememe x j within wi. Note that, as the characters\\nand the words are not embedded into the same semantic space, new sememe embed-\\ndings are learned instead of using those learned in SPSE, hence different notations\\nare used for the sake of distinction. bc\\nk and b\\xe2\\x80\\xb2\\xe2\\x80\\xb2\\nj denote the biases of ck and x j, and\\n\\xce\\xbb\\xe2\\x80\\xb2 is the hyperparameter adjusting the two parts. Finally, the score function of word\\nw = c1...c|w| is de\\xef\\xac\\x81ned as\\nP(x j|w) \\xe2\\x88\\xbccr\\xe2\\x88\\x97\\nk\\xe2\\x88\\x97\\xc2\\xb7\\n\\x08\\nx\\xe2\\x80\\xb2\\nj + \\xc2\\xafx\\xe2\\x80\\xb2\\nj\\n\\t\\n.\\n(6.31)\\n150\\n6\\nSememe Knowledge Representation\\nSPWE\\nSPSE\\nSPWCF\\nSPCSE\\nLegend\\nhigh-frequency words\\nlow-frequency words\\nword\\nExternal\\nInternal\\nCSP\\nFig. 6.11 An illustration of model ensembling in sememe prediction\\nModel Ensembling. SPWCF/SPCSE and SPWE/SPSE take different sources\\nof information as input, which means that they have different characteristics:\\nSPWCF/SPCSE only have access to internal information, while SPWE/SPSE can\\nonly make use of external information. On the other hand, just like the difference\\nbetween SPWE and SPSE, SPWCF originates from collaborative \\xef\\xac\\x81ltering, whereas\\nSPCSE uses matrix factorization. All of those methods have in common that they tend\\nto recommend the sememes of similar words, but they diverge in their interpretation\\nof similar.\\nTherefore,toobtainbetterpredictionperformance,itisnecessarytocombinethese\\nmodels.WedenotetheensembleofSPWCFandSPCSEastheinternalmodel,andthe\\nensemble of SPWE and SPSE as the external model. The ensemble of the internal\\nand the external models is the novel framework CSP. In practice, for words with\\nreliable word embeddings, i.e., high-frequency words, we can use the integration of\\nthe internal and the external models; for words with extremely low frequencies (e.g.,\\nhaving no reliable word embeddings), we can just use the internal model and ignore\\nthe external model, because the external information is noisy in this case. Figure6.11\\nshows model ensembling in different scenarios. For the sake of comparison, we use\\nthe integration of SPWCF, SPCSE, SPWE, and SPSE as CSP in all experiments.\\nAnd two models are integrated by simple weighted addition.\\n6.3.4.5\\nCross-Lingual Sememe Prediction\\nMost languages do not have sememe-based linguistic KBs such as HowNet, which\\nprevents us from understanding and utilizing human languages to a greater extent.\\nTherefore, it is important to build sememe-based linguistic KBs for various lan-\\nguages.\\nTo address the issue of the high labor cost of manual annotation, Qi et al. [56]\\npropose a new task, cross-lingual lexical sememe prediction (CLSP) which aims to\\n6.3 Applications\\n151\\nautomatically predict lexical sememes for words in other languages. There are two\\ncritical challenges for CLSP:\\n(1) There is not a consistent one-to-one match between words in different lan-\\nguages. For example, English word \\xe2\\x80\\x9cbeautiful\\xe2\\x80\\x9d can refer to Chinese words of either\\nor\\n. Hence, we cannot simply translate HowNet into another language.\\nAnd how to recognize the semantic meaning of a word in other languages becomes\\na critical problem.\\n(2) Since there is a gap between the semantic meanings of words and sememes,\\nwe need to build semantic representations for words and sememes to capture the\\nsemantic relatedness between them.\\nTo tackle these challenges, Qi et al. [56] propose a novel model for CLSP, which\\naims to transfer sememe-based linguistic KBs from source language to target lan-\\nguage. Their model contains three modules: (1) monolingual word embedding learn-\\ning which is intended for learning semantic representations of words for source and\\ntarget languages, respectively; (2) cross-lingual word embedding alignment which\\naims to bridge the gap between the semantic representations of words in two lan-\\nguages; (3) sememe-based word embedding learning whose objective is to incorpo-\\nrate sememe information into word representations.\\nThey take Chinese as source language and English as the target language to\\nshow the effectiveness of their model. Experimental results show that the proposed\\nmodel could effectively predict lexical sememes for words with different frequencies\\nin other languages and their model has consistent improvements on two auxiliary\\nexperiments including bilingual lexicon induction and monolingual word similarity\\ncomputation by jointly learning the representations of sememes, words in source and\\ntarget languages.\\nThe model consists of three parts: monolingual word representation learning,\\ncross-lingual word embedding alignment, and sememe-based word representation\\nlearning. Hence, they de\\xef\\xac\\x81ne the objective function of our method corresponding to\\nthe three parts:\\nL = Lmono + Lcross + Lsememe.\\n(6.32)\\nHere, the monolingual term Lmono is designed for learning monolingual word\\nembeddings from nonparallel corpora for source and target languages, respectively.\\nThe cross-lingual term Lcross aims to align cross-lingual word embeddings in a\\nuni\\xef\\xac\\x81ed semantic space. And Lsememe can draw sememe information into word rep-\\nresentation learning and conduce to better word embeddings for sememe prediction.\\nIn the following paragraphs, we will introduce the three parts in detail.\\nMonolingualWordRepresentation.Monolingualwordrepresentationisrespon-\\nsible for explaining regularities in monolingual corpora of source and target lan-\\nguages. Since the two corpora are nonparallel, Lmono comprises two monolingual\\nsubmodels that are independent of each other:\\nLmono = L S\\nmono + L T\\nmono,\\n(6.33)\\nwhere the superscripts S and T denote source and target languages, respectively.\\n152\\n6\\nSememe Knowledge Representation\\nAs a common practice, the well-established Skip-gram model is chosen to obtain\\nmonolingual word embeddings. The Skip-gram model is aimed at maximizing the\\npredictive probability of context words conditioned on the centered word. Formally,\\ntaking the source side, for example, given a training word sequence {wS\\n1, . . . , wS\\nn},\\nSkip-gram model intends to minimize\\nL S\\nmono = \\xe2\\x88\\x92\\nn\\xe2\\x88\\x92K\\n\\x02\\nc=K+1\\n\\x02\\n\\xe2\\x88\\x92K\\xe2\\x89\\xa4k\\xe2\\x89\\xa4K,k\\xcc\\xb8=0\\nlog P(wS\\nc+k|wS\\nc ),\\n(6.34)\\nwhere K is the size of the sliding window. P(wS\\nc+k|wS\\nc ) stands for the predictive prob-\\nability of one of the context words conditioned on the centered word wS\\nc , formalized\\nby the following softmax function:\\nP(wS\\nc+k|wS\\nc ) =\\nexp(wS\\nc+k \\xc2\\xb7 wS\\nc )\\n\\x03\\nwSs \\xe2\\x88\\x88V S exp(wSs \\xc2\\xb7 wSc ),\\n(6.35)\\nin which V s indicates the word vocabulary of source language. L T\\nmono can be formu-\\nlated similarly.\\nCross-lingual Word Embedding Alignment. Cross-lingual word embedding\\nalignment aims to build a uni\\xef\\xac\\x81ed semantic space for the words in source and target\\nlanguages. Inspired by [69], the cross-lingual word embeddings are aligned with\\nsignals of a seed lexicon and self-matching.\\nFormally, Lcross is composed of two terms including alignment by seed lexicon\\nLseed and alignment by matching Lmatch:\\nLcross = \\xce\\xbbsLseed + \\xce\\xbbmLmatch,\\n(6.36)\\nwhere \\xce\\xbbs and \\xce\\xbbm are hyperparameters for controlling relative weightings of the two\\nterms.\\n(1) Alignment by Seed Lexicon\\nThe seed lexicon term Lseed encourages word embeddings of translation pairs in\\na seed lexicon D to be close, which can be achieved via an L2 regularizer:\\nLseed =\\n\\x02\\n\\xe2\\x9f\\xa8wSs ,wT\\nt \\xe2\\x9f\\xa9\\xe2\\x88\\x88D\\n\\xe2\\x88\\xa5wS\\ns \\xe2\\x88\\x92wT\\nt \\xe2\\x88\\xa52,\\n(6.37)\\nin which wS\\ns and wT\\nt indicate the words in source and target languages in the seed\\nlexicon, respectively.\\n(2) Alignment by Matching Mechanism\\nAs for the matching process, it is found on the assumption that each target word\\nshould be matched to a single source word or a special empty word, and vice versa.\\nThe goal of the matching process is to \\xef\\xac\\x81nd the matched source (target) word for each\\n6.3 Applications\\n153\\ntarget (source) word and maximize the matching probabilities for all the matched\\nword pairs. The loss of this part can be formulated as\\nLmatch = L T2S\\nmatch + L S2T\\nmatch,\\n(6.38)\\nwhere L T 2S\\nmatch is the term for target-to-source matching and L S2T\\nmatch is the term for\\nsource-to-target matching.\\nNext, a detailed explanation of target-to-source matching is given, and the source-\\nto-target matching is de\\xef\\xac\\x81ned in the same way. A latent variable mt \\xe2\\x88\\x88{0, 1, . . . , |V S|}\\n(t = 1, 2, . . . , |V T |) is \\xef\\xac\\x81rst introduced for each target word wT\\nt , where |V S| and |V T |\\nindicate the vocabulary size of source and target languages, respectively. Here, mt\\nspeci\\xef\\xac\\x81es the index of the source word that wT\\nt matches with, and mt = 0 signi\\xef\\xac\\x81es the\\nempty word is matched. Then we have m = {m1, m2, . . . , m|V T |}, and can formalize\\nthe target-to-source matching term:\\nL T 2S\\nmatch = \\xe2\\x88\\x92log P(C T |C S) = \\xe2\\x88\\x92log\\n\\x02\\nm\\nP(C T , m|C S),\\n(6.39)\\nwhere C T and C S denote the target and source corpus, respectively. Here, they simply\\nassume that the matching processes of target words are independent of each other.\\nTherefore, we have\\nP(C T , m|C S) =\\n\\x07\\nwT \\xe2\\x88\\x88C T\\nP(wT , m|C S) =\\n|V T |\\n\\x07\\nt=1\\nP(wT\\nt |wS\\nmt)c(wT\\nt ),\\n(6.40)\\nwhere wS\\nmt is the source word that wT\\nt matches with, and c(wT\\nt ) is the number of\\ntimes wT\\nt occurs in the target corpus.\\n6.3.5\\nOther Sememe-Guided Applications\\n6.3.5.1\\nChinese LIWC Lexicon Expansion\\nLinguistic Inquiry and Word Count (LIWC) [52] has been widely used for comput-\\nerized text analysis in social science. Not only can LIWC be used to analyze text\\nfor classi\\xef\\xac\\x81cation and prediction, but it has also been used to examine the underlying\\npsychological states of a writer or speaker. In the beginning, LIWC was developed\\nto address content analytic issues in experimental psychology. Nowadays, there is\\nan increasing number of applications across \\xef\\xac\\x81elds such as computational linguistics\\n[22], demographics [48], health diagnostics [11], and social relationship [32].\\nChinese is the most spoken language in the world, but we cannot use the original\\nLIWC to analyze Chinese text. Fortunately, Chinese LIWC [28] has been released\\n154\\n6\\nSememe Knowledge Representation\\nto \\xef\\xac\\x81ll the vacancy. In this part, we mainly focus on Chinese LIWC and using LIWC\\nto stand for Chinese LIWC if not otherwise speci\\xef\\xac\\x81ed.\\nWhile LIWC has been used in a variety of \\xef\\xac\\x81elds, its lexicon only contains less than\\n7,000 words. This is insuf\\xef\\xac\\x81cient because according to [39], there are at least 56,008\\ncommon words in Chinese. Moreover, LIWC lexicon does not consider emerging\\nwords and phrases on the Internet. Therefore, it is reasonable and necessary to\\nexpand the LIWC lexicon so that it is more accurate and comprehensive for sci-\\nenti\\xef\\xac\\x81c research. One way to expand LIWC lexicon is to annotate the new words\\nmanually. However, it is too time-consuming and often requires language expertise\\nto add new words. Hence, expanding LIWC lexicon automatically is proposed.\\nIn LIWC lexicon, words are labeled with different categories and categories form\\na certain hierarchy. Therefore, hierarchical classi\\xef\\xac\\x81cation algorithms can be naturally\\napplied to LIWC lexicon. Reference [15] proposes Hierarchical SVM (Support Vec-\\ntor Machine), which is a modi\\xef\\xac\\x81ed version of SVM based on the hierarchical problem\\ndecomposition approach. In [6], the authors presented a novel algorithm which can\\nbe used on both tree- and Directed Acyclic Graph (DAG)-structured hierarchies.\\nSome recent works [12, 33] attempted to use neural networks in the hierarchical\\nclassi\\xef\\xac\\x81cation.\\nHowever, these methods are often too generic without considering the special\\nproperties of words and LIWC lexicon. Many words and phrases have multiple\\nmeaningsandaretherebyclassi\\xef\\xac\\x81edintomultipleleafcategories.Thisisoftenreferred\\nto as polysemy. Additionally, many categories in LIWC are \\xef\\xac\\x81ne-grained, thus making\\nit more dif\\xef\\xac\\x81cult to distinguish them. To address these issues, we introduce several\\nmodels to incorporate sememe information when expanding the lexicon, which will\\nbe discussed after the introduction of the basic model.\\nBasic Decoder for Hierarchical Classi\\xef\\xac\\x81cation. First, we introduce the basic\\nmodel for Chinese LIWC lexicon expansion. The well-known Sequence-to-Sequence\\ndecoder [64] is exploited for hierarchical classi\\xef\\xac\\x81cation. The original Sequence-to-\\nSequence decoder is often trained to predict the next word wt with consideration of\\nall the previously predicted words {w1, . . . , wt\\xe2\\x88\\x921}. This is a useful feature since an\\nimportant difference between \\xef\\xac\\x82at multilabel classi\\xef\\xac\\x81cation and hierarchical classi\\xef\\xac\\x81-\\ncation is that there are explicit connections among hierarchical labels. This property\\nis utilized by transforming hierarchical labels into a sequence. Let Y denote the label\\nset and \\xcf\\x80: Y \\xe2\\x86\\x92Y denote the parent relationship where \\xcf\\x80(y) is the parent node of\\ny \\xe2\\x88\\x88Y. Given a word w, its labels form a tree structure hierarchy. We then choose\\neach path from the root node to the leaf node, and transform it into a sequence\\n{y1, y2, . . . , yL} where \\xcf\\x80(yi) = yi\\xe2\\x88\\x921, \\xe2\\x88\\x80i \\xe2\\x88\\x88[2, L] and L is the number of levels in\\nthe hierarchy. In this way, when the model predicts a label yi, it takes into consid-\\neration the probability of parent label sequence {y1,. . . ,yi\\xe2\\x88\\x921}. Formally, the decoder\\nde\\xef\\xac\\x81nes a probability over the label sequence:\\nP(y1, y2, . . . , yL) =\\nL\\n\\x07\\ni=1\\nP(yi|(y1, . . . , yi\\xe2\\x88\\x921), w).\\n(6.41)\\n6.3 Applications\\n155\\nA common approach for decoder is to use LSTM [27] so that each conditional\\nprobability is computed as\\nP(yi|(y1, . . . , yi\\xe2\\x88\\x921), w) = g(yi\\xe2\\x88\\x921, si) = oi \\xe2\\x8a\\x99tanh(hi),\\n(6.42)\\nwhere\\nhi = fi \\xe2\\x8a\\x99hi\\xe2\\x88\\x921 + ii \\xe2\\x8a\\x99\\xcb\\x9chi,\\n\\xcb\\x9chi = tanh(Wh[hi\\xe2\\x88\\x921; yi\\xe2\\x88\\x921] + bh),\\noi = Sigmoid(Wo[hi\\xe2\\x88\\x921; yi\\xe2\\x88\\x921] + bo),\\nzi = Sigmoid(Wz[hi\\xe2\\x88\\x921; yi\\xe2\\x88\\x921] + bz),\\nfi = Sigmoid(W f [hi\\xe2\\x88\\x921; yi\\xe2\\x88\\x921] + b f ),\\n(6.43)\\nwhere \\xe2\\x8a\\x99is an element-wise multiplication and hi is the ith hidden state of the RNN.\\nWh, Wo, Wz, W f are weights and bh, bo, bz, b f are biases. oi, zi, and fi are known\\nas output gate layer, input gate layer, and forget gate layer, respectively.\\nTo take advantage of word embeddings, the initial state h0 = w is de\\xef\\xac\\x81ned where\\nw represents the embedding of the word. In other words, the word embeddings are\\napplied as the initial state of the decoder.\\nSpeci\\xef\\xac\\x81cally, the inputs of our model are word embeddings and label embeddings.\\nFirst, raw words are transformed into word embeddings by an embedding matrix E \\xe2\\x88\\x88\\nR|V |\\xc3\\x97dw, where dw is the word embedding dimension. Then, at each time step, label\\nembeddings y are fed to the model, which is obtained by a label embedding matrix\\nY \\xe2\\x88\\x88R|Y|\\xc3\\x97dy, where dy is the label embedding dimension. Here word embeddings are\\npretrained and \\xef\\xac\\x81xed during training.\\nGenerally speaking, the decoder is expected to decode word labels hierarchically\\nbased on word embeddings. At each time step, the decoder will predict the current\\nlabel depending on previously predicted labels.\\nHierarchical Decoder with Sememe Attention. The basic decoder uses word\\nembeddings as the initial state, then predicts word labels hierarchically as sequences.\\nHowever, each word in the basic decoder model has only one representation. This\\nis insuf\\xef\\xac\\x81cient because many words are polysemous and many categories are \\xef\\xac\\x81ne-\\ngrained in the LIWC lexicon. It is dif\\xef\\xac\\x81cult to handle these properties using a single\\nreal-valued vector. Therefore, Zeng et al. [68] propose to incorporate sememe infor-\\nmation.\\nBecause different sememes represent different meanings of a word, they should\\nhave different weights when predicting word labels. Moreover, we believe that the\\nsame sememe should have different weights in different categories. Take the word\\napex in Fig.6.12, for example. The sememe location should have a relatively\\nhigher weight when the decoder chooses among the subclasses of relative. When\\nchoosing among the subclasses of PersonalConcerns, location should have\\na lower weight because it represents a relatively irrelevant sense vertex.\\n156\\n6\\nSememe Knowledge Representation\\n2\\n1\\nSense  (acme)\\nSense  (vertex)\\n(apex)\\n(Boundary)\\n(Location)\\n(Entity)\\n(Angular)\\n(Dot)\\n(Most)\\n(GreaterThanNormal)\\nhost\\nmodifier\\nbelong\\nmodifier\\ndegree\\nFig. 6.12 Example word apex and its senses and sememes in HowNet annotation\\nTo achieve these goals, the utilization of attention mechanism [1] is proposed\\nto incorporate sememe information when decoding the word label sequence. The\\nstructure of the model is illustrated in Fig.6.13.\\nSimilar to the basic decoder approach, word embeddings are applied as the initial\\nstate of the decoder. The primary difference is that the conditional probability is\\nde\\xef\\xac\\x81ned as\\nP(yi|(y1, . . . , yi\\xe2\\x88\\x921), w, ci) = g([yi\\xe2\\x88\\x921; ci], hi),\\n(6.44)\\nwhere ci is known as context vector. The context vector ci depends on a set of sememe\\nembeddings {x1, . . . , xN}, acquired by a sememe embedding matrix X \\xe2\\x88\\x88R|S|\\xc3\\x97ds,\\nwhere ds is the sememe embedding dimension.\\nTo be more speci\\xef\\xac\\x81c, the context vector ci is computed as a weighted sum of the\\nsememe embedding x j:\\nci =\\nN\\n\\x02\\nj=1\\n\\xce\\xb1i jx j.\\n(6.45)\\nThe weight \\xce\\xb1i j of each sememe embedding x j is de\\xef\\xac\\x81ned as\\n\\xce\\xb1i j =\\nexp(v \\xc2\\xb7 tanh(W1yi\\xe2\\x88\\x921 + W2x j))\\n\\x03N\\nk=1 exp(v \\xc2\\xb7 tanh(W1yi\\xe2\\x88\\x921 + W2xk))\\n,\\n(6.46)\\n6.3 Applications\\n157\\nWord\\nEmbedding\\nSememe\\nEmbedding\\nSememe\\nEmbedding\\nSememe\\nEmbedding\\nSoftmax\\nLabel 1\\n<GO>\\n<EOS>\\nLabel 1.1\\nLabel 1.1.1\\nLabel 1\\nLabel 1.1\\nLabel 1.1.1\\n\\xe2\\x80\\xa6\\nFig. 6.13 The architecture of sememe attention decoder with word embeddings as the initial state\\nwhere v \\xe2\\x88\\x88Ra is a trainable parameter, W1 \\xe2\\x88\\x88Ra\\xc3\\x97dy and W2 \\xe2\\x88\\x88Ra\\xc3\\x97ds are weight\\nmatrices, and a is the number of hidden units in attention model.\\nIntuitively, at each time step, the decoder chooses which sememes to pay atten-\\ntion to when predicting the current word label. In this way, different sememes can\\nhave different weights, and the same sememe can have different weights in differ-\\nent categories. With the support of sememe attention, the decoder can differentiate\\nmultiple meanings in a word and the \\xef\\xac\\x81ne-grained categories and thus can expand a\\nmore accurate and comprehensive lexicon.\\n6.4\\nSummary\\nIn this chapter, we \\xef\\xac\\x81rst give an introduction to the most well-known sememe knowl-\\nedge base, HowNet, which uses about 2, 000 prede\\xef\\xac\\x81ned sememes to annotate over\\n100, 000 Chinese and English words and phrases. Different from other linguistic\\nknowledge bases like WordNet, HowNet is based on the minimum semantics units\\n(sememes) and captures the compositional relations between sememes and words.\\nTo learn the representations of sememe knowledge, we elaborate on three models,\\nnamely Simple Sememe Aggregation model (SSA), Sememe Attention over Context\\nmodel (SAC), and Sememe Attention over Target model (SAT). These models not\\nonly learn the representations of sememes but also help improve the representations\\nofwords.Next,wedescribesomeapplicationsofsememeknowledge,includingword\\n158\\n6\\nSememe Knowledge Representation\\nrepresentation, semantic composition, and language modeling. We also detail how to\\nautomatically predict sememes for both monolingual and cross-lingual unannotated\\nwords.\\nFor further learning of sememe knowledge-based NLP, you can read the book\\nwritten by the authors of HowNet [18]. You can also \\xef\\xac\\x81nd more related papers in\\nthis paper list https://github.com/thunlp/SCPapers. You can use the open source API\\nOpenHowNet https://github.com/thunlp/OpenHowNet to access HowNet data.\\nIn the future, there are some research directions worth exploring:\\n(1) Utilizing Structures of Sememe Annotations. The sememe annotations in\\nHowNet are hierarchical, and sememes annotated to a word are actually organized\\nas a tree. However, existing studies still do not utilize the structural information of\\nsememes. Instead, in current methods, sememes are simply regarded as semantic\\nlabels. In fact, the structures of sememes also incorporate abundant semantic infor-\\nmation and will be helpful to the deep understanding of lexical semantics. Besides,\\nexisting sememe prediction studies also predict unstructured sememes only, and it\\nis an interesting task to conduct structured sememe predictions.\\n(2) Leveraging Sememes in Low-data Regimes. One of the most important and\\ntypical characteristics of sememes is that limited sememes can represent unlimited\\nsemantics, which can play an important and positive role in tackling the low-data\\nregimes. In word representation learning, the representations of low-frequency words\\ncan be improved by their sememes, which have been well learned with the high-\\nfrequency words they annotate. We believe sememes will be bene\\xef\\xac\\x81cial to other\\nlow-data regimes, e.g., low-resource language NLP tasks.\\n(3)BuildingSememeKnowledgeBasesforOtherLanguages.OriginalHowNet\\nannotates sememes for only two languages: Chinese and English. As far as we\\nknow, there are not sememe knowledge bases like HowNet in other languages. Since\\nHowNet and its sememe knowledge have been veri\\xef\\xac\\x81ed helpful for better understand-\\ning human languages, it will be of great signi\\xef\\xac\\x81cance to annotate sememes for words\\nand phrases in other languages. In the section, we have described a study on cross-\\nlingual sememe prediction. And we think it is promising to make efforts toward this\\ndirection.\\nReferences\\n1. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. In Proceedings of ICLR, 2015.\\n2. Michele Banko, Vibhu O Mittal, and Michael J Witbrock. Headline generation based on sta-\\ntistical translation. In Proceedings of ACL, 2000.\\n3. MarcoBaroniandRobertoZamparelli.Nounsarevectors,adjectivesarematrices:Representing\\nadjective-noun constructions in semantic space. In Proceedings of EMNLP, 2010.\\n4. Yoshua Bengio, R\\xc3\\xa9jean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic\\nlanguage model. Journal of Machine Learning Research, 3(Feb):1137\\xe2\\x80\\x931155, 2003.\\n5. Adam Berger and John Lafferty. Information retrieval as statistical translation. In Proceedings\\nof SIGIR, 1999.\\nReferences\\n159\\n6. Wei Bi and James T Kwok. Multi-label classi\\xef\\xac\\x81cation on tree-and dag-structured hierarchies.\\nIn Proceedings of ICML, 2011.\\n7. William Blacoe and Mirella Lapata. A comparison of vector-based representations for semantic\\ncomposition. In Proceedings of EMNLP-CoNLL, 2012.\\n8. Leonard Bloom\\xef\\xac\\x81eld. A set of postulates for the science of language. Language, 2(3):153\\xe2\\x80\\x93164,\\n1926.\\n9. Thorsten Brants, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean. Large language\\nmodels in machine translation. In Proceedings of EMNLP, 2007.\\n10. Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Fredrick Jelinek,\\nJohn D Lafferty, Robert L Mercer, and Paul S Roossin. A statistical approach to machine\\ntranslation. Computational Linguistics, 16(2):79\\xe2\\x80\\x9385, 1990.\\n11. Wilma Bucci and Bernard Maskit. Building a weighted dictionary for referential activity. Com-\\nputing Attitude and Affect in Text, pages 49\\xe2\\x80\\x9360, 2005.\\n12. Ricardo Cerri, Rodrigo C Barros, and Andr\\xc3\\xa9 CPLF De Carvalho. Hierarchical multi-label\\nclassi\\xef\\xac\\x81cation using local neural networks. Journal of Computer and System Sciences, 80(1):39\\xe2\\x80\\x93\\n56, 2014.\\n13. Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. A uni\\xef\\xac\\x81ed model for word sense representation\\nand disambiguation. In Proceedings of EMNLP, 2014.\\n14. Xinxiong Chen, Lei Xu, Zhiyuan Liu, Maosong Sun, and Huanbo Luan. Joint learning of\\ncharacter and word embeddings. In Proceedings of IJCAI, 2015.\\n15. Yangchi Chen, Melba M Crawford, and Joydeep Ghosh. Integrating support vector machines\\nin a hierarchical output space decomposition framework. In Proceedings of IGARSS, 2004.\\n16. Lei Dang and Lei Zhang. Method of discriminant for chinese sentence sentiment orientation\\nbased on hownet. Application Research of Computers, 4:43, 2010.\\n17. Zhendong Dong and Qiang Dong. Hownet-a hybrid language and knowledge resource. In\\nProceedings of NLP-KE, 2003.\\n18. Zhendong Dong and Qiang Dong. HowNet and the Computation of Meaning (With CD-Rom).\\nWorld Scienti\\xef\\xac\\x81c, 2006.\\n19. Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A\\nSmith. Retro\\xef\\xac\\x81tting word vectors to semantic lexicons. In Proceedings of NAACL-HLT, 2015.\\n20. Xianghua Fu, Guo Liu, Yanyan Guo, and Zhiqiang Wang. Multi-aspect sentiment analysis for\\nchinese online social reviews based on topic modeling and hownet lexicon. Knowledge-Based\\nSystems, 37:186\\xe2\\x80\\x93195, 2013.\\n21. Edward Grefenstette and Mehrnoosh Sadrzadeh. Experimental support for a categorical com-\\npositional distributional model of meaning. In Proceedings of EMNLP, 2011.\\n22. Justin Grimmer and Brandon M Stewart. Text as data: The promise and pitfalls of automatic\\ncontent analysis methods for political texts. Political analysis, 21(3):267\\xe2\\x80\\x93297, 2013.\\n23. Yihong Gu, Jun Yan, Hao Zhu, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Fen Lin, and Leyu\\nLin. Language modeling with sparse product of sememe experts. In Proceedings of EMNLP,\\npages 4642\\xe2\\x80\\x934651, 2018.\\n24. Djoerd Hiemstra. A linguistically motivated probabilistic model of information retrieval. In\\nProceedings of TPDL, 1998.\\n25. G. E Hinton. Products of experts. In Proceedings of ICANN, 1999.\\n26. Geoffrey E Hinton. Learning distributed representations of concepts. In Proceedings of CogSci,\\n1986.\\n27. Sepp Hochreiter and J\\xc3\\xbcrgen Schmidhuber. Long short-term memory. Neural Computation,\\n9(8):1735\\xe2\\x80\\x931780, 1997.\\n28. Chin-Lan Huang, CK Chung, Natalie Hui, Yi-Cheng Lin, Yi-Tai Seih, WC Chen, and JW Pen-\\nnebaker. The development of the chinese linguistic inquiry and word count dictionary. Chinese\\nJournal of Psychology, 54(2):185\\xe2\\x80\\x93201, 2012.\\n29. Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. Improving word\\nrepresentations via global context and multiple word prototypes. In Proceedings of ACL, 2012.\\n30. Huiming Jin, Hao Zhu, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Fen Lin, and Leyu Lin.\\nIncorporating chinese characters of words for lexical sememe prediction. In Proceedings of\\nACL, 2018.\\n160\\n6\\nSememe Knowledge Representation\\n31. Dan Jurafsky. Speech & language processing. 2000.\\n32. Ewa Kacewicz, James W Pennebaker, Matthew Davis, Moongee Jeon, and Arthur C Graesser.\\nPronoun use re\\xef\\xac\\x82ects standings in social hierarchies. Journal of Language and Social Psychol-\\nogy, 33(2):125\\xe2\\x80\\x93143, 2014.\\n33. Sanjeev Kumar Karn, Ulli Waltinger, and Hinrich Sch\\xc3\\xbctze. End-to-end trainable attentive\\ndecoder for hierarchical entity classi\\xef\\xac\\x81cation. Proceedings of EACL, 2017.\\n34. Slava Katz. Estimation of probabilities from sparse data for the language model component of a\\nspeech recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400\\xe2\\x80\\x93\\n401, 1987.\\n35. Thomas Kober, Julie Weeds, Jeremy Ref\\xef\\xac\\x81n, and David Weir. Improving sparse word repre-\\nsentations with distributional inference for semantic composition. In Proceedings of EMNLP,\\n2016.\\n36. Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recom-\\nmender systems. Computer, 42(8), 2009.\\n37. Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In\\nProceedings of NeurIPS, 2014.\\n38. Wei Li, Xuancheng Ren, Damai Dai, Yunfang Wu, Houfeng Wang, and Xu Sun. Sememe\\nprediction: Learning semantic knowledge from unstructured textual wiki descriptions. arXiv\\npreprint arXiv:1808.05437, 2018.\\n39. Xingjian Li et al. Lexicon of common words in contemporary chinese, 2008.\\n40. Qun Liu. Word similarity computing based on hownet. Computational linguistics and Chinese\\nlanguage processing, 7(2):59\\xe2\\x80\\x9376, 2002.\\n41. Shu Liu, Jingjing Xu, Xuancheng Ren, and Xu Sun. Evaluating semantic rationality of a\\nsentence: A sememe-word-matching neural network based on hownet. 2019.\\n42. Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher\\nPotts. Learning word vectors for sentiment analysis. In Proceedings of ACL, 2011.\\n43. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\\xef\\xac\\x81cient estimation of word\\nrepresentations in vector space. In Proceedings of ICLR, 2013.\\n44. Tomas Mikolov, Martin Kara\\xef\\xac\\x81\\xc3\\xa1t, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recur-\\nrent neural network based language model. In Proceedings of InterSpeech, 2010.\\n45. David RH Miller, Tim Leek, and Richard M Schwartz. A hidden markov model information\\nretrieval system. In Proceedings of SIGIR, 1999.\\n46. JeffMitchell andMirella Lapata.Vector-basedmodelsofsemantic composition.InProceedings\\nof ACL, 2008.\\n47. Jeff Mitchell and Mirella Lapata. Language models based on semantic composition. In Pro-\\nceedings of EMNLP, 2009.\\n48. Matthew L Newman, Carla J Groom, Lori D Handelman, and James W Pennebaker. Gen-\\nder differences in language use: An analysis of 14,000 text samples. Discourse Processes,\\n45(3):211\\xe2\\x80\\x93236, 2008.\\n49. Yilin Niu, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Improved word representation learn-\\ning with sememes. In Proceedings of ACL, 2017.\\n50. Francis Jeffry Pelletier. The principle of semantic compositionality. Topoi, 13(1):11\\xe2\\x80\\x9324, 1994.\\n51. Francis Jeffry Pelletier. Semantic Compositionality, volume 1. 2016.\\n52. James W Pennebaker, Roger J Booth, and Martha E Francis. Linguistic inquiry and word count:\\nLiwc [computer software]. Austin, TX: liwc. net, 2007.\\n53. Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\\nrepresentation. In Proceedings of EMNLP, 2014.\\n54. Jay M Ponte and W Bruce Croft. A language modeling approach to information retrieval. In\\nProceedings of SIGIR, 1998.\\n55. Fanchao Qi, Junjie Huang, Chenghao Yang, Zhiyuan Liu, Xiao Chen, Qun Liu, and Maosong\\nSun. Modeling semantic compositionality with sememe knowledge. In Proceedings of ACL,\\n2019.\\n56. Fanchao Qi, Yankai Lin, Maosong Sun, Hao Zhu, Ruobing Xie, and Zhiyuan Liu. Cross-lingual\\nlexical sememe prediction. In Proceedings of EMNLP, 2018.\\nReferences\\n161\\n57. Alexander M Rush, Sumit Chopra, and Jason Weston. A neural attention model for abstractive\\nsentence summarization. In Proceedings of EMNLP, 2015.\\n58. Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. Item-based collaborative\\n\\xef\\xac\\x81ltering recommendation algorithms. In Proceedings of WWW, 2001.\\n59. Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. Parsing with com-\\npositional vector grammars. In Proceedings of ACL, 2013.\\n60. Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic compo-\\nsitionality through recursive matrix-vector spaces. In Proceedings of EMNLP-CoNLL, 2012.\\n61. Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D. Manning, Andrew Y\\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over a senti-\\nment treebank. In Proceedings of EMNLP, 2013.\\n62. Jingguang Sun, Dongfeng Cai, Dexin Lv, and Yanju Dong. Hownet based chinese question\\nautomatic classi\\xef\\xac\\x81cation. Journal of Chinese Information Processing, 21(1):90\\xe2\\x80\\x9395, 2007.\\n63. Maosong Sun and Xinxiong Chen. Embedding for words and word senses based on human\\nannotatedknowledgebase:Acasestudyonhownet.JournalofChineseInformationProcessing,\\n30:1\\xe2\\x80\\x936, 2016.\\n64. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\\nnetworks. In Proceedings of NeurIPS, 2014.\\n65. DavidWeir,JulieWeeds,JeremyRef\\xef\\xac\\x81n,andThomasKober.Aligningpackeddependencytrees:\\na theory of composition for distributional semantics. Computational Linguistics, 42(4):727\\xe2\\x80\\x93\\n761, December 2016.\\n66. Ruobing Xie, Xingchi Yuan, Zhiyuan Liu, and Maosong Sun. Lexical sememe prediction via\\nword embeddings and matrix factorization. In Proceedings of IJCAI, 2017.\\n67. Binyong Yin. Quantitative research on Chinese morphemes. Studies of the Chinese Language,\\n5:338\\xe2\\x80\\x93347, 1984.\\n68. Xiangkai Zeng, Cheng Yang, Cunchao Tu, Zhiyuan Liu, and Maosong Sun. Chinese liwc\\nlexicon expansion via hierarchical classi\\xef\\xac\\x81cation of word embeddings with sememe attention.\\nIn Proceedings of AAAI, 2018.\\n69. Meng Zhang, Haoruo Peng, Yang Liu, Huan-Bo Luan, and Maosong Sun. Bilingual lexicon\\ninduction from non-parallel data with minimal supervision. In Proceedings of AAAI, 2017.\\n70. Yuntao Zhang, Ling Gong, and Yongcheng Wang. Chinese word sense disambiguation using\\nhownet. In Proceedings of ICNC,\\n71. Yu Zhao, Zhiyuan Liu, and Maosong Sun. Phrase type sensitive tensor indexing model for\\nsemantic composition. In Proceedings of AAAI, 2015.\\n72. Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. DAG-Structured long short-term memory\\nfor semantic compositionality. In Proceedings of NAACL-HLT, 2016.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 7\\nWorld Knowledge Representation\\nAbstract World knowledge representation aims to represent entities and relations\\nin the knowledge graph in low-dimensional semantic space, which have been widely\\nused in large knowledge-driven tasks. In this chapter, we \\xef\\xac\\x81rst introduce the concept\\nof the knowledge graph. Next, we introduce the motivations and give an overview\\nof the existing approaches for knowledge graph representation. Further, we discuss\\nseveral advanced approaches that aim to deal with the current challenges of knowl-\\nedge graph representation. We also review the real-world applications of knowledge\\ngraph representation, such as language modeling, question answering, information\\nretrieval, and recommender systems.\\n7.1\\nIntroduction\\nKnowledge Graph (KG), which is also named as Knowledge Base (KB), is a signif-\\nicant multi-relational dataset for modeling concrete entities and abstract concepts in\\nthe real world. It provides useful structured information and plays a crucial role in\\nlots of real-world applications such as web search and question answering. It is not\\nexaggerated to say that knowledge graphs teach us how to model the entities as well\\nas the relationships among them in this complicated real world.\\nTo encode knowledge into a real-world application, knowledge graph represen-\\ntation, which represents entities and relations in knowledge graphs with distributed\\nrepresentations, has been proposed and applied to various real-world arti\\xef\\xac\\x81cial intel-\\nligence \\xef\\xac\\x81elds including question answering, information retrieval, and dialogue sys-\\ntem. That is, knowledge graph representation learning plays a vital role as a bridge\\nbetween knowledge graphs and knowledge-driven tasks.\\nIn this section, we will introduce the concept of knowledge graph, several typical\\nknowledge graphs, knowledge graph representation learning, and several typical\\nknowledge-driven tasks.\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_7\\n163\\n164\\n7\\nWorld Knowledge Representation\\n7.1.1\\nWorld Knowledge Graphs\\nIn ancient times, knowledge was stored and inherited through books and letters\\nwritten on parchment or bamboo slip. With the Internet thriving in the twenty-\\xef\\xac\\x81rst\\ncentury, millions of thousands of messages have \\xef\\xac\\x82ooded into the World Wide Web,\\nand knowledge was transferred to the semi-structured textual information on the\\nweb. However, due to the information explosion, it is not easy to extract knowledge\\nwe want from the signi\\xef\\xac\\x81cant, noisy plain text on the Internet. To obtain knowledge\\neffectively, people notice that the world is not only made of strings but also made of\\nentities and relations. Knowledge Graph, which arranges structured multi-relational\\ndata of concrete entities and abstract concepts in the real world, is blooming in recent\\nyears and attracts wide attention in both academia and industry.\\nKGs are usually constructed from existing Semantic Web datasets in Resource\\nDescription Framework (RDF) with the help of manual annotation, while it can\\nalso be automatically enriched by extracting knowledge from large plain texts on\\nthe Internet. A typical KG usually contains two elements, including entities (i.e.,\\nconcrete entities and abstract concepts in the real world) and relations between\\nentities. It usually represents knowledge with large quantities of triple facts in the\\ntriple form of \\xe2\\x9f\\xa8head entity, relation, tail entity\\xe2\\x9f\\xa9abridged as \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9. For example,\\nWilliam Shakespeare is a famous English poet and playwright, who is widely\\nregarded as the greatest writer in the English language, and Romeo and Juliet\\nis one of his masterpieces. In knowledge graph, we will represent this knowledge as\\n\\xe2\\x9f\\xa8William Shakespeare, works_written, Romeo and Juliet\\xe2\\x9f\\xa9. Note\\nthat in the real world, the same head entity and relation may have multiple tail\\nentities (e.g., William Shakespeare also wrote Hamlet and A Midsummer\\nNight\\xe2\\x80\\x99s Dream), and reversely the same situation will happen when tail entity\\nand relation are \\xef\\xac\\x81xed. Even it is possible when both the head entity and tail entity are\\nmultiple (e.g., in relations like actor_in_movie). However, in KG, all knowl-\\nedge can be represented in triple facts regardless of the types of entities and relations.\\nThrough these triples, we can generate a huge directed graph whose nodes corre-\\nspond to entities and edges correspond to relations to model the real world. With the\\nwell-structured united knowledge representation, KGs are widely used in a variety\\nof applications to enhance their system performance.\\nThere are several KGs widely utilized nowadays in applications of information\\nretrieval and question answering. In this subsection, we will introduce some famous\\nKGs such as Freebase, DBpedia, Yago, and WordNet. In fact, there are also lots\\nof comparatively smaller KGs in speci\\xef\\xac\\x81c \\xef\\xac\\x81elds of knowledge functioned in vertical\\nsearch.\\n7.1.1.1\\nFreebase\\nFreebase is one of the most popular knowledge graphs in the world. It is a large\\ncommunity-curated database consisting of well-known people, places, and things,\\n7.1 Introduction\\n165\\nFig. 7.1 An example of search results in Freebase\\nwhich is composed of existing databases and its community members. Freebase\\nwas \\xef\\xac\\x81rst developed by Metaweb, an American software company, and ran since\\nMarch 2007. In July 2010, Metaweb was acquired by Google, and Freebase was\\ncombined to power up Google\\xe2\\x80\\x99s Knowledge Graph. In December 2014, the Freebase\\nteam of\\xef\\xac\\x81cially announced that the website, as well as the API of Freebase, would\\nbe shut down by June 30, 2015. While the data in Freebase would be transferred\\nto Wikidata, which is another collaboratively edited knowledge base operated by\\nWikimedia Foundation. Up to March 24, 2016, Freebase arranged 58,726,427 topics\\nand 3,197,653,841 facts.\\nFreebase contains well-structured data representing relationships between entities\\naswellastheattributesofentitiesintheformoftriplefacts(Fig.7.1).DatainFreebase\\nwas mainly harvested from various sources, including Wikipedia, Fashion Model\\nDirectory, NNDB, MusicBrainz, and so on. Moreover, the community members also\\ncontributed a lot to Freebase. Freebase is an open and shared database that aims to\\nconstruct a global database which encodes the world\\xe2\\x80\\x99s knowledge. It announced an\\nopen API, RDF endpoint, and a database dump for its users for both commercial and\\nnoncommercial use. As described by Tim O\\xe2\\x80\\x99Reilly, Freebase is the bridge between\\nthe bottom-up vision of Web 2.0 collective intelligence and the more structured world\\nof the Semantic Web.\\n7.1.1.2\\nDBpedia\\nDBpedia is a crowd-sourced community effort aiming to extract structured content\\nfrom Wikipedia and make this information accessible on the web. It was started by\\nresearchers at Free University of Berlin, Leipzig University and OpenLink Software,\\n166\\n7\\nWorld Knowledge Representation\\ninitially released to the public in January 2007. DBpedia allows users to ask semantic\\nqueries associated with Wikipedia resources, even including links to other related\\ndatasets, which makes it easier for us to fully utilize the massive amount of informa-\\ntion in Wikipedia in a novel and effective way. DBpedia is also an essential part of\\nthe Linked Data effort described by Tim Berners-Lee.\\nThe English version of DBpedia describes 4.58 million entities, out of which\\n4.22 million are classi\\xef\\xac\\x81ed in a consistent ontology, including 1,445,000 persons,\\n735,000 places, 411,000 creative works, 251,000 species, 241,000 organizations, and\\n6,000 diseases. There are also localized versions of DBpedia in 125 languages, all of\\nwhich contain 38.3 million entities. Besides, DBpedia also contains a great number of\\ninternal and external links, including 80.9 million links to Wikipedia categories, 41.2\\nmillion links to YAGO categories, 25.2 million links to images, and 29.8 million links\\nto external web pages. Moreover, DBpedia maintains a hierarchical, cross-domain\\nontology covering overall 685 classes, which has been manually created based on\\nthe commonly used infoboxes in Wikipedia.\\nDBpedia has several advantages over other KGs. First, DBpedia has a close con-\\nnection to Wikipedia and can automatically evolve as Wikipedia changes. It makes\\nthe update process of DBpedia more ef\\xef\\xac\\x81cient. Second, DBpedia is multilingual that\\nis convenient for users over the world with their native languages.\\n7.1.1.3\\nYAGO\\nYAGO, which is short for Yet Another Great Ontology, is a high-quality KG devel-\\nopedbyMaxPlanckInstituteforComputerScienceinSaarbru\\xc3\\xbcckeninitiallyreleased\\nin 2008. Knowledge in YAGO is automatically extracted from Wikipedia, WordNet,\\nand GeoNames, whose accuracy has been manually evaluated and proves a con-\\n\\xef\\xac\\x81rmed accuracy of 95%. YAGO is special not only because of the con\\xef\\xac\\x81dence value\\nevery fact possesses depending on the manual evaluation but also because that YAGO\\nis anchored in space and time, which can provide a spatial dimension or temporal\\ndimension to part of its entities.\\nCurrently, YAGO has more than 10 million entities, including persons, organi-\\nzations, and locations, with over 120 million facts about these entities. YAGO also\\ncombines knowledge extracted from Wikipedias of 10 different languages and clas-\\nsi\\xef\\xac\\x81es them into approximately 350,000 classes according to the Wikipedia category\\nsystem and the taxonomy of WordNet. YAGO has also joined the linked data project\\nand been linked to the DBpedia ontology and the SUMO ontology (Fig.7.2).\\n7.2\\nKnowledge Graph Representation\\nKnowledge Graphs provide us with a novel aspect to describe the world with entities\\nand triple facts, which attract growing attention from researchers. Large KGs such\\nas Freebase, DBpedia, and YAGO have been constructed and widely used in an\\nenormous amount of applications such as question answering and Web search.\\n7.2 Knowledge Graph Representation\\n167\\nFig. 7.2 An example of search results in YAGO\\nHowever, with KG size increasing, we are facing two main challenges: data spar-\\nsity and computational inef\\xef\\xac\\x81ciency. Data sparsity is a general problem in lots of\\n\\xef\\xac\\x81elds like social network analysis or interest mining. It is because that there are too\\nmany nodes (e.g., users, products, or entities) in a large graph, while too few edges\\n(e.g., relationships) between these nodes, since the number of relations of a node is\\nlimited in the real world. Computational ef\\xef\\xac\\x81ciency is another challenge we need to\\novercome with the increasing size of knowledge graphs.\\nTo tackle these problems, representation learning is introduced to knowledge rep-\\nresentation. Representation learning in KGs aims to project both entities and relations\\ninto a low-dimensional continuous vector space to get their distributed representa-\\ntions, whose performance has been con\\xef\\xac\\x81rmed in word representation and social rep-\\nresentation. Compared with the traditional one-hot representation, distributed repre-\\nsentation has much fewer dimensions, and thus lowers the computational complexity.\\nWhat is more, distributed representation can explicitly show the similarity between\\nentities through some distance calculated by the low-dimensional embeddings, while\\nall embeddings in one-hot representation are orthogonal, making it dif\\xef\\xac\\x81cult to tell\\nthe potential relations between entities.\\nWith the advantages above, knowledge graph representation learning is blooming\\nin knowledge applications, signi\\xef\\xac\\x81cantly improving the ability of KGs on the task\\nof knowledge completion, knowledge fusion, and reasoning. It is considered as the\\nbridge between knowledge construction, knowledge graphs, and knowledge-driven\\napplications. Up till now, a high number of methods have been proposed using a\\ndistributed representation for modeling knowledge graphs, with the learned knowl-\\nedge representations widely utilized in various knowledge-driven tasks like question\\nanswering, information retrieval, and dialogue system.\\n168\\n7\\nWorld Knowledge Representation\\nIn summary, Knowledge graph Representation Learning (KRL) aims to construct\\ndistributed knowledge representations for entities and relations, projecting knowl-\\nedge into low-dimensional semantic vector spaces. Recent years have witnessed sig-\\nni\\xef\\xac\\x81cant advances in knowledge graph representation learning with a large amount of\\nKRL methods proposed to construct knowledge representations, among which the\\ntranslation-based methods achieve state-of-the-art performance in many KG tasks,\\nwith a right balance in both effectiveness and ef\\xef\\xac\\x81ciency.\\nIn this section, we will \\xef\\xac\\x81rst describe the notations that we will use in KRL. Then,\\nwe will introduce TransE, which is the fundamental version of translation-based\\nmethods. Next, we will explore the various extension methods of TransE in detail.\\nAt last, we will take a brief look over other representation learning methods utilized\\nin modeling knowledge graphs.\\n7.2.1\\nNotations\\nFirst, we introduce the general notations used in the rest of this section. We use\\nG = (E, R, T ) to denote the whole KG, in which E = {e1, e2, . . . , e|E|} stands for\\nthe entity set, R = {r1,r2, . . . ,r|R|} stands for the relation set, and T stands for the\\ntriple set. |E| and |R| are the corresponding entity and relation numbers in their\\noverall sets. As stated above, we represent knowledge in the form of triple fact\\n\\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, where h \\xe2\\x88\\x88E means the head entity, t \\xe2\\x88\\x88E means the tail entity, and r \\xe2\\x88\\x88R\\nmeans the relation between h and t.\\n7.2.2\\nTransE\\nTransE [7] is a translation-based model for learning low-dimensional embeddings of\\nentities and relations. It projects entities as well as relations into the same semantic\\nembeddingspace,andthenconsidersrelationsastranslationsintheembeddingspace.\\nFirst, we will start with the motivations of this method, and then discuss the details\\nin how knowledge representations are trained under TransE. Finally, we will explore\\nthe advantages and disadvantages of TransE for a deeper understanding.\\n7.2.2.1\\nMotivation\\nThere are three main motivations behind the translation-based knowledge graph\\nrepresentation learning method. The primary motivation is that it is natural to con-\\nsider relationships between entities as translating operations. Through distributed\\nrepresentations, entities are projected to a low-dimensional vector space. Intuitively,\\nwe agree that a reasonable projection should map entities with similar semantic\\nmeanings to the same \\xef\\xac\\x81eld, while entities with different meanings should belong to\\n7.2 Knowledge Graph Representation\\n169\\ndistinct clusters in the vector space. For example, William Shakespeare and\\nJane Austen may be in the same cluster of writers, Romeo, and Juliet and\\nPride and Prejudice may be in another cluster of books. In this case, they\\nshare the same relation works_written, and the translations between writers and\\nbooks in the vector space are similar.\\nThe secondary motivation of TransE derives from the breakthrough in word repre-\\nsentation by Word2vec [49]. Word2vec proposes two simple models, Skip-gram and\\nCBOW, to learn word embeddings from large-scale corpora, signi\\xef\\xac\\x81cantly improv-\\ning the performance in word similarity and analogy. The word embeddings learned\\nby Word2vec have some interesting phenomena: if two word-pairs share the same\\nsemantic or syntactic relationships, their subtraction embeddings in each word pair\\nwill be similar. For instance, we have\\nw(king) \\xe2\\x88\\x92w(man) \\xe2\\x89\\x88w(queen) \\xe2\\x88\\x92w(woman),\\n(7.1)\\nwhich indicates that the latent semantic relation between king and man, which is\\nsimilar to the relation between queen and woman, is successfully embedded in\\nthe word representation. This approximate relation could be found not only with the\\nsemantic relations but also with the syntactic relations. We have\\nw(bigger) \\xe2\\x88\\x92w(big) \\xe2\\x89\\x88w(smaller) \\xe2\\x88\\x92w(small).\\n(7.2)\\nThe phenomenon found in word representation strongly implies that there may\\nexist an explicit method to represent relationships between entities as translating\\noperations in vector space.\\nThe last motivation comes from the consideration of computational complexity.\\nOn the one hand, a substantial increase in model complexity will result in high\\ncomputational costs and obscure model interpretability. Moreover, a complex model\\nmay lead to over\\xef\\xac\\x81tting. On the other hand, experimental results on model complexity\\ndemonstrate that the simpler models perform almost as good as more expressive mod-\\nels in most KG applications, in the condition that there are sizeable multi-relational\\ndataset and a relatively large amount of relations. As KG size increases, computa-\\ntional complexity becomes the primary challenge in the knowledge graph represen-\\ntation. The intuitive assumption of translation leads to a better trade-off between\\naccuracy and ef\\xef\\xac\\x81ciency.\\n7.2.2.2\\nMethodology\\nAs illustrated in Fig.7.3, TransE projects entities and relations into the same low-\\ndimensional space. All embeddings take values in Rd, where d is a hyperparameter\\nindicating the dimension of embeddings. With the translation assumption, for each\\ntriple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9in T , we want the summation embedding h + r to be the nearest\\nneighbor of tail embedding t. The score function of TransE is then de\\xef\\xac\\x81ned as follows:\\n170\\n7\\nWorld Knowledge Representation\\nFig. 7.3 The architecture of\\nTransE model [47]\\nh\\nt\\nr\\nE (h,r, t) = \\xe2\\x88\\xa5h + r \\xe2\\x88\\x92t\\xe2\\x88\\xa5.\\n(7.3)\\nMore speci\\xef\\xac\\x81cally, to learn such embeddings of entities and relations, TransE\\nformalizes a margin-based loss function with negative sampling as objective for\\ntraining. The pair-wise function is de\\xef\\xac\\x81ned as follows:\\nL =\\n\\x02\\n\\xe2\\x9f\\xa8h,r,t\\xe2\\x9f\\xa9\\xe2\\x88\\x88T\\n\\x02\\n\\xe2\\x9f\\xa8h\\xe2\\x80\\xb2,r\\xe2\\x80\\xb2,t\\xe2\\x80\\xb2\\xe2\\x9f\\xa9\\xe2\\x88\\x88T \\xe2\\x88\\x92\\nmax(\\xce\\xb3 + E (h,r, t)) \\xe2\\x88\\x92E (h\\xe2\\x80\\xb2,r\\xe2\\x80\\xb2, t\\xe2\\x80\\xb2), 0),\\n(7.4)\\nin which E (h,r, t) is the score of energy function for a positive triple (i.e., triple in\\nT ) and E (h\\xe2\\x80\\xb2,r\\xe2\\x80\\xb2, t\\xe2\\x80\\xb2) is that of a negative triple. The energy function E can be either\\nmeasured by L1 or L2 distance. \\xce\\xb3 > 0 is a hyperparameter of margin and a bigger\\n\\xce\\xb3 means a wider gap between positive and the corresponding negative scores. T \\xe2\\x88\\x92is\\nthe negative triple set with respect to T .\\nSince there are no explicit negative triples in knowledge graphs, we de\\xef\\xac\\x81ne T \\xe2\\x88\\x92as\\nfollows:\\nT \\xe2\\x88\\x92= {\\xe2\\x9f\\xa8h\\xe2\\x80\\xb2,r, t\\xe2\\x9f\\xa9|h\\xe2\\x80\\xb2 \\xe2\\x88\\x88E} \\xe2\\x88\\xaa{\\xe2\\x9f\\xa8h,r\\xe2\\x80\\xb2, t\\xe2\\x9f\\xa9|r\\xe2\\x80\\xb2 \\xe2\\x88\\x88R} \\xe2\\x88\\xaa{\\xe2\\x9f\\xa8h,r, t\\xe2\\x80\\xb2\\xe2\\x9f\\xa9|t\\xe2\\x80\\xb2 \\xe2\\x88\\x88E},\\n\\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9\\xe2\\x88\\x88T,\\n(7.5)\\nwhich means the negative triple set T \\xe2\\x88\\x92is composed of the positive triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9\\nwith head entity, relation, or tail entity randomly replaced by any other entities or\\nrelations in KG. Note that the new triple generated after replacement will not be\\nconsidered as a negative sample if it has already been in T .\\nTransE is optimized using mini-batch stochastic gradient descent (SGD), with\\nentities and relations randomly initialized. Knowledge completion, which is a link\\nprediction task aiming to predict the third element in a triple (could be either entity\\nor relation) with the given rest two elements, is designed to evaluate the learned\\nknowledge representations.\\n7.2 Knowledge Graph Representation\\n171\\n7.2.2.3\\nDisadvantages and Challenges\\nTransE is effective and ef\\xef\\xac\\x81cient and has shown its power on link prediction. However,\\nit still has several disadvantages and challenges to be further explored.\\nFirst, in knowledge completion, we may have multiple correct answers with the\\ngiven two elements in a triple. For instance, with the given head entity William\\nShakespeare and the relation works_written, we will get a list of master-\\npiecesincludingRomeo and Juliet,Hamletand A Midsummer Night\\xe2\\x80\\x99s\\nDream. These books share the same information in the writer while differing in many\\nother \\xef\\xac\\x81elds such as theme, background, and famous roles in the book. However,\\nwith the translation assumption in TransE, every entity has only one embedding\\nin all triples, which signi\\xef\\xac\\x81cantly limits the ability of TransE in knowledge graph\\nrepresentations. In [7], the authors categorize all relations into four classes, 1-to-1,\\n1-to-Many, Many-to-1, Many-to-Many, according to the cardinalities of their head\\nand tail arguments. A relation is considered as 1-to-1 if most heads appear with\\none tail, 1-to-Many if a head can appear with many tails, Many-to-1 if a tail can\\nappear with many heads, and Many-to-Many if multiple heads appear with multiple\\ntails. Statistics demonstrate that the 1-to-Many, Many-to-1, Many-to-Many relations\\noccupy a large proportion. TransE does well in 1-to-1, but it has issues when dealing\\nwith 1-to-Many, Many-to-1, Many-to-Many relations. Similarly, TransE may also\\nstruggle with re\\xef\\xac\\x82exive relations.\\nSecond, the translating operation is intuitive and effective, only considering the\\nsimple one-step translation, which may limit the ability to model KGs. Taking enti-\\nties as nodes and relations as edges, we can construct a huge knowledge graph\\nwith the triple facts. However, TransE focuses on minimizing the energy function\\nE (h,r, t) = \\xe2\\x88\\xa5h + r \\xe2\\x88\\x92t\\xe2\\x88\\xa5, which only utilize the one-step relation information in\\nknowledge graphs, regardless of the latent relationships located in long-distance\\npaths. For example, if we know the triple fact that \\xe2\\x9f\\xa8The forbidden city,\\nlocate_in, Beijing\\xe2\\x9f\\xa9and \\xe2\\x9f\\xa8Beijing, capital_of, China\\xe2\\x9f\\xa9, we can infer\\nthat The forbidden city locates in China. TransE can be further enhanced\\nwith the favor of multistep information.\\nThird, the representation and the dissimilarity function in TransE are oversimpli-\\n\\xef\\xac\\x81ed for the consideration of ef\\xef\\xac\\x81ciency. Therefore, TransE may not be capable enough\\nof modeling those complicated entities and relations in knowledge graphs. There still\\nexist challenges on how to balance the effectiveness and ef\\xef\\xac\\x81ciency, avoiding both\\nover\\xef\\xac\\x81tting and under\\xef\\xac\\x81tting.\\nBesides the disadvantages and challenges stated above, multisource information\\nsuch as textual information and hierarchical type/label information is of great sig-\\nni\\xef\\xac\\x81cance, which will be further discussed in the following.\\n172\\n7\\nWorld Knowledge Representation\\n7.2.3\\nExtensions of TransE\\nThere are lots of extension methods following TransE to address the challenges\\nabove. Speci\\xef\\xac\\x81cally, TransH, TransR, TransD, and TranSparse are proposed to solve\\nthe challenges in modeling 1-to-Many, Many-to-1, and Many-to-Many relations,\\nPTransE is proposed to encode long-distance information located in multistep paths,\\nand CTransR, TransA, TransG, and KG2E further extend the oversimpli\\xef\\xac\\x81ed model\\nof TransE. We will discuss these extension methods in detail.\\n7.2.3.1\\nTransH\\nWith distributed representation, entities are projected to the semantic vector space,\\nand similar entities tend to be in the same cluster. However, it seems that William\\nShakespeare should be in the neighborhood of Isaac Newton when talking\\nabout nationality, while it should be next to Mark Twain when talking about occu-\\npation. To accomplish this, we want entities to show different preferences in different\\nsituations, that is, to have multiple representations in different triples.\\nTo address the issue when modeling 1-to-Many, Many-to-1, Many-to-Many, and\\nre\\xef\\xac\\x82exive relations, TransH [77] enables an entity to have multiple representations\\nwhen involved in different relations. As illustrated in Fig.7.4, TransH proposes a\\nrelation-speci\\xef\\xac\\x81c hyperplane wr for each relation, and judge dissimilarities on the\\nhyperplane instead of the original vector space of entities. Given a triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9,\\nTransH \\xef\\xac\\x81rst projects h and t to the corresponding hyperplane wr to get the projection\\nh\\xe2\\x8a\\xa5and t\\xe2\\x8a\\xa5, and the translation vector r is used to connect h\\xe2\\x8a\\xa5and t\\xe2\\x8a\\xa5on the hyperplane.\\nThe score function is de\\xef\\xac\\x81ned as follows:\\nE (h,r, t) = \\xe2\\x88\\xa5h\\xe2\\x8a\\xa5+ r \\xe2\\x88\\x92t\\xe2\\x8a\\xa5\\xe2\\x88\\xa5,\\n(7.6)\\nin which we have\\nh\\xe2\\x8a\\xa5= h \\xe2\\x88\\x92w\\xe2\\x8a\\xa4\\nr hwr,\\nt\\xe2\\x8a\\xa5= t \\xe2\\x88\\x92w\\xe2\\x8a\\xa4\\nr twr,\\n(7.7)\\nwhere wr is a vector and \\xe2\\x88\\xa5wr\\xe2\\x88\\xa52 is restricted to 1. As for training, TransH also\\nminimizes the margin-based loss function with negative sampling which is similar\\nto TransE, and use mini-batch SGD to learn representations.\\n7.2.3.2\\nTransR/CTransR\\nTransH enables entities to have multiple representations in different relations with\\nthe favor of hyperplanes, while entities and relations are still restricted in the same\\nsemantic vector space, which may limit the ability for modeling entities and relations.\\nTransR [39] assumes that entities and relations should be arranged in distinct spaces,\\nthat is, entity space for all entities and relation space for each relation.\\n7.2 Knowledge Graph Representation\\n173\\nhr\\ntr\\nr\\nh\\nt\\nFig. 7.4 The architecture of TransH model [47]\\nAs illustrated in Fig.7.5, For a triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, h, t \\xe2\\x88\\x88Rk and r \\xe2\\x88\\x88Rd, TransR \\xef\\xac\\x81rst\\nprojects h and t from entity space to the corresponding relation space of r. That is\\nto say, every entity has a relation-speci\\xef\\xac\\x81c representation for each relation, and the\\ntranslating operation is processed in the speci\\xef\\xac\\x81c relation space. The energy function\\nof TransR is de\\xef\\xac\\x81ned as follows:\\nE (h,r, t) = \\xe2\\x88\\xa5hr + r \\xe2\\x88\\x92tr\\xe2\\x88\\xa5,\\n(7.8)\\nwhere hr and tr stand for the relation-speci\\xef\\xac\\x81c representation for h and tr in the\\ncorresponding relation space of r. The projection from entity space to relation space\\nis\\nhr = hMr,\\ntr = tMr,\\n(7.9)\\nwhere Mr \\xe2\\x88\\x88Rk\\xc3\\x97d is a projection matrix mapping entities from the entity space to\\nthe relation space of r. TransR also constrains the norms of the embeddings and has\\n\\xe2\\x88\\xa5h\\xe2\\x88\\xa52 \\xe2\\x89\\xa41, \\xe2\\x88\\xa5t\\xe2\\x88\\xa52 \\xe2\\x89\\xa41, \\xe2\\x88\\xa5r\\xe2\\x88\\xa52 \\xe2\\x89\\xa41, \\xe2\\x88\\xa5hr\\xe2\\x88\\xa52 \\xe2\\x89\\xa41, \\xe2\\x88\\xa5tr\\xe2\\x88\\xa52 \\xe2\\x89\\xa41. As for training, TransR shares\\nthe same margin-based score function as TransE.\\nFurthermore, the author found that some relations in knowledge graphs could\\nbe divided into a few sub-relations that give more precise information. The dif-\\nferences between those sub-relations can be learned from corresponding entity\\npairs. For instance, the relation location_contains has head-tail patterns like\\ncity-street, country-city, and even country-university, showing\\ndifferent attributes in cognition. With the sub-relations being considered, entities\\nmay be projected to more precise positions in the semantic vector space.\\nCluster-based TransR (CTransR), which is an enhanced version of TransR with the\\nsub-relations into consideration, is then proposed. More speci\\xef\\xac\\x81cally, for each relation\\nr, all entity pairs (h, t) are \\xef\\xac\\x81rst clustered into several groups. The clustering of entity\\npairs depends on the subtraction result of t \\xe2\\x88\\x92h, in which h and t are pretrained by\\nTransE. Next, we learn a distinct sub-relation vector rc for each cluster according to\\nthe corresponding entity pairs, and the original energy function is modi\\xef\\xac\\x81ed as\\n174\\n7\\nWorld Knowledge Representation\\nt\\nh\\ntr\\nhr\\nr\\nMr\\nMr\\nentity space\\nrelation space of r\\nFig. 7.5 The architecture of TransR model [47]\\nE (h,r, t) = \\xe2\\x88\\xa5hr + rc \\xe2\\x88\\x92tr\\xe2\\x88\\xa5+ \\xce\\xb1\\xe2\\x88\\xa5rc \\xe2\\x88\\x92r\\xe2\\x88\\xa5,\\n(7.10)\\nwhere \\xe2\\x88\\xa5rc \\xe2\\x88\\x92r\\xe2\\x88\\xa5wants the sub-relation vector rc not to be too distinct from the uni\\xef\\xac\\x81ed\\nrelation vector r.\\n7.2.3.3\\nTransD\\nTransH and TransR focus on the multiple representations of entities in different\\nrelations, improving the performance on knowledge completion and triple classi-\\n\\xef\\xac\\x81cation. However, both models only project entities according to the relations in\\ntriples, ignoring the diversity of entities. Moreover, the projection operation with\\nmatrix-vector multiplication leads to a higher computational complexity compared\\nto TransE, which is time consuming when applied on large-scale graphs. To address\\nthis problem, TransD [32] proposes a novel projection method with a dynamic map-\\nping matrix depending on both entity and relation, which takes the diversity of entities\\nas well as relations into consideration.\\nTransD de\\xef\\xac\\x81nes two vectors for each entity and relation, i.e., the original vector that\\nis also used in TransE, TransH, and TransR for distributed representation of entities\\nandrelations,andtheprojectionvectorthatisusedinconstructingprojectionmatrices\\nfor mapping entities from entity space to relation space. As illustrated in Fig.7.6,\\nTransD uses h, t, r to represent the original vectors, while hp, tp, and rp are used to\\nrepresent the projection vectors. There are two projection matrices Mrh, Mrt \\xe2\\x88\\x88Rm\\xc3\\x97n\\nused to project from entity space to relation space, and the projection matrices are\\ndynamically constructed as follows:\\nMrh = rph\\xe2\\x8a\\xa4\\np + Im\\xc3\\x97n,\\nMrt = rpt\\xe2\\x8a\\xa4\\np + Im\\xc3\\x97n,\\n(7.11)\\n7.2 Knowledge Graph Representation\\n175\\nt2\\nh1\\nh1r\\nMrhi\\nMrti\\nentity space\\nrelation space of r\\nt3\\nt1\\nh2\\nh3\\nt1r\\nh2r\\nt2r\\nt3r\\nh3r\\nFig. 7.6 The architecture of TransD model [47]\\nwhich means the projection vectors of entity and relation are combined to determine\\nthe dynamic projection matrix. The score function is then de\\xef\\xac\\x81ned as\\nE (h,r, t) = \\xe2\\x88\\xa5Mrhh + r \\xe2\\x88\\x92Mrtt\\xe2\\x88\\xa5.\\n(7.12)\\nThe projection matrices are initialized with identity matrices, and there are also\\nsome normalization constraints as in TransR.\\nTransD proposes a dynamic method to construct projection matrices with the\\nconsideration of diversity in both entities and relations, achieving better performance\\ncompared to existing methods in link prediction and triple classi\\xef\\xac\\x81cation. Moreover,\\nit lowers both computational and spatial complexity compared to TransR.\\n7.2.3.4\\nTranSparse\\nThe extension methods of TransE stated above focus on the multiple representa-\\ntions for entities in different relations and entity pairs. However, there are still two\\nchallenges ignored: (1) The heterogeneity. Relations in knowledge graphs differ in\\ngranularity. Some complex relations may link to many entity pairs, while some rela-\\ntively simple relations not. (2) The unbalance. Some relations may have more links\\nto head entities and fewer links to tail entities, and vice versa. The performance will\\nbe further improved if we consider these rather than merely treat all relations equally.\\nExisting methods like TransR build projection matrices for each relation, while\\nthese projection matrices have the same number of parameters, regardless of the vari-\\nety in the complexity of relations. TranSparse [33] is then proposed to address the\\nissues. The underlying assumption of TranSparse is that complex relations should\\nhave more parameters to learn while simple relations have fewer, where the complex-\\nity of a relation is judged from the number of triples or entities linked by the relation.\\n176\\n7\\nWorld Knowledge Representation\\nTo accomplish this, two models, i.e., TranSparse(share) and TranSparse(separate),\\nare proposed for avoiding over\\xef\\xac\\x81tting and under\\xef\\xac\\x81tting.\\nInspired by TransR, TranSparse(share) builds a projection matrix Mr(\\xce\\xb8r) for each\\nrelationr.Thisprojectionmatrixissparse,andthesparsedegree\\xce\\xb8r mainlydependson\\nthe number of entity pairs linked tor. Suppose Nr is the number of linked entity pairs,\\nN \\xe2\\x88\\x97\\nr represents the maximum number of Nr, and \\xce\\xb8min denotes the minimum sparse\\ndegree of projection matrix Mr that 0 \\xe2\\x89\\xa4\\xce\\xb8min \\xe2\\x89\\xa41. The sparse degree of relation r is\\nde\\xef\\xac\\x81ned as follows:\\n\\xce\\xb8r = 1 \\xe2\\x88\\x92(1 \\xe2\\x88\\x92\\xce\\xb8min)Nr/N \\xe2\\x88\\x97\\nr .\\n(7.13)\\nBoth head and tail entities share the same sparse projection matrix Mr(\\xce\\xb8r) in\\ntranslation. The score function is\\nE (h,r, t) = \\xe2\\x88\\xa5Mr(\\xce\\xb8r)h + r \\xe2\\x88\\x92Mr(\\xce\\xb8r)t\\xe2\\x88\\xa5.\\n(7.14)\\nDiffering from TranSparse(share), TranSparse(separate) builds two different\\nsparse matrices Mrh(\\xce\\xb8rh) and Mrt(\\xce\\xb8rt) for head and tail entities. The sparse degree\\n\\xce\\xb8rh (or \\xce\\xb8rt) then depends on the number of head (or tail) entities linked by relation\\nr. We have Nrh (or Nrt) to represent the number of head (or tail) entities, as well as\\nN \\xe2\\x88\\x97\\nrh (or N \\xe2\\x88\\x97\\nrt) to represent the maximum number of Nrh (or Nrt). And \\xce\\xb8min will also\\nbe set as the minimum sparse degree of projection matrices that 0 \\xe2\\x89\\xa4\\xce\\xb8min \\xe2\\x89\\xa41. We\\nhave\\n\\xce\\xb8rh = 1 \\xe2\\x88\\x92(1 \\xe2\\x88\\x92\\xce\\xb8min)Nrh/N \\xe2\\x88\\x97\\nrh,\\n\\xce\\xb8rt = 1 \\xe2\\x88\\x92(1 \\xe2\\x88\\x92\\xce\\xb8min)Nrt/N \\xe2\\x88\\x97\\nrt.\\n(7.15)\\nThe score function of TranSparse(separate) is\\nE (h,r, t) = \\xe2\\x88\\xa5Mrh(\\xce\\xb8rh)h + r \\xe2\\x88\\x92Mrt(\\xce\\xb8rt)t\\xe2\\x88\\xa5.\\n(7.16)\\nThrough the sparse projection matrix, TranSparse solves the heterogeneity and\\nthe unbalance simultaneously.\\n7.2.3.5\\nPTransE\\nThe extension models of TransE stated above are mainly focused on the challenge of\\nmultiple representations of entities in different scenarios. However, those extension\\nmodelsonlyconsiderthesimpleone-steppaths(i.e.,relation)intranslatingoperation,\\nignoring the rich global information located in the whole knowledge graphs. Consid-\\nering the multistep relational path is a potential method to utilize the global informa-\\ntion. For instance, if we notice the multistep relational path that \\xe2\\x9f\\xa8The forbidden\\ncity, locate_in, Beijing\\xe2\\x9f\\xa9\\xe2\\x86\\x92\\xe2\\x9f\\xa8Beijing, capital_of, China\\xe2\\x9f\\xa9, we can\\ninference with con\\xef\\xac\\x81dence that the triple \\xe2\\x9f\\xa8The forbidden city, locate_in,\\nChina\\xe2\\x9f\\xa9may exist. The relational path provides us with a powerful way to con-\\n7.2 Knowledge Graph Representation\\n177\\nstruct better knowledge graph representations and even get a better understanding of\\nknowledge reasoning.\\nThere are two main challenges when encoding the information in multistep rela-\\ntional paths. First, how to select reliable and meaningful relational paths among\\nenormous path candidates in KGs, since there are lots of relation sequence patterns\\nwhich do not indicate reasonable relationships. Let us just consider the relational\\npath \\xe2\\x9f\\xa8The forbidden city, locate_in, Beijing\\xe2\\x9f\\xa9\\xe2\\x86\\x92\\xe2\\x9f\\xa8Beijing, held,\\n2008 Summer Olympics\\xe2\\x9f\\xa9, it is hard to describe the relationship between The\\nforbidden city and 2008 Summer Olympics. Second, how to model\\nthose meaningful relational paths once we get them since it is dif\\xef\\xac\\x81cult to solve\\nthis composition semantic problem in relational paths.\\nPTransE [38] is then proposed to model the multistep relational paths. To select\\nmeaningful relational paths, the authors propose a Path-Constraint Resource Alloca-\\ntion (PCRA) algorithm to judge the relation path reliability. Suppose there is infor-\\nmation (or resource) in head entity h which will \\xef\\xac\\x82ow to tail entity t through some\\ncertain relational paths. The basic assumption of PCRA is that: the reliability of path\\n\\xe2\\x84\\x93depends on the resource amount that \\xef\\xac\\x81nally \\xef\\xac\\x82ows from head to tail. Formally, we\\nset \\xe2\\x84\\x93= (r1, . . . ,rl) for a certain path between h and t. The resource travels from h\\nto t and the path could be represented as S0/h\\nr1\\xe2\\x88\\x92\\xe2\\x86\\x92S1\\nr2\\xe2\\x88\\x92\\xe2\\x86\\x92. . .\\nrl\\xe2\\x88\\x92\\xe2\\x86\\x92Sl/t. For an entity\\nm \\xe2\\x88\\x88Si, the resource amount of m is de\\xef\\xac\\x81ned as follows:\\nR\\xe2\\x84\\x93(m) =\\n\\x02\\nn\\xe2\\x88\\x88Si\\xe2\\x88\\x921(\\xc2\\xb7,m)\\n1\\n|Si(n, \\xc2\\xb7)| R\\xe2\\x84\\x93(n),\\n(7.17)\\nwhere Si\\xe2\\x88\\x921(\\xc2\\xb7, m) indicates all direct predecessors of entity m along with relation ri in\\nSi\\xe2\\x88\\x921, and Si(n, \\xc2\\xb7) indicates all direct successors of n \\xe2\\x88\\x88Si\\xe2\\x88\\x921 with relation r. Finally,\\nthe resource amount of tail R\\xe2\\x84\\x93(t) is used to measure the reliability of \\xe2\\x84\\x93in the given\\ntriple \\xe2\\x9f\\xa8h, \\xe2\\x84\\x93, t\\xe2\\x9f\\xa9.\\nOnce we have learned the reliability and select those meaningful relational path\\ncandidates, the next challenge is how to model the meaning of those multistep paths.\\nPTransE proposes three types of composition operation, namely, Addition, Multipli-\\ncation, and recurrent neural networks, to get the representation l of \\xe2\\x84\\x93= (r1, . . . ,rl)\\nthrough those relations. The score function of the path triple \\xe2\\x9f\\xa8h, \\xe2\\x84\\x93, t\\xe2\\x9f\\xa9is de\\xef\\xac\\x81ned as\\nfollows:\\nE (h, \\xe2\\x84\\x93, t) = \\xe2\\x88\\xa5l \\xe2\\x88\\x92(t \\xe2\\x88\\x92h)\\xe2\\x88\\xa5\\xe2\\x89\\x88\\xe2\\x88\\xa5l \\xe2\\x88\\x92r\\xe2\\x88\\xa5= E (\\xe2\\x84\\x93,r),\\n(7.18)\\nwhere r indicates the golden relation between h and t. Since PTransE wants to meet\\nthe assumption in TransE that r \\xe2\\x89\\x88t \\xe2\\x88\\x92h simultaneously, PTransE directly utilizes r\\nin training. The optimization objective of PTransE is\\nL =\\n\\x02\\n(h,r,t)\\xe2\\x88\\x88S\\n[L (h,r, t) + 1\\nZ\\n\\x02\\n\\xe2\\x84\\x93\\xe2\\x88\\x88P(h,t)\\nR(\\xe2\\x84\\x93|h, t)L (\\xe2\\x84\\x93,r)],\\n(7.19)\\n178\\n7\\nWorld Knowledge Representation\\nt2\\nh1\\n(a)\\nt1\\nh2\\nt3\\nr1\\nr2\\nt2\\nh1\\n(b)\\nt1\\nh2\\nt3\\nr1\\nr2\\nFig. 7.7 The architecture of TransA model [47]\\nwhere L (h,r, t) is the margin-based score function with E (h,r, t) and L (\\xe2\\x84\\x93,r) is\\nthe margin-based score function with E (\\xe2\\x84\\x93,r). The reliability R(\\xe2\\x84\\x93|h, t) of \\xe2\\x84\\x93in (h, \\xe2\\x84\\x93, t)\\nis well considered in the overall loss function.\\nBesides PTransE, similar ideas such as [21, 22] also consider the multistep rela-\\ntional paths on different tasks such as knowledge completion and question answer-\\ning successfully. These works demonstrate that there is plentiful information located\\nin multi-step relational paths, which could signi\\xef\\xac\\x81cantly improve the performance\\nof knowledge graph representation, and further explorations on more sophisticated\\nmodels for relational paths are still promising.\\n7.2.3.6\\nTransA\\nTransA [78] is proposed to solve the following problems in TransE and other\\nextensions: (1) TransE and its extensions only consider the Euclidean distance in\\ntheir energy functions, which seems to be less \\xef\\xac\\x82exible. (2) Existing methods regard\\neach dimension in the semantic vector space identically whatever the triple is, which\\nmay bring in errors when calculating dissimilarities. To solve these problems, as\\nillustrated in Fig.7.7, TransA replaces the in\\xef\\xac\\x82exible Euclidean distance with adaptive\\nMahalanobis distance, which is more adaptive and \\xef\\xac\\x82exible. The energy function of\\nTransA is as follows:\\nE (h,r, t) = (|h + r \\xe2\\x88\\x92t|)\\xe2\\x8a\\xa4Wr(|h + r \\xe2\\x88\\x92t|),\\n(7.20)\\nwhere Wr is a relation-speci\\xef\\xac\\x81c nonnegative symmetric matrix corresponding to the\\nadaptive matric. Note that the |h + r \\xe2\\x88\\x92t| stands for a nonnegative vector that each\\ndimension is the absolute value of the translating operation. We have\\n(|h + r \\xe2\\x88\\x92t|) \\xe2\\x89\\x9c(|h1 + r1 \\xe2\\x88\\x92t1|, |h2 + r2 \\xe2\\x88\\x92t2|, . . . |hn + rn \\xe2\\x88\\x92tn|).\\n(7.21)\\n7.2 Knowledge Graph Representation\\n179\\n7.2.3.7\\nKG2E\\nExisting translation-based models usually consider entities and relations as vectors\\nembeddedinlow-dimensionalsemanticspaces.However,asexplainedabove,entities\\nand relations in KGs are various with different granularities. Therefore, the margin\\nin the margin-based score function that is used to distinguish positive triples from\\nnegative triples should be more \\xef\\xac\\x82exible due to the diversity, and the uncertainties of\\nentities and relations should be taken into consideration.\\nTo solve this, KG2E [30] is proposed, introducing the multidimensional Gaus-\\nsian distributions to KG representations. As illustrated in Fig.7.8, KG2E represents\\neach entity and relation with a Gaussian distribution. Speci\\xef\\xac\\x81cally, the mean vector\\ndenotes the entity/relation\\xe2\\x80\\x99s central position, and the covariance matrix denotes its\\nuncertainties. To learn the Gaussian distributions for entities and relations, KG2E\\nalso follows the score function proposed in TransE. For a triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, the Gaussian\\ndistributions of entity and relation are de\\xef\\xac\\x81ned as follows:\\nh \\xe2\\x88\\xbcN (\\xce\\xbch, \\xce\\xa3h),\\nt \\xe2\\x88\\xbcN (\\xce\\xbct, \\xce\\xa3t),\\nr \\xe2\\x88\\xbcN (\\xce\\xbcr, \\xce\\xa3r).\\n(7.22)\\nNote that the covariances are diagonal for the consideration of ef\\xef\\xac\\x81ciency. KG2E\\nhypothesizes that the head and tail entity are independent with speci\\xef\\xac\\x81c relations, then\\nthe translation h \\xe2\\x88\\x92t could be de\\xef\\xac\\x81ned as\\nh \\xe2\\x88\\x92t = e \\xe2\\x88\\xbcN (\\xce\\xbch \\xe2\\x88\\x92\\xce\\xbct, \\xce\\xa3h + \\xce\\xa3t).\\n(7.23)\\nTo measure the dissimilarity between e and r, KG2E proposes two methods con-\\nsidering both asymmetric similarity and symmetric similarity.\\nThe asymmetric similarity is based on the KL divergence between e and r, which\\nis a straightforward method to measure the similarity between two probability dis-\\ntributions. The energy function is as follows:\\nE (h,r, t) = DKL(e\\xe2\\x88\\xa5r)\\n=\\n\\x03\\nx\\xe2\\x88\\x88Rke N (x; \\xce\\xbcr, \\xce\\xa3r) log N (x; \\xce\\xbce, \\xce\\xa3e)\\nN (x; \\xce\\xbcr, \\xce\\xa3r)dx\\n= 1\\n2\\n\\x04\\ntr(\\xce\\xa3\\xe2\\x88\\x921\\nr \\xce\\xa3r) + (\\xce\\xbcr \\xe2\\x88\\x92\\xce\\xbce)\\xe2\\x8a\\xa4\\xce\\xa3\\xe2\\x88\\x921\\nr (\\xce\\xbcr \\xe2\\x88\\x92\\xce\\xbce) \\xe2\\x88\\x92log det(\\xce\\xa3e)\\ndet(\\xce\\xa3r) \\xe2\\x88\\x92ke\\n\\x05\\n,\\n(7.24)\\nwhere tr(\\xce\\xa3) indicates the trace of \\xce\\xa3, and \\xce\\xa3\\xe2\\x88\\x921 indicates the inverse.\\nThe symmetric similarity is based on the expected likelihood or probability prod-\\nuct kernel. KE2G takes the inner product between Pe and Pr as the measurement of\\nsimilarity. The logarithm of energy function is\\n180\\n7\\nWorld Knowledge Representation\\nFig. 7.8 The architecture of\\nKG2E model [47]\\nBill Clinton\\nHillary Clinton\\nspouse\\nUSA\\nNationality\\nArkansas\\nBorn on\\nE (h,r, t) =\\n\\x03\\nx\\xe2\\x88\\x88Rke N (x; \\xce\\xbce, \\xce\\xa3e)N (x; \\xce\\xbcr, \\xce\\xa3r)dx\\n= log N (0; \\xce\\xbce \\xe2\\x88\\x92\\xce\\xbcr, \\xce\\xa3e + \\xce\\xa3r)\\n= 1\\n2\\n\\x06\\n(\\xce\\xbce \\xe2\\x88\\x92\\xce\\xbcr)\\xe2\\x8a\\xa4(\\xce\\xa3e + \\xce\\xa3r)\\xe2\\x88\\x921(\\xce\\xbce \\xe2\\x88\\x92\\xce\\xbcr) + log det(\\xce\\xa3e + \\xce\\xa3r) + ke log(2\\xcf\\x80)\\n\\x07\\n.\\n(7.25)\\nThe optimization objective of KG2E is also margin-based similar to TransE. Both\\nasymmetric and symmetric similarities are constrained by some regularization to\\navoid over\\xef\\xac\\x81tting:\\n\\xe2\\x88\\x80l \\xe2\\x88\\x88E \\xe2\\x88\\xaaR,\\n\\xe2\\x88\\xa5\\xce\\xbcl\\xe2\\x88\\xa52 \\xe2\\x89\\xa41,\\ncminI \\xe2\\x89\\xa4\\xce\\xa3l \\xe2\\x89\\xa4cmaxI,\\ncmin > 0.\\n(7.26)\\nFigure7.8 shows a brief example of representations in KG2E.\\n7.2.3.8\\nTransG\\nWe have discussed the problem of TransE in the session of TransR/CTransR that some\\nrelations in knowledge graphs such as location_contains or has_part may\\nhave multiple sub-meanings. These relations are more likely to be some combina-\\ntions that could be divided into several more precise relations. To address this issue,\\nCTransR is proposed with a preprocess of clustering for each relation r depending\\non the entity pairs (h, t). TransG [79] also focuses on this issue more elegantly by\\nintroducing a generative model. As illustrated in Fig.7.9, it assumes that different\\nsemantic component embeddings should follow a Gaussian Mixture Model. The\\ngenerative process is as follows:\\n1. For each entity e \\xe2\\x88\\x88E, TransG sets a standard normal distribution: \\xce\\xbce \\xe2\\x88\\xbcN (0, I).\\n2. For a triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, TransG uses Chinese Restaurant Process to automatically\\ndetect semantic components (i.e., sub-meanings in a relation): \\xcf\\x80r,n \\xe2\\x88\\xbcCRP(\\xce\\xb2).\\n7.2 Knowledge Graph Representation\\n181\\nt2\\nh\\n(b)\\nt1\\nt3\\nr\\nr\\nt4\\nt5\\nt2\\nh\\n(a)\\nt1\\nt3\\nr\\nt4\\nt5\\nFig. 7.9 The architecture of TransG model [47]\\n3. Drawtheheadembeddingtoformastandardnormaldistribution: h \\xe2\\x88\\xbcN (\\xce\\xbch, \\xcf\\x83 2\\nh I).\\n4. Draw the tail embedding to form a standard normal distribution: t \\xe2\\x88\\xbcN (\\xce\\xbct, \\xcf\\x83 2\\nt I).\\n5. Draw the relation embedding for this semantic component: \\xce\\xbcr,n = t \\xe2\\x88\\x92h \\xe2\\x88\\xbc\\nN (\\xce\\xbct \\xe2\\x88\\x92\\xce\\xbch, (\\xcf\\x83 2\\nh + \\xcf\\x83 2\\nt )I).\\n\\xce\\xbc is the mean embedding and \\xcf\\x83 is the variance. Finally, the score function is\\nE (h,r, t) \\xe2\\x88\\x9d\\nNr\\n\\x02\\nn=1\\n\\xcf\\x80r,nN (\\xce\\xbct \\xe2\\x88\\x92\\xce\\xbch, (\\xcf\\x83 2\\nh + \\xcf\\x83 2\\nt )I),\\n(7.27)\\nin which Nr is the number of semantic components of r, and \\xcf\\x80r,n is the weight of ith\\ncomponent generated by the Chinese Restaurant Process.\\nFigure7.9 shows the advantages of the generative Gaussian Mixture Model.\\n7.2.3.9\\nManifoldE\\nKG2E and TransG introduce Gaussian distributions to knowledge graph represen-\\ntation learning, improving the \\xef\\xac\\x82exibility and diversity with the various forms of\\nentity and relation representation. However, TransE and its most extensions view the\\ngolden triples as almost points in the low-dimensional vector space, following the\\nassumption of translation. This point assumption may lead to two problems: being\\nan ill-posed algebraic system and being over-strict with the geometric form.\\nManifoldE [80] is proposed to address this issue, considering the possible position\\nof the golden candidate in vector space as a manifold instead of one point. The overall\\nscore function of ManifoldE is de\\xef\\xac\\x81ned as follows:\\nE (h,r, t) = \\xe2\\x88\\xa5M (h,r, t) \\xe2\\x88\\x92D2\\nr \\xe2\\x88\\xa52,\\n(7.28)\\n182\\n7\\nWorld Knowledge Representation\\nin which D2\\nr is a relation-speci\\xef\\xac\\x81c manifold parameter indicating the bias. Two kinds\\nofmanifoldsarethenproposedinManifoldE.ManifoldE(Sphere)isastraightforward\\nmanifold that supposes t should be located in the sphere which has h + r to be the\\ncenter and Dr to be the radius. We have\\nM (h,r, t) = \\xe2\\x88\\xa5h + r \\xe2\\x88\\x92t\\xe2\\x88\\xa52\\n2.\\n(7.29)\\nThe second manifold utilized is the hyperplane for it is much easier for two\\nhyperplanes to intersect. The function of ManifoldE(Hyperplane) is\\nM (h,r, t) = (h + rh)\\xe2\\x8a\\xa4(t + rt),\\n(7.30)\\nin which rh and rt represent the two relation embeddings. This indicates that for\\na triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, the tail entity t should locate in the hyperplane whose direction\\nis h + rh with the bias to be D2\\nr . Furthermore, ManifoldE(Hyperplane) considers\\nabsolute values in M (h,r, t) as |h + rh|\\xe2\\x8a\\xa4|t + rt| to double the solution number of\\npossible tails. For both manifolds, the author applies kernel forms on Reproducing\\nKernel Hilbert Space.\\n7.2.4\\nOther Models\\nTranslation-based methods such as TransE are simple but effective, whose power\\nhas been consistently veri\\xef\\xac\\x81ed on various tasks like knowledge graph completion and\\ntriple classi\\xef\\xac\\x81cation, achieving state-of-the-art performance. However, there are also\\nsome other representation learning methods performing well on knowledge graph\\nrepresentation. In this part, we will take a brief look at these methods as inspiration.\\n7.2.4.1\\nStructured Embeddings\\nStructured Embeddings (SE) [8] is a classical representation learning method for\\nKGs. In SE, each entity is projected to a d-dimensional vector space. SE designs two\\nrelation-speci\\xef\\xac\\x81c matrices Mr,1, Mr,2 \\xe2\\x88\\x88Rd\\xc3\\x97d for each relation r, projecting both head\\nand tail entities with these relation-speci\\xef\\xac\\x81c matrices when calculating the similarities.\\nThe score function of SE is de\\xef\\xac\\x81ned as follows:\\nE (h,r, t) = \\xe2\\x88\\xa5Mr,1h \\xe2\\x88\\x92Mr,2t\\xe2\\x88\\xa51,\\n(7.31)\\nin which both h and t are transformed into a relation-speci\\xef\\xac\\x81c vector space with\\nthose projection matrices. The assumption of SE is that the projected head and tail\\nembeddings should be as similar as possible according to the loss function. Different\\nfrom the translation-based methods, SE models entities as embeddings and relations\\n7.2 Knowledge Graph Representation\\n183\\nas projection matrices. In training, SE considers all triples in the training set and\\nminimizes the overall loss function.\\n7.2.4.2\\nSemantic Matching Energy\\nSemantic Matching Energy (SME) [5, 6] proposes a more complicated representation\\nlearning method. Differing from SE, SME considers both entities and relations as\\nlow-dimensional vectors. For a triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, h and r are combined using a projection\\nfunction g to get a new embedding lh,r, and the same with t and r to get lt,r. Next, a\\npoint-wise multiplication function is used on the two combined embeddings lh,r and\\nlt,r to get the score of this triple. SME proposes two different projection functions in\\nthe second step, among which the linear form is\\nE (h,r, t) = (M1h + M2r + b1)\\xe2\\x8a\\xa4(M3t + M4r + b2),\\n(7.32)\\nand the bilinear form is:\\nE (h,r, t) = ((M1h \\xe2\\x8a\\x99M2r) + b1)\\xe2\\x8a\\xa4((M3t \\xe2\\x8a\\x99M4r) + b2),\\n(7.33)\\nwhere \\xe2\\x8a\\x99is the element-wise (Hadamard) product. M1, M2, M3, M4 are weight\\nmatrices in the projection function, and b1 and b2 are the bias. Bordes et al. [6] is\\nbased on SME and improves the bilinear form with three-way tensors instead of\\nmatrices.\\n7.2.4.3\\nLatent Factor Model\\nLatent Factor Model (LFM) is proposed for modeling large multi-relational datasets.\\nLFM is based on a bilinear structure, which models entities as embeddings and\\nrelations as matrices. It could share sparse latent factors among different relations,\\nsigni\\xef\\xac\\x81cantly reducing the model and computational complexity. The score function\\nof LFM is de\\xef\\xac\\x81ned as follows:\\nE (h,r, t) = h\\xe2\\x8a\\xa4Mrt,\\n(7.34)\\nin which Mr is the representation of the relation r. Moreover, [92] proposes DIST-\\nMULT model, which restricts Mr to be a diagonal matrix. This enhanced model not\\nonly reduces the parameter number of LFM and thus lowers the model\\xe2\\x80\\x99s computa-\\ntional complexity, but also achieves better performance.\\n184\\n7\\nWorld Knowledge Representation\\n7.2.4.4\\nRESCAL\\nRESCAL is a knowledge graph representation learning method based on matrix\\nfactorization [54, 55]. In RESCAL, to represent all triple facts in knowledge graphs,\\nthe authors employ a three-way tensor \\xe2\\x88\\x92\\xe2\\x86\\x92\\nX \\xe2\\x88\\x88Rd\\xc3\\x97d\\xc3\\x97k in which d is the dimension of\\nentities and k is that of relations. In the three-way tensor \\xe2\\x88\\x92\\xe2\\x86\\x92\\nX , two modes stand for\\nthe head and tail entities while the third mode represents the relations. The entries of\\n\\xe2\\x88\\x92\\xe2\\x86\\x92\\nX are based on the existence of the corresponding triple facts. That is, \\xe2\\x88\\x92\\xe2\\x86\\x92\\nX i jm = 1 if\\nthe triple \\xe2\\x9f\\xa8ith entity, mth relation, jth entity\\xe2\\x9f\\xa9holds in the training set, and otherwise\\n\\xe2\\x88\\x92\\xe2\\x86\\x92\\nX i jm = 0 if the triple is nonexisting.\\nTo capture the inherent structure of all triples, a tensor factorization model named\\nRESCAL is then proposed. Suppose \\xe2\\x88\\x92\\xe2\\x86\\x92\\nX = {X1, . . . , Xk}, for each slice Xn, we have\\nthe following rank-r factorization:\\nXn \\xe2\\x89\\x88ARnA\\xe2\\x8a\\xa4,\\n(7.35)\\nwhere A \\xe2\\x88\\x88Rd\\xc3\\x97r stands for the r-dimensional entity representations, and Rn \\xe2\\x88\\x88Rr\\xc3\\x97r\\nrepresents the interactions of the r latent components for n-th relation. The assump-\\ntion in this factorization is similar to LFM, while RESCAL also optimizes the nonex-\\nisting triples where \\xe2\\x88\\x92\\xe2\\x86\\x92\\nX i jm = 0 instead of only considering the positive instances.\\nFollowing this tensor factorization assumption, the loss function of RESCAL is\\nde\\xef\\xac\\x81ned as follows:\\nL = 1\\n2\\n\\x08\\x02\\nn\\n\\xe2\\x88\\xa5Xn \\xe2\\x88\\x92ARnA\\xe2\\x8a\\xa4\\xe2\\x88\\xa52\\nF\\n\\t\\n+ 1\\n2\\xce\\xbb\\n\\x08\\n\\xe2\\x88\\xa5A\\xe2\\x88\\xa52\\nF +\\n\\x02\\nn\\n\\xe2\\x88\\xa5Rn\\xe2\\x88\\xa52\\nF\\n\\t\\n,\\n(7.36)\\nin which the second term is a regularization term and \\xce\\xbb is a hyperparameter.\\n7.2.4.5\\nHOLE\\nRESCAL works well with multi-relational data but suffers from high computational\\ncomplexity. To leverage both effectiveness and ef\\xef\\xac\\x81ciency, Holographic Embeddings\\n(HOLE) is proposed as an enhanced version of RESCAL [53].\\nHOLE employs an operation named circular correlation to generate compositional\\nrepresentations, which is similar to those holographic models of associative memory.\\nThe circular correlation operation \\xe2\\x8b\\x86: Rd \\xc3\\x97 Rd \\xe2\\x86\\x92Rd between two entities h and t\\nis as follows:\\nh \\xe2\\x8b\\x86ttk =\\nd\\xe2\\x88\\x921\\n\\x02\\ni=0\\nhit(k+i)mod d.\\n(7.37)\\nFigure7.10a also demonstrates a simple instance of this operation. The probability\\nof a triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9is then de\\xef\\xac\\x81ned as\\n7.2 Knowledge Graph Representation\\n185\\nhead\\ntail\\neh0\\nr\\neh1\\neh2\\net0\\net1\\net2\\nP(\\nr(h,t))\\n(a)HOLE\\nhead\\ntail\\neh0\\nr\\neh1\\neh2\\net0\\net1\\net2\\nP(\\nr(h,t))\\n(b)RESCAL\\nFig. 7.10 The architecture of RESCAL and HOLE models\\nP(\\xcf\\x86r(h, t) = 1) = Sigmoid(r\\xe2\\x8a\\xa4(h \\xe2\\x8b\\x86t)).\\n(7.38)\\nConsidering circular correlation brings in lots of advantages: (1) unlike other\\noperations like multiplication or convolution, circular correlation is noncommutative\\n(i.e., h \\xe2\\x8b\\x86t \\xcc\\xb8= t \\xe2\\x8b\\x86h), which is capable of modeling asymmetric relations in knowledge\\ngraphs. (2) Circular correlation has lower computational complexity compared to\\ntensor product in RESCAL. What\\xe2\\x80\\x99s more, the circular correlation could further speed\\nup with the help of Fast Fourier Transform (FFT), which is formalized as follows:\\nh \\xe2\\x8b\\x86t = F \\xe2\\x88\\x921(F(h) \\xe2\\x8a\\x99F(b)).\\n(7.39)\\n186\\n7\\nWorld Knowledge Representation\\nF(\\xc2\\xb7) and F(\\xc2\\xb7)\\xe2\\x88\\x921 represent the FFT and its inverse, while F(\\xc2\\xb7) denotes the complex\\nconjugate in Cd, and \\xe2\\x8a\\x99stands for the element-wise (Hadamard) product. Due to\\nFFT, the computational complexity of circular correlation is O(d log d), which is\\nmuch lower than that of tensor product.\\n7.2.4.6\\nComplex Embedding (ComplEx)\\nComplEx [70] employs an eigenvalue decomposition model, which makes use of\\ncomplex valued embeddings. The composition of complex embeddings can handle a\\nlarge variety of binary relations, among the symmetric and antisymmetric relations.\\nFormally, the log-odd of the probability that the fact \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9is true is\\nfr(h, t) = Sigmoid(Xhrt),\\n(7.40)\\nwhere fr(h, t) is expected to be 1 when (h,r, t) holds, otherwise \\xe2\\x88\\x921. Here, Xhrt is\\ncalculated as follows:\\nXhrt = Re(\\xe2\\x9f\\xa8r, h, t\\xe2\\x9f\\xa9)\\n= \\xe2\\x9f\\xa8Re(r), Re(h), Re(t)\\xe2\\x9f\\xa9+ \\xe2\\x9f\\xa8Re(r), Im(h), Im(t)\\xe2\\x9f\\xa9\\n\\xe2\\x88\\x92\\xe2\\x9f\\xa8Im(r), Re(h), Im(t)\\xe2\\x9f\\xa9\\xe2\\x88\\x92\\xe2\\x9f\\xa8Im(r), Im(h), Re(t)\\xe2\\x9f\\xa9,\\n(7.41)\\nwhere \\xe2\\x9f\\xa8x, y, z\\xe2\\x9f\\xa9= \\ni xi yizi denotes the trilinear dot product, Re(x) and Im(x) indi-\\ncate the real part and the imaginary part of the number x respectively. In fact, Com-\\nplEx can be viewed as an extension of RESCAL, which assigns complex embedding\\nof the entities and relations.\\nBesides, [29] has proved that HolE is mathematically equivalent to ComplEx\\nrecently.\\n7.2.4.7\\nConvolutional 2D Embeddings (ConvE)\\nConvE [16] uses 2D convolution over embeddings and multiple layers of nonlinear\\nfeatures to model knowledge graphs. It is the \\xef\\xac\\x81rst nonlinear model that signi\\xef\\xac\\x81cantly\\noutperforms previous linear models.\\nSpeci\\xef\\xac\\x81cally, ConvE uses convolutional and fully connected layers to model the\\ninteractions between input entities and relationships. After that, the obtained features\\nare \\xef\\xac\\x82attened, transformed through a fully connected layer, and the inner product is\\ntaken with all object entity vectors to generate a score for each triple.\\nFor each triple \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, ConvE de\\xef\\xac\\x81nes its score function as\\nfr(h, t) = f (vec( f ([\\xc2\\xafh; \\xc2\\xafr] \\xe2\\x88\\x97\\xcf\\x89))W)t,\\n(7.42)\\n7.2 Knowledge Graph Representation\\n187\\nwhere \\xe2\\x88\\x97denotes the convolution operator, and vec(\\xc2\\xb7) means compressing a matrix\\ninto a vector. r \\xe2\\x88\\x88Rk is a relation parameter depending on r, \\xc2\\xafh and \\xc2\\xafr denote a 2D\\nreshaping of h and r, respectively: if h, r \\xe2\\x88\\x88Rk, then \\xc2\\xafh, \\xc2\\xafr \\xe2\\x88\\x88Rka\\xc3\\x97kb, where k = kakb.\\nConvE can be seen as an improvement on HolE. Compared with HolE, it learns\\nmultiple layers of nonlinear features, and thus theoretically more expressive than\\nHolE.\\n7.2.4.8\\nRotation Embeddings (RotatE)\\nRotatE [67] de\\xef\\xac\\x81nes each relation as a rotation from the head entity to the tail entity in\\nthe complex vector space. Thus, it is able to model and infer various relation patterns,\\nincluding symmetry/antisymmetry, inversion, and composition. Formally, the score\\nfunction of the fact \\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9of RotatE is de\\xef\\xac\\x81ned as\\nfr(h, t) = \\xe2\\x88\\xa5h \\xe2\\x8a\\x99r \\xe2\\x88\\x92t\\xe2\\x88\\xa5,\\n(7.43)\\nwhere \\xe2\\x8a\\x99denotes the element-wise (Hadamard) product, h, r, t \\xe2\\x88\\x88Ck and |ri| = 1.\\nRotatE is simple but achieves quite good performance. Compared with previous\\nwork, it is the \\xef\\xac\\x81rst model that is capable of modeling and inferring all the three\\nrelation patterns above.\\n7.2.4.9\\nNeural Tensor Network\\nSocher et al. [65] propose Neural Tensor Network (NTN) as well as Single Layer\\nModel (SLM), while NTN is an enhanced version of SLM. Inspired by the previous\\nattempts in KRL, SLM represents both entities and relations as low-dimensional\\nvectors, and also designs relation-speci\\xef\\xac\\x81c projection matrices to map entities from\\nentity space to relation space. Similar to SE, the score function of SLM is as follows:\\nE (h,r, t) = r\\xe2\\x8a\\xa4tanh(Mr,1h + Mr,2t),\\n(7.44)\\nwhere h, t \\xe2\\x88\\x88Rd represent head and tail embeddings, r \\xe2\\x88\\x88Rk represents relation\\nembedding, and Mr,1, Mr,2 \\xe2\\x88\\x88Rd\\xc3\\x97k stand for the relation-speci\\xef\\xac\\x81c matrices.\\nAlthough SLM has introduced relation embeddings as well as a nonlinear layer\\ninto the score function, the model representation capability is still restricted. Neural\\ntensor network is then proposed with tensors being introduced into the SLM frame-\\nwork. Besides the original linear neural network layer that projects entities to the\\nrelation space, NTN also adds another tensor-based neural layer which combines\\nhead and tail embeddings with a relation-speci\\xef\\xac\\x81c tensor, as illustrated in Fig.7.11.\\nThe score function of NTN is then de\\xef\\xac\\x81ned as follows:\\nE (h,r, t) = r\\xe2\\x8a\\xa4tanh(h\\xe2\\x8a\\xa4\\xe2\\x88\\x92\\xe2\\x86\\x92\\nMrt + Mr,1h + Mr,2t + br),\\n(7.45)\\n188\\n7\\nWorld Knowledge Representation\\nt\\nh\\nword space\\nentity space\\nr\\nScore\\nNeural \\nTensor\\nNetwork\\nFig. 7.11 The architecture of NTN model [47]\\nwhere \\xe2\\x88\\x92\\xe2\\x86\\x92\\nMr \\xe2\\x88\\x88Rd\\xc3\\x97d\\xc3\\x97k is a 3-way relation-speci\\xef\\xac\\x81c tensor, br is the bias, and Mr,1,\\nMr,2 \\xe2\\x88\\x88Rd\\xc3\\x97k is the relation-speci\\xef\\xac\\x81c matrices similar to SLM. Note that SLM is the\\nsimpli\\xef\\xac\\x81ed version of NTN if the tensor and bias are set to zero.\\nBesides the improvements in score function, NTN also attempts to utilize the\\nlatent textual information located in entity names and successfully achieves signif-\\nicant improvements. Differing from previous RL models that provide each entity\\nwith a vector, NTN represents each entity as the average of its entity name\\xe2\\x80\\x99s word\\nembeddings. For example, the entity Bengal tiger will be represented as the\\naverage word embeddings of Bengal and tiger. It is apparent that the entity\\nname will provide valuable information for understanding an entity, since Bengal\\ntiger may come from Bengal and be related to other tigers. Moreover, the number\\nof words is far less than that of entities. Therefore, using the average word embed-\\ndings of entity names will also lower the computational complexity and alleviate the\\nissue of data sparsity.\\nNTN utilizes tensor-based neural networks to model triple facts and achieves\\nexcellent successes. However, the overcomplicated method will lead to higher com-\\nputational complexity compared to other methods, and the vast number of parameters\\nwill limit the performance on rather sparse and large-scale KGs.\\n7.2.4.10\\nNeural Association Model (NAM)\\nNAM [43] adopts multilayer nonlinear activations in the deep neural network to\\nmodel the conditional probabilities between head and tail entities. NAM studies\\ntwo model structures Deep Neural Network (DNN) and Relation Modulated Neural\\nNetwork (RMNN).\\nNAM-DNN feeds the head and tail entities\\xe2\\x80\\x99 embeddings into an MLP with L fully\\nconnected layers, which is formalized as follows:\\n7.2 Knowledge Graph Representation\\n189\\nz(l) = Sigmoid(Mlz(l\\xe2\\x88\\x921) + b(l)), l = 1, . . . , L,\\n(7.46)\\nwhere z(0) = [h; r], M(l) and b(l) is the weight matrix and bias vector for the l-th\\nfully connected layer, respectively. And \\xef\\xac\\x81nally the score function of NAM-DNN is\\nde\\xef\\xac\\x81ned as\\nfr(h, t) = Sigmoid(t\\xe2\\x8a\\xa4z(L)).\\n(7.47)\\nDifferent from NAM-DNN, NAM-RMNN feds the relation embedding r into\\neach layer of the deep neural network as follows:\\nz(l) = Sigmoid(M(l)z(l\\xe2\\x88\\x921) + B(l)r), l = 1, . . . , L,\\n(7.48)\\nwhere z(0) = [h; r], M(l) and B(l) indicate the weight matrices. The score function\\nof NAM-RMNN is de\\xef\\xac\\x81ned as\\nfr(h, t) = Sigmoid(t\\xe2\\x8a\\xa4z(L) + B(l+1)r).\\n(7.49)\\n7.3\\nMultisource Knowledge Graph Representation\\nWe are living in a complicated pluralistic real world, in which we can get information\\nthrough all senses and learn knowledge not only from structured knowledge graphs\\nbut also from plain texts, categories, images, and videos. This cross-modal infor-\\nmation is considered as multisource information. Besides the structured knowledge\\ngraph which is well utilized in previous KRL methods, we will introduce the other\\nkinds of KRL methods utilizing multisource information:\\n1. Plain text is one of the most common information we deliver, receive, and\\nanalyze every day. There are vast amounts of plain texts we possess remaining to be\\ndetected, in which the signi\\xef\\xac\\x81cant knowledge that structured knowledge graphs may\\nnot include locates. Entity description is a special kind of textual information that\\ndescribes the corresponding entity within a few sentences or a short paragraph. Usu-\\nally, entity descriptions are maintained by some knowledge graphs (i.e., Freebase)\\nor could be automatically extracted from huge databases like Wikipedia.\\n2. Entity type is another important structured information for building knowledge\\nrepresentations. To learn new objects within our prior knowledge systems, human\\nbeings tend to systemize those objects into existing categories. An entity type is usu-\\nally represented with hierarchical structures, which consist of different granularities\\nof entity subtypes. It is natural that entities in the real world usually have multiple\\nentity types. Most of the existing famous knowledge graphs own their customized\\nhierarchical structures of entity types.\\n3. Images provide intuitive visual information to describe what the entity looks\\nlike, which is con\\xef\\xac\\x81rmed to be the most signi\\xef\\xac\\x81cant information we receive and process\\nevery day. The latent information located in images helps a lot, especially when\\ndealing with concrete entities. For instance, we may \\xef\\xac\\x81nd out the potential relationship\\n190\\n7\\nWorld Knowledge Representation\\nbetween Cherry and Plum (there are both plants belonging to Rosaceae) from\\ntheir appearances. Images could be downloaded from websites, and there are also\\nsubstantial image datasets like ImageNet.\\nMultisourceinformationlearningprovidesanovelmethodtolearnknowledgerep-\\nresentations not only from the internal information of structured knowledge graphs\\nbut also from the external information of plain texts, hierarchical types, and images.\\nMoreover, the exploration in multisource information learning helps to further under-\\nstand human cognition with all senses in the real world. The cross-modal represen-\\ntations learned based on knowledge graphs will also provide possible relationships\\nbetween different kinds of information.\\n7.3.1\\nKnowledge Graph Representation with Texts\\nTextual information is one of the most common and widely used information these\\ndays. There are large plain texts generated every day on the web and easy to be\\nextracted. Words are compressed symbols of our thoughts and can provide the con-\\nnections between entities, which are of great signi\\xef\\xac\\x81cance in KRL.\\n7.3.1.1\\nKnowledge Graph and Text Joint Embedding\\nWang et al. [76] attempt to utilize textual information by jointly embedding entities,\\nrelations, and words into the same low-dimensional continuous vector space. Their\\njoint model contains three parts, namely, the knowledge model, the text model, and\\nthe alignment model. More speci\\xef\\xac\\x81cally, the knowledge model is learned based on the\\ntriple facts in KGs by translation-based models, while the text model is learned based\\non the concurrences of words in the large corpus by Skip-gram. As for the alignment\\nmodel, two methods are proposed utilizing Wikipedia anchors and entity names. The\\nmain idea of alignment by Wikipedia anchors is replacing the word-word pair (w, v)\\nwith the word-entity pair (w, ev) according to the anchors in Wiki pages, while the\\nmain idea of alignment by entity names is replacing the entities in original triple\\n\\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9with the corresponding entity names \\xe2\\x9f\\xa8wh,r, t\\xe2\\x9f\\xa9, \\xe2\\x9f\\xa8h,r, wt\\xe2\\x9f\\xa9, and \\xe2\\x9f\\xa8wh,r, wt\\xe2\\x9f\\xa9.\\nModeling entities and words into the same vector space are capable of encoding\\nboth information in knowledge graphs and that in plain texts, while the performance\\nof this joint model depends on the completeness of Wikipedia anchors and may suffer\\nfrom the weak interactions merely based on entity names. To address this issue,\\n[101] proposes a new joint embedding based on [76] and improves the alignment\\nmodel with entity descriptions into consideration, assuming that entities should be\\nsimilar to all words in their descriptions. These joint models learn knowledge and text\\njoint embeddings, improving evaluation performance in both word and knowledge\\nrepresentations.\\n7.3 Multisource Knowledge Graph Representation\\n191\\nCNN/\\nCBOW\\ndescription of head\\nw1\\nw2\\nwn\\nw1\\nw2\\nwn\\ndescription of tail\\nhead\\nrelation\\n+\\n=\\n+\\n=\\ntail\\nCNN/\\nCBOW\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nFig. 7.12 The architecture of DKRL model\\n7.3.1.2\\nDescription-Embodied Knowledge Graph Representation\\nAnother way of utilizing textual information is directly constructing knowledge rep-\\nresentations from entity descriptions instead of merely considering the alignments.\\nXie et al. [82] proposes Description-embodied Knowledge Graph Representation\\nLearning (DKRL) that provides two kinds of knowledge representations: the \\xef\\xac\\x81rst is\\nthe structure-based representation hS and tS, which can directly represent entities\\nwidely used in previous methods, and the second is the description-based represen-\\ntation hD and tD which derives from entity descriptions. The energy function derives\\nfrom translation-based framework:\\nE (h,r, t) = \\xe2\\x88\\xa5hS + r \\xe2\\x88\\x92tS\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5hS + r \\xe2\\x88\\x92tD\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5hD + r \\xe2\\x88\\x92tS\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5hD + r \\xe2\\x88\\x92tD\\xe2\\x88\\xa5.\\n(7.50)\\nThe description-based representation is constructed via CBOW or CNN encoders\\nthat encode rich textual information from plain texts into knowledge representations.\\nThe architecture of DKRL is shown in Fig.7.12.\\nCompared to conventional translation-based methods, the two types of entity\\nrepresentations in DKRL are constructed with both structural information and textual\\ninformation, and thus could get better performance in knowledge graph completion\\nand type classi\\xef\\xac\\x81cation. Besides, DKRL could represent an entity even if it is not in\\nthe training set, as long as there are a few sentences to describe the entity. As their\\nmillions of new entities come up every day, DKRL is capable of handling zero-shot\\nlearning.\\n192\\n7\\nWorld Knowledge Representation\\n7.3.2\\nKnowledge Graph Representation with Types\\nEntity types, which serve as a kind of category information of entities and are usu-\\nally arranged with hierarchical structures, could provide structured information to\\nunderstand entities in KRL better.\\n7.3.2.1\\nType-Constraint Knowledge Graph Representation\\nKrompa\\xc3\\x9f et al. [36] take type information as type constraints, and improves exist-\\ning methods like RESCAL and TransE via type constraints. It is intuitive that in a\\nparticular relation, the head or tail entities should belong to some speci\\xef\\xac\\x81c types. For\\nexample, the head entities of the relation write_books should be a human (or\\nmore precisely an author), and the tail entities should be a book.\\nSpeci\\xef\\xac\\x81cally, in RESCAL, the original factorization Xr \\xe2\\x89\\x88ARrA\\xe2\\x8a\\xa4is modi\\xef\\xac\\x81ed to\\nX\\xe2\\x80\\xb2\\nr \\xe2\\x89\\x88A[headr,:]RrA\\xe2\\x8a\\xa4\\n[tailr,:],\\n(7.51)\\nin which headr, tailr are the set of entities \\xef\\xac\\x81tting the type constraints of head or tail\\nand X\\xe2\\x80\\xb2r is a sparse adjacency matrix of shape |headr| \\xc3\\x97 |tailr|. In the enhanced ver-\\nsion, only the entities that \\xef\\xac\\x81t type constraints will be considered during factorization.\\nIn TransE, type constraints are utilized in negative sampling. The margin-based\\nscore functions of translation-based methods need negative instances, which are\\ngenerated through randomly replacing head or tail entities with another entity in\\ntriples. With type constraints, the negative samples are chosen by\\nh\\xe2\\x80\\xb2 \\xe2\\x88\\x88E[headr] \\xe2\\x8a\\x86E,\\nt\\xe2\\x80\\xb2 \\xe2\\x88\\x88E[tailr] \\xe2\\x8a\\x86E,\\n(7.52)\\nwhere E[headr] is the subset of entities following type constraints for head in relation\\nr, and E[tailr] is that for tail.\\n7.3.2.2\\nType-Embodied Knowledge Graph Representation\\nConsidering type information as constraints is simple but effective, while the per-\\nformance is still limited. Instead of merely viewing type information as type con-\\nstraints, Xie et al. [83] propose Type-embodied Knowledge Graph Representation\\nLearning (TKRL), utilizing hierarchical-type structures to instruct the construction\\nof projection matrices. Inspired by TransR that every entity should have multiple\\nrepresentations in different scenarios, the energy function of TKRL is de\\xef\\xac\\x81ned as\\nfollows:\\nE (h,r, t) = \\xe2\\x88\\xa5Mrhh + r \\xe2\\x88\\x92Mrtt\\xe2\\x88\\xa5,\\n(7.53)\\n7.3 Multisource Knowledge Graph Representation\\n193\\nh\\nh\\nt\\nt\\nhch\\n(m)\\nhch\\nhch\\ntct\\nRHE\\nWHE\\nr\\ntct\\nr\\nMct\\n(m)\\nMch\\n(m-1)\\nMct\\n(m-1)\\nMct\\n(1)\\nMct\\n(1)\\n\\xe2\\x88\\x91i\\xce\\xb2iMCh\\n(1)\\n\\xe2\\x88\\x91i\\xce\\xb2iMCt\\n(1)\\nFig. 7.13 The architecture of TKRL model\\nin which Mrh and Mrt are two projection matrices for h and t that depend on their\\ncorresponding hierarchical types in this triple. Two hierarchical-type encoders are\\nproposed to learn the projection matrices, regarding all subtypes in the hierarchy\\nas projection matrices, in which Recursive Hierarchy Encoder is based on matrix\\nmultiplication, while Weighted Hierarchy Encoder is based on matrix summation:\\nMRH Ec =\\nm\\n\\x0b\\ni=1\\nMc(i) = Mc(1)Mc(2) . . . Mc(m),\\n(7.54)\\nMW H Ec =\\nm\\n\\x02\\ni=1\\n\\xce\\xb2iMc(i) = \\xce\\xb21Mc(1) + \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 + \\xce\\xb2mMc(m),\\n(7.55)\\nwhere Mc(i) stands for the projection matrix of the ith subtype of the hierarchical\\ntype c, \\xce\\xb2i is the corresponding weight of the subtype. Figure7.13 demonstrates a\\nsimple illustration of TKRL. Taking RHE, for instance, given an entity William\\nShakespeare, it is \\xef\\xac\\x81rst projected to a rather general sub-type space like human,\\nand then sequentially projected to a more precise subtype like author or English\\nauthor. Moreover, TKRL also proposes an enhanced soft-type constraint to alle-\\nviate the problems caused by type information incompleteness.\\n194\\n7\\nWorld Knowledge Representation\\nArmet\\nSuit of \\narmour\\nhas part\\nFig. 7.14 Examples of entity images [81]\\n7.3.3\\nKnowledge Graph Representation with Images\\nImages could provide intuitive visual information of their corresponding entities\\xe2\\x80\\x99\\noutlook, which may give signi\\xef\\xac\\x81cant hints suggesting some latent attributes of entities\\nfrom certain aspects. For instance, Fig.7.14 demonstrates some examples of entity\\nimages of their corresponding entities Suit of armour and Armet. The left\\nside shows the triple facts that \\xe2\\x9f\\xa8Suit of armour, has_a_part, Armet\\xe2\\x9f\\xa9, and\\nsurprisingly, we can infer this knowledge directly from the images.\\n7.3.3.1\\nImage-Embodied Knowledge Graph Representation\\nXie et al. [81] propose Image-embodied Knowledge Graph Representation Learning\\n(IKRL) to take visual information into consideration when constructing knowledge\\nrepresentations. Inspired by the multiple entity representations in [82], IKRL also\\nproposes the image-based representation hI and tI besides the original structure-\\nbased representation, and jointly learn both two types of entity representations simul-\\ntaneously within the translation-based framework.\\nE (h,r, t) = \\xe2\\x88\\xa5hS + r \\xe2\\x88\\x92tS\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5hS + r \\xe2\\x88\\x92tI\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5hI + r \\xe2\\x88\\x92tS\\xe2\\x88\\xa5+ \\xe2\\x88\\xa5hI + r \\xe2\\x88\\x92tI\\xe2\\x88\\xa5.\\n(7.56)\\nMore speci\\xef\\xac\\x81cally, IKRL \\xef\\xac\\x81rst constructs the image representations for all entity\\nimages with neural networks, and then project these image representations from\\nimage space to entity space via a projection matrix. Since most entities may have\\nmultiple images with different qualities, IKRL selects the more informative and\\ndiscriminative images via an attention-based method. The evaluation results of\\nIKRL not only con\\xef\\xac\\x81rm the signi\\xef\\xac\\x81cance of visual information in understanding\\n7.3 Multisource Knowledge Graph Representation\\n195\\n-\\n-\\n\\xe2\\x89\\x88\\ndresser\\ndrawer\\npianoforte\\nkeyboard\\n-\\n\\xe2\\x89\\x88\\n-\\ncat (Felidae)\\ntiger\\ntoothed whale\\ndolphin\\nw(part_of)\\nw(hypernym)\\n\\xe2\\x89\\x88\\n\\xe2\\x89\\x88\\nFig. 7.15 An example of semantic regularities in word space [81]\\nentities but also show the possibility of a joint heterogeneous semantic space.\\nMoreover, the authors also \\xef\\xac\\x81nd some interesting semantic regularities such as\\nw(man) \\xe2\\x88\\x92w(king) \\xe2\\x89\\x88w(woman) \\xe2\\x88\\x92w(queen) found in word space, which are\\nshown in Fig.7.15.\\n7.3.4\\nKnowledge Graph Representation with Logic Rules\\nTypicalknowledgegraphsstoreknowledgeintheformoftriplefactswithonerelation\\nlinkingtwoentities.MostexistingKRLmethodsonlyconsidertheinformationwithin\\ntriple facts separately, ignoring the possible interactions and correlations between dif-\\nferent triples. Logic rules, which are certain kinds of summaries deriving from human\\nbeings\\xe2\\x80\\x99 prior knowledge, could help us with knowledge inference and reasoning. For\\ninstance, if we know the triple fact that \\xe2\\x9f\\xa8Beijing, is_capital_of, China\\xe2\\x9f\\xa9,\\nwe can easily infer with high con\\xef\\xac\\x81dence that \\xe2\\x9f\\xa8Beijing, located_in, China\\xe2\\x9f\\xa9,\\nsince we know the logic rule that the relation is_capital_of \\xe2\\x87\\x92located_in.\\nSome works are focusing on introducing logic rules to knowledge acquisition and\\ninference, among which Markov Logic Networks are intuitively utilized to address\\nthis challenge [3, 58, 75]. The path-based TransE [38] stated above also implicitly\\nconsiders the latent logic rules between different relations via relation paths.\\n7.3.4.1\\nKALE\\nKALE is a translation-based KRL method that jointly learns knowledge representa-\\ntions with logic rules [24]. The joint learning consists of two parts, namely, the triple\\nmodeling and the rule modeling. For triple modeling, KALE follows the translation\\nassumption with minor alteration in scoring function as follows:\\n196\\n7\\nWorld Knowledge Representation\\nE (h,r, t) = 1 \\xe2\\x88\\x92\\n1\\n3\\n\\xe2\\x88\\x9a\\nd\\n\\xe2\\x88\\xa5h + r \\xe2\\x88\\x92t\\xe2\\x88\\xa5,\\n(7.57)\\nin which d stands for the dimension of knowledge embeddings. E (h,r, t) takes value\\nin [0, 1] for the convenience of joint learning.\\nFor the newly added rule modeling, KALE uses the t-norm fuzzy logics proposed\\nin [25] that represent the truth value of a complex formula with the truth values of its\\nconstituents. Specially, KALE focuses on two typical types of logic rules. The \\xef\\xac\\x81rst is\\n\\xe2\\x88\\x80h, t : \\xe2\\x9f\\xa8h,r1, t\\xe2\\x9f\\xa9\\xe2\\x87\\x92\\xe2\\x9f\\xa8h,r2, t\\xe2\\x9f\\xa9(e.g., given \\xe2\\x9f\\xa8Beijing, is_capital_of, China\\xe2\\x9f\\xa9,\\nwecaninferthat\\xe2\\x9f\\xa8Beijing,located_in,China\\xe2\\x9f\\xa9).KALErepresentsthescoring\\nfunction of this logic rule f1 via speci\\xef\\xac\\x81c t-norm based logical connectives as follows:\\nE ( f1) = E (h,r1, t)E (h,r2, t) \\xe2\\x88\\x92E (h,r1, t) + 1.\\n(7.58)\\nThesecondis\\xe2\\x88\\x80h, e, t : \\xe2\\x9f\\xa8h,r1, e\\xe2\\x9f\\xa9\\xe2\\x88\\xa7\\xe2\\x9f\\xa8e,r2, t\\xe2\\x9f\\xa9\\xe2\\x87\\x92\\xe2\\x9f\\xa8h,r3, t\\xe2\\x9f\\xa9(e.g.,given\\xe2\\x9f\\xa8Tsinghua,\\nlocated_in, Beijing\\xe2\\x9f\\xa9) and \\xe2\\x9f\\xa8Beijing, located_in, China\\xe2\\x9f\\xa9, we can infer\\nthat \\xe2\\x9f\\xa8Tsinghua, located_in, China\\xe2\\x9f\\xa9). And KALE de\\xef\\xac\\x81nes the second scoring\\nfunction as\\nE ( f2) = E (h,r1, e)E (e,r2, t)E (h,r3, t) \\xe2\\x88\\x92E (h,r1, e)E (e,r2, t) + 1.\\n(7.59)\\nThe joint training contains all positive formulae, including triple facts as well as\\nlogic rules. Note that for the consideration of logic rule qualities, KALE ranks all\\npossible logic rules by their truth values with pretrained TransE and manually \\xef\\xac\\x81lters\\nsome rules ranked at the top.\\n7.4\\nApplications\\nRecent years have witnessed the great thrive in knowledge-driven arti\\xef\\xac\\x81cial intelli-\\ngence, such as QA systems and chatbot. AI agents are expected to accurately and\\ndeeply understand user demands, and then appropriately and \\xef\\xac\\x82exibly give responses\\nand solutions. Such kind of work cannot be done without certain forms of knowledge.\\nTo introduce knowledge to AI agents, researchers \\xef\\xac\\x81rst extract knowledge from\\nheterogeneous information like plain texts, images, and structured knowledge bases.\\nThese various kinds of heterogeneous information are then fused and stored with\\ncertain structures like knowledge graphs. Next, the knowledge is projected to a low-\\ndimensional semantic space following some KRL methods. And \\xef\\xac\\x81nally, these learned\\nknowledge representations are utilized in various knowledge applications like infor-\\nmation retrieval and dialogue system. Figure7.16 demonstrates a brief pipeline of\\nknowledge-driven applications from scratch.\\nFrom the illustration, we can observe that knowledge graph representation learn-\\ning is the critical component in the whole knowledge-driven application\\xe2\\x80\\x99s pipeline.\\nIt bridges the gap between knowledge graphs that store knowledge and knowledge\\n7.4 Applications\\n197\\nHamlet is a tragedy\\nwritten by Willianm\\nShakespeare at an\\nuncertain date between\\n1599 and 1602,...\\nheterogeneous\\ninformation\\nknowledge graph\\nknowledge\\nconstruction\\nknowledge \\nrepresentation\\nKRL\\nmethods\\n embedding\\nmodels\\n knowledge\\napplications\\n information\\nretrieval\\n question\\nanswering\\n dialogue\\nsystem\\nFig. 7.16 An illustration of knowledge-driven applications\\napplications that use knowledge. Knowledge representations with distributed meth-\\nods, compared to those with symbolic methods, are able to solve the data sparsity and\\nmodeling the similarities between entities and relations. Moreover, embedding-based\\nmethods are convenient to be used with deep learning methods and are naturally \\xef\\xac\\x81t\\nfor the combination with heterogeneous information.\\nIn this section, we will introduce possible applications of knowledge represen-\\ntations mainly from two aspects. First, we will introduce the usage of knowledge\\nrepresentations for knowledge-driven applications, and then we will show the power\\nof knowledge representations for knowledge extraction and construction.\\n7.4.1\\nKnowledge Graph Completion\\nKnowledge graph completion aims to build structured knowledge bases by extract-\\ning knowledge from heterogeneous sources such as plain texts, existing knowledge\\nbases, and images. Knowledge construction consists of several subtasks like relation\\nextraction and information extraction, making the fundamental step in the whole\\nknowledge-driven framework.\\nRecently, automatic knowledge construction has attracted considerable attention\\nsince it is incredibly time consuming and labor intensive to deal with enormous\\nexisting and new information. In the following section, we will introduce some\\nexplorations on neural relation extraction, and concentrate on the combination of\\nknowledge representations.\\n7.4.1.1\\nKnowledge Representations for Relation Extraction\\nRelation extraction focuses on predicting the correct relation between two entities\\ngiven a short plain text containing the two entities. Generally, all relations to predict\\nare prede\\xef\\xac\\x81ned, which is different to open information extraction. Entities are usually\\nmarked with named entity recognition systems or extracted according to anchor texts,\\nor automatically generated via distance supervision [50].\\n198\\n7\\nWorld Knowledge Representation\\nKG\\nFact\\nText\\nWord\\nPosition\\nEncoder\\nPlaceOfBirth\\nwas\\nborn\\nin\\n[Mark Twain]\\nMark Twain\\nh\\n+\\n=\\nr\\nt\\n,\\n,\\n[Florida]\\nFlorida\\nFig. 7.17 The architecture of joint representation learning framework for knowledge acquisition\\nConventional methods for relation extraction and classi\\xef\\xac\\x81cation are mainly based\\non statistical machine learning, which strongly depends on the qualities of extracted\\nfeatures. Zeng et al. [96] \\xef\\xac\\x81rst introduce CNN to relation classi\\xef\\xac\\x81cation and achieve\\ngreat improvements. Lin et al. [40] further improves neural relation extraction models\\nwith attention-based models over instances.\\nHan et al. [27, 28] propose a novel joint representation learning framework for\\nknowledge acquisition. The key idea is that the joint model learns knowledge and text\\nrepresentations within a uni\\xef\\xac\\x81ed semantic space via KG-text alignments. Figure7.17\\nshows the brief framework of the KG-text joint model. For the text part, the sen-\\ntence with two entities Mark Twain and Florida is regarded as the input for\\na CNN encoder, and the output of CNN is considered to be the latent relation\\nplace_of_birth of this sentence. While for the KG part, entity and relation\\nrepresentations are learned via translation-based methods. The learned representa-\\ntions of KG and text parts are aligned during training. This work is the \\xef\\xac\\x81rst attempt\\nto encode knowledge representations from existing KGs to knowledge construc-\\ntion tasks and achieves improvements in both knowledge completion and relation\\nextraction.\\n7.4 Applications\\n199\\nFig. 7.18 The architecture of KNET model\\n7.4.2\\nKnowledge-Guided Entity Typing\\nEntity typing is the task of detecting semantic types for a named entity (or entity men-\\ntion) in plain text. For example, given a sentence Jordan played 15 seasons\\nin the NBA,entitytypingaimstoinferthat Jordaninthissentenceisa person,\\nan athlete, and even a basketball player. Entity typing is important for\\nnamed entity disambiguation since it can narrow down the range of candidates for an\\nentity mention [10]. Moreover, entity typing also bene\\xef\\xac\\x81ts massive Natural Language\\nProcessing (NLP) tasks such as relation extraction [46], question answering [90],\\nand knowledge base population [9].\\nConventional named entity recognition models [69, 73] typically classify entity\\nmentions into a small set of coarse labels (e.g., person, organization,\\nlocation, and others). Since these entity types are too coarse grained for many\\nNLP tasks, a number of works [15, 41, 94, 95] have been proposed to introduce a\\nmuch larger set of \\xef\\xac\\x81ne-grained types, which are typically subtypes of those coarse-\\ngrained types. Previous \\xef\\xac\\x81ne-grained entity typing methods usually derive features\\nusing NLP tools such as POS tagging and parsing, and inevitably suffer from error\\npropagation. Dong et al. [18] make the \\xef\\xac\\x81rst attempt to explore deep learning in entity\\ntyping. The method only employs word vectors as features, discarding complicated\\nfeature engineering. Shimaoka et al. [63] further introduce the attention scheme into\\nneural models for \\xef\\xac\\x81ne-grained entity typing.\\nNeural models have achieved state-of-the-art performance for \\xef\\xac\\x81ne-grained entity\\ntyping. However, these methods face the following nontrivial challenges:\\n(1) Entity-Context Separation. Existing methods typically encode context\\nwords without utilizing crucial correlations between entity and context. How-\\never, it is intuitive that the importance of words in the context for entity typ-\\n200\\n7\\nWorld Knowledge Representation\\ning is signi\\xef\\xac\\x81cantly in\\xef\\xac\\x82uenced by which entity mentions we concern about. For\\nexample, in a sentence In 1975, Gates and Paul Allen co-founded\\nMicrosoft, which became the world\\xe2\\x80\\x99s largest PC software\\ncompany, the word company is much more important for determining the type of\\nMicrosoft than for the type of Gates.\\n(2) Entity-Knowledge Separation. Existing methods only consider text informa-\\ntion of entity mentions for entity typing. In fact, Knowledge Graphs (KGs) provide\\nrich and effective additional information for determining entity types. For example,\\nin the above sentence In 1975, Gates ... Microsoft ... company,\\neven if we have no type information of Microsoft in KG, entities similar to\\nMicrosoft (such as IBM) will also provide supplementary information.\\nIn order to address the issues of entity-context separation and entity-knowledge\\nseparation, we propose Knowledge-guided Attention (KNET) Neural Entity Typing.\\nAs illustrated in Fig.7.18, KNET mainly consists of two parts. Firstly, KNET builds\\na neural network, including a Long Short-Term Memory (LSTM) and a fully con-\\nnected layer, to generate context and named entity representations. Secondly, KNET\\nintroduces knowledge attention to emphasize those critical words and improve the\\nquality of context representations. Here we introduce the knowledge attention in\\ndetail.\\nKnowledge graphs provide rich information about entities in the form of triples\\n\\xe2\\x9f\\xa8h,r, t\\xe2\\x9f\\xa9, where h and t are entities and r is the relation between them. Many KRL\\nworks have been devoted to encoding entities and relations into real-valued semantic\\nvector space based on triple information in KGs. KRL provides us with an ef\\xef\\xac\\x81cient\\nway to exploit KG information for entity typing.\\nKNET employs the most widely used KRL method TransE to obtain entity embed-\\nding e for each entity e. During the training scenario, it is known that the entity men-\\ntion m indicates the corresponding e in KGs with embedding e, and hence, KNET\\ncan directly compute knowledge attention as follows:\\n\\xce\\xb1KA\\ni\\n= f\\n\\x08\\neWKA\\n\\x0c\\xe2\\x88\\x92\\xe2\\x86\\x92\\nhi\\n\\xe2\\x86\\x90\\xe2\\x88\\x92\\nhi\\n\\r\\t\\n,\\n(7.60)\\nwhere WKA is a bilinear parameter matrix, and aKA\\ni\\nis the attention weight for the\\nith word.\\nKnowledge Attention in Testing. The challenge is that, in the testing scenario,\\nwe do not know the corresponding entity in the KG of a certain entity mention. A\\nsolution is to perform entity linking, but it will introduce linking errors. Besides,\\nin many cases, KGs may not contain the corresponding entities for many entity\\nmentions.\\nTo address this challenge, we build an additional text-based representation for\\nentities in KGs during training. Concretely, for an entity e and its context sentence\\ns, we encode its left and right context into cl and cr using an one-directional LSTM,\\nand further learn the text-based representation \\xcb\\x86e as follows:\\n7.4 Applications\\n201\\n\\xcb\\x86e = tanh\\n\\xe2\\x8e\\x9b\\n\\xe2\\x8e\\x9dW\\n\\xe2\\x8e\\xa1\\n\\xe2\\x8e\\xa3\\nm\\ncl\\ncr\\n\\xe2\\x8e\\xa4\\n\\xe2\\x8e\\xa6\\n\\xe2\\x8e\\x9e\\n\\xe2\\x8e\\xa0,\\n(7.61)\\nwhere W is the parameter matrix, and m is the mention representation. Note that,\\nLSTM used here is different from those in context representation in order to prevent\\ninterference. In order to bridge text-based and KG-based representations, in the\\ntraining scenario, we simultaneously learn \\xcb\\x86e by putting an additional component in\\nthe objective function:\\nOKG(\\xce\\xb8) = \\xe2\\x88\\x92\\n\\x02\\ne\\n\\xe2\\x88\\xa5e \\xe2\\x88\\x92\\xcb\\x86e\\xe2\\x88\\xa52.\\n(7.62)\\nIn this way, in the testing scenario, we can directly use Eq.7.61 to obtain the corre-\\nsponding entity representation and compute knowledge attention using Eq.7.60.\\n7.4.3\\nKnowledge-Guided Information Retrieval\\nThe emergence of large-scale knowledge graphs has motivated the development of\\nentity-oriented search, which utilizes knowledge graphs to improve search engines.\\nRecent progresses in entity-oriented search include better text representations with\\nentity annotations [61, 85], richer ranking features [14], entity-based connections\\nbetween query and documents [45, 84], and soft-match query and documents through\\nknowledgegraphrelationsorembeddings[19, 88].Theseapproachesbringinentities\\nand semantics from knowledge graphs and have greatly improved the effectiveness\\nof feature-based search systems.\\nAnother frontier of information retrieval is the development of neural ranking\\nmodels (neural-IR). Deep learning techniques have been used to learn distributed\\nrepresentations of queries and documents that capture their relevance relations\\n(representation-based) [62], or to model the query-document relevancy directly from\\ntheir word-level interactions (interaction-based) [13, 23, 87]. Neural-IR approaches,\\nespecially the interaction-based ones, have greatly improved the ranking accuracy\\nwhen large-scale training data are available [13].\\nEntity-oriented search and neural-IR push the boundary of search engines from\\ntwo different aspects. Entity-oriented search incorporates human knowledge from\\nentities and knowledge graph semantics. It has shown promising results on feature-\\nbased ranking systems. On the other hand, neural-IR leverages distributed repre-\\nsentations and neural networks to learn more sophisticated ranking models form\\nlarge-scale training data. Entity-Duet Neural Ranking Model (EDRM), as shown in\\nFig.7.19, incorporates entities in interaction-based neural ranking models. EDRM\\n\\xef\\xac\\x81rst learns the distributed representations of entities using their semantics from\\nknowledge graphs: descriptions and types. Then it follows a recent state-of-the-art\\nentity-oriented search framework, the word-entity duet [86], and matches documents\\nto queries with both bag-of-words and bag-of-entities. Instead of manual features,\\n202\\n7\\nWorld Knowledge Representation\\nObama\\nfamily\\ntree\\nObama\\nDescription\\nType\\nFamily Tree\\nDescription\\nType\\nAttention\\nCNN\\nCNN\\nQuery\\nDocument\\nUnigrams\\nBigrams\\nTrigrams\\nEnriched-entity\\nEmbedding\\nInteraction Matrix\\nSoft Match Feature\\nN-gram\\nEmbedding\\nEnriched-entity\\nEmbedding\\nKernel\\nPooling\\nFinal\\nRanking\\nScore\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nvwq\\nveq\\nvwd\\nved\\nMww\\nMew\\nMwe\\nMee\\nM\\n\\xce\\xa6 (M)\\nFig. 7.19 The architecture of EDRM model\\nEDRM uses interaction-based neural models [13] to match the query and documents\\nwith word-entity duet representations. As a result, EDRM combines entity-oriented\\nsearch and the interaction-based neural-IR; it brings the knowledge graph semantics\\nto neural-IR and enhances entity-oriented search with neural networks.\\n7.4.3.1\\nInteraction-Based Ranking Models\\nGiven a query q and a document d, interaction-based models \\xef\\xac\\x81rst build the word-\\nlevel translation matrix between q and d. The translation matrix describes word-\\npairs similarities using word correlations, which are captured by word embedding\\nsimilarities in interaction-based models.\\nTypically, interaction-based ranking models \\xef\\xac\\x81rst map each word w in q and d to\\nan L-dimensional embedding vw.\\nvw = Embw(w).\\n(7.63)\\nIt then constructs the interaction matrix M based on query and document embed-\\ndings. Each element Mi j in the matrix, compares the ith word in q and the jth word\\nin d, e.g., using the cosine similarity of word embeddings:\\nMi j = cos(vwq\\ni , vwd\\nj ).\\n(7.64)\\nWith the translation matrix describing the term level matches between query and\\ndocuments, the next step is to calculate the \\xef\\xac\\x81nal ranking score from the matrix. Many\\napproaches have been developed in interaction-based neural ranking models, but in\\ngeneral, that would include a feature extractor on M and then one or several ranking\\nlayers to combine the features to the ranking score.\\n7.4 Applications\\n203\\n7.4.3.2\\nSemantic Entity Representation\\nEDRM incorporates the semantic information about an entity from the knowledge\\ngraphs into its representation. The representation includes three embeddings: entity\\nembedding, description embedding, and type embedding, all in L dimension and are\\ncombined to generate the semantic representation of the entity.\\nEntity Embedding uses an L-dimensional embedding layer Embe to get the entity\\nembedding e for e:\\nve = Embe(e).\\n(7.65)\\nDescription Embedding encodes an entity description which contains m words\\nand explains the entity. EDRM \\xef\\xac\\x81rst employs the word embedding layer Embv to\\nembed the description word v to v. Then it combines all embeddings in the text to\\nan embedding matrix V. Next, it leverages convolutional \\xef\\xac\\x81lters to slide over the text\\nand compose the l length n-gram as g j\\ne:\\ng j\\ne = ReLU(WCNN \\xc2\\xb7 V j: j+h\\nw\\n+ bCNN),\\n(7.66)\\nwhere WCNN and bCNN are two parameters of the convolutional \\xef\\xac\\x81lter.\\nThen we use max pooling after the convolution layer to generate the description\\nembedding vdes\\ne :\\nvdes\\ne\\n= max(g1\\ne, ..., g j\\ne, ..., gm\\ne ).\\n(7.67)\\nType Embedding encodes the categories of entities. Each entity e has n kinds of\\ntypes Fe = { f1, ..., f j, ..., fn}. EDRM \\xef\\xac\\x81rst gets the f j embedding v f j through the\\ntype embedding layer Embtype:\\nvemb\\nf j\\n= Embtype(e).\\n(7.68)\\nThen EDRM utilizes an attention mechanism to combine entity types to the type\\nembedding vtype\\ne\\n:\\nvtype\\ne\\n=\\nn\\n\\x02\\nj\\n\\xce\\xb1 jv f j,\\n(7.69)\\nwhere \\xce\\xb1 j is the attention score, calculated as:\\n\\xce\\xb1 j =\\nexp(y j)\\n\\nn\\nl exp(yl),\\n(7.70)\\ny j =\\n\\x08\\x02\\ni\\nWbowvti\\n\\t\\n\\xc2\\xb7 v f j,\\n(7.71)\\n204\\n7\\nWorld Knowledge Representation\\nwhere y j is the dot product of the query or document representation and type embed-\\nding f j. We leverage bag-of-words for query or document encoding. Wbow is a\\nparameter matrix.\\nCombination. The three embeddings are combined by a linear layer to generate\\nthe semantic representation of the entity:\\nvsem\\ne\\n= vemb\\ne\\n+ We[vdes\\ne ; vtype\\ne\\n]\\xe2\\x8a\\xa4+ be,\\n(7.72)\\nin which We is an L \\xc3\\x97 2L matrix and be is an L-dimensional vector.\\n7.4.3.3\\nNeural Entity-Duet Framework\\nWord-entity duet [86] is a recently developed framework in entity-oriented search. It\\nutilizes the duet representation of bag-of-words and bag-of-entities to match question\\nq and document d with handcrafted features. This work introduces it to neural-IR.\\nThey \\xef\\xac\\x81rst construct bag-of-entities qe and de with entity annotation as well as\\nbag-of-words qw and dw for q and d. The duet utilizes a four-way interaction: query\\nwords to document words (qw-dw), query words to documents entities (qw-de), query\\nentities to document words (qe-dw), and query entities to document entities (qe-de).\\nInstead of features, EDRM uses a translation layer that calculates the similarity\\nbetween a pair of query-document terms: (vi\\nwq or vi\\neq) and (v j\\nwd or v j\\ned). It constructs the\\ninteraction matrix M = {Mww, Mwe, Mew, Mee}. And Mww, Mwe, Mew, Mee denote\\ninteractions of qw-dw, qw-de, qe-dw, qe-de respectively. And elements in them are\\nthe cosine similarities of corresponding terms:\\nMi j\\nww = cos(vi\\nwq, v j\\nwd); Mi j\\nee = cos(vi\\neq, v j\\ned)\\nMi j\\new = cos(vi\\neq, v j\\nwd); Mi j\\nwe = cos(vi\\nwq, v j\\ned).\\n(7.73)\\nThe \\xef\\xac\\x81nal ranking feature \\r(M) is a concatenation of four cross matches (\\xcf\\x86(M)):\\n\\r(M) = [\\xcf\\x86(Mww); \\xcf\\x86(Mwe); \\xcf\\x86(Mew); \\xcf\\x86(Mee)],\\n(7.74)\\nwhere the \\xcf\\x86 can be any function used in interaction-based neural ranking models.\\nThe entity-duet presents an effective way to crossly match query and document\\nin entity and word spaces. In EDRM, it introduces the knowledge graph semantics\\nrepresentations into neural-IR models.\\nThe duet translation matrices provided by EDRM can be plugged into any standard\\ninteraction-basedneuralrankingmodelssuchasK-NRM[87]andConv-KNRM[13].\\nWith suf\\xef\\xac\\x81cient training data, the whole model is optimized end-to-end with back-\\npropagation. During the process, the integration of the knowledge graph semantics,\\nentity embedding, description embeddings, type embeddings, and matching with\\nentities is learned jointly with the ranking neural network.\\n7.4 Applications\\n205\\n7.4.4\\nKnowledge-Guided Language Models\\nKnowledge is an important external information for language modeling. It is because\\nthe statistical co-occurrences cannot instruct the generation of all kinds of knowledge,\\nespecially for those named entities with low frequencies. Researchers try to incorpo-\\nrate external knowledge into language models for better performance on generation\\nand representation.\\n7.4.4.1\\nNKLM\\nLanguage models aim to learn the probability distribution over sequences of words,\\nwhich is a classical and essential NLP task widely studied. Recently, sequence to\\nsequence neural models (seq2seq) are blooming and widely utilized in sequential\\ngenerative tasks like machine translation [68] and image caption generation [72].\\nHowever, most seq2seq models have signi\\xef\\xac\\x81cant limitations when modeling and using\\nbackground knowledge.\\nTo address this problem, Ahn et al. [1] propose a Neural Knowledge Language\\nModel (NKLM) that considers knowledge provided by knowledge graphs when\\ngenerating natural language sequences with RNN language models. The key idea is\\nthat NKLM has two ways to generate a word. The \\xef\\xac\\x81rst is the same way as conventional\\nseq2seq models that generate a \\xe2\\x80\\x9cvocabulary word\\xe2\\x80\\x9d according to the probabilities of\\nsoftmax, and the second is to generate a \\xe2\\x80\\x9cknowledge word\\xe2\\x80\\x9d according to the external\\nknowledge graphs.\\nSpeci\\xef\\xac\\x81cally, the NKLM model takes LSTM as the framework of generating\\n\\xe2\\x80\\x9cvocabulary word\\xe2\\x80\\x9d. For external knowledge graph information, NKLM denotes the\\ntopic knowledge as K = {a1, . . . a|K |}, in which ai represents the entities (i.e.,\\nnamed as \\xe2\\x80\\x9ctopic\\xe2\\x80\\x9d in [1]) that appear in the same triple of a certain entity. At each step\\nt, NKLM takes both \\xe2\\x80\\x9cvocabulary word\\xe2\\x80\\x9d wv\\nt\\xe2\\x88\\x921 and \\xe2\\x80\\x9cknowledge word\\xe2\\x80\\x9d wo\\nt\\xe2\\x88\\x921 as well as\\nthe fact at\\xe2\\x88\\x921 predicted at step t \\xe2\\x88\\x921 as the inputs of LSTM. Next, the hidden state of\\nLSTM ht is combined with the knowledge context e to get the fact key kt via an MLP\\nmodule. The knowledge context ek derives from the mean embeddings of all related\\nfacts of fact k. The fact key kt is then used to extract the most appropriate fact at from\\nthe corresponding topic knowledge. And \\xef\\xac\\x81nally, the selected fact at is combined with\\nhidden state ht to predict (1) both \\xe2\\x80\\x9cvocabulary word\\xe2\\x80\\x9d wv\\nt and \\xe2\\x80\\x9cknowledge word\\xe2\\x80\\x9d wo\\nt ,\\nand (2) which word to generate at this step. The architecture of NKLM is shown in\\nFig.7.20.\\nThe NKLM model explores a novel neural model that combines the symbolic\\nknowledge information in external knowledge graphs with seq2seq language models.\\nHowever, the topic of knowledge is given when generating natural languages, which\\nmakes NKLM less practical and scalable for more general free talks. Nevertheless,\\nwe still believe that it is promising to encode knowledge into language models with\\nsuch methods.\\n206\\n7\\nWorld Knowledge Representation\\nFig. 7.20 The architecture\\nof NKLM model\\nTopic Knowledge\\ncopy\\nfact search\\nxt\\na1\\na2\\na3\\na4\\n\\xe2\\x80\\xa6\\naN\\nNaF\\no1\\no2\\no3\\no4\\n\\xe2\\x80\\xa6\\noN\\nht\\ne\\nkt\\nat\\nwt\\nv\\nwt\\no\\nht-1\\nat-1\\nwv\\nt-1\\nwo\\nt-1\\nLSTM\\nzt\\n7.4.4.2\\nERNIE\\nPretrained language models like BERT [17] have a strong ability to represent\\nlanguage information from text. With rich language representation, pretrained\\nmodels obtain state-of-the-art results on various NLP applications. However, the\\nexisting pretrained language models rarely consider incorporating external knowl-\\nedge to provide related background information for better language understanding.\\nFor example, given a sentence Bob Dylan wrote Blowin\\xe2\\x80\\x99 in the Wind\\nand Chronicles: Volume One,\\nwithout\\nknowing\\nBlowin\\xe2\\x80\\x99 in the\\nWind and Chronicles: Volume One are song and book respectively, it\\nis dif\\xef\\xac\\x81cult to recognize the two occupations of Bob Dylan, i.e., songwriter\\nand writer.\\nTo enhance language representation models with external knowledge, Zhang et\\nal. [100] propose an enhanced language representation model with informative enti-\\nties (ERNIE). Knowledge Graphs (KGs) are important external knowledge resources,\\nand they think informative entities in KGs can be the bridge to enhance language\\nrepresentation with knowledge. ERNIE considers overcoming two main challenges\\nfor incorporating external knowledge: Structured Knowledge Encoding and Hetero-\\ngeneous Information Fusion.\\nFor extracting and encoding knowledge information, ERNIE \\xef\\xac\\x81rstly recognizes\\nnamed entity mentions in text and then aligns these mentions to their corresponding\\nentities in KGs. Instead of directly using the graph-based facts in KGs, ERNIE\\nencodes the graph structure of KGs with knowledge embedding algorithms like\\nTransE [7], and then takes the informative entity embeddings as input. Based on the\\nalignments between text and KGs, ERNIE integrates entity representations in the\\nknowledge module into the underlying layers of the semantic module.\\n7.4 Applications\\n207\\nbob\\ndylan\\nwrote\\n\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\n1962\\nMulti-Head Attention\\nMulti-Head Attention \\nInformation Fusion\\n\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\nToken Input\\nEntity Input\\nToken Output\\nEntity Output\\nBob Dylan wrote Blowin\\xe2\\x80\\x99 in the Wind in 1962\\nblow\\nMulti-Head\\nAttention\\nFeed\\nForward\\nNx\\nMulti-Head\\nAttention\\nInformation\\nFusion\\nToken Input\\nMulti-Head\\nAttention\\nEntity Input\\nMx\\nToken Output\\nEntity Output\\nBlowin\\xe2\\x80\\x99 in the Wind\\nBob Dylan\\nAggregator\\nTransformer\\nAggregator\\n(a) Model Achitecture\\n(b) Aggregator\\nK-Encoder\\nT-Encoder\\nw1\\ni-1\\nw2\\ni-1\\nw3\\ni-1\\nw4\\ni-1\\nwn\\ni-1\\ne1\\ni-1\\ne2\\ni-1\\nw1\\ni-1\\nw2\\ni-1\\nw3\\ni-1\\nw4\\ni-1\\nwn\\ni-1\\ne1\\ni-1\\ne2\\ni-1\\ne1\\ni-1\\ne2\\ni-1\\n~\\n~\\n~\\n~\\n~\\n~\\n~\\n~\\n~\\n\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7\\nw1\\ni\\nw2\\ni\\nw3\\ni\\nw4\\ni\\nwn\\ni\\ne1\\ni\\ne2\\ni\\ne1\\ni\\ne2\\ni\\nFig. 7.21 The architecture of ERNIE model\\nSimilar to BERT, ERNIE adopts the masked language model and the next sen-\\ntence prediction as the pretraining objectives. Besides, for the better fusion of textual\\nand knowledge features, ERNIE uses a new pretraining objective (denoising entity\\nauto-encoder) by randomly masking some of the named entity alignments in the\\ninput text and training to select appropriate entities from KGs to complete the align-\\nments. Unlike the existing pre-trained language representation models only utilizing\\nlocal context to predict tokens, these objectives require ERNIE to aggregate both\\ncontext and knowledge facts for predicting both tokens and entities, and lead to a\\nknowledgeable language representation model.\\nFigure7.21 is the overall architecture. The left part shows that ERNIE consists\\nof two encoders (T-Encoder and K-Encoder), where T-Encoder is stacked by several\\nclassical transformer layers and K-Encoder is stacked by the new aggregator layers\\ndesigned for knowledge integration. The right part is the detail of the aggregator layer.\\nIn the aggregator layer, the input token embeddings and entity embeddings from the\\npreceding aggregator are fed into two multi-head self-attention, respectively. Then,\\nthe aggregator adopts an information fusion layer for the mutual integration of the\\ntoken and entity sequence and computes the output embedding for each token and\\nentity.\\nERNIE explores how to incorporate knowledge information into language repre-\\nsentation models. The experimental results demonstrate that ERNIE has more pow-\\nerful abilities of both denoising distantly supervised data and \\xef\\xac\\x81ne-tuning on limited\\ndata than BERT.\\n7.4.4.3\\nKALM\\nPre-trained language models can do many tasks without supervised training data, like\\nreading comprehension, summarization, and translation [60]. However, traditional\\nlanguage models are unable to ef\\xef\\xac\\x81ciently model entity names observed in text. To\\n208\\n7\\nWorld Knowledge Representation\\nsolve this problem, Liu et al. [42] propose a new language model architecture, called\\nKnowledge-Augmented Language Model (KALM), to use the entity types of words\\nfor better language modeling.\\nKALM is a language model with the option to generate words from a set of entities\\nfrom a knowledge database. An individual word can either come from a general\\nword dictionary as in the traditional language model or be generated as a name of an\\nentity from a knowledge database. The training objectives just supervise the output\\nand ignore the decision of the word type. Entities in the knowledge database are\\npartitioned by type and they use the database to build the types of words. According\\nto the context observed so far, the model decides whether the word is a general term\\nor a named entity in a given type. Thus, KALM learns to predict whether the context\\nobserved is indicative of a named entity and what tokens are likely to be entities of\\na given type.\\nWith the language modeling, KALM learns a named entity recognizer without any\\nexplicit supervision by using only plain text and the potential types of words. And,\\nit achieves a comparable performance with the state-of-the-art supervised methods.\\n7.4.5\\nOther Knowledge-Guided Applications\\nKnowledge enables AI agents to understand, infer, and address user demands, which\\nisessentialinmostknowledge-drivenapplicationslikeinformationretrieval,question\\nanswering, and dialogue system. The behavior of AI agents will be more reasonable\\nand accurate with the favor of knowledge representations. In the following subsec-\\ntions, we will introduce the great improvements made by knowledge representation\\nin question answering.\\n7.4.5.1\\nKnowledge-Guided Question Answering\\nQuestion answering aims to give correct answers according to users\\xe2\\x80\\x99 questions,\\nwhich needs the capabilities of both natural language understanding of questions\\nand inference on answer selection. Therefore, combining knowledge with question\\nanswering is a straightforward application for knowledge representations. Most con-\\nventional question answering systems directly utilize knowledge graphs as certain\\ndatabases, ignoring the latent relationships between entities and relations. Recently,\\nwith the thriving in deep learning, explorations have focused on neural models for\\nunderstanding questions and even generating answers.\\nConsideringthe\\xef\\xac\\x82exibilityanddiversityofgeneratedanswersinnaturallanguages,\\nYin et al. [93] propose a neural Generative Question Answering model (GENQA),\\nwhich explores on generating answers to simple factoid questions in natural lan-\\nguages. Figure7.22 demonstrates the work\\xef\\xac\\x82ow of GENQA. First, a bidirectional\\nRNN is regarded as the Interpreter to transform question q from natural language\\nto compressed representation Hq. Next, Enquirer takes HQ as the key to rank rel-\\n7.4 Applications\\n209\\nFig. 7.22 The architecture\\nof GENQA model\\nQ:\\xe2\\x80\\x9cHow tall is Yao Ming?\\xe2\\x80\\x9d\\nA:\\xe2\\x80\\x9cHe is 2.29m  and visible from space\\xe2\\x80\\x9d\\nAnswerer\\nGenerator\\nEnquirer\\ninterpreter\\nLong-term Memory\\n(Knowledge-base)\\nAtt.Model\\nShort-term Mernory\\nevant triples facts of q in knowledge graphs and retrieves possible entities in rq.\\nFinally, Answerer combines Hq and rq to generate answers in the form of natural\\nlanguages. Similar to [1], at each step, Answerer \\xef\\xac\\x81rst decides whether to generate\\ncommon words or knowledge words according to a logistic regression model. For\\ncommon words, Answerer acts in the same way as RNN decoders with Hq selected\\nby attention-based methods. As for knowledge words, Answerer directly generates\\nentities with higher ranks.\\nThere are gradually more efforts focusing on encoding knowledge representations\\ninto knowledge-driven tasks like information retrieval and dialogue systems. How-\\never, how to \\xef\\xac\\x82exibly and effectively combine knowledge with AI agents remains to\\nbe explored in the future.\\n7.4.5.2\\nKnowledge-Guided Recommendation System\\nDue to the rapid growth of web information, recommendation systems have been\\nplaying an essential role in the web application. The recommendation system aims\\nto predict the \\xe2\\x80\\x9crating\\xe2\\x80\\x9d or \\xe2\\x80\\x9cpreference\\xe2\\x80\\x9d that users may give to items. And since KGs\\ncan provide rich information, including both structured and unstructured data, rec-\\nommendation systems have utilized more and more knowledge from KGs to enrich\\ntheir contexts.\\nCheekula et al. [11] explore to utilize the hierarchical knowledge from the DBpe-\\ndia category structure in the recommendation system and employs the spreading\\nactivation algorithm to identify entities of interest to the user. Besides, Passant [56]\\nmeasures the semantic relatedness of the artist entity in a KG to build music recom-\\nmendation systems. However, most of these systems mainly investigate the problem\\nby leveraging the structure of KGs. Recently, with the development of representation\\n210\\n7\\nWorld Knowledge Representation\\nlearning, [98] proposes to jointly learn the latent representations in a collaborative\\n\\xef\\xac\\x81ltering recommendation system as well as entities\\xe2\\x80\\x99 representations in KGs.\\nExcept the tasks stated above, there are gradually more efforts focusing on encod-\\ning knowledge graph representations into other tasks such as dialogue system [37,\\n103], entity disambiguation [20, 31], knowledge graph alignment [12, 102], depen-\\ndency parsing [35], etc. Moreover, the idea of KRL has also motivated the research\\non visual relation extraction [2, 99] and social relation extraction [71].\\n7.5\\nSummary\\nIn this chapter, we \\xef\\xac\\x81rst introduce the concept of the knowledge graph. Knowledge\\ngraph contains both entities and the relationships among them in the form of triple\\nfacts, providing an effective way of human beings learning and understanding the\\nreal world. Next, we introduce the motivations of knowledge graph representation,\\nwhich is considered as a useful and convenient method for a large amount of data and\\nis widely explored and utilized in multiple knowledge-based tasks and signi\\xef\\xac\\x81cantly\\nimproves the performance. And we describe existing approaches for knowledge\\ngraph representation. Further, we discuss several advanced approaches that aim to\\ndeal with the current challenges of knowledge graph representation. We also review\\nthe real-world applications of knowledge graph representation such as language\\nmodeling, question answering, information retrieval, and recommendation systems.\\nFor further understanding of knowledge graph representation, you can \\xef\\xac\\x81nd more\\nrelated papers in this paper list https://github.com/thunlp/KRLPapers. There are also\\nsome recommended surveys and books including:\\n\\xe2\\x80\\xa2 Bengio et al. Representation learning: A review and new perspectives [4].\\n\\xe2\\x80\\xa2 Liu et al. Knowledge representation learning: A review [47].\\n\\xe2\\x80\\xa2 Nickel et al. A review of relational machine learning for knowledge graphs [52].\\n\\xe2\\x80\\xa2 Wang et al. Knowledge graph embedding: A survey of approaches and applications\\n[74].\\n\\xe2\\x80\\xa2 Ji et al. A survey on knowledge graphs: representation, acquisition and applications\\n[34].\\nIn the future, for better knowledge graph representation, there are some directions\\nrequiring further efforts:\\n(1) Utilizing More Knowledge. Current KRL approaches focus on represent-\\ning triple-based knowledge from world knowledge graphs such as Freebase, Wiki-\\ndata, etc. In fact, there are various kinds of knowledge in the real world such as\\nfactual knowledge, event knowledge, commonsense knowledge, etc. What\\xe2\\x80\\x99s more,\\nthe knowledge is stored with different formats, such as attributions, quanti\\xef\\xac\\x81er, text,\\nand so on. The researchers have formed a consensus that utilizing more knowledge\\nis a potential way toward more interpretable and intelligent NLP. Some existing\\nworks [44, 82] have made some preliminary attempts of utilizing more knowledge\\n7.5 Summary\\n211\\nin KRL. Beyond these works, is it possible to represent different knowledge in a\\nuni\\xef\\xac\\x81ed semantic space, which can be easily applied in downstream NLP tasks?\\n(2) Performing Deep Fusion of knowledge and language. There is no doubt\\nthat the joint learning of knowledge and language information can further bene\\xef\\xac\\x81t\\ndownstream NLP tasks. Existing works [76, 89, 97] have preliminarily veri\\xef\\xac\\x81ed the\\neffectiveness of joint learning. Recently, ERINE [100] and KnowBERT [57] further\\nprovide us a novel perspective to fuse knowledge and language in pretraining. Soares\\net al. [64] learn the relational similarity in text with the guidance of KGs, which is\\nalso a pioneer of knowledge fusion. Besides designing novel pretraining objectives,\\nwe could also design novel model architectures for downstream tasks, which are\\nmore suitable to utilize KRL, such as memory-based models [48, 91] and graph\\nnetwork-based models [66]. Nevertheless, it still remains an unsolved problem for\\neffectively performing the deep fusion of knowledge and language.\\n(3)OrientingHeterogeneousModalities.WiththefastdevelopmentoftheWorld\\nWide Web, the data size of audios, images, and videos on the Web have become\\nlarger and larger, which are also important resources for KRL besides texts. Some\\npioneer works [51, 81] explore to learn knowledge representations on a multi-modal\\nknowledge graph, but are still preliminary attempts. Intuitively, audio and visual\\nknowledge can provide complementary information, which bene\\xef\\xac\\x81ts related NLP\\ntasks. To the best of our knowledge, there still lacks research on applying multi-modal\\nKRL in downstream tasks. How to ef\\xef\\xac\\x81ciently and effectively integrate multi-modal\\nknowledge is becoming a critical and challenging problem for KRL.\\n(4) Exploring Knowledge Reasoning. Most of the existing KRL methods rep-\\nresent knowledge information in low-dimensional semantic space, which is feasible\\nfor the computation of complex knowledge graphs in neural-based NLP models.\\nAlthough bene\\xef\\xac\\x81ting from the usability of low-dimensional embeddings, KRL cannot\\nperform explainable reasoning such as symbolic rules, which is of great importance\\nfor downstream NLP tasks. Recently, there has been increasing interest in the com-\\nbination of embedding methods and symbolic reasoning methods [26, 59], aiming at\\ntaking both advantages of them. Beyond these works, there remain lots of unsolved\\nproblems for developing better knowledge reasoning ability for KRL.\\nReferences\\n1. Sungjin Ahn, Heeyoul Choi, Tanel P\\xc3\\xa4rnamaa, and Yoshua Bengio. A neural knowledge lan-\\nguage model. arXiv preprint arXiv:1608.00318, 2016.\\n2. Stephan Baier, Yunpu Ma, and Volker Tresp. Improving visual relationship detection using\\nsemantic modeling of scene descriptions. In Proceedings of ISWC, 2017.\\n3. Islam Beltagy and Raymond J Mooney. Ef\\xef\\xac\\x81cient markov logic inference for natural language\\nsemantics. In Proceedings of AAAI Workshop, 2014.\\n4. Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and\\nnew perspectives. TPAMI, 35(8):1798\\xe2\\x80\\x931828, 2013.\\n5. Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. Joint learning of words\\nand meaning representations for open-text semantic parsing. In Proceedings of AISTATS,\\n2012.\\n212\\n7\\nWorld Knowledge Representation\\n6. Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. A semantic matching\\nenergy function for learning with multi-relational data. Machine Learning, 94(2):233\\xe2\\x80\\x93259,\\n2014.\\n7. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana\\nYakhnenko. Translating embeddings for modeling multi-relational data. In Proceedings of\\nNeurIPS, 2013.\\n8. Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. Learning structured\\nembeddings of knowledge bases. In Proceedings of AAAI, 2011.\\n9. Andrew Carlson, Justin Betteridge, Richard C Wang, Estevam R Hruschka Jr, and Tom M\\nMitchell. Coupled semi-supervised learning for information extraction. In Proceedings of\\nWSDM, 2010.\\n10. Mohamed Chabchoub, Michel Gagnon, and Amal Zouaq. Collective disambiguation and\\nsemantic annotation for entity linking and typing. In Proceedings of SWEC, 2016.\\n11. Siva Kumar Cheekula, Pavan Kapanipathi, Derek Doran, Prateek Jain, and Amit P Sheth.\\nEntity recommendations using hierarchical knowledge bases. 2015.\\n12. Muhao Chen, Yingtao Tian, Mohan Yang, and Zaniolo Carlo. Multilingual knowledge graph\\nembeddings for cross-lingual knowledge alignment. In Proceedings of IJCAI, 2017.\\n13. Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. Convolutional neural networks\\nfor soft-matching n-grams in ad-hoc search. In Proceedings of WSDM, 2018.\\n14. Jeffrey Dalton, Laura Dietz, and James Allan. Entity query feature expansion using knowledge\\nbase links. In Proceedings of SIGIR, 2014.\\n15. Luciano Del Corro, Abdalghani Abujabal, Rainer Gemulla, and Gerhard Weikum. Finet:\\nContext-aware \\xef\\xac\\x81ne-grained named entity typing. In Proceedings of EMNLP, 2015.\\n16. Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional\\n2d knowledge graph embeddings. In Proceedings of AAAI, 2018.\\n17. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019.\\n18. Li Dong, Furu Wei, Hong Sun, Ming Zhou, and Ke Xu. A hybrid neural model for type\\nclassi\\xef\\xac\\x81cation of entity mentions. In Proceedings of IJCAI, 2015.\\n19. Faezeh Ensan and Ebrahim Bagheri. Document retrieval model through semantic linking. In\\nProceedings of WSDM, 2017.\\n20. Wei Fang, Jianwen Zhang, Dilin Wang, Zheng Chen, and Ming Li. Entity disambiguation by\\nknowledge and text jointly embedding. In Proceedings of CoNLL, 2016.\\n21. Alberto Garc\\xc3\\xada-Dur\\xc3\\xa1n, Antoine Bordes, and Nicolas Usunier. Composing relationships with\\ntranslations. In Proceedings of EMNLP, 2015.\\n22. Kelvin Gu, John Miller, and Percy Liang. Traversing knowledge graphs in vector space. In\\nProceedings of EMNLP, 2015.\\n23. Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. Semantic matching by non-linear\\nword transportation for information retrieval. In Proceedings of CIKM, 2016.\\n24. Shu Guo, Quan Wang, Lihong Wang, Bin Wang, and Li Guo. Jointly embedding knowledge\\ngraphs and logical rules. In Proceedings of EMNLP, 2016.\\n25. Petr H\\xc3\\xa1jek. Metamathematics of fuzzy logic, volume 4. Springer Science & Business Media,\\n1998.\\n26. Will Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. Embedding\\nlogical queries on knowledge graphs. In Proceedings of NIPS, 2018.\\n27. Xu Han, Zhiyuan Liu, and Maosong Sun. Joint representation learning of text and knowledge\\nfor knowledge graph completion. arXiv preprint arXiv:1611.04125, 2016.\\n28. Xu Han, Zhiyuan Liu, and Maosong Sun. Neural knowledge acquisition via mutual attention\\nbetween knowledge graph and text. In Proceedings of AAAI, pages 4832\\xe2\\x80\\x934839, 2018.\\n29. Katsuhiko Hayashi and Masashi Shimbo. On the equivalence of holographic and complex\\nembeddings for link prediction. In Proceedings of ACL, 2017.\\n30. Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao. Learning to represent knowledge graphs\\nwith gaussian embedding. In Proceedings of CIKM, 2015.\\nReferences\\n213\\n31. Hongzhao Huang, Larry Heck, and Heng Ji. Leveraging deep neural networks and knowledge\\ngraphs for entity disambiguation. arXiv preprint arXiv:1504.07678, 2015.\\n32. Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. Knowledge graph embedding\\nvia dynamic mapping matrix. In Proceedings of ACL, 2015.\\n33. Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao. Knowledge graph completion with adaptive\\nsparse transfer matrix. In Proceedings of AAAI, 2016.\\n34. Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S Yu. A survey on knowl-\\nedge graphs: Representation, acquisition and applications. arXiv preprint arXiv:2002.00388,\\n2020.\\n35. A-Yeong Kim, Hyun-Je Song, Seong-Bae Park, and Sang-Jo Lee. A re-ranking model for\\ndependency parsing with knowledge graph embeddings. In Proceedings of IALP, 2015.\\n36. Denis Krompa\\xc3\\x9f, Stephan Baier, and Volker Tresp. Type-constrained representation learning\\nin knowledge graphs. In Proceedings of ISWC, 2015.\\n37. Phong Le, Marc Dymetman, and Jean-Michel Renders. Lstm-based mixture-of-experts for\\nknowledge-aware dialogues. In Proceedings of ACL Workshop, 2016.\\n38. Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, and Song Liu. Modeling\\nrelation paths for representation learning of knowledge bases. In Proceedings of EMNLP,\\n2015.\\n39. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation\\nembeddings for knowledge graph completion. In Proceedings of AAAI, 2015.\\n40. Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. Neural relation\\nextraction with selective attention over instances. In Proceedings of ACL, 2016.\\n41. Xiao Ling and Daniel S Weld. Fine-grained entity recognition. In Proceedings of AAAI, 2012.\\n42. Angli Liu, Jingfei Du, and Veselin Stoyanov. Knowledge-augmented language model and its\\napplication to unsupervised named-entity recognition. In Proceedings of NAACL, 2019.\\n43. Quan Liu, Hui Jiang, Andrew Evdokimov, Zhen-Hua Ling, Xiaodan Zhu, Si Wei, and\\nYu Hu. Probabilistic reasoning via deep learning: Neural association models. arXiv preprint\\narXiv:1603.07704, 2016.\\n44. Quan Liu, Hui Jiang, Zhen-Hua Ling, Xiaodan Zhu, Si Wei, and Yu Hu. Commonsense\\nknowledge enhanced embeddings for solving pronoun disambiguation problems in winograd\\nschema challenge. arXiv preprint arXiv:1611.04146, 2016.\\n45. Xitong Liu and Hui Fang. Latent entity space: A novel retrieval approach for entity-bearing\\nqueries. Information Retrieval Journal, 18(6):473\\xe2\\x80\\x93503, 2015.\\n46. Yang Liu, Kang Liu, Liheng Xu, Jun Zhao, et al. Exploring \\xef\\xac\\x81ne-grained entity type constraints\\nfor distantly supervised relation extraction. In Proceedings of COLING, 2014.\\n47. Zhiyuan Liu, Maosong Sun, Yankai Lin, and Ruobing Xie. Knowledge representation learn-\\ning: a review. JCRD, 53(2):247\\xe2\\x80\\x93261, 2016.\\n48. Todor Mihaylov and Anette Frank. Knowledgeable reader: Enhancing cloze-style reading\\ncomprehension with external commonsense knowledge. In Proceedings of ACL, 2018.\\n49. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\\xef\\xac\\x81cient estimation of word\\nrepresentations in vector space. In Proceedings of ICLR, 2013.\\n50. Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. Distant supervision for relation\\nextraction without labeled data. In Proceedings of ACL-IJCNLP, 2009.\\n51. Hatem Mousselly-Sergieh, Teresa Botschen, Iryna Gurevych, and Stefan Roth. A multimodal\\ntranslation-based approach for knowledge graph representation learning. In Proceedings of\\nJCLCS, 2018.\\n52. Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. A review of\\nrelational machine learning for knowledge graphs. 2015.\\n53. Maximilian Nickel, Lorenzo Rosasco, and Tomaso Poggio. Holographic embeddings of\\nknowledge graphs. In Proceedings of AAAI, 2016.\\n54. Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective\\nlearning on multi-relational data. In Proceedings of ICML, 2011.\\n55. Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. Factorizing yago: scalable machine\\nlearning for linked data. In Proceedings of WWW, 2012.\\n214\\n7\\nWorld Knowledge Representation\\n56. Alexandre Passant. dbrec\\xe2\\x80\\x93music recommendations using dbpedia. In Proceedings of ISWC,\\n2010.\\n57. Matthew E. Peters, Mark Neumann, Robert L Logan, Roy Schwartz, Vidur Joshi, Sameer\\nSingh, and Noah A. Smith. Knowledge enhanced contextual word representations. In Pro-\\nceedings of EMNLP-IJCNLP, 2019.\\n58. Jay Pujara, Hui Miao, Lise Getoor, and William W Cohen. Knowledge graph identi\\xef\\xac\\x81cation.\\nIn Proceedings of ISWC, 2013.\\n59. Meng Qu and Jian Tang. Probabilistic logic neural networks for reasoning. In Proceedings of\\nNIPS, 2019.\\n60. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\\nLanguage models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.\\n61. Hadas Raviv, Oren Kurland, and David Carmel. Document retrieval using entity-based lan-\\nguage models. In Proceedings of SIGIR, 2016.\\n62. Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gr\\xc3\\xa9goire Mesnil. A latent semantic\\nmodelwithconvolutional-poolingstructureforinformationretrieval.In ProceedingsofCIKM,\\n2014.\\n63. Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. An attentive neural\\narchitecture for \\xef\\xac\\x81ne-grained entity type classi\\xef\\xac\\x81cation. In Proceedings of AKBC Workshop,\\n2016.\\n64. Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. Matching the\\nBlanks: Distributional similarity for relation learning. In Proceedings of ACL, pages 2895\\xe2\\x80\\x93\\n2905, 2019.\\n65. Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. Reasoning with neural\\ntensor networks for knowledge base completion. In Proceedings of NeurIPS, 2013.\\n66. Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Ruslan Salakhutdinov, and\\nWilliam Cohen. Open domain question answering using early fusion of knowledge bases and\\ntext. In Proceedings of EMNLP, 2018.\\n67. Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embed-\\nding by relational rotation in complex space. In Proceedings of ICLR, 2019.\\n68. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\\nnetworks. In Proceedings of NeurIPS, 2014.\\n69. Erik F Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task:\\nLanguage-independent named entity recognition. In Proceedings of HLT-NAACL, 2003.\\n70. Th\\xc3\\xa9o Trouillon, Johannes Welbl, Sebastian Riedel, \\xc3\\x89ric Gaussier, and Guillaume Bouchard.\\nComplex embeddings for simple link prediction. In Proceedings of ICML, 2016.\\n71. Cunchao Tu, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Transnet: translation-based\\nnetwork representation learning for social relation extraction. In Proceedings of IJCAI, 2017.\\n72. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural\\nimage caption generator. In Proceedings of CVPR, 2015.\\n73. Nina Wacholder, Yael Ravin, and Misook Choi. Disambiguation of proper names in text. In\\nProceedings of ANLP, 1997.\\n74. Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph embedding: A survey\\nof approaches and applications. TKDE, 29(12):2724\\xe2\\x80\\x932743, 2017.\\n75. Quan Wang, Bin Wang, and Li Guo. Knowledge base completion using embeddings and rules.\\nIn Proceedings of IJCAI, 2015.\\n76. Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph and text jointly\\nembedding. In Proceedings of EMNLP, pages 1591\\xe2\\x80\\x931601, 2014.\\n77. Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by\\ntranslating on hyperplanes. In Proceedings of AAAI, 2014.\\n78. Han Xiao, Minlie Huang, Yu Hao, and Xiaoyan Zhu. Transa: An adaptive approach for\\nknowledge graph embedding. arXiv preprint arXiv:1509.05490, 2015.\\n79. Han Xiao, Minlie Huang, Yu Hao, and Xiaoyan Zhu. Transg: A generative mixture model for\\nknowledge graph embedding. arXiv preprint arXiv:1509.05488, 2015.\\nReferences\\n215\\n80. Han Xiao, Minlie Huang, and Xiaoyan Zhu. From one point to a manifold: Knowledge graph\\nembedding for precise link prediction. In Proceedings of IJCAI, 2016.\\n81. Ruobing Xie, Zhiyuan Liu, Tat-seng Chua, Huanbo Luan, and Maosong Sun. Image-embodied\\nknowledge representation learning. In Proceedings of IJCAI, 2016.\\n82. Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. Representation learning\\nof knowledge graphs with entity descriptions. In Proceedings of AAAI, 2016.\\n83. Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Representation learning of knowledge graphs\\nwith hierarchical types. In Proceedings of IJCAI, 2016.\\n84. Chenyan Xiong and Jamie Callan. EsdRank: Connecting query and documents through exter-\\nnal semi-structured data. In Proceedings of CIKM, 2015.\\n85. Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. Bag-of-entities representation for ranking.\\nIn Proceedings of ICTIR, 2016.\\n86. Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. Word-entity duet representations for docu-\\nment ranking. In Proceedings of SIGIR, 2017.\\n87. Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end\\nneural ad-hoc ranking with kernel pooling. In Proceedings of SIGIR, 2017.\\n88. Chenyan Xiong, Russell Power, and Jamie Callan. Explicit semantic ranking for academic\\nsearch via knowledge graph embedding. In Proceedings of WWW, 2017.\\n89. Jiacheng Xu, Xipeng Qiu, Kan Chen, and Xuanjing Huang. Knowledge graph representation\\nwith jointly structural and textual encoding. In Proceedings of IJCAI, 2017.\\n90. Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, and Gerhard Weikum. Robust question\\nanswering over the web of linked data. In Proceedings of CIKM, 2013.\\n91. Bishan Yang and Tom Mitchell. Leveraging knowledge bases in LSTMs for improving\\nmachine reading. In Proceedings of ACL, 2017.\\n92. Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities\\nand relations for learning and inference in knowledge bases. In Proceedings of ICLR, 2015.\\n93. Jun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang, Hang Li, and Xiaoming Li. Neural gener-\\native question answering. In Proceedings of IJCAI, 2016.\\n94. Dani Yogatama, Daniel Gillick, and Nevena Lazic. Embedding methods for \\xef\\xac\\x81ne grained entity\\ntype classi\\xef\\xac\\x81cation. In Proceedings of ACL, 2015.\\n95. Mohamed Amir Yosef, Sandro Bauer, Johannes Hoffart, Marc Spaniol, and Gerhard Weikum.\\nHYENA: Hierarchical type classi\\xef\\xac\\x81cation for entity names. In Proceedings of COLING, 2012.\\n96. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. Relation classi\\xef\\xac\\x81cation\\nvia convolutional deep neural network. In Proceedings of COLING, 2014.\\n97. Dongxu Zhang, Bin Yuan, Dong Wang, and Rong Liu. Joint semantic relevance learning with\\ntext data and graph knowledge. In Proceedings of ACL-IJCNLP, 2015.\\n98. Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. Collaborative\\nknowledge base embedding for recommender systems. In Proceedings of SIGKDD, 2016.\\n99. HanwangZhang,ZawlinKyaw,Shih-FuChang,andTat-SengChua.Visualtranslationembed-\\nding network for visual relation detection. In Proceedings of CVPR, 2017.\\n100. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie:\\nEnhanced language representation with informative entities. In Proceedings of ACL, 2019.\\n101. Huaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan, and Zheng Chen. Aligning knowledge\\nand text embeddings by entity descriptions. In Proceedings of EMNLP, 2015.\\n102. Hao Zhu, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Iterative entity alignment via joint\\nknowledge embeddings. In Proceedings of IJCAI, 2017.\\n103. Wenya Zhu, Kaixiang Mo, Yu Zhang, Zhangbin Zhu, Xuezheng Peng, and Qiang Yang.\\nFlexible end-to-end dialogue system for knowledge grounded conversation. arXiv preprint\\narXiv:1709.04264, 2017.\\n216\\n7\\nWorld Knowledge Representation\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 8\\nNetwork Representation\\nAbstract Network representation learning aims to embed the vertexes in a network\\ninto low-dimensional dense representations, in which similar vertices in the net-\\nwork should have \\xe2\\x80\\x9cclose\\xe2\\x80\\x9d representations (usually measured by cosine similarity or\\nEuclidean distance of their representations). The representations can be used as the\\nfeature of vertices and applied to many network study tasks. In this chapter, we will\\nintroduce network representation learning algorithms in the past decade. Then we\\nwill talk about their extensions when applied to various real-world networks. Finally,\\nwe will introduce some common evaluation tasks of network representation learning\\nand relevant datasets.\\n8.1\\nIntroduction\\nAs a natural way to represent objects and their relationships, the network is ubiqui-\\ntous in our daily lives. The rapid development of social networks like Facebook and\\nTwitter encourage researchers to design effective and ef\\xef\\xac\\x81cient algorithms on network\\nstructure. A key problem of network study is how to represent the network informa-\\ntion properly. Traditional representations of networks are usually high dimensional\\nand sparse, which becomes a weakness when people apply statistical learning to\\nnetworks. With the development of machine learning, feature learning of vertices in\\na network is becoming an emerging task. Therefore, network representation learn-\\ning algorithms turn network information into low-dimensional dense real-valued\\nvectors, which can be used as input for existing machine learning algorithms. For\\nexample, the representations of vertices can be fed to a classi\\xef\\xac\\x81er like Support Vector\\nMachine (SVM) for the vertex classi\\xef\\xac\\x81cation task. Also, the representations can be\\nused for visualization by taking the representations as points in Euclidean space. In\\nthis section, we will formalize the network representation learning problem.\\nThe original version of this chapter was revised: The \\xef\\xac\\x81rst paragraph in Section 8.3 was updated.\\nThe correction to this chapter can be found at https://doi.org/10.1007/978-981-15-5573-2_12.\\n\\xc2\\xa9 The Author(s) 2020, corrected publication 2023\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_8\\n217\\n218\\n8\\nNetwork Representation\\nFig. 8.1 A visualization of vertex embeddings learned by DeepWalk model [93]\\nDenote a network as G = (V, E) where V is the vertex set and E is the edge set.\\nAn edge e = (vi, v j) \\xe2\\x88\\x88E where vi, v j \\xe2\\x88\\x88V is a directed edge from vertex vi to v j.\\nThe outdegree of vertex vi is de\\xef\\xac\\x81ned as degO(vi) = |{v j|(vi, v j) \\xe2\\x88\\x88E}|. Similarly,\\nthe indegree of vertex vi is degI(vi) = |{v j|(v j, vi) \\xe2\\x88\\x88E}|. For undirected network,\\nwe have deg(vi) = degO(vi) = degI(vi). Taking social network as an example, a\\nvertex represents a user and an edge represents the friendship between two users.\\nThe indegree and outdegree represent the number of followers and followees of a\\nuser, respectively.\\nAdjacency matrix A \\xe2\\x88\\x88R|V |\\xc3\\x97|V | is a matrix where Ai j = 1 if (vi, v j) \\xe2\\x88\\x88E and\\nAi j = 0 otherwise. We can easily generalize adjacency matrix to weighted network\\nby setting Ai j to the weight of edge (vi, v j). The adjacency matrix is a simple\\nand straightforward representation of the network. Each row of adjacency matrix\\nA denotes the relationship between a vertex and other vertices and can be seen as\\nthe representation of the corresponding vertex.\\nThoughconvenientandstraightforward,therepresentationoftheadjacencymatrix\\nsuffers from the scalability problem. Adjacency matrix A takes |V | \\xc3\\x97 |V | space to\\nstore, and it is usually unacceptable when |V | grows large. Also, the adjacency matrix\\nis very sparse, which means most of its entries are zeros. The data sparsity makes\\ndiscrete algorithms applicable, but it is still hard to develop ef\\xef\\xac\\x81cient algorithms for\\nstatistic learning [93].\\nTherefore, people come up with the idea to learn low-dimensional dense rep-\\nresentations for vertices in a network. Formally, the goal of network representation\\nlearning is to learn a real-valued vector v \\xe2\\x88\\x88Rd for vertex v \\xe2\\x88\\x88V where dimension d is\\nmuch smaller than the number of vertices |V |. The idea is that similar vertices should\\nhave close representations as shown in Fig.8.1. Network representation learning can\\nbe unsupervised or semi-supervised. The representations are automatically learned\\nwithout feature engineering and can be further used for speci\\xef\\xac\\x81c tasks like classi-\\n\\xef\\xac\\x81cations once they are learned. These representations are low dimensional, which\\nenables ef\\xef\\xac\\x81cient algorithms to be designed over the representations without consid-\\nering the network structure itself. We will discuss more details about the evaluation\\nof network representations later in this chapter.\\n8.2 Network Representation\\n219\\n8.2\\nNetwork Representation\\nIn this section, we will introduce several kinds of network representation learning\\nalgorithms in detail.\\n8.2.1\\nSpectral Clustering Based Methods\\nSpectral clustering based methods are a group of algorithms that compute \\xef\\xac\\x81rst k\\neigenvectors or singular vectors of an af\\xef\\xac\\x81nity matrix, such as adjacency or Lapla-\\ncian matrix of the network. These methods depend heavily on the construction of the\\naf\\xef\\xac\\x81nity matrix. The evaluation result of different af\\xef\\xac\\x81nity matrices varies a lot. Gener-\\nally speaking, spectral clustering based methods have a high complexity because the\\ncomputations of eigenvectors and singular vectors have a nonlinear time complexity.\\nOn the other hand, spectral clustering based methods need to save an af\\xef\\xac\\x81nity\\nmatrix in the memory during the computation. Thus the space complexity cannot be\\nignored, either. These disadvantages limit the large-scale and online generalization of\\nthese methods. Now we will present several algorithms based on spectral clustering.\\nLocally Linear Embedding (LLE) [98] assumes that the representations of vertices\\nare sampled from a manifold. More speci\\xef\\xac\\x81cally, LLE supposes that the representa-\\ntions of a vertex and its neighbors lie in a locally linear patch of the manifold. That\\nis to say, a vertex\\xe2\\x80\\x99s representation can be approximated by a linear combination of\\nthe representation of its neighbors. LLE uses the linear combination of neighbors to\\nreconstruct the center vertex. Formally, the reconstruction error of all vertices can be\\nexpressed as\\nL (W, V) =\\n|V |\\n\\x02\\ni=1\\n\\x03\\x03\\x03\\x03\\x03\\x03\\nvi \\xe2\\x88\\x92\\n|V |\\n\\x02\\nj=1\\nWi jv j\\n\\x03\\x03\\x03\\x03\\x03\\x03\\n2\\n,\\n(8.1)\\nwhere V \\xe2\\x88\\x88R|V |\\xc3\\x97d is the vertex embedding matrix and Wi j is the contribution coef-\\n\\xef\\xac\\x81cient of vertex v j to vi. LLE enforces Wi j = 0 if vi and v j are not connected,\\ni.e., (vi, v j) /\\xe2\\x88\\x88E. Further, the summation of a row of matrix W is set to 1, i.e.,\\n\\x04|V |\\nj=1 Wi j = 1.\\nEquation8.1 is solved by alternatively optimizing weight matrix W and represen-\\ntation V. The optimization over W can be solved as a least-squares problem. The\\noptimization over representation V leads to the following optimization problem:\\nL (W, V) =\\n|V |\\n\\x02\\ni=1\\n\\x03\\x03\\x03\\x03\\x03\\x03\\nvi \\xe2\\x88\\x92\\n|V |\\n\\x02\\nj=1\\nWi jv j\\n\\x03\\x03\\x03\\x03\\x03\\x03\\n2\\n,\\n(8.2)\\n220\\n8\\nNetwork Representation\\ns.t.\\n|V |\\n\\x02\\ni=1\\nvi = 0,\\n(8.3)\\nand |V |\\xe2\\x88\\x921\\n|V |\\n\\x02\\ni=1\\nv\\xe2\\x8a\\xa4\\ni vi = Id,\\n(8.4)\\nwhere Id denotes d \\xc3\\x97 d identity matrix. The conditions Eqs.8.3 and 8.4 ensure\\nthe uniqueness of the solution. The \\xef\\xac\\x81rst condition enforces the center of all vertex\\nembeddings to zero point and the second condition guarantees different coordinates\\nhave the same scale, i.e., equal contribution to the reconstruction error.\\nThe optimization problem can be formulated as the computation of eigenvectors\\nof matrix (I|V | \\xe2\\x88\\x92W\\xe2\\x8a\\xa4)(I|V | \\xe2\\x88\\x92W), which is an easily solvable eigenvalue problem.\\nMore details can be found in the note [22].\\nLaplacian Eigenmap [8] algorithm simply follows the idea that the representations\\nof two connected vertices should be close. Speci\\xef\\xac\\x81cally, the \\xe2\\x80\\x9ccloseness\\xe2\\x80\\x9d is measured\\nby the square of Euclidean distance. We use D to denote diagonal degree matrix\\nwhere D is a |V | \\xc3\\x97 |V | diagonal matrix and the ith diagonal entry Dii is the degree\\nof vertex vi. The Laplacian matrix L of a graph is de\\xef\\xac\\x81ned as the difference of diagonal\\nmatrix D and adjacency matrix A, i.e., L = D \\xe2\\x88\\x92A.\\nLaplacian Eigenmap algorithm wants to minimize the following cost function:\\nL (V) =\\n\\x02\\n{i, j|(vi,v j)\\xe2\\x88\\x88E}\\n\\xe2\\x88\\xa5vi \\xe2\\x88\\x92v j\\xe2\\x88\\xa52,\\n(8.5)\\ns.t. V\\xe2\\x8a\\xa4DV = Id.\\n(8.6)\\nThe cost function is the summation of square loss of all connected vertex pairs\\nand the condition prevents the trivial all-zero solution caused by arbitrary scale.\\nEquation8.5 can be reformulated in matrix form as\\nV\\xe2\\x88\\x97= arg\\nmin\\nV\\xe2\\x8a\\xa4DV=Id\\ntr(V\\xe2\\x8a\\xa4LV).\\n(8.7)\\nAlgebraic knowledge tells us that the optimal solution V\\xe2\\x88\\x97of Eq.8.7 is the cor-\\nresponding eigenvectors of d smallest nonzero eigenvalues of Laplacian matrix L.\\nNotethat theLaplacianEigenmapalgorithmcanbeeasilygeneralizedtotheweighted\\ngraph.\\nBoth LLE and Laplacian Eigenmap have a symmetric cost function which indi-\\ncates that both algorithms cannot be applied to the directed graph. Directed Graph\\nEmbedding (DGE) [17] was proposed to generalize Laplacian Eigenmap.\\nFor both directed and undirected graph, we can de\\xef\\xac\\x81ne a transition probability\\nmatrix P \\xe2\\x88\\x88R|V |\\xc3\\x97|V |, where Pi j denotes the probability that vertex vi walks to v j.\\n8.2 Network Representation\\n221\\nTable 8.1 Applicability of LLE, Laplacian Eigenmap, and DGE algorithms on undirected,\\nweighted, and directed graph\\nAlgorithm\\nCapability\\nUndirected\\nWeighted\\nDirected\\nLLE\\n\\xe2\\x9c\\x93\\n\\xe2\\x80\\x93\\n\\xe2\\x80\\x93\\nLaplacian Eigenmap\\n\\xe2\\x9c\\x93\\n\\xe2\\x9c\\x93\\n\\xe2\\x80\\x93\\nDGE\\n\\xe2\\x9c\\x93\\n\\xe2\\x9c\\x93\\n\\xe2\\x9c\\x93\\nThe transition matrix de\\xef\\xac\\x81nes a Markov random walk through the graph. We denote\\nthe stationary value of vertex vi as \\xcf\\x80i where \\x04\\ni \\xcf\\x80i = 1. The stationary distribution\\nof random walk is commonly used in many ranking algorithms such as PageRank.\\nDGE designs a new cost function which emphasizes the important vertices, which\\nhave a higher stationary value:\\nL (V) =\\n|V |\\n\\x02\\ni=1\\n\\xcf\\x80i\\n|V |\\n\\x02\\nj=1\\nPi j\\xe2\\x88\\xa5vi \\xe2\\x88\\x92v j\\xe2\\x88\\xa52.\\n(8.8)\\nBy denoting M = diag(\\xcf\\x801, \\xcf\\x802, . . . , \\xcf\\x80|V |), the cost function Eq.8.8 can be refor-\\nmulated as\\nL (V) = 2tr(V\\xe2\\x8a\\xa4BV),\\n(8.9)\\ns.t. V\\xe2\\x8a\\xa4MV = Id,\\n(8.10)\\nwhere\\nB = M \\xe2\\x88\\x92MP \\xe2\\x88\\x92P\\xe2\\x8a\\xa4M\\n2\\n.\\n(8.11)\\nThe condition Eq.8.10 is added to remove an arbitrary scaling factor. Similar to\\nLaplacian Eigenmap, the optimization problem can also be solved as a generalized\\neigenvector problem.\\nFor comparisons between the above three network embedding learning algo-\\nrithms, we conclude the following table to illustrate their applicability (Table8.1).\\nUnlike previous works which minimize the distance between vertex representa-\\ntions, Tang and Liu [112] introduces modularity [85] into the cost function instead.\\nModularity is a measurement which characterizes how far the graph is away from\\na uniform random graph. Given graph G = (V, E), we assume that vertices V are\\ndivided into k nonoverlapping communities. By \\xe2\\x80\\x9cuniform random graph\\xe2\\x80\\x9d, we mean\\nvertices connect to each other based on a uniform distribution given their degrees.\\nThen the expected edges between vi and v j is deg(vi) deg(vj)\\n2|E|\\n. Then the modularity of a\\ngraph Q is de\\xef\\xac\\x81ned as\\n222\\n8\\nNetwork Representation\\nQ =\\n1\\n2|E|\\n\\x02\\ni, j\\n\\x05\\nAi j \\xe2\\x88\\x92deg(vi) deg(v j)\\n2|E|\\n\\x06\\n\\xce\\xb4(vi, v j),\\n(8.12)\\nwhere \\xce\\xb4(vi, v j) = 1 if vi and v j belong to the same community and \\xce\\xb4(vi, v j) = 0\\notherwise. A larger modularity indicates that the subgraphs inside communities are\\ndenser, which follows the intuition that a community is a dense well-connected\\ncluster. Then the problem is to \\xef\\xac\\x81nd a partition that maximizes the modularity Q.\\nHowever, a hard clustering on modularity maximization is proved to be NP hard.\\nTherefore, they relax the problem to a soft case. Let d \\xe2\\x88\\x88Z|V |\\n+ denotes the degree of\\nall vertices and 1 \\xe2\\x88\\x88{0, 1}|V |\\xc3\\x97k denotes the community indicator matrix where\\n1i j =\\n\\x07\\n1\\nif vertex i belongs to community j,\\n0\\notherwise.\\n(8.13)\\nThen we de\\xef\\xac\\x81ne modularity matrix B as\\nB = A \\xe2\\x88\\x92ddT\\n2|E|,\\n(8.14)\\nand modularity Q can be reformulated as\\nQ =\\n1\\n2|E|tr(1\\xe2\\x8a\\xa4B1).\\n(8.15)\\nBy relaxing 1 to a continuous matrix, it has been proved that the optimal solution\\n1 is the top-k eigenvectors of modularity matrix B [84].\\nAs an alternatively cost function, Tang and Liu also proposed another algorithm\\n[113] by optimizing over normalized cut of the graph. Similarly, the algorithm turns\\nto the computation of top-k eigenvectors of normalized graph Laplacian \\x08L:\\n\\x08L = D\\xe2\\x88\\x921\\n2 L D\\xe2\\x88\\x921\\n2 = I \\xe2\\x88\\x92D\\xe2\\x88\\x921\\n2 AD\\xe2\\x88\\x921\\n2 .\\n(8.16)\\nThen the community indicator matrix 1 is taken as a k-dimensional vertex repre-\\nsentation.\\nToconcludespectralclusteringmethodsfornetworkrepresentationlearning,these\\nmethods often de\\xef\\xac\\x81ne a cost function that is linear or quadratic to the vertex embed-\\nding. Then they reformulate the cost function as a matrix form and \\xef\\xac\\x81gure out that\\nthe optimal solutions are eigenvectors of a particular matrix according to algebra\\nknowledge. The major drawback of spectral clustering methods is the complexity:\\nthe computation of eigenvectors for large-scale matrices is both time consuming and\\nspace consuming.\\n8.2 Network Representation\\n223\\n8.2.2\\nDeepWalk\\nAs shown in previous subsections, accurate computation of the optimal solution, such\\nas eigenvector computation, is not very ef\\xef\\xac\\x81cient for large-scale problems. Meantime,\\nneural network approaches have proved their effectiveness in many areas such as\\nnatural language and image processing. Though the gradient descent method cannot\\nalways guarantee an optimal solution of the neural network models, the implemen-\\ntation and learning of neural networks are relatively fast, and they usually have good\\nperformances. On the other hand, neural network models can let people get rid of\\nfeature engineering and are mostly data driven. Thus, the exploration of the neural\\nnetwork approach on representation learning is becoming an emerging task.\\nDeepWalk [93] proposes a novel approach that introduces deep learning tech-\\nniques into network representation learning for the \\xef\\xac\\x81rst time. The bene\\xef\\xac\\x81ts of model-\\ning truncated random walks instead of the adjacency matrix are twofold: \\xef\\xac\\x81rst, random\\nwalks need only local information and thus enable discrete and online algorithms on\\nit while modeling of adjacency matrix may need to store everything in memory and\\nthus be space consuming; second, modeling random walks can alleviate the variance\\nand uncertainty of modeling original binary adjacency matrix. We will look insight\\ninto DeepWalk in the next subsection.\\nUnsupervised representation learning algorithms have been widely studied and\\napplied in the natural language processing area. The authors show that the vertex fre-\\nquency in short random walks also follows the power law as words in documents do.\\nShowing the connection between vertex to the word and random walks to sentences,\\nthe authors adapted a well-known word representation learning algorithm word2vec\\n[80] into vertex representation learning. Now, we will introduce DeepWalk algo-\\nrithms in detail.\\nGiven graph G = (V, E), we denote a random walk started at vertex vi as \\xe2\\x84\\x93vi.\\nWe use \\xe2\\x84\\x93k\\nvi to represent the kth vertex in the random walk \\xe2\\x84\\x93vi. The next vertex \\xe2\\x84\\x93k+1\\nvi\\nis generated by uniformly random selection from neighbors of vertex \\xe2\\x84\\x93k\\nvi. Random\\nwalk sequences have been used for many network analysis tasks, such as similarity\\nmeasurement and community detection [2, 32].\\nDeepWalk follows the idea of language modeling to model short random walk\\nsequences. That is to estimate the likelihood of observing vertex vi given all previous\\nvertices in the random walk:\\nP(vi|(v1, v2, . . . , vi\\xe2\\x88\\x921)).\\n(8.17)\\nTo the extent of vertex representation learning, we turn to predict vertex vi given\\nthe representations of all previous vertices:\\nP(vi|(v1, v2, . . . , vi\\xe2\\x88\\x921)).\\n(8.18)\\nA relaxation of this formula in language modeling turns to use vertex vi to predict\\nits neighboring vertices vi\\xe2\\x88\\x92w, . . . , vi\\xe2\\x88\\x921, vi+1, . . . , vi+w where w is the window size.\\n224\\n8\\nNetwork Representation\\nThis part of model is named as Skip-gram model in word embedding learning. The\\nneighboring vertices are also called context vertices of the center vertex. As another\\nsimpli\\xef\\xac\\x81cation, DeepWalk ignores the order and offset of the vertices and thus predict\\nvi\\xe2\\x88\\x92w and vi\\xe2\\x88\\x921 in the same way. The optimization function of a single vertex of a\\nrandom walk can be formulated as\\nmin\\nv \\xe2\\x88\\x92log P({vi\\xe2\\x88\\x92w, . . . , vi\\xe2\\x88\\x921, vi+1, . . . , vi+w}|vi).\\n(8.19)\\nBased on independent assumption, the loss function can be rewritten as\\nmin\\nv\\nw\\n\\x02\\nk=\\xe2\\x88\\x92w,k\\xcc\\xb8=0\\n\\xe2\\x88\\x92log P(vi+k|vi).\\n(8.20)\\nThe overall loss function can be obtained by adding up over every vertex in every\\nrandom walk.\\nNow we talk about how to predict a single vertex v j given center vertex vi. In\\nDeepWalk, each vertex vi has two representations with the same dimension: ver-\\ntex representation vi \\xe2\\x88\\x88Rd and context representation ci \\xe2\\x88\\x88Rd. The probability of\\nprediction P(v j|vi) is de\\xef\\xac\\x81ned by a softmax function over all vertices:\\nP(v j|vi) =\\nexp(vic\\xe2\\x8a\\xa4\\nj )\\n\\x04|V |\\nk=1 exp(vic\\xe2\\x8a\\xa4\\nk )\\n.\\n(8.21)\\nHere we come to the parameter learning phase of DeepWalk. We \\xef\\xac\\x81rst present the\\npseudocode of the DeepWalk framework in Algorithm 8.1.\\nAlgorithm 8.1 DeepWalk algorithm\\nGiven graph G = (V, E), window size w, embedding size d, walks per vertex n and walk length l\\nfor i = 1, 2, . . . , n do\\nfor vi \\xe2\\x88\\x88V do\\n\\xe2\\x84\\x93vi =RandomWalk(G, vi,l)\\nSkip-gram(V, \\xe2\\x84\\x93vi , w)\\nend for\\nend for\\nwhere RandomWalk(G, vi,l) generates a random walk rooted at vi with length l and\\nSkip-gram(V, \\xe2\\x84\\x93vi , w) function is de\\xef\\xac\\x81ned in Algorithm 8.2, where \\xce\\xb1l is the learning\\nrate of stochastic gradient descent.\\nNote that the parameter updating rule V = V \\xe2\\x88\\x92\\xce\\xb1l \\xe2\\x88\\x82J\\n\\xe2\\x88\\x82V in Skip-gram has a com-\\nplexity of O(|V |) because in the computation of the gradient of P(vk|v j) (as shown\\nin Eq.8.21), the denominator has |V | terms to compute. This complexity is unac-\\nceptable for large-scale networks.\\n8.2 Network Representation\\n225\\nAlgorithm 8.2 Skip-gram(R, Wvi , w)\\nfor v j \\xe2\\x88\\x88\\xe2\\x84\\x93vi do\\nfor vk \\xe2\\x88\\x88\\xe2\\x84\\x93vi [ j \\xe2\\x88\\x92w : j + w] do\\nif vk \\xcc\\xb8= v j then\\nJ(V) = \\xe2\\x88\\x92log P(vk|V j)\\nV = V \\xe2\\x88\\x92\\xce\\xb1l \\xe2\\x88\\x82J\\n\\xe2\\x88\\x82V\\nend if\\nend for\\nend for\\nTable 8.2 Analogy of DeepWalk and word2vec\\nMethod\\nObject\\nInput\\nOutput\\nWord2vec\\nWord\\nSentence\\nWord embedding\\nDeepWalk\\nVertex\\nRandom walk\\nVertex embedding\\nTo address this problem, people proposed Hierarchical Softmax as a variant of\\noriginal softmax function. The core idea is to map the vertices to a balanced binary\\ntree, where each vertex corresponds to a leaf of the tree. Then the prediction of a\\nvertex turns to the prediction of the path from the root to the corresponding leaf.\\nAssume that the path from root to vertex vk is denoted by a sequence of tree nodes\\nb1, b2 . . . , b\\xe2\\x8c\\x88log |V |\\xe2\\x8c\\x89and then we have\\nlog P(vk|v j) =\\n\\xe2\\x8c\\x88log |V |\\xe2\\x8c\\x89\\n\\x02\\ni=1\\nlog P(bi|v j).\\n(8.22)\\nA logistic function can easily implement a binary decision on a tree node. Hence,\\nthe time complexity reduces to O(log |V |) from O(|V |). We can accelerate the\\nalgorithm by using Huffman coding to map frequent vertices to the tree nodes that\\nare close to the root. We can also use negative sampling which is used in word2vec\\nto replace hierarchical softmax for speeding up.\\nSo far, we have \\xef\\xac\\x81nished the introduction of the DeepWalk algorithm. Deep-\\nWalk introduces ef\\xef\\xac\\x81cient deep learning techniques into network embedding learning.\\nTable8.2 gives an analogy between DeepWalk and Word2vec. DeepWalk outper-\\nforms traditional network representation learning methods on network classi\\xef\\xac\\x81cation\\ntasks and is also ef\\xef\\xac\\x81cient for large-scale networks. Besides, the generation of random\\nwalks can be generalized to nonrandom walk, such as the information propagation\\nstreams. In the next subsection, we will give a detailed proof to demonstrate the\\ncorrelation between DeepWalk and matrix factorization.\\n226\\n8\\nNetwork Representation\\n8.2.2.1\\nMatrix Factorization Comprehension of DeepWalk\\nPerozzi et al. introduced the Skip-gram model into the study of social network for\\nthe \\xef\\xac\\x81rst time, and designed an algorithm named DeepWalk [93] for learning vertex\\nrepresentation on a graph. In this subsection, we prove that the DeepWalk algorithm\\nwith Skip-gram and softmax model is actually factoring a matrix M where each\\nentry Mi j is the logarithm of the average probability that vertex vi randomly walks\\nto vertex v j in \\xef\\xac\\x81x steps. We will explain it later.\\nSince the Skip-gram model does not consider the offset of context vertex and\\npredict context vertices independently, we can regard the random walks as a set of\\nvertex-context pairs. The useful information on random walks is the co-occurrence\\nof vertex pairs inside a window. Given network G = (V, E), we suppose that vertex-\\ncontext set D is generated from random walks, where each piece of D is a vertex-\\ncontext pair (v, c). Let V be the set of nodes, and VC be the set of context nodes. In\\nmost cases, V = VC.\\nConsider a vertex-context pair (v, c):\\nN(v,c) denotes the number of times (v, c) appears in D. Nv = \\x04\\nc\\xe2\\x80\\xb2\\xe2\\x88\\x88VC N(v,c\\xe2\\x80\\xb2) and\\nNc = \\x04\\nv\\xe2\\x80\\xb2\\xe2\\x88\\x88V N(v\\xe2\\x80\\xb2,c) denotes the number of times v and c appears in D. Note that\\n|D| = \\x04\\nv\\xe2\\x80\\xb2\\xe2\\x88\\x88V\\n\\x04\\nc\\xe2\\x80\\xb2\\xe2\\x88\\x88VC N(v\\xe2\\x80\\xb2,c\\xe2\\x80\\xb2).\\nA context vertex c \\xe2\\x88\\x88VC is represented by a d-dimension vector c \\xe2\\x88\\x88Rd and C\\nis a |VC| \\xc3\\x97 d matrix, where row j is vector cj. Our goal is to \\xef\\xac\\x81gure out a matrix\\nM = VC\\xe2\\x8a\\xa4.\\nPerozzi et al. implemented the DeepWalk algorithm with the Skip-gram and Hier-\\narchical Softmax model. Note that Hierarchical Softmax is a variant of softmax for\\nspeeding the training time. In this subsection, we give proofs for both negative sam-\\npling and softmax with the Skip-gram model.\\nNegative sampling approximately maximizes the probability of softmax function\\nby randomly choosing k negative samples from the context set. Levy and Goldberg\\nshowed that Skip-gram with the Negative Sampling model (SGNS) is implicitly fac-\\ntorizing a word-context matrix [69] by assuming that dimensionality d is suf\\xef\\xac\\x81ciently\\nlarge. In other words, we can assign each product v \\xc2\\xb7 c a value independent of the\\nothers.\\nIn SGNS model, we have\\nP((v, c) \\xe2\\x88\\x88D) = Sigmoid(v \\xc2\\xb7 c) =\\n1\\n1 + e\\xe2\\x88\\x92v\\xc2\\xb7c .\\n(8.23)\\nSuppose we choose k negative samples for each vertex-context pair (v, c) accord-\\ning to the distribution PD(cN) =\\nNcN\\n|D| . Then, the objective function for SGNS can be\\nwritten as\\n8.2 Network Representation\\n227\\nO =\\n\\x02\\nv\\xe2\\x88\\x88V\\n\\x02\\nc\\xe2\\x88\\x88VC\\nN(v,c)(log Sigmoid(v \\xc2\\xb7 c) + kEcN \\xe2\\x88\\xbcPD[log Sigmoid(\\xe2\\x88\\x92v \\xc2\\xb7 c)])\\n=\\n\\x02\\nv\\xe2\\x88\\x88V\\n\\x02\\nc\\xe2\\x88\\x88VC\\nN(v,c) log Sigmoid(v \\xc2\\xb7 c) + k\\n\\x02\\nv\\xe2\\x88\\x88V\\nNv\\n\\x02\\ncN \\xe2\\x88\\x88VC\\nNcN\\n|D| log Sigmoid(\\xe2\\x88\\x92v \\xc2\\xb7 c)\\n=\\n\\x02\\nv\\xe2\\x88\\x88V\\n\\x02\\nc\\xe2\\x88\\x88VC\\nN(v,c) log Sigmoid(v \\xc2\\xb7 c) + kNv\\nNc\\n|D| log Sigmoid(\\xe2\\x88\\x92v \\xc2\\xb7 c).\\n(8.24)\\nDenote x = v \\xc2\\xb7 c. By solving \\xe2\\x88\\x82O\\n\\xe2\\x88\\x82x = 0, we have\\nv \\xc2\\xb7 c = x = log N(v,c)|D|\\nNvNc\\n\\xe2\\x88\\x92log k.\\n(8.25)\\nThus we have Mi j = log\\nN(vi ,c j )\\n|D|\\nNvi\\n|D|\\nNc j\\n|D|\\n\\xe2\\x88\\x92log k. Mi j can be interpreted as Point-wise\\nMutual Information(PMI) of vertex-context pair (vi, c j) shifted by log k.\\nSince both negative sampling and hierarchical softmax are variants of softmax,\\nwe pay more attention to the softmax model and give a further discussion on it. We\\nalso assume that the values of v \\xc2\\xb7 c are independent.\\nIn softmax model,\\nP((v, c) \\xe2\\x88\\x88D) =\\nev\\xc2\\xb7c\\n\\x04\\nc\\xe2\\x80\\xb2\\xe2\\x88\\x88VC ev\\xc2\\xb7c\\xe2\\x80\\xb2 .\\n(8.26)\\nAnd the objective function is\\nO =\\n\\x02\\nv\\xe2\\x88\\x88V\\n\\x02\\nc\\xe2\\x88\\x88VC\\nN(v,c) log\\nev\\xc2\\xb7c\\n\\x04\\nc\\xe2\\x80\\xb2\\xe2\\x88\\x88VC ev\\xc2\\xb7c\\xe2\\x80\\xb2 .\\n(8.27)\\nAfter extracting all terms associated to v \\xc2\\xb7 c as O(v, c), we have\\nO(v, c) = N(v,c) log\\nev\\xc2\\xb7c\\n\\x04\\nc\\xe2\\x80\\xb2\\xe2\\x88\\x88VC,c\\xe2\\x80\\xb2\\xcc\\xb8=c ev\\xc2\\xb7c\\xe2\\x80\\xb2 + ev\\xc2\\xb7c +\\n\\x02\\n\\xcb\\x9cc\\xe2\\x88\\x88VC,\\xcb\\x9cc\\xcc\\xb8=c\\nN(v,\\xcb\\x9cc) log\\nev\\xc2\\xb7\\xcb\\x9cc\\n\\x04\\nc\\xe2\\x80\\xb2\\xe2\\x88\\x88VC,c\\xe2\\x80\\xb2\\xcc\\xb8=c ev\\xc2\\xb7c\\xe2\\x80\\xb2 + ev\\xc2\\xb7c .\\n(8.28)\\nNote that O =\\n1\\n|VC|\\n\\x04\\nv\\xe2\\x88\\x88V\\n\\x04\\nc\\xe2\\x88\\x88VC O(v, c). Denote x = v \\xc2\\xb7 c. By solving \\xe2\\x88\\x82O\\n\\xe2\\x88\\x82x = 0\\nfor all such x, we have\\nv \\xc2\\xb7 c = x = log N(v,c)\\nNv\\n+ bv,\\n(8.29)\\nwhere bv can be any real constant since it will be canceled when we compute\\nP((v, c) \\xe2\\x88\\x88D). Thus, we have Mi j = log\\nN(vi ,c j )\\nN(vi ) + bvi. We will discuss what Mi j\\nrepresents in next section.\\n228\\n8\\nNetwork Representation\\nIt is clear that the method of sampling vertex-context pairs, i.e., random walks\\ngeneration, will affect matrix M. In this section, we will discuss Nv\\n|D|,\\nNc\\n|D| and N(v,c)\\nNv\\nbased on an ideal sampling method for DeepWalk algorithm.\\nAssume the graph is connected and undirected, and the window size is w. The\\nsampling algorithm is illustrated in Algorithm 8.3. We can easily generalize this\\nsampling method to the directed graph by only adding (RWi, RW j) into D.\\nAlgorithm 8.3 Ideal vertex-context pair sampling algorithm\\nGenerate an in\\xef\\xac\\x81nite long random walk \\xe2\\x84\\x93.\\nDenote \\xe2\\x84\\x93i as the vertex on position i of \\xe2\\x84\\x93, where i = 0, 1, 2, . . .\\nfor i = 0, 1, 2, . . . do\\nfor j \\xe2\\x88\\x88[i + 1, i + w] do\\nadd (\\xe2\\x84\\x93i, \\xe2\\x84\\x93j) into D\\nadd (\\xe2\\x84\\x93j, \\xe2\\x84\\x93i) into D\\nend for\\nend for\\nEach appearance of vertex i will be recorded 2w times in D for undirected graph\\nand w times for directed graph. Thus, we can \\xef\\xac\\x81gure out that\\nNvi\\n|D| is the frequency of\\nvi that appears in the random walk, which is exactly the PageRank value of vi. Also\\nnote that\\nN(vi ,v j )\\nNvi /2w is the expectation times that v j is observed in left/right w neighbors\\nof vi.\\nDenote the transition matrix in PageRank algorithm be P. More formally, let\\ndeg(vi) be the degree of vertex i. Pi j =\\n1\\ndeg(vi) if (i, j) \\xe2\\x88\\x88E and Pi j = 0 otherwise.\\nWe use ei to denote a |V |-dimension row vector, where all entries are zero except\\nthe ith entry is 1.\\nSuppose that we start a random walk from vertex i and use ei to denote the\\ninitial state. Then eiP is the distribution over all the vertices where jth entry is the\\nprobability that vertex vi walks to vertex v j. Hence, jth entry of eiPw is the probability\\nthat vertex vi walks to vertex v j at exactly w steps. Thus [ei(P + P2 + \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 + Pw)] j\\nis the expectation times that v j appears in right w neighbors of vi.\\nHence\\nN(vi,v j)\\nNvi/2w = 2[ei(P + P2 + \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 + Pw)] j,\\nN(vi,v j)\\nNvi\\n= [ei(P + P2 + \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 + Pw)] j\\nw\\n.\\n(8.30)\\nThis equality also holds for a directed graph.\\nBy setting bvi = log 2w for all i, Mi j = log\\nN(vi ,v j )\\nNvi /2w is logarithm of the expectation\\ntimes that v j appears in left/right w neighbors of vi.\\nBy setting bvi = 0 for all i, Mi j = log\\nN(vi ,v j )\\nNvi\\n= log [ei(A+A2+\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7+Aw)] j\\nw\\nis logarithm\\nof the average probability that vertex vi randomly walks to vertex v j in w steps.\\n8.2 Network Representation\\n229\\n8.2.2.2\\nDiscussion\\nSo far we have seen many different network representation learning algorithms and\\nwe can \\xef\\xac\\x81gure out some patterns that how network representation methods share.\\nThen we will move forward and see how these patterns match some recent network\\nembedding algorithms.\\nMost network representation algorithms try to reconstruct a data matrix generated\\nfrom the graph with vertex embeddings. The simplest matrix would be the adjacency\\nmatrix. However, recovering the adjacency matrix may not be the best choice. First,\\nreal-world networks are mostly very sparse which means O(|E|) = O(|V |). There-\\nfore, the adjacency matrix will be very sparse as well. Though the sparseness enables\\nan ef\\xef\\xac\\x81cient algorithm, it can harm the performance of vertex representation learning\\nbecause of the de\\xef\\xac\\x81ciency of useful information. Second, the adjacency matrix may\\nbe noisy and sensitive. A single missing link can completely change the correlation\\nbetween two vertices.\\nHence people seek to \\xef\\xac\\x81nd an alternative matrix to replace the adjacency matrix\\nthough implicitly. Take DeepWalk as an example, DeepWalk models the following\\nmatrix based on matrix factorization comprehension of DeepWalk:\\nM = P + P2 + \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 + Pw,\\n(8.31)\\nwhere\\nPi j =\\n\\x07\\n1/deg(vi)\\nif (vi, v j) \\xe2\\x88\\x88E,\\n0\\notherwise.\\n(8.32)\\nCompared with the adjacency matrix A, the matrix M modeled by DeepWalk is\\nmuch denser. Furthermore, the window size parameter w can adjust the density: a\\nlarger window size models a denser matrix but will slow down the algorithm. Hence,\\nthe window size w works as a harmonic factor to balance ef\\xef\\xac\\x81ciency and effectiveness.\\nOn the other hand, the matrix M can alleviate the noises in the adjacency matrix.\\nConsider two similar vertices vi and v j, even though the edge between them is\\nmissing, they can still have many co-occurrences by appearing inside a window size\\nof the same random walks.\\nIn a real-world application, direct computation of M may have a high time com-\\nplexity when window size w grows. Thus, it is essential to choose a proper w. How-\\never, window size w is a discrete parameter, and thus the matrix M may grow from\\ntoo sparse to too dense by changing w by 1. Here, we can see another bene\\xef\\xac\\x81t of\\nrandom walks. Random walks used by DeepWalk serve as Monte Carlo simulations\\nfor approximating matrix M. The more random walks you walk, the more likely you\\ncan approximate the matrix.\\nAfter we choose a matrix to model, we need to correlate the matrix entry with\\nvertex representations pairs. There are two widely used measurements of vertices\\npairs: Euclidean distance and inner product. Assume that we want to model the entry\\nMi j given vertex representations vi and v j, we can employ\\n230\\n8\\nNetwork Representation\\nMi j = f (\\xe2\\x88\\xa5vi \\xe2\\x88\\x92v j\\xe2\\x88\\xa52),\\nMi j = f (vi \\xc2\\xb7 v j),\\n(8.33)\\nwhere function f can be any reasonable matching functions such as sigmoid function\\nor linear function for our propose. Actually, the inner product vi \\xc2\\xb7 v j is used more\\nwidely and would correspond to equivalent matrix factorization methods.\\nThe next phase is to design a proper loss function between Mi j and f (vi \\xc2\\xb7 v j).\\nSeveral loss functions such as square loss and hinge loss can be employed. You can\\nalso design a generative model and maximize the likelihood of matrix M.\\nThe \\xef\\xac\\x81nal step of a network representation learning algorithm would be parameter\\nlearning. The most frequently used parameter learning method would be Stochastic\\nGradient Descent (SGD). Other variants of SGD such as AdaGrad and AdaDelta can\\nmake the learning phase converge faster. In the next subsection, we will see some\\nrecent network representation learning algorithms which follow DeepWalk. We will\\n\\xef\\xac\\x81nd that their models can match all these phases above and have some innovations\\non building matrix M, modifying function f , and changing loss function.\\n8.2.3\\nMatrix Factorization Based Methods\\nWe will focus on two network representation learning algorithms LINE and GraRep\\n[13, 111] in this subsection. They both follow the framework introduced in the last\\nsubsection.\\n8.2.3.1\\nLINE\\nTang et al. [111] proposed a network embedding model named as LINE. LINE algo-\\nrithm can handle large-scale networks with arbitrary types: (un)directed or weighted.\\nTo model the interaction between vertices, LINE models \\xef\\xac\\x81rst-order proximity which\\nis represented by observed links and second-order proximity which is determined by\\nshared neighbors but not links between vertices.\\nBefore we introduce the details of the algorithm, we can move one step back\\nand see how the idea works. The modeling of \\xef\\xac\\x81rst-order proximity, i.e., observed\\nlinks, is the modeling of the adjacency matrix. As we said in the last subsection,\\nthe adjacency matrix is usually too sparse. Hence the modeling of second-order\\nproximity, i.e., vertices with shared neighbors, can serve as complement information\\nto enrich the adjacency matrix and make it denser. The enumeration of all vertex\\npairs which have common neighbors is time consuming. Thus, it is necessary to\\ndesign a sampling phase to handle large-scale networks. The sampling phase works\\nlike Monte Carlo simulation to approximate the ideal matrix.\\n8.2 Network Representation\\n231\\nNow we only have two questions: how to de\\xef\\xac\\x81ne \\xef\\xac\\x81rst-order and second-order\\nproximity and how to de\\xef\\xac\\x81ne the loss function. In other words, it is equal to how to\\nde\\xef\\xac\\x81ne M and loss function.\\nFirst-order proximity between vertex u and v is de\\xef\\xac\\x81ned as the weight wuv on\\nedge (u, v). If there is no edge between vertex u and v, then the \\xef\\xac\\x81rst-order proximity\\nbetween them is 0.\\nSecond-order proximity between vertex u and v is de\\xef\\xac\\x81ned as the similarity\\nbetween their neighborhood network. Let pu = (wu,1, . . . , wu,|V |) denote the \\xef\\xac\\x81rst-\\norder proximity between vertex u and all other vertices. Then the second-order prox-\\nimity between u and v is de\\xef\\xac\\x81ned as the similarity of pu and pv. If they have no shared\\nneighbors, then the second-order proximity is zero.\\nThen we can introduce LINE model more speci\\xef\\xac\\x81cally. The joint probability\\nbetween vi and v j is\\np1(vi, v j) =\\n1\\n1 + exp(\\xe2\\x88\\x92vi \\xc2\\xb7 v j),\\n(8.34)\\nwhere vi and v j are d-dimensional row vectors which indicate the representations\\nof vertex vi and v j.\\nTo supervise the probabilities, empirical probability is de\\xef\\xac\\x81ned as \\xcb\\x86p1(i, j) = wi j\\nW ,\\nwhere W = \\x04\\n(vi,v j)\\xe2\\x88\\x88E wi j. Thus our goal is to \\xef\\xac\\x81nd vertex embeddings to approximate\\nwi j\\nW with\\n1\\n1+exp(\\xe2\\x88\\x92vi\\xc2\\xb7v j). Following the idea in last subsection, it is equivalent to say\\nvi \\xc2\\xb7 v j = Mi j = \\xe2\\x88\\x92log( W\\nwi j \\xe2\\x88\\x921).\\nThe loss function between joint probability p1 and its empirical probability \\xcb\\x86p1 is\\nL1 = DKL( \\xcb\\x86p1 || p1),\\n(8.35)\\nwhere DKL(\\xc2\\xb7 || \\xc2\\xb7) is KL-divergence of two probability distributions.\\nOn the other hand, we de\\xef\\xac\\x81ne the probability that vertex v j appears in vi\\xe2\\x80\\x99s context:\\np2(v j|vi) =\\nexp(c j \\xc2\\xb7 vi)\\n\\x04|V |\\nk=1 exp(ck \\xc2\\xb7 vi)\\n.\\n(8.36)\\nSimilarly, the empirical probability is de\\xef\\xac\\x81ned as \\xcb\\x86p2(v j|vi) = wi j\\ndi\\nwhere di =\\n\\x04\\nk wik and the loss function is\\nL2 =\\n\\x02\\ni\\ndi DKL( \\xcb\\x86p2(\\xc2\\xb7, vi) || p2(\\xc2\\xb7, vi)).\\n(8.37)\\nThe \\xef\\xac\\x81rst-order and second-order proximity embeddings are trained separately,\\nand we concatenate the embeddings together after the training phase as vertex rep-\\nresentations.\\n232\\n8\\nNetwork Representation\\n8.2.3.2\\nGraRep\\nNow we turn to another network representation learning algorithm, GraRep, which\\ndirectly follows the proof of matrix factorization form of DeepWalk. Recall that\\nwe prove DeepWalk is actually factorizing a matrix M where M = log A+A2+\\xc2\\xb7\\xc2\\xb7\\xc2\\xb7+Aw\\nw\\n.\\nGraRep algorithm can be divided into 3 steps:\\n\\xe2\\x80\\xa2 Get k-step transition probability matrix Ak for each k = 1, 2, . . . , K.\\n\\xe2\\x80\\xa2 Get each k-step representation.\\n\\xe2\\x80\\xa2 Concatenate all k-step representations.\\nGraRep uses a simple idea, i.e., SVD decomposition on Ak, in the second step to\\nget embeddings. As K gets large, the matrix M gets denser and thus outputs a better\\nrepresentation. However, this algorithm is not very ef\\xef\\xac\\x81cient especially when K gets\\nlarge.\\n8.2.4\\nStructural Deep Network Methods\\nDifferent from previous methods that use a shallow neural network model to char-\\nacterize the network representations, Structural Deep Network Embedding (SDNE)\\n[125] employs the deeper neural model to model the nonlinearity between vertex\\nembeddings. As shown in Fig.8.2, the whole model can be divided into two parts:\\n(1) the \\xef\\xac\\x81rst part is supervised by Laplacian Eigenmaps, which models the \\xef\\xac\\x81rst-order\\nproximity; (2) the second part is unsupervised deep neural autoencoder which char-\\nacterizes the second-order proximity. Finally, the algorithm takes the intermediate\\nlayer which is used for the supervised part as the network representation.\\nFirst, we will give a brief introduction to deep neural autoencoder. A neural\\nautoencoder requires that the output vector should be as similar to the input vector.\\nGenerally speaking, the output cannot be the same with the input vector because\\nthe dimension of intermediate layers of the autoencoder is much smaller than that\\nof the input and output layer. That is to say, a deep autoencoder \\xef\\xac\\x81rst compresses\\nthe input into a low-dimensional intermediate vector and then tries to reconstruct\\nthe original input vector from the low-dimensional intermediate vector. Once the\\ndeep autoencoder is trained, we can say that the intermediate layer is an excellent\\nlow-dimensional representation of the original inputs since we can recover the input\\nvector from it.\\nMore formally, we assume the input vector is xi. Then the hidden representation\\nof each layer is de\\xef\\xac\\x81ned as\\ny(1)\\ni\\n= Sigmoid(W(1)xi + b(1)),\\ny(k)\\ni\\n= Sigmoid(W(k)y(k\\xe2\\x88\\x921)\\ni\\n+ b(k)), k = 2, 3 . . . ,\\n(8.38)\\n8.2 Network Representation\\n233\\nUnsupervised Component\\nLocal structure preserved cost\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nUnsupervised Component\\nLocal structure preserved cost\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nxi\\nyi\\n(1)\\nyi\\n(k)\\nyi\\n(1)\\nxi\\n^\\n^\\nxj\\nyj\\n(1)\\nyj\\n(k)\\nyj\\n(1)\\nxj\\n^\\n^\\nLaplacian\\nEigenmaps\\nparameter sharing\\nparameter sharing\\nSupervised Component\\n(Global structure\\npreserved cost)\\nVertex i\\nVertex j\\nFig. 8.2 The architecture of structural deep network embedding model\\nwhere W(k) and b(k) are weighted matrix and bias vector of kth layer. We assume\\nthat the hidden representation of the Kth layer has the minimum dimension. After\\nobtaining y(K)\\ni\\n, we can get the output \\xcb\\x86xi by reversing the calculation process. Then the\\noptimization objective of autoencoder is to minimize the difference between input\\nvector xi and output vector \\xcb\\x86xi:\\nL (W, b) =\\nn\\n\\x02\\ni=1\\n\\xe2\\x88\\xa5\\xcb\\x86xi \\xe2\\x88\\x92xi\\xe2\\x88\\xa52,\\n(8.39)\\nwhere n is the number of input instances.\\nBack to the network representation problem, SDNE applies the autoencoder to\\nevery vertex. The input vector xi of each vertex vi is de\\xef\\xac\\x81ned as follows: if vertex vi\\nand v j are connected, then the jth entry xi j > 0, otherwise xi j = 0. For unweighed\\ngraph, if vertex (vi, v j) \\xe2\\x88\\x88E, xi j = 1. Then the intermediate layer y(K)\\ni\\ncan be seen\\nas the low-dimension representation of vertex vi. Also note that there are much more\\nzero entries in input vectors than positive entries due to the sparity of real-world\\nnetwork. Therefore, the loss of positive entries should be emphasized. Therefore, the\\n\\xef\\xac\\x81nal optimization objective of second proximity modeling can be written as\\nL2nd =\\n|V |\\n\\x02\\ni=1\\n\\xe2\\x88\\xa5(\\xcb\\x86xi \\xe2\\x88\\x92xi) \\xe2\\x8a\\x99bi\\xe2\\x88\\xa52,\\n(8.40)\\n234\\n8\\nNetwork Representation\\nwhere \\xe2\\x8a\\x99denotes element-wise multiplication and bi j = 1 if xi j = 0 while bi j =\\n\\xce\\xb2 > 1 if xi j > 0.\\nWe have introduced the unsupervised part modeled by deep autoencoder. Now we\\nturn to the supervised part. The supervised part simply requires that the representation\\nof connected vertices should be close to each other. Thus, the loss function of this\\npart is\\nL1st =\\n|V |\\n\\x02\\ni, j=1\\nxi j\\xe2\\x88\\xa5y(K)\\ni\\n\\xe2\\x88\\x92y(K)\\nj\\n\\xe2\\x88\\xa52.\\n(8.41)\\nFinally, the overall loss function included regularization term is\\nL = L2nd + \\xce\\xb1L1st + \\xce\\xbbLreg,\\n(8.42)\\nwhere \\xce\\xb1 and \\xce\\xbb are harmonic hyperparameter and regularization loss Lreg is the sum\\nof the square of all parameters. The model can be optimized by back-propagation\\nin a standard neural network way. After the training process, y(K)\\ni\\nis taken as the\\nrepresentation of vertex vi.\\n8.2.5\\nExtensions\\n8.2.5.1\\nNetwork Representation with Internal Information\\nAsymmetric Transitivity Preserving Network Representation. Existing network\\nrepresentation learning algorithms mostly focus on an undirected graph. Most of\\nthe methods cannot handle the directed graph well because they do not accurately\\ncharacterize the asymmetric property. High-Order Proximity preserved Embedding\\n(HOPE) [89] is proposed to preserve high-order proximities of large-scale graphs and\\ncapture the asymmetric transitivity. The algorithm further derives a general formula-\\ntion that covers multiple popular high-order proximity measurements and provides\\nan approximate algorithm with an upper bound of RMSE (Root Mean Squared Error).\\nNetwork embedding assumes that the more and the shorter paths from vi to v j,\\nthe more similar should be their representation vectors. In particular, the algorithm\\nassigns two vectors, i.e., source and target vectors for each vertex. We denote adja-\\ncency matrix as A and the user representations as U = [Us, Ut], where Us \\xe2\\x88\\x88R|V |\\xc3\\x97d\\nand Ut \\xe2\\x88\\x88R|V |\\xc3\\x97d are source and target vertex embeddings, respectively. We de\\xef\\xac\\x81ne\\na high-order proximity matrix as S, where Si j is the proximity between vi and v j.\\nThen our goal is to approximate the matrix S with the product of Us and Ut. The\\noptimization objective can be written as\\nmin\\nUs,Ut \\xe2\\x88\\xa5S \\xe2\\x88\\x92UsUt \\xe2\\x8a\\xa4\\xe2\\x88\\xa52\\nF.\\n(8.43)\\n8.2 Network Representation\\n235\\nMany high-order proximity measurements which characterize the asymmetric\\ntransitivity share a general formulation which can be used for the approximation of\\nthe proximities:\\nS = M\\xe2\\x88\\x921\\ng Ml,\\n(8.44)\\nwhere Mg and Ml are both polynomials of matrices. Now we will take three com-\\nmonly used high-order proximity measurements to illustrate the formula.\\n\\xe2\\x80\\xa2 Katz Index Katz Index is a weighted summation over the path set between two\\nvertices. The computation of the Katz Index can be written recurrently:\\nS := \\xce\\xb2 AS + \\xce\\xb2 A,\\n(8.45)\\nwhere the decay parameter \\xce\\xb2 represents how fast the weight decreases when the\\nlength of paths grows.\\n\\xe2\\x80\\xa2 Rooted PageRank For rooted PageRank, Si j is the probability that a random walk\\nfrom vertex vi will locate at v j in the stable state. The formula can be written as\\nS := \\xce\\xb1SP + (1 \\xe2\\x88\\x92\\xce\\xb1)I,\\n(8.46)\\nwhere \\xce\\xb1 is the probability that a random walk returns to its start point and P is the\\ntransition matrix.\\n\\xe2\\x80\\xa2 Common Neighbors Si j is the number of vertexes which is the target of an edge\\nfrom vi and the source of an edge to v j. The matrix S can be expressed as\\nS = A2.\\n(8.47)\\nFor the three high-order proximity measurements introduced above, we summa-\\nrize their equivalent form S = M\\xe2\\x88\\x921\\ng Ml in the following table (Table8.3).\\nA simple idea of approximating S with the product of matrices is SVD decom-\\nposition. However, the direct computation of SVD decomposition of matrix S has a\\ncomplexity of O(|V |3). By writing matrix S as M\\xe2\\x88\\x921\\ng Ml, we do not need to compute\\nmatrix S directly. Instead, we can do JDGSVD decomposition on Mg and Ml inde-\\npendently and then use their results to derive the decomposition of S. The complexity\\nreduces to |E|d2 for each iteration of JDGSVD.\\nCommunity Preserving Network Representation. While previous methods aim\\nat preserving the microscopic structure of a network such as \\xef\\xac\\x81rst- and second-order\\nTable 8.3 General formula for high-order proximity measurements\\nMeasurement\\nMg\\nMl\\nKatz index\\nI \\xe2\\x88\\x92\\xce\\xb2 A\\n\\xce\\xb2 A\\nRooted PageRank\\nI \\xe2\\x88\\x92\\xce\\xb1P\\n(1 \\xe2\\x88\\x92\\xce\\xb1)I\\nCommon neighbors\\nI\\nA2\\n236\\n8\\nNetwork Representation\\nproximities. Wang et al. [127] proposed Modularized Nonnegative Matrix Factor-\\nization (M-NMF), which encodes the mesoscopic community structure information\\ninto the network representations. The basic idea is to consider the modularity as part\\nof the optimization function. Recall that the modularity is formulated in Eq.8.15 and\\nS is the community indicator matrix. Then the loss function of modularity part is to\\nminimize \\xe2\\x88\\x92tr(S\\xe2\\x8a\\xa4BS).\\nSimilar to previous methods, M-NMF also factorizes an af\\xef\\xac\\x81nity matrix which\\nencodes \\xef\\xac\\x81rst-order and second-order proximities. Speci\\xef\\xac\\x81cally, M-NMF takes adja-\\ncency matrix A as the \\xef\\xac\\x81rst-order proximity matrix A1 and computes the cosine\\nsimilarity of corresponding rows of adjacency matrix A as the second-order prox-\\nimity matrix A2. M-NMF uses a mixture of A1 and A2 as the similarity matrix. To\\nconclude, the overall optimization function of M-NMF is\\nmin\\nM,U,S,C\\n\\x03\\x03A1 + \\xce\\xb7A2 \\xe2\\x88\\x92MU\\xe2\\x8a\\xa4\\x03\\x032\\nF + \\xce\\xb1\\n\\x03\\x03S \\xe2\\x88\\x92UC\\xe2\\x8a\\xa4\\x03\\x032\\nF \\xe2\\x88\\x92\\xce\\xb2tr(S\\xe2\\x8a\\xa4BS),\\n(8.48)\\nwhereS \\xe2\\x88\\x88R|V |\\xc3\\x97k, M, U\\xe2\\x88\\x88R|V |\\xc3\\x97m, C\\xe2\\x88\\x88Rk\\xc3\\x97m, Mi j, Ui j, Si j, Ci j \\xe2\\x89\\xa50, \\xe2\\x88\\x80i\\xe2\\x88\\x80j, tr(S\\xe2\\x8a\\xa4S) =\\n|V | and \\xce\\xb1, \\xce\\xb2, \\xce\\xb7 > 0 are harmonic hyperparameters. Subscript F denotes Frobenius\\nnorm. Here similarity matrix A1 + \\xce\\xb7A2 is factorized into two nonnegative matrices\\nM and U. Then community representation matrix C in the second term bridges the\\nmatrix factorization part and the modularity part.\\nA concurrent algorithm Community-enhanced NRL (CNRL) [116, 117] is a\\npipeline algorithm that learns node-community assignment at \\xef\\xac\\x81rst and then reforms\\nthe DeepWalk algorithm to incorporate community information. Speci\\xef\\xac\\x81cally, in the\\n\\xef\\xac\\x81rst phase, CNRL made an analogy between community detection and topic model-\\ning. Then CNRL started by generating random walks and fed these vertex sequences\\ninto Latent Dirichlet Allocation (LDA) algorithm. By taking a vertex as a word and a\\ntopic as a community, CNRL can get a soft-assignment of vertex-community mem-\\nbership. Then in the second phase, both the embedding of a center node and the\\nembedding of its community are used to predict the neighborhood vertices in the\\nrandom walk sequences. The illustration \\xef\\xac\\x81gure is shown in Fig.8.3.\\n8.2.5.2\\nNetwork Representation with External Information\\nNetwork Representation with Text Information. We will present the network\\nembedding algorithm TADW, which further generalizes the matrix factorization\\nframework to take advantage of text information. Text-Associated DeepWalk\\n(TADW) [136] incorporates text features of vertices into network representation\\nlearning under the framework of matrix factorization. The matrix factorization view\\nof DeepWalk enables the introduction of text information into matrix factorization for\\nnetwork representation learning. Figure8.4 shows the main idea of TADW: factorize\\nvertex af\\xef\\xac\\x81nity matrix M \\xe2\\x88\\x88R|V |\\xc3\\x97|V | into the product of three matrices: W \\xe2\\x88\\x88Rk\\xc3\\x97|V |,\\nH \\xe2\\x88\\x88Rk\\xc3\\x97 ft, and text features T \\xe2\\x88\\x88R ft\\xc3\\x97|V |. Then TADW concatenates W and HT as\\n2k-dimensional representations of vertices.\\n8.2 Network Representation\\n237\\nVetex\\nembeddings\\nSliding window\\nCommunity\\nembeddings\\nCommunity\\nembedding\\nAssigned\\ncommunities\\n1\\n1\\n2 \\n2 \\n3 \\n3 \\n4 \\n4 \\n5\\n5\\n6\\n6\\n7\\n7\\nVetex sequence\\nRandom walks\\non a network\\nCommunity 1\\nCommunity 2\\nCommunity 3\\nFig. 8.3 The architecture of community preserving network embedding model\\nFig. 8.4 The architecture of\\ntext-associated DeepWalk\\nmodel\\n\\xc3\\x97\\n\\xc3\\x97\\nV\\nV\\nV\\nk\\nM\\nWT\\nH\\nT\\nft\\nThen the question is how to build vertex af\\xef\\xac\\x81nity matrix M and how to extract\\ntext feature T from the text information. Following the proof of matrix factorization\\nform of DeepWalk, TADW set vertex af\\xef\\xac\\x81nity matrix M to a tradeoff between speed\\nand accuracy: factorize the matrix M = (A + A2)/2 where A is the row-normalized\\nadjacency matrix. For text feature matrix T, TADW \\xef\\xac\\x81rst constructs the TF-IDF matrix\\nfrom the text and then reduces the dimension of the TF-IDF matrix to 200 via SVD\\ndecomposition.\\n238\\n8\\nNetwork Representation\\nReconstructed\\nEdge Vector\\nEdge Autoencoder\\nLabel#2\\nLabel#5\\nu\\nl\\nu\\nv\\nv\\nTranslation\\nMechanism\\nBinary Edge\\nVertor\\nEdge with\\nMulti-labels\\nFig. 8.5 The architecture of TransNet model\\nFormally, the model of TADW minimizes the following optimization function:\\nmin\\nW,H \\xe2\\x88\\xa5M \\xe2\\x88\\x92W\\xe2\\x8a\\xa4HT\\xe2\\x88\\xa52\\nF + \\xce\\xbb\\n2(\\xe2\\x88\\xa5W\\xe2\\x88\\xa52\\nF + \\xe2\\x88\\xa5H\\xe2\\x88\\xa52\\nF),\\n(8.49)\\nwhere \\xce\\xbb is the regularization factor. The optimization of parameters are processed\\nby updating W and H iteratively via conjugate gradient descent.\\nTransNet. Most existing NRL methods neglect the semantic information of edges\\nand simplify the edge as a binary or continuous value. TransNet algorithm [119]\\nconsiders the label information on the edges instead of nodes. In particular, TransNet\\nis based on translation mechanism shown in Fig.8.5.\\nIn the settings of TransNet, each edge has a number of binary labels on it. Then the\\nloss function of TransNet consists of two parts: one part is the translation loss which\\nmeasures the distance between u + e and v where u, e, v stand for the embeddings\\nof head vertex, edge, and tail vertex; another part is the reconstruction loss of the\\nautoencoder which encodes the labels of an edge into its embedding e and restore\\nthe labels from the embedding. After the learning phase, we can compute the edge\\nembedding by subtracting two vertices and use the decoder part of the autoencoder\\nto predict the labels of an unobserved edge.\\n8.2 Network Representation\\n239\\nSemi-supervised Network Representation. In this part, we introduce several\\nsemi-supervised network representation learning methods that are applied to het-\\nerogeneous networks. All methods learn vertex embeddings and their classi\\xef\\xac\\x81cation\\nlabels simultaneously.\\n(1) LSHM The \\xef\\xac\\x81rst algorithm LSHM (Latent Space Heterogeneous Model) [52],\\nfollows the manifold assumption which assumes that two connected nodes tend to\\nhave similar node embeddings. Thus, the regularization loss which forces connected\\nnodes to have similar representations can be formulated as\\n\\x02\\ni, j\\nwi j\\xe2\\x88\\xa5vi \\xe2\\x88\\x92v j\\xe2\\x88\\xa52,\\n(8.50)\\nwhere wi j is the weight of edge (vi, v j).\\nAs a semi-supervised representation learning algorithm, LHSM also needs to\\npredict the classi\\xef\\xac\\x81cation labels for unlabeled vertices. To train the classi\\xef\\xac\\x81ers, LHSM\\ncomputes the loss of observed labels as\\n\\x02\\ni\\n\\xce\\x94( f\\xce\\xb8(vi), yi),\\n(8.51)\\nwhere f\\xce\\xb8(vi) is the predicted label for vertex vi, yi is the observed label for vi and\\n\\xce\\x94(\\xc2\\xb7, \\xc2\\xb7) is the loss function between predicted label and ground truth label. Speci\\xef\\xac\\x81-\\ncally, f\\xce\\xb8(\\xc2\\xb7) is a linear function and \\xce\\x94(\\xc2\\xb7, \\xc2\\xb7) is set to hinge loss.\\nFinally, the objective function is\\nL (V, \\xce\\xb8) =\\n\\x02\\ni\\n\\xce\\x94( f\\xce\\xb8(vi), yi) + \\xce\\xbb\\n\\x02\\ni, j\\nwi j\\xe2\\x88\\xa5vi \\xe2\\x88\\x92v j\\xe2\\x88\\xa52,\\n(8.52)\\nwhere \\xce\\xbb is a harmonic hyperparameter. The algorithm is optimized via stochastic\\ngradient descent.\\n(2) node2vec Node2vec [38] modi\\xef\\xac\\x81es DeepWalk by changing the generation\\nof random walks. As shown in previous subsections, DeepWalk generates rooted\\nrandom walks by choosing the next vertex according to a uniform distribution, which\\ncould be improved by using a well-designed random walk generation strategy.\\nNode2vec \\xef\\xac\\x81rst considers two extreme cases of vertex visiting sequences: Breadth-\\nFirst Search (BFS) and Depth-First Search (DFS). By restricting the search to nearby\\nnodes, BFS characterizes the nearby neighborhoods of center vertices and obtains\\na microscopic view of the neighborhood of every node. Vertices in the sampled\\nneighborhoods of BFS tend to repeat many times, which can reduce the variance in\\ncharacterizing the distribution of neighboring vertices of the source node. In contrast,\\nthesamplednodesinDFSre\\xef\\xac\\x82ectamacro-viewoftheneighborhoodwhichisessential\\nin inferring communities based on homophily.\\nNode2vec designs a neighborhood sampling strategy which can smoothly inter-\\npolate between BFS and DFS. More speci\\xef\\xac\\x81cally, consider a random walk that just\\nwalks through edge (t, v) and now stays at vertex v. The walk evaluates the transition\\n240\\n8\\nNetwork Representation\\nprobabilities of edge (v, x) to decide the next step. Node2vec sets the unnormalized\\ntransition probability to \\xcf\\x80vx = \\xce\\xb1pq(t, x) \\xc2\\xb7 wvx, where\\n\\xce\\xb1pq(t, x) =\\n\\xe2\\x8e\\xa7\\n\\xe2\\x8e\\xa8\\n\\xe2\\x8e\\xa9\\n1\\np if dtx = 0,\\n1 if dtx = 1,\\n1\\nq if dtx = 2,\\n(8.53)\\nand dtx denotes the shortest path distance between vertices t and x. p and q are\\nparameters that guide the random walk and control how fast the walk explores and\\nleaves the neighborhood of starting vertex. A low p will increase the probability of\\nrevisiting a vertex and make the random walk focus on local neighborhoods while a\\nlowq will encourage the random walk to explore further vertices. After the generation\\nof the random walks, the rest of the algorithm is almost the same as that of DeepWalk.\\n(3) MMDW Max-Margin DeepWalk (MMDW) [118] utilizes the max-margin\\nstrategy in SVM to generalize DeepWalk algorithm for semi-supervised learning.\\nSpeci\\xef\\xac\\x81cally, MMDW employs the matrix factorization form of DeepWalk proved\\nin TADW [136] and further add the max-margin constraint which requires that the\\nembeddings of nodes from different labels should be far from each other. The opti-\\nmization function can be written as\\nmin\\nX,Y,W,\\xce\\xbe L =\\nmin\\nX,Y,W,\\xce\\xbe LDW + 1\\n2\\xe2\\x88\\xa5W\\xe2\\x88\\xa52 + C\\nT\\n\\x02\\ni=1\\n\\xce\\xbei,\\ns.t. w\\xe2\\x8a\\xa4\\nli xi \\xe2\\x88\\x92w\\xe2\\x8a\\xa4\\nj xi \\xe2\\x89\\xa5e j\\ni \\xe2\\x88\\x92\\xce\\xbei, \\xe2\\x88\\x80i, j,\\n(8.54)\\nwhere W = [w1, w2, . . . , wm]T is the weight matrix of SVM, \\xce\\xbe is the slack variables,\\ne j\\ni = 1 if li \\xcc\\xb8= j and e j\\ni = 0 otherwise, and LDW is the matrix factorization form\\nDeepWalk loss function:\\nLDW = \\xe2\\x88\\xa5M \\xe2\\x88\\x92X\\xe2\\x8a\\xa4Y\\xe2\\x88\\xa52\\n2 + \\xce\\xbb\\n2(\\xe2\\x88\\xa5X\\xe2\\x88\\xa52\\n2 + \\xe2\\x88\\xa5Y\\xe2\\x88\\xa52\\n2),\\n(8.55)\\nwhich is introduced in previous sections.\\nFigure8.6 shows the visualization result of the DeepWalk and MMDW algorithm\\non the Wiki dataset [103]. We can see that the embeddings of nodes from different\\nclasses are more separable with the help of semi-supervised max-margin represen-\\ntation learning.\\n(4)PTEAnotheralgorithmcalledPTE(PredictiveTextEmbedding)[110]focuses\\non text network such as the bibliography network where a paper is a vertex, and\\nthe citation relationship between papers forms the edges. PTE considers network\\nstructure together with plain text and observed vertex labels. PTE proposes a semi-\\nsupervised framework to learn vertex representation and predict unobserved vertex\\nlabels.\\n8.2 Network Representation\\n241\\nFig. 8.6 A visualization of t-SNE 2D representations on Wiki dataset (left: DeepWalk, right:\\nMMDW) [118]\\nA text network is divided into three bipartite networks: word-word, word-\\ndocument, and word-label networks. We will introduce the de\\xef\\xac\\x81nition of the three\\nnetworks in more detail.\\nFor the word-word network, the weight wi j of the edge between word vi and v j is\\nde\\xef\\xac\\x81ned as the number of times that the two words co-occur in the same context win-\\ndows. For word-document network, the weight wi j between word vi and document\\nd j is de\\xef\\xac\\x81ned as the number of times vi appears in document d j. For the word-label\\nnetwork, the weight wi j of the edge between word vi and class c j is de\\xef\\xac\\x81ned as:\\nwi j = \\x04\\nd:ld= j ndi, where ndi is the term frequency of word vi in document d, and ld\\nis the class label of document d.\\nThen following previous work LINE, given bipartite network G = (VA \\xe2\\x88\\xaaVB, E),\\nthe conditional probability of generating vi \\xe2\\x88\\x88VA from v j \\xe2\\x88\\x88VB is de\\xef\\xac\\x81ned as\\nP(vi|v j) =\\nexp(v j \\xc2\\xb7 vi)\\n\\x04|V |\\nk=1 exp(vk \\xc2\\xb7 vi)\\n.\\n(8.56)\\nSimilar to LINE model, the loss function is de\\xef\\xac\\x81ned as the KL-divergence between\\nempirical distribution and conditional distribution. The optimization objective can\\nbe further formulated as\\nL = \\xe2\\x88\\x92\\n\\x02\\n(vi,v j)\\xe2\\x88\\x88E\\nwi j log P(vi|v j).\\n(8.57)\\n242\\n8\\nNetwork Representation\\nThen the \\xef\\xac\\x81nal objective can be obtained by summing all three bipartite networks:\\nLpte = Lww + Lwd + Lwl,\\n(8.58)\\nwhere\\nLww = \\xe2\\x88\\x92\\n\\x02\\n(vi,v j)\\xe2\\x88\\x88Eww\\nwi j log P(vi|v j),\\n(8.59)\\nLwd = \\xe2\\x88\\x92\\n\\x02\\n(vi,v j)\\xe2\\x88\\x88Ewd\\nwi j log P(vi|d j),\\n(8.60)\\nLwl = \\xe2\\x88\\x92\\n\\x02\\n(vi,v j)\\xe2\\x88\\x88Ewl\\nwi j log P(vi|l j).\\n(8.61)\\nThen the optimization can be done by stochastic gradient descent.\\n8.2.5.3\\nTask-Speci\\xef\\xac\\x81c Network Representation\\nNetwork Representation for Community Detection. As shown in spectral cluster-\\ning methods, people make their effort to learn community indicator matrix based on\\nmodularity and normalized graph cut. The continuous community indicator matrix\\ncan be seen as a k-dimensional vertex representation, where k is the number of com-\\nmunities. Note that modularity and graph cut is de\\xef\\xac\\x81ned for nonoverlapping commu-\\nnities. By alternating a cost function for overlapping communities, the idea can also\\nwork for overlapping community detection. In this subsection, we will introduce sev-\\neral community detection algorithms. These community detection algorithms start\\nby learning a k-dimensional nonnegative vertex-community af\\xef\\xac\\x81nity matrix and then\\nderive a hard community assignment for vertices based on the matrix. Therefore, the\\nkey procedure of these algorithms can be regarded as an unsupervised k-dimensional\\nnonnegative vertex embedding learning.\\nBIGCLAM [140] is an overlapping community detection method. It assumes that\\nmatrix F \\xe2\\x88\\x88R|V |\\xc3\\x97k is the user-community af\\xef\\xac\\x81nity matrix, where Fvc is the strength\\nbetween vertex v and community c. Matrix F is nonnegative and Fvc = 0 indicates\\nno af\\xef\\xac\\x81liation. BIGCLAM builds a generative model by modeling the probability that\\nvertexvi connectsv j givenuser-communityaf\\xef\\xac\\x81nitymatrixF.Morespeci\\xef\\xac\\x81cally,given\\nmatrix F, BIGCLAM generates an edge between vertex vi and v j with a probability\\nP(vi, v j) = 1 \\xe2\\x88\\x92exp(\\xe2\\x88\\x92Fvi \\xc2\\xb7 Fv j),\\n(8.62)\\nwhere Fvi is the corresponding row of matrix F for vertex vi and can be seen as the\\nrepresentation of vi. Note that the probability P(vi, v j) has an increasing relationship\\nwith Fvi \\xc2\\xb7 F\\xe2\\x8a\\xa4\\nv j = \\x04\\nc Fvi,cFv j,c, which indicates that the more communities a pair of\\nnodes shared, the more likely they are connected.\\n8.2 Network Representation\\n243\\nFor the case that Fvi \\xc2\\xb7 Fv j = 0, BIGCLAM adds a background probability \\xcf\\xb5 =\\n2|E|\\n|V |(|V |\\xe2\\x88\\x921) to the pair of nodes to avoid a zero probability.\\nThen BIGCLAM tries to maximize the log-likelihood of the graph G = (V, E):\\nO(F) =\\n\\x02\\ni, j:(vi,v j)\\xe2\\x88\\x88E\\nlog P(vi, v j) +\\n\\x02\\ni, j:(vi,v j)/\\xe2\\x88\\x88E\\nlog(1 \\xe2\\x88\\x92P(vi, v j)),\\n(8.63)\\nwhich can be reformulated as\\nO(F) =\\n\\x02\\ni, j:(vi,v j)\\xe2\\x88\\x88E\\nlog(1 \\xe2\\x88\\x92exp(\\xe2\\x88\\x92Fvi \\xc2\\xb7 Fv j)) \\xe2\\x88\\x92\\n\\x02\\ni, j:(vi,v j)/\\xe2\\x88\\x88E\\nFvi \\xc2\\xb7 Fv j.\\n(8.64)\\nThe parameters F are learned by projected gradient descent. Note that the train-\\ning objective can be regarded as a variant of nonnegative matrix factorization. The\\nmaximization of log-likelihood function is an approximation of adjacency matrix A\\nby FF\\xe2\\x8a\\xa4. Compared with L2-norm loss function, the gradient of Eq.8.64 can be com-\\nputed more ef\\xef\\xac\\x81ciently for a sparse matrix A which is the most case in the real-world\\ndataset.\\nThe model can also be generalized to asymmetric case [141]. That is to replace\\nEq.8.62 by\\nP(vi, v j) = 1 \\xe2\\x88\\x92exp(\\xe2\\x88\\x92Fvi \\xc2\\xb7 Hv j),\\n(8.65)\\nwhere H is another matrix that has the same size with the matrix F. The generative\\nmodel can also consider attributes of vertices by adding attribute terms to Eq.8.62\\n[79].\\n8.2.5.4\\nNetwork Representation for Visualization\\nDifferent from previous algorithms that focus on machine learning tasks, the algo-\\nrithms introduced in this subsection are designed for visualization. As a commonly\\nused data structure, the visualization of networks is an important task. The dimen-\\nsions of representations of vertices are usually 2 or 3 to draw the graph.\\nRepresentation learning for network visualization generally follows the following\\naesthetic criteria [30]:\\n\\xe2\\x80\\xa2 Distribute the vertices evenly in the frame.\\n\\xe2\\x80\\xa2 Minimize edge crossings.\\n\\xe2\\x80\\xa2 Make edge lengths uniform.\\n\\xe2\\x80\\xa2 Re\\xef\\xac\\x82ect inherent symmetry.\\n\\xe2\\x80\\xa2 Conform to the frame.\\nFollowing these criteria, graph visualization algorithms build a force-directed\\ngraph drawing framework. The basic assumption is that there is a spring between\\neach pair of vertices. Then the optimization objective is to minimize the energy of\\nthe graph according to Hooke\\xe2\\x80\\x99s law:\\n244\\n8\\nNetwork Representation\\nE =\\n\\x02\\ni, j\\n1\\n2ki j(\\xe2\\x88\\xa5vi \\xe2\\x88\\x92v j\\xe2\\x88\\xa5\\xe2\\x88\\x92li j)2,\\n(8.66)\\nwhere ki j is spring constant, vi is the position of vertex vi and li j is the length of\\nshortest path between vertex vi and v j. The intuition is straightforward: close vertices\\nshould have close positions in the drawing. Several algorithms have been proposed\\nto improve this framework [34, 54, 60] by changing the setting of spring constant ki j\\nor the energy function. The parameters can be easily learned via gradient descent.\\n8.2.5.5\\nEmbedding Enhancement via High-Order Proximity\\nApproximation\\nYang et al. [137] summarize several existing NRL methods into a uni\\xef\\xac\\x81ed two-step\\nframework, including proximity matrix construction and dimension reduction. They\\nconclude that an NRL method can be improved by exploring higher order proximities\\nwhen building the proximity matrix. Then they propose Network Embedding Update\\n(NEU) algorithm, which implicitly approximates higher order proximities with the-\\noretical approximation bound and can be applied to any NRL methods to enhance\\ntheir performances. NEU can make a consistent and signi\\xef\\xac\\x81cant improvement over\\nsome NRL methods with almost negligible running time.\\nThe two-step framework is summarized as follows:\\nStep 1: Proximity Matrix Construction. Compute a proximity matrix M \\xe2\\x88\\x88\\nR|V |\\xc3\\x97|V |, which encodes the information of k-order proximity matrix where k =\\n1, 2 . . . , K. For example, M = 1\\nK A + 1\\nK A2 \\xc2\\xb7 \\xc2\\xb7 \\xc2\\xb7 + 1\\nK AK stands for an average com-\\nbination of k-order proximity matrix for k = 1, 2 . . . , K. The proximity matrix M is\\nusually represented by a polynomial of normalized adjacency matrix A of degree K,\\nand we denote the polynomial as f (A) \\xe2\\x88\\x88R|V |\\xc3\\x97|V |. Here the degree K of polynomial\\nf (A) corresponds to the maximum order of proximities encoded in the proximity\\nmatrix. Note that the storage and computation of proximity matrix M doesn\\xe2\\x80\\x99t nec-\\nessarily take O(|V |2) time because we only need to save and compute the nonzero\\nentries.\\nStep 2: Dimension Reduction. Find network embedding matrix V \\xe2\\x88\\x88R|V |\\xc3\\x97d and\\ncontext embedding C \\xe2\\x88\\x88R|V |\\xc3\\x97d so that the product VC\\xe2\\x8a\\xa4approximates proximity\\nmatrix M. Here different algorithms may employ different distance functions to\\nminimize the distance between M and VC\\xe2\\x8a\\xa4. For example, we can naturally use the\\nnorm of matrix M \\xe2\\x88\\x92VC\\xe2\\x8a\\xa4to measure the distance and minimize it.\\nSpectral Clustering, DeepWalk, and GraRep can be formalized into the two-step\\nframework. Now we focus on the \\xef\\xac\\x81rst step and study how to de\\xef\\xac\\x81ne the right proximity\\nmatrix for NRL.\\nWe summarize the comparisons among Spectral Clustering (SC), DeepWalk, and\\nGraRep in Table8.4 and conclude the following observations.\\n8.2 Network Representation\\n245\\nTable 8.4 Comparisons among three NRL methods\\nSC\\nDeepWalk\\nGraRep\\nProximity matrix\\nL\\n\\x04K\\nk=1\\nAk\\nK\\nAk, k = 1 . . . K\\nComputation\\nAccurate\\nApproximate\\nAccurate\\nScalability\\nYes\\nYes\\nNo\\nPerformance\\nLow\\nMiddle\\nHigh\\nObservation 8.1 Modeling higher order and accurate proximity matrix can improve\\nthe quality of network representation. In other words, NRL can bene\\xef\\xac\\x81t from exploring\\na polynomial proximity matrix f (A) of a higher degree.\\nFrom the development of NRL methods, it can be seen that DeepWalk outperforms\\nSpectral Clustering because DeepWalk considers higher order proximity matrices,\\nand the higher order proximity matrices can provide complementary information for\\nlower order proximity matrices. GraRep outperforms DeepWalk because GraRep\\naccurately calculates the k-order proximity matrix rather than approximating it by\\nMonte Carlo simulation as DeepWalk does.\\nObservation 8.2 Accurate computation of high-order proximity matrix is not fea-\\nsible for large-scale networks.\\nThe major drawback of GraRep is the computation complexity of calculating the\\naccurate k-order proximity matrix. In fact, the computation of high-order proximity\\nmatrix takes O(|V |2) time and the time complexity of SVD decomposition also\\nincreases as k-order proximity matrix gets dense when k grows. In summary, the\\ntime complexity of O(|V |2) is too expensive to handle large-scale networks.\\nThe \\xef\\xac\\x81rst observation provides the motivation to explore higher order proximity\\nmatrices in NRL models, but the second observation indicates that an accurate infer-\\nence of higher order proximity matrices isn\\xe2\\x80\\x99t acceptable. Therefore, how to learn\\nnetwork embeddings from approximate higher order proximity matrices ef\\xef\\xac\\x81ciently\\nbecomes important. To be more ef\\xef\\xac\\x81cient, the network representations which encode\\nthe information of lower order proximity matrices can be used as our basis to avoid\\nrepeated computations. The problem is formalized below.\\nProblem Formalization. Assume that we have normalized adjacency matrix A\\nas the \\xef\\xac\\x81rst-order proximity matrix, network embedding V, and context embedding C,\\nwhere V, C \\xe2\\x88\\x88R|V |\\xc3\\x97d. Suppose that the embeddings V and C are learned by the above\\nNRL framework which indicates that the product VC\\xe2\\x8a\\xa4approximates a polynomial\\nproximity matrix f (A) of degree K. The goal is to learn a better representation V\\xe2\\x80\\xb2\\nand C\\xe2\\x80\\xb2, which approximates a polynomial proximity matrix g(A) with higher degree\\nthan f (A). Also, the algorithm should be ef\\xef\\xac\\x81cient in the linear time of |V |. Note\\nthat the lower bound of time complexity is O(|V |d) which is the size of embedding\\nmatrix R.\\nThere is a simple, ef\\xef\\xac\\x81cient, and effective iterative updating algorithm to solve the\\nabove problem.\\n246\\n8\\nNetwork Representation\\nMethod. Given hyperparameter \\xce\\xbb \\xe2\\x88\\x88(0, 1\\n2], normalized adjacency matrix A, we\\nupdate V and C as follows:\\nV\\xe2\\x80\\xb2 = V + \\xce\\xbbAV,\\nC\\xe2\\x80\\xb2 = C + \\xce\\xbbA\\xe2\\x8a\\xa4C.\\n(8.67)\\nThe time complexity of computing AV and A\\xe2\\x8a\\xa4C is O(|V |d) because matrix A\\nis sparse and has O(|V |) nonzero entries. Thus the overall time complexity of one\\niteration of operation (Eq.8.67) is O(|V |d).\\nRecall that product of previous embedding V and C approximates polynomial\\nproximity matrix f (A) of degree K. It can be proved that the algorithm can learn\\nbetter embeddings V\\xe2\\x80\\xb2 and C\\xe2\\x80\\xb2, where the product V\\xe2\\x80\\xb2C\\xe2\\x80\\xb2\\xe2\\x8a\\xa4approximates a polynomial\\nproximity matrix g(A) of degree K + 2 bounded by matrix in\\xef\\xac\\x81nite norm.\\nTheorem Denote the network and context embedding by V and C, and suppose\\nthat the approximation between VC\\xe2\\x8a\\xa4and proximity matrix M = f (A) is bounded\\nby r = \\xe2\\x88\\xa5f (A) \\xe2\\x88\\x92VC\\xe2\\x8a\\xa4\\xe2\\x88\\xa5\\xe2\\x88\\x9eand f (\\xc2\\xb7) is a polynomial of degree K. Then the product\\nof updated embeddings V\\xe2\\x80\\xb2 and C\\xe2\\x80\\xb2 from Eq.8.67 approximates a polynomial g(A) =\\nf (A) + 2\\xce\\xbbAf (A) + \\xce\\xbb2 A2 f (A) of degree K + 2 with approximation bound r\\xe2\\x80\\xb2 =\\n(1 + 2\\xce\\xbb + \\xce\\xbb2)r \\xe2\\x89\\xa49\\n4r.\\nProof Assume that S = f (A) \\xe2\\x88\\x92VC\\xe2\\x8a\\xa4and thus r = \\xe2\\x88\\xa5S\\xe2\\x88\\xa5\\xe2\\x88\\x9e.\\n\\xe2\\x88\\xa5g(A) \\xe2\\x88\\x92V\\xe2\\x80\\xb2C\\xe2\\x80\\xb2\\xe2\\x8a\\xa4\\xe2\\x88\\xa5\\xe2\\x88\\x9e= \\xe2\\x88\\xa5g(A) \\xe2\\x88\\x92(V + \\xce\\xbbAV)(C\\xe2\\x8a\\xa4+ \\xce\\xbbC\\xe2\\x8a\\xa4A)\\xe2\\x88\\xa5\\xe2\\x88\\x9e\\n= \\xe2\\x88\\xa5g(A) \\xe2\\x88\\x92VC\\xe2\\x8a\\xa4\\xe2\\x88\\x92\\xce\\xbbAVC\\xe2\\x8a\\xa4\\xe2\\x88\\x92\\xce\\xbbVC\\xe2\\x8a\\xa4A \\xe2\\x88\\x92\\xce\\xbb2 AVC\\xe2\\x8a\\xa4A\\xe2\\x88\\xa5\\xe2\\x88\\x9e\\n= \\xe2\\x88\\xa5S + \\xce\\xbbAS + \\xce\\xbbSA + \\xce\\xbb2 ASA\\xe2\\x88\\xa5\\xe2\\x88\\x9e\\n\\xe2\\x89\\xa4\\xe2\\x88\\xa5S\\xe2\\x88\\xa5\\xe2\\x88\\x9e+ \\xce\\xbb\\xe2\\x88\\xa5A\\xe2\\x88\\xa5\\xe2\\x88\\x9e\\xe2\\x88\\xa5S\\xe2\\x88\\xa5\\xe2\\x88\\x9e+ \\xce\\xbb\\xe2\\x88\\xa5S\\xe2\\x88\\xa5\\xe2\\x88\\x9e\\xe2\\x88\\xa5A\\xe2\\x88\\xa5\\xe2\\x88\\x9e+ \\xce\\xbb2\\xe2\\x88\\xa5S\\xe2\\x88\\xa5\\xe2\\x88\\x9e\\xe2\\x88\\xa5A\\xe2\\x88\\xa52\\n\\xe2\\x88\\x9e\\n= r + 2\\xce\\xbbr + \\xce\\xbb2r,\\n(8.68)\\nwhere the second last equality replaces g(A) and f (A) \\xe2\\x88\\x92VC\\xe2\\x8a\\xa4by the de\\xef\\xac\\x81nitions\\nof g(A) and S and the last equality uses the fact that \\xe2\\x88\\xa5A\\xe2\\x88\\xa5\\xe2\\x88\\x9e= maxi\\n\\x04\\nj |Ai j| = 1\\nbecause the summation of each row of A equals to 1.\\nIn the experimental settings, it is assumed that the weight of lower order proxim-\\nities should be larger than higher order proximities because they are more directly\\nrelated to the original network. Therefore, given g(A) = f (A) + 2\\xce\\xbbAf (A) + \\xce\\xbb2 A2\\nf (A), we have 1 \\xe2\\x89\\xa52\\xce\\xbb \\xe2\\x89\\xa5\\xce\\xbb2 > 0 which indicates that \\xce\\xbb \\xe2\\x88\\x88(0, 1\\n2]. The proof indicates\\nthat the updated embedding can implicitly approximate a polynomial g(A) of 2 more\\ndegrees within 9\\n4 times matrix in\\xef\\xac\\x81nite norm of previous embeddings.\\n\\xe2\\x96\\xa0\\nAlgorithm. The update Eq.8.67 can be further generalized in two directions. First\\nwe can update embeddings V and C according to Eq.8.69:\\nV\\xe2\\x80\\xb2 = V + \\xce\\xbb1A V + \\xce\\xbb2 A (A V),\\nC\\xe2\\x80\\xb2 = C + \\xce\\xbb1A\\xe2\\x8a\\xa4C + \\xce\\xbb2 A\\xe2\\x8a\\xa4(A\\xe2\\x8a\\xa4C).\\n(8.69)\\n8.2 Network Representation\\n247\\nThe time complexity is still O(|V |d) but Eq.8.69 can obtain higher proximity\\nmatrix approximation than Eq.8.67 in one iteration. More complex update formulas\\nthat explore further higher proximities than Eq.8.69 can also be applied but Eq.8.69\\nis used in current experiments as a cost-effective choice.\\nAnother direction is that the update equation can be processed for T rounds to\\nobtain higher proximity approximation. However, the approximation bound would\\ngrow exponentially as the number of rounds T grows and thus the update cannot be\\ndonein\\xef\\xac\\x81nitely.NotethattheupdateoperationofV andCarecompletelyindependent.\\nTherefore, only updating network embedding V is enough for NRL. The above\\nalgorithm (NEU) avoids an accurate computation of high-order proximity matrix\\nbut can yield network embeddings that actually approximate high-order proximities.\\nHence, this algorithm can improve the quality of network embeddings ef\\xef\\xac\\x81ciently.\\nIntuitively, Eqs.8.67 and 8.69 allow the learned embeddings to further propagate to\\ntheir neighbors. Hence, the proximities of longer distances between vertices will be\\nembedded.\\n8.2.6\\nApplications\\nIn this part, we will introduce common applications for network representation learn-\\ning and their evaluation metrics.\\n8.2.6.1\\nMulti-label Classi\\xef\\xac\\x81cation\\nA multi-label classi\\xef\\xac\\x81cation task is the most widely used network representation\\nlearning evaluation task. The representations of vertices are considered as vertex\\nfeatures and applied to classi\\xef\\xac\\x81ers to predict vertex labels. More formally, we assume\\nthat there are K labels in total. The vertex-label relationship can be expressed as a\\nbinary matrix M \\xe2\\x88\\x88{0, 1}|V |\\xc3\\x97K where Mi j = 1 indicates that vertex vi has jth label\\nand Mi j = 0 otherwise. Speci\\xef\\xac\\x81cally, for the multiclass classi\\xef\\xac\\x81cation problem, each\\nvertex has exactly one label, which means there is only an \\xe2\\x80\\x9c1\\xe2\\x80\\x9d in each row of matrix\\nM. For the evaluation task, we set a training ratio which indicates how much percent\\nof vertices have observed labels. Then our goal is to predict the labels for the vertices\\nin the test set.\\nFor unsupervised network representation learning algorithms, the labels of the\\ntraining set are not used for embedding learning. The network representations are\\nfed to classi\\xef\\xac\\x81ers like SVM or logistic regression. Each label will have its classi-\\n\\xef\\xac\\x81er. For semi-supervised learning methods, they take the observed vertex labels into\\naccount in the representation learning period. These algorithms will have their spe-\\nci\\xef\\xac\\x81c classi\\xef\\xac\\x81ers for label prediction.\\nOnce the label prediction is done, we can move to compute the evaluation met-\\nrics. For multiclass classi\\xef\\xac\\x81cation, we assume that the number of correctly predicted\\nvertices is |Vr|. Then the classi\\xef\\xac\\x81cation accuracy is de\\xef\\xac\\x81ned as the ratio of correctly\\n248\\n8\\nNetwork Representation\\npredicted vertices which can be formulated as |Vr|/|V |. For multi-label classi\\xef\\xac\\x81ca-\\ntion, the precision, recall, and F1 are the most popular metrics, which are computed\\nas follows:\\nPrecision = Ncorrectly predicted labels\\nNpredicted labels\\n,\\nRecall = Ncorrectly predicted labels\\nNunobserved labels\\n,\\nF1-Score = 2Precision \\xc3\\x97 Recall\\nPrecision + Recall .\\n(8.70)\\n8.2.6.2\\nLink Prediction\\nLink prediction is another important evaluation task for network representation learn-\\ning because a good network embedding should have the ability to model the af\\xef\\xac\\x81nity\\nbetween vertices. For evaluation, we randomly pick up edges as training set and leave\\nthe rest as test set. Cross-validation can also be employed for training and testing.\\nTo make link prediction given the vertex representations, we \\xef\\xac\\x81rst need to evaluate\\nthe strength of a pair of vertices. The strength between two vertices is evaluated\\nby computing the similarity between their representations. This similarity is usually\\ncomputed by cosine similarity, inner product, or square loss, which depends on the\\nalgorithm. For example, if an algorithm uses \\xe2\\x88\\xa5Vi \\xe2\\x88\\x92C j\\xe2\\x88\\xa52\\n2 in their objective function,\\nthen square loss should be used to measure the similarity between vertex represen-\\ntations. Then after we get the similarity of all unobserved links, we can rank them\\nfor link prediction. There are two signi\\xef\\xac\\x81cant metrics for link prediction: area under\\nthe receiver operating characteristic curve (AUC) and precision.\\nAUC. The AUC value is the probability that a randomly chosen missing link has\\na higher score than a randomly chosen nonexistent link. For implementation, we\\nrandomly select a missing link and a nonexistent link and compare their similarity\\nscore. Assume that there are n1 times that missing link having a higher score and n2\\ntimes they have the same score among n independent comparisons. Then the AUC\\nvalue is\\nAUC = n1 + 0.5n2\\nn\\n.\\n(8.71)\\nNote that for a random network representation, the AUC value should be 0.5.\\nPrecision. Given the ranking of all the non-observed links, we predict the links\\nwith top-L highest score as predicted ones. Assume that there are Lr links that are\\nmissing links, then the precision is de\\xef\\xac\\x81ned as Lr/L.\\n8.2.6.3\\nCommunity Detection\\nFor the network representation based community detection algorithm, we \\xef\\xac\\x81rst need\\nto convert the nonnegative vertex representation into the hard assignment of commu-\\n8.2 Network Representation\\n249\\nnities. Assume that we have network representation matrix V \\xe2\\x88\\x88R+|V |\\xc3\\x97k where row i\\nof V is the nonnegative embedding of vertex vi. For community detection, we regard\\neach dimension of the embeddings as a community. That is to say, Vi j denotes the\\naf\\xef\\xac\\x81nity between vertex vi and community c j. For each column of matrix V, we set\\na threshold \\xce\\x94 and the vertices with af\\xef\\xac\\x81nity score higher than the threshold will be\\nconsidered as a member of the corresponding community. The threshold can be set\\nin various ways. For example, we can set \\xce\\xb4 so that a vertex belongs to a community c\\nif the node is connected to other members of c with an edge probability higher than\\n1/N: [140]\\n1\\nN \\xe2\\x89\\xa41 \\xe2\\x88\\x92exp(\\xe2\\x88\\x92\\xce\\x942),\\n(8.72)\\nwhich indicates that \\xce\\x94 =\\n\\x0c\\n\\xe2\\x88\\x92log(1 \\xe2\\x88\\x921/N).\\nFor evaluation metrics, we have two choices: modularity and matching score.\\nModularity. Recall that the modularity of a graph Q is de\\xef\\xac\\x81ned as\\nQ =\\n1\\n2|E|\\n\\x02\\ni, j\\n\\x05\\nAi j \\xe2\\x88\\x92deg(vi)deg(v j)\\n2|E|\\n\\x06\\n\\xce\\xb4(vi, v j),\\n(8.73)\\nwhere \\xce\\xb4(vi, v j) = 1 if vi and v j belong to the same community and \\xce\\xb4(vi, v j) = 0\\notherwise. A larger modularity indicates a better community detection algorithm.\\nMatching Score. This is a more sophisticated evaluation metric for community\\ndetection. To compare a set of ground truth communities C\\xe2\\x88\\x97to a set of detected\\ncommunities C, we \\xef\\xac\\x81rst need to match each detected community to the most similar\\nground truth community. On the other side, we also \\xef\\xac\\x81nd the most similar detected\\ncommunity for each ground truth community. Then the \\xef\\xac\\x81nal performance is evaluated\\nby the average of both sides:\\n1\\n2|C\\xe2\\x88\\x97|\\n\\x02\\nc\\xe2\\x88\\x97\\ni \\xe2\\x88\\x88C\\xe2\\x88\\x97\\nmax\\nc j\\xe2\\x88\\x88C \\xce\\xb4(c\\xe2\\x88\\x97\\ni , c j) +\\n1\\n2|C|\\n\\x02\\nc j\\xe2\\x88\\x88C\\nmax\\nc\\xe2\\x88\\x97\\ni \\xe2\\x88\\x88C\\xe2\\x88\\x97\\xce\\xb4(c\\xe2\\x88\\x97\\ni , c j),\\n(8.74)\\nwhere \\xce\\xb4(c\\xe2\\x88\\x97\\ni , c j) is a similarity measurement of ground truth community c\\xe2\\x88\\x97\\ni and\\ndetected community c j, such as Jaccard similarity. The score is between 0 and 1,\\nwhere 1 indicates a perfect matching of ground truth communities.\\n8.2.6.4\\nRecommender System\\nRecommender systems aim at recommending items (e.g., products, movies, or loca-\\ntions) for users and cover a wide range of applications. In many cases, an application\\ncomes with an associated social network between users. Now we will present an\\nexample to show how to use the idea of network representation for building recom-\\nmender systems in location-based social networks.\\n250\\n8\\nNetwork Representation\\n(a) Friendship Network\\n(b) User Trajectory\\nFig. 8.7 An illustrative example for the data in LBSNs: a Link connections represent the friendship\\nbetween users. b A trajectory generated by a user is a sequence of chronologically ordered check-in\\nrecords [138]\\nThe accelerated growth of mobile trajectories in location-based services brings\\nvaluable data resources to understand users\\xe2\\x80\\x99 moving behaviors. Apart from recording\\nthe trajectory data, another major characteristic of these location-based services is\\nthat they also allow the users to connect whomever they like or are interested in. As\\nshown in Fig.8.7, a combination of social networking and location-based services\\nis called as Location-Based Social Networks (LBSN). As shown in [21], locations\\nthat are frequently visited by socially related persons tend to be correlated, which\\nindicates the close association between social connections and trajectory behaviors\\nof users in LBSNs. In order to better analyze and mine LBSN data, we need to have\\na comprehensive view to analyze and mine the information from the two aspects,\\ni.e., the social network and mobile trajectory data.\\nSpeci\\xef\\xac\\x81cally, JNTM [138] is proposed to model both social networks and mobile\\ntrajectories jointly. The model consists of two components: the construction of social\\nnetworks and the generation of mobile trajectories. First, JNTM adopts a network\\nembedding method for the construction of social networks where a networking rep-\\nresentation can be derived for a user. Secondly, JNTM considers four factors that\\nin\\xef\\xac\\x82uence the generation process of mobile trajectories, namely, user visit prefer-\\nence, in\\xef\\xac\\x82uence of friends, short-term sequential contexts, and long-term sequential\\ncontexts. Then JNTM uses real-valued representations to encode the four factors and\\nset two different user representations to model the \\xef\\xac\\x81rst two factors: a visit interest\\nrepresentation and a network representation. To characterize the last two contexts,\\nJNTM employs the RNN and GRU models to capture the sequential relatedness in\\nmobile trajectories at different levels, i.e., short term or long term. Finally, the two\\ncomponents are tied by sharing user network representations. The overall model is\\nillustrated in Fig.8.8.\\n8.2 Network Representation\\n251\\nLayer 1\\n(Output)\\nLayer 2\\n(Representation)\\nLayer 3\\n(Deeper Neural Network)\\nGRU\\nRNN\\nFriendship\\nUser Interest\\nLong-term Context\\nShort-term Context\\nNetwork G\\nTrajectory T\\nFig. 8.8 The architecture of JNTM model\\n8.2.6.5\\nInformation Diffusion Prediction\\nInformation diffusion prediction is an important task which studies how information\\nitems spread among users. The prediction of information diffusion, also known as\\ncascade prediction, has been studied over a wide range of applications, such as\\nproduct adoption [67], epidemiology [124], social networks [63], and the spread of\\nnews and opinions [68].\\nAs shown in Fig.8.9, microscopic diffusion prediction aims at guessing the next\\ninfected user, while macroscopic diffusion prediction estimates the total numbers of\\ninfected users during the diffusion process. Also, an underlying social graph among\\nusers will be available when information diffusion occurs on a social network service.\\nThe social graph will be considered as additional structural inputs for diffusion\\nprediction.\\nFOREST [139] is the \\xef\\xac\\x81rst work to address both microscopic and macroscopic\\npredictions. As shown in Fig.8.10, FOREST proposes a structural context extrac-\\nFig. 8.9 Illustrative examples for microscopic next infected user prediction (left) and macroscopic\\ncascade size prediction (right) [139]\\n252\\n8\\nNetwork Representation\\nFig. 8.10 An illustrative example of structural context extraction of the orange node by neighbor\\nsampling and feature aggregation [139]\\ntion algorithm that was originally introduced for accelerating graph convolutional\\nnetworks [41] to build an RNN-based microscopic cascade model. For each user\\nv, we \\xef\\xac\\x81rst sample Z users {u1, u2 . . . , uZ} from v and its neighbors N (v). Then\\nwe update its feature vector by aggregating the neighborhood features. The updated\\nuser feature vector encodes structural information by aggregating features from v\\xe2\\x80\\x99s\\n\\xef\\xac\\x81rst-order neighbors. The operation can also be processed recursively to explore a\\nlarger neighborhood of user v. Empirically, a two-step neighborhood exploration is\\ntime ef\\xef\\xac\\x81cient and enough to give promising results.\\nFOREST further incorporates the ability of macroscopic prediction, i.e., esti-\\nmating the eventual size of a cascade into the model by reinforcement learning. The\\nmethod can be divided into four steps: (a) encode observed K users by a microscopic\\ncascade model; (b) enable the microscopic cascade model to predict the size of a cas-\\ncade by cascade simulations; (c) use Mean-Square Log-Transformed Error (MSLE)\\nas the supervision signal for macroscopic predictions; and (d) employ a reinforce-\\nment learning framework to update parameters through policy gradient algorithm.\\nThe overall work\\xef\\xac\\x82ow is illustrated in Fig.8.11.\\n8.3\\nGraph Neural Networks\\nWe now give a short introduction to Graph Neural Networks for NRL, partially based\\non our review [161] and tutorial [162] whose publishing agreement allows the authors\\nto reuse these contents.\\n8.3 Graph Neural Networks\\n253\\n\\xe2\\x80\\xa6\\n\\xe2\\x80\\xa6\\nh1\\nh2\\nhK\\nMicroscopic Cascade Model\\nu1\\nu2\\nuK\\n(a) Feed observed K users into\\nmicroscopic cascade model\\n<STOP>\\n(b) cascade simulations by sampling\\n(d) Update parameters by\\npolicy gradients\\n(c) MSLE\\nReward\\nFig. 8.11 The work\\xef\\xac\\x82ow of adopting microscopic cascade model for macroscopic size prediction\\nby reinforcement learning\\n8.3.1\\nMotivations\\nGraph Neural Networks (GNNs) are deep learning based methods that operate on\\ngraph domain. Due to its convincing performance and high interpretability, GNN\\nhas been a widely applied graph analysis method recently. In this subsection, we will\\nillustrate the fundamental motivations of graph neural networks.\\nIn recent years, CNNs [65] have made breakthroughs in various machine learning\\nareas, especially in the area of computer vision, and started the revolution of deep\\nlearning [64]. CNNs are capable of extracting multiscale localized features and these\\nfeatures are used to generate more expressive representations. As we are going deeper\\ninto CNNs and graphs, we found the keys of CNNs: local connection, shared weights,\\nand the use of multilayer [64]. These are also of great importance in solving problems\\nof graph domain, because (1) graphs are the most typical locally connected structure,\\n(2) shared weights reduce the computational cost compared with traditional spectral\\ngraph theory [23], and (3) multilayer structure is the key to deal with hierarchical\\npatterns, which captures the features of various sizes. However, CNNs can only\\noperate on regular Euclidean data like images (2D grid) and text (1D sequence)\\nwhile these data structures can be regarded as instances of graphs. Therefore, it is\\nstraightforward to think of \\xef\\xac\\x81nding the generalization of CNNs to graphs. As shown\\nin Fig.8.12, it is hard to de\\xef\\xac\\x81ne localized convolutional \\xef\\xac\\x81lters and pooling operators,\\nwhich hinders the transformation of CNN from Euclidean domain to non-Euclidean\\ndomain.\\nThe other motivation comes from network embedding [12, 24, 37, 42, 149]. In\\nthe \\xef\\xac\\x81eld of graph analysis, traditional machine learning approaches usually rely on\\nhand-engineered features and are limited by its in\\xef\\xac\\x82exibility and high cost. Follow-\\ning the idea of representation learning and the success of word embedding [81],\\nDeepWalk [93], which is regarded as the \\xef\\xac\\x81rst graph embedding method based on\\n254\\n8\\nNetwork Representation\\nFig. 8.12 Left: image in Euclidean space. Right: graph in non-Euclidean space [155]\\nrepresentation learning, applies Skip-gram model [81] on the generated random\\nwalks. Similar approaches such as node2vec [38], LINE [111], and TADW [136]\\nalso achieved breakthroughs. However, these methods suffer from two severe draw-\\nbacks [42]. First, no parameters are shared between nodes in the encoder, which leads\\nto computational inef\\xef\\xac\\x81ciency, since it means the number of parameters grows linearly\\nwith the number of nodes. Second, the direct embedding methods lack the ability of\\ngeneralization, which means they cannot deal with dynamic graphs or generalize to\\nnew graphs.\\nBased on CNNs and network embedding, Graph Neural Networks (GNNs) are\\nproposed to collectively aggregate information from graph structure. Thus, they can\\nmodel input and/or output consisting of elements and their dependency. Further, the\\ngraph neural networks can simultaneously model the diffusion process on the graph\\nwith the RNN kernel.\\nIn the rest of this section, we will \\xef\\xac\\x81rst introduce several typical variants of graph\\nneural networks such as Graph Convolutional Networks (GCNs), Graph Attention\\nNetworks (GATs), and Graph Recurrent Networks (GRNs). Then we will introduce\\nseveral extensions to the original model and \\xef\\xac\\x81nally, we will give some examples of\\napplications that utilize graph neural networks.\\n8.3.2\\nGraph Convolutional Networks\\nGraph Convolutional Networks (GCNs) aim to generalize convolutions to the graph\\ndomain. Advances in this direction are often categorized as spectral approaches and\\nspatial (nonspectral) approaches.\\n8.3 Graph Neural Networks\\n255\\n8.3.2.1\\nSpectral Approaches\\nSpectral approaches work with a spectral representation of the graphs.\\nSpectral Network. Bruna et al. [11] proposes the spectral network. The convolu-\\ntion operation is de\\xef\\xac\\x81ned in the Fourier domain by computing the eigendecomposition\\nof the graph Laplacian. The operation can be de\\xef\\xac\\x81ned as the multiplication of a signal\\nx \\xe2\\x88\\x88RN (a scalar for each node) with a \\xef\\xac\\x81lter g\\xce\\xb8 =diag(\\xce\\xb8) parameterized by \\xce\\xb8 \\xe2\\x88\\x88RN:\\ng\\xce\\xb8 \\xe2\\x8b\\x86x = Ug\\xce\\xb8(\\x0f)U T x,\\n(8.75)\\nwhere U is the matrix of eigenvectors of the normalized graph Laplacian L = IN \\xe2\\x88\\x92\\nD\\xe2\\x88\\x921\\n2 AD\\xe2\\x88\\x921\\n2 = U\\x0fU T (D is the degree matrix and A is the adjacency matrix of the\\ngraph), with a diagonal matrix of its eigenvalues \\x0f.\\nThis operation results in potentially intense computations and non-spatially local-\\nized \\xef\\xac\\x81lters. Henaff et al. [47] attempts to make the spectral \\xef\\xac\\x81lters spatially localized\\nby introducing a parameterization with smooth coef\\xef\\xac\\x81cients.\\nChebNet. Hammond et al. [43] suggests that g\\xce\\xb8(\\x0f) can be approximated by a\\ntruncated expansion in terms of Chebyshev polynomials Tk(x) up to Kth order. Thus,\\nthe operation is\\ng\\xce\\xb8 \\xe2\\x8b\\x86x \\xe2\\x89\\x88\\nK\\n\\x02\\nk=0\\n\\xce\\xb8kTk( \\xcb\\x9cL)x,\\n(8.76)\\nwith \\xcb\\x9cL = 2/\\xce\\xbbmax L \\xe2\\x88\\x92IN. \\xce\\xbbmax denotes the largest eigenvalue of L. \\xce\\xb8 \\xe2\\x88\\x88RK is now\\na vector of Chebyshev coef\\xef\\xac\\x81cients. The Chebyshev polynomials are de\\xef\\xac\\x81ned as\\nTk(x) = 2xTk\\xe2\\x88\\x921(x) \\xe2\\x88\\x92Tk\\xe2\\x88\\x922(x), with T0(x) = 1 and T1(x) = x. It can be observed\\nthat the operation is K-localized since it is a Kth-order polynomial in the Laplacian.\\nDefferrard et al. [28] proposes the ChebNet. It uses this K-localized convolution to\\nde\\xef\\xac\\x81ne a convolutional neural network, which could remove the need to compute the\\neigenvectors of the Laplacian.\\nGCN. Kipf and Welling [59] limits the layer-wise convolution operation to K = 1\\nto alleviate the problem of over\\xef\\xac\\x81tting on local neighborhood structures for graphs\\nwith very wide node degree distributions. It further approximates \\xce\\xbbmax \\xe2\\x89\\x882 and the\\nequation simpli\\xef\\xac\\x81es to\\ng\\xce\\xb8\\xe2\\x80\\xb2 \\xe2\\x8b\\x86x \\xe2\\x89\\x88\\xce\\xb8\\xe2\\x80\\xb2\\n0x + \\xce\\xb8\\xe2\\x80\\xb2\\n1 (L \\xe2\\x88\\x92IN) x = \\xce\\xb8\\xe2\\x80\\xb2\\n0x \\xe2\\x88\\x92\\xce\\xb8\\xe2\\x80\\xb2\\n1D\\xe2\\x88\\x921\\n2 AD\\xe2\\x88\\x921\\n2 x,\\n(8.77)\\nwith two free parameters \\xce\\xb8\\xe2\\x80\\xb2\\n0 and \\xce\\xb8\\xe2\\x80\\xb2\\n1. After constraining the number of parameters\\nwith \\xce\\xb8 = \\xce\\xb8\\xe2\\x80\\xb2\\n0 = \\xe2\\x88\\x92\\xce\\xb8\\xe2\\x80\\xb2\\n1, we can obtain the following expression:\\ng\\xce\\xb8 \\xe2\\x8b\\x86x \\xe2\\x89\\x88\\xce\\xb8\\n\\r\\nIN + D\\xe2\\x88\\x921\\n2 AD\\xe2\\x88\\x921\\n2\\n\\x0e\\nx.\\n(8.78)\\nNote that stacking this operator could lead to numerical instabilities and\\nexploding/vanishing\\ngradients,\\n[59]\\nintroduces\\nthe\\nrenormalization\\ntrick:\\n256\\n8\\nNetwork Representation\\nIN + D\\xe2\\x88\\x921\\n2 AD\\xe2\\x88\\x921\\n2 \\xe2\\x86\\x92\\xcb\\x9cD\\xe2\\x88\\x921\\n2 \\xcb\\x9cA \\xcb\\x9cD\\xe2\\x88\\x921\\n2 , with \\xcb\\x9cA = A + IN and \\xcb\\x9cDii = \\x04\\nj \\xcb\\x9cAi j. Finally, [59]\\ngeneralizes the de\\xef\\xac\\x81nition to a signal X \\xe2\\x88\\x88RN\\xc3\\x97C with C input channels and F \\xef\\xac\\x81lters\\nfor feature maps as follows:\\nH = f ( \\xcb\\x9cD\\xe2\\x88\\x921\\n2 \\xcb\\x9cA \\xcb\\x9cD\\xe2\\x88\\x921\\n2 XW),\\n(8.79)\\nwhere W \\xe2\\x88\\x88RC\\xc3\\x97F is a matrix of \\xef\\xac\\x81lter parameters, H \\xe2\\x88\\x88RN\\xc3\\x97F is the convolved signal\\nmatrix and f (\\xc2\\xb7) is the activation function.\\nThe GCN layer can be stacked for multiple times so that we have the equation:\\nH(t) = f ( \\xcb\\x9cD\\xe2\\x88\\x921\\n2 \\xcb\\x9cA \\xcb\\x9cD\\xe2\\x88\\x921\\n2 H(t\\xe2\\x88\\x921)W(t\\xe2\\x88\\x921)),\\n(8.80)\\nwhere the superscripts t and t \\xe2\\x88\\x921 denote the layers of the matrices, the initial matrix\\nH(0) could be X. After L layers, we can use the \\xef\\xac\\x81nal embedding matrix H(L) and a\\nreadout function to get the \\xef\\xac\\x81nal output matrix Z:\\nZ = Readout(H (L)),\\n(8.81)\\nwhere the readout function can be any machine learning methods, such as MLP.\\nFinally, as a semi-supervised algorithm, GCN uses the feature matrix at the top\\nlayer Z which has the same dimension with the total number of labels to predict the\\nlabels of all observed labels. The loss function can be written as\\nL = \\xe2\\x88\\x92\\n\\x02\\nl\\xe2\\x88\\x88yL\\n\\x02\\nf\\nYl f ln Zl f ,\\n(8.82)\\nwhere yL is the set of node indices that have observed labels. Figure8.13 shows the\\nalgorithm of GCN.\\nAnswer:green\\noutput layer\\ninput layer\\nhidden\\nlayers\\nC\\nF\\nx1\\nZ1\\nZ2\\nZ3\\nZ4\\nY1\\nY4\\nx2\\nx3\\nx4\\nFig. 8.13 The architecture of graph convolutional network model\\n8.3 Graph Neural Networks\\n257\\n8.3.2.2\\nSpatial Approaches\\nIn all of the spectral approaches mentioned above, the learned \\xef\\xac\\x81lters depend on\\nthe Laplacian eigenbasis, which depends on the graph structure, that is, a model\\ntrained on a speci\\xef\\xac\\x81c structure could not be directly applied to a graph with a different\\nstructure.\\nSpatial approaches de\\xef\\xac\\x81ne convolutions directly on the graph, operating on spa-\\ntially close neighbors. The major challenge of spatial approaches is de\\xef\\xac\\x81ning the con-\\nvolution operation with differently sized neighborhoods and maintaining the local\\ninvariance of CNNs.\\nNeural FPs. Duvenaud et al. [31] uses different weight matrices for nodes with\\ndifferent degrees\\nx(t) = h(t\\xe2\\x88\\x921)\\nv\\n+\\n|Nv|\\n\\x02\\ni=1\\nh(t\\xe2\\x88\\x921)\\ni\\n,\\nh(t)\\nv\\n= f (W(t)\\n|Nv|x(t)),\\n(8.83)\\nwhere W(t)\\n|Nv| is the weight matrix for nodes with degree |Nv| at layer t. And the main\\ndrawback of the method is that it cannot be applied to large-scale graphs with more\\nnode degrees.\\nIn the following description of other models, we use h(t)\\nv\\nto denote the hidden\\nstate of node v at layer t. Nv denotes the neighbor set of node v and |Nv| denotes the\\nsize of the set.\\nDCNN. Atwood and Towsley [4] proposes the Diffusion-Convolutional Neural\\nNetworks (DCNNs). Transition matrices are used to de\\xef\\xac\\x81ne the neighborhood for\\nnodes in DCNN. For node classi\\xef\\xac\\x81cation, it has\\nH = f\\n\\r\\nWc \\xe2\\x8a\\x99\\xe2\\x88\\x92\\xe2\\x86\\x92\\nP X\\n\\x0e\\n,\\n(8.84)\\nwhere \\xe2\\x8a\\x99is the element-wise multiplication and X is an N \\xc3\\x97 F matrix of input\\nfeatures. \\xe2\\x88\\x92\\xe2\\x86\\x92\\nP is an N \\xc3\\x97 K \\xc3\\x97 N tensor which contains the power series {P, P2,\\xe2\\x80\\xa6,\\nPK} of matrix P. And P is the degree-normalized transition matrix from the graphs\\nadjacency matrix A. Each entity is transformed to a diffusion-convolutional rep-\\nresentation, which is a K \\xc3\\x97 F matrix de\\xef\\xac\\x81ned by K hops of graph diffusion over\\nF features. And then it will be de\\xef\\xac\\x81ned by a K \\xc3\\x97 F weight matrix and a nonlin-\\near activation function f . Finally H (which is N \\xc3\\x97 K \\xc3\\x97 F) denotes the diffusion\\nrepresentations of each node in the graph.\\nDGCN. Zhuang and Ma [158] proposes the Dual Graph Convolutional Network\\n(DGCN) to consider the local consistency and global consistency of graphs jointly. It\\nuses two convolutional networks to capture the local/global consistency and adopts\\nan unsupervised loss to ensemble them. The \\xef\\xac\\x81rst convolutional network is the same\\nas Eq.8.80. And the second network replaces the adjacency matrix with Positive\\nPoint-wise Mutual Information (PPMI) matrix:\\n258\\n8\\nNetwork Representation\\nH(t) = f (D\\n\\xe2\\x88\\x921\\n2\\nP X P D\\n\\xe2\\x88\\x921\\n2\\nP H (t\\xe2\\x88\\x921)W),\\n(8.85)\\nwhere X P is the PPMI matrix and DP is the diagonal degree matrix of X P.\\nGraphSAGE. Hamilton et al. [41] proposes the GraphSAGE, a general induc-\\ntive framework. The framework generates embeddings by sampling and aggregating\\nfeatures from a node\\xe2\\x80\\x99s local neighborhood.\\nh(t)\\nNv = AGGREGATE(t)({h(t\\xe2\\x88\\x921)\\nu\\n, \\xe2\\x88\\x80u \\xe2\\x88\\x88Nv}),\\nh(t)\\nv\\n= f (W(t)[h(t\\xe2\\x88\\x921)\\nv\\n; h(t)\\nNv]).\\n(8.86)\\nHowever, [41] does not utilize the full set of neighbors in Eq.8.86 but a \\xef\\xac\\x81xed-\\nsize set of neighbors by uniformly sampling. And [41] suggests three aggregator\\nfunctions.\\n\\xe2\\x80\\xa2 Mean aggregator. It could be viewed as an approximation of the convolutional\\noperation from the transductive GCN framework [59], so that the inductive version\\nof the GCN variant could be derived by\\nh(t)\\nv\\n= f\\n\\x0f\\nW \\xc2\\xb7 MEAN\\n\\x0f\\n{h(t\\xe2\\x88\\x921)\\nv\\n} \\xe2\\x88\\xaa{h(t\\xe2\\x88\\x921)\\nu\\n|\\xe2\\x88\\x80u \\xe2\\x88\\x88Nv}\\n\\x10\\x10\\n.\\n(8.87)\\nThe mean aggregator is different from other aggregators because it does not per-\\nform the concatenation operation which concatenates ht\\xe2\\x88\\x921\\nv\\nand ht\\nNv in Eq.8.86. It\\ncan be viewed as a form of \\xe2\\x80\\x9cskip connection\\xe2\\x80\\x9d [46] and can achieve better perfor-\\nmance.\\n\\xe2\\x80\\xa2 LSTM aggregator. Hamilton et al. [41] also uses an LSTM-based aggregator which\\nhas a larger expressive capability. However, LSTMs process inputs in a sequential\\nmanner so that they are not permutation invariant. Hamilton et al. [41] adapts\\nLSTMs to operate on an unordered set by permutating node\\xe2\\x80\\x99s neighbors.\\n\\xe2\\x80\\xa2 Pooling aggregator. In the pooling aggregator, each neighbor\\xe2\\x80\\x99s hidden state is fed\\nthrough a fully connected layer and then a max-pooling operation is applied to the\\nset of the node\\xe2\\x80\\x99s neighbors.\\nh(t)\\nNv = max({ f (Wpoolh(t\\xe2\\x88\\x921)\\nu\\n+ b), \\xe2\\x88\\x80u \\xe2\\x88\\x88Nv}).\\n(8.88)\\nNote that any symmetric functions could be used in place of the max-pooling\\noperation here.\\nOther methods. There are still many other spatial methods. The PATCHY-SAN\\nmodel [86] \\xef\\xac\\x81rst extracts exactly k nodes for each node and normalizes them. Then\\nthe convolutional operation is applied to the normalized neighborhood. LGCN [35]\\nleverages CNNs as aggregators. It performs max-pooling on nodes\\xe2\\x80\\x99 neighborhood\\nmatrices to get top-k feature elements and then applies 1-D CNN to compute hid-\\nden representations. Monti et al. [82] proposes a spatial-domain model (MoNet)\\non non-Euclidean domains which could generalize several previous techniques.\\n8.3 Graph Neural Networks\\n259\\nThe Geodesic CNN (GCNN) [78] and Anisotropic CNN (ACNN) [10] on manifolds\\nor GCN [59] and DCNN [4] on graphs could be formulated as particular instances\\nof MoNet. Our readers can refer to their papers for more details.\\n8.3.3\\nGraph Attention Networks\\nThe attention mechanism has been successfully used in many sequence-based tasks\\nsuch as machine translation [5, 36, 121], machine reading [19], etc. Many works\\nfocus on generalizing the attention mechanism to the graph domain.\\nGAT. Velickovic et al. [122] proposes a Graph Attention Network (GAT) which\\nincorporates the attention mechanism into the propagation step. Speci\\xef\\xac\\x81cally, it uses\\nthe self-attention strategy and each node\\xe2\\x80\\x99s hidden state is computed by attending\\nover its neighbors.\\nVelickovic et al. [122] de\\xef\\xac\\x81nes a single graph attentional layer and constructs\\narbitrary graph attention networks by stacking this layer. The layer computes the\\ncoef\\xef\\xac\\x81cients in the attention mechanism of the node pair (i, j) by:\\n\\xce\\xb1i j =\\nexp\\n\\r\\nLeakyReLU\\n\\r\\na\\xe2\\x8a\\xa4[Wh(t\\xe2\\x88\\x921)\\ni\\n; Wh(t\\xe2\\x88\\x921)\\nj\\n]\\n\\x0e\\x0e\\n\\x04\\nk\\xe2\\x88\\x88Ni exp\\n\\r\\nLeakyReLU\\n\\r\\na\\xe2\\x8a\\xa4[Wh(t\\xe2\\x88\\x921)\\ni\\n; Wh(t\\xe2\\x88\\x921)\\nk\\n]\\n\\x0e\\x0e,\\n(8.89)\\nwhere \\xce\\xb1i j is the attention coef\\xef\\xac\\x81cient of node j to i. W \\xe2\\x88\\x88RF\\xe2\\x80\\xb2\\xc3\\x97F is the weight matrix\\nof a shared linear transformation which applied to every node, a \\xe2\\x88\\x88R2F\\xe2\\x80\\xb2 is the weight\\nvector. It is normalized by a softmax function and the LeakyReLU nonlinearity (with\\nnegative input slop 0.2) is applied.\\nThen the \\xef\\xac\\x81nal output features of each node can be obtained by (after applying a\\nnonlinearity f ):\\nh(t)\\ni\\n= f\\n\\xe2\\x8e\\x9b\\n\\xe2\\x8e\\x9d\\x02\\nj\\xe2\\x88\\x88Ni\\n\\xce\\xb1i jWh(t\\xe2\\x88\\x921)\\nj\\n\\xe2\\x8e\\x9e\\n\\xe2\\x8e\\xa0.\\n(8.90)\\nMoreover, the layer utilizes the multi-head attention similarly to [121] to stabilize\\nthe learning process. It applies K independent attention mechanisms to compute the\\nhidden states and then concatenates their features(or computes the average), resulting\\nin the following two output representations:\\nh(t)\\ni\\n= \\xe2\\x88\\xa5K\\nk=1 f\\n\\xe2\\x8e\\x9b\\n\\xe2\\x8e\\x9d\\x02\\nj\\xe2\\x88\\x88Ni\\n\\xce\\xb1k\\ni jWkh(t\\xe2\\x88\\x921)\\nj\\n\\xe2\\x8e\\x9e\\n\\xe2\\x8e\\xa0,\\n(8.91)\\nh(t)\\ni\\n= f\\n\\xe2\\x8e\\x9b\\n\\xe2\\x8e\\x9d1\\nK\\nK\\n\\x02\\nk=1\\n\\x02\\nj\\xe2\\x88\\x88Ni\\n\\xce\\xb1k\\ni jWkh(t\\xe2\\x88\\x921)\\nj\\n\\xe2\\x8e\\x9e\\n\\xe2\\x8e\\xa0,\\n(8.92)\\n260\\n8\\nNetwork Representation\\nwhere \\xce\\xb1k\\ni j is normalized attention coef\\xef\\xac\\x81cient computed by the kth attention mecha-\\nnism, \\xe2\\x88\\xa5is the concatenation operation.\\nThe attention architecture in [122] has several properties: (1) the computation of\\nthe node-neighbor pairs is parallelizable thus the operation is ef\\xef\\xac\\x81cient; (2) it can\\ndeal with nodes that have different degrees by assigning reasonable weights to their\\nneighbors; (3) it can be applied to the inductive learning problems easily.\\nGAAN. Besides GAT, Gated Attention Network (GAAN) [150] also uses the\\nmulti-head attention mechanism. However, it uses a self-attention mechanism to\\ngather information from different heads to replace the average operation of GAT.\\n8.3.4\\nGraph Recurrent Networks\\nSeveral works are attempting to use the gate mechanism like GRU [20] or LSTM [48]\\nin the propagation step to release the limitations induced by the vanilla GNN architec-\\nture and improve the effectiveness of the long-term information propagation across\\nthe graph. We call these methods Graph Recurrent Networks (GRNs) and we will\\nintroduce some variants of GRNs in this subsection.\\nGGNN.Lietal.[72]proposesthegatedgraphneuralnetwork(GGNN)whichuses\\nthe Gate Recurrent Units (GRU) in the propagation step. It follows the computation\\nsteps from recurrent neural networks for a \\xef\\xac\\x81xed number of L steps, then it back-\\npropagates through time to compute gradients.\\nSpeci\\xef\\xac\\x81cally, the basic recurrence of the propagation model is\\na(t)\\nv\\n=A\\xe2\\x8a\\xa4\\nv [h(t\\xe2\\x88\\x921)\\n1\\n. . . h(t\\xe2\\x88\\x921)\\nN\\n]\\xe2\\x8a\\xa4+ b,\\nz(t)\\nv\\n= Sigmoid\\n\\x0f\\nWza(t)\\nv + Uzh(t\\xe2\\x88\\x921)\\nv\\n\\x10\\n,\\nr(t)\\nv\\n= Sigmoid\\n\\x0f\\nWra(t)\\nv + Urh(t\\xe2\\x88\\x921)\\nv\\n\\x10\\n,\\n(8.93)\\n\\x08h(t)\\nv\\n= tanh\\n\\x0f\\nWa(t)\\nv + U\\n\\x0f\\nr(t)\\nv \\xe2\\x8a\\x99h(t\\xe2\\x88\\x921)\\nv\\n\\x10\\x10\\n,\\nh(t)\\nv\\n=\\n\\x0f\\n1 \\xe2\\x88\\x92z(t)\\nv\\n\\x10\\n\\xe2\\x8a\\x99h(t\\xe2\\x88\\x921)\\nv\\n+ z(t)\\nv \\xe2\\x8a\\x99\\x08h(t)\\nv .\\nThe node v \\xef\\xac\\x81rst aggregates message from its neighbors, where Av is the sub-\\nmatrix of the graph adjacency matrix A and denotes the connection of node v with\\nits neighbors. Then the hidden state of the node is updated by the GRU-like function\\nusing the information from its neighbors and the hidden state from the previous\\ntimestep. a gathers the neighborhood information of node v, z and r are the update\\nand reset gates.\\nLSTMs are also used similarly as GRU through the propagation process based on\\na tree or a graph.\\nTree-LSTM. Tai et al. [109] proposes two extensions to the basic LSTM architec-\\nture: the Child-Sum Tree-LSTM and the N-ary Tree-LSTM. Like in standard LSTM\\nunits, each Tree-LSTM unit (indexed by v) contains input and output gates iv and ov,\\na memory cell cv and hidden state hv. The Tree-LSTM unit replaces the single forget\\n8.3 Graph Neural Networks\\n261\\ngate by a forget gate fvk for each child k, allowing node v to select information from\\nits children accordingly. The equations of the Child-Sum Tree-LSTM are\\n\\x08ht\\xe2\\x88\\x921\\nv\\n=\\n\\x02\\nk\\xe2\\x88\\x88Nv\\nht\\xe2\\x88\\x921\\nk\\n,\\nit\\nv = Sigmoid\\n\\r\\nWixt\\nv + Ui\\x08ht\\xe2\\x88\\x921\\nv\\n+ bi\\x0e\\n,\\nft\\nvk = Sigmoid\\n\\r\\nW f xt\\nv + U f ht\\xe2\\x88\\x921\\nk\\n+ b f \\x0e\\n,\\not\\nv = Sigmoid\\n\\r\\nWoxt\\nv + Uo\\x08ht\\xe2\\x88\\x921\\nv\\n+ bo\\x0e\\n,\\n(8.94)\\nut\\nv = tanh\\n\\r\\nWuxt\\nv + Uu\\x08ht\\xe2\\x88\\x921\\nv\\n+ bu\\x0e\\n,\\nct\\nv = it\\nv \\xe2\\x8a\\x99ut\\nv +\\n\\x02\\nk\\xe2\\x88\\x88Nv\\nft\\nvk \\xe2\\x8a\\x99ct\\xe2\\x88\\x921\\nk\\n,\\nht\\nv = ot\\nv \\xe2\\x8a\\x99tanh(ct\\nv),\\nwhere xt\\nv is the input vector at time t in the standard LSTM setting.\\nIn a speci\\xef\\xac\\x81c case, if each node\\xe2\\x80\\x99s number of children is at most K and these children\\ncan be ordered from 1 to K, then the N-ary Tree-LSTM can be applied. For node\\nv, ht\\nvk and ct\\nvk denote the hidden state and memory cell of its kth child at time t\\nrespectively. The transition equations are the following:\\nit\\nv = Sigmoid\\n\\r\\nWixt\\nv +\\nK\\n\\x02\\nl=1\\nUi\\nlht\\xe2\\x88\\x921\\nvl\\n+ bi\\x0e\\n,\\nft\\nvk = Sigmoid\\n\\r\\nW f xt\\nv +\\nK\\n\\x02\\nl=1\\nU f\\nklht\\xe2\\x88\\x921\\nvl\\n+ b f \\x0e\\n,\\not\\nv = Sigmoid\\n\\r\\nWoxt\\nv +\\nK\\n\\x02\\nl=1\\nUo\\nl ht\\xe2\\x88\\x921\\nvl\\n+ bo\\x0e\\n,\\n(8.95)\\nut\\nv = tanh\\n\\r\\nWuxt\\nv +\\nK\\n\\x02\\nl=1\\nUu\\nl ht\\xe2\\x88\\x921\\nvl\\n+ bu\\x0e\\n,\\nct\\nv = it\\nv \\xe2\\x8a\\x99ut\\nv +\\nK\\n\\x02\\nl=1\\nft\\nvl \\xe2\\x8a\\x99ct\\xe2\\x88\\x921\\nvl ,\\nht\\nv = ot\\nv \\xe2\\x8a\\x99tanh(ct\\nv).\\nCompared to the Child-Sum Tree-LSTM, the N-ary Tree-LSTM introduces sep-\\narate parameters for each child k. These parameters allow the model to learn more\\n\\xef\\xac\\x81ne-grained representations conditioning on each node\\xe2\\x80\\x99s children.\\nGraph LSTM. The two types of Tree-LSTMs can be easily adapted to the graph.\\nThe graph-structured LSTM in [148] is an example of the N-ary Tree-LSTM applied\\n262\\n8\\nNetwork Representation\\nto the graph. However, it is a simpli\\xef\\xac\\x81ed version since each node in the graph has at\\nmost 2 incoming edges (from its parent and sibling predecessor). Peng et al. [92]\\nproposes another variant of the Graph LSTM based on the relation extraction task.\\nThe main difference between graphs and trees is that edges of graphs have their\\nlabels, and [92] utilizes different weight matrices to represent different labels.\\nit\\nv = Sigmoid\\n\\r\\nWixt\\nv +\\n\\x02\\nk\\xe2\\x88\\x88Nv\\nUi\\nm(v,k)ht\\xe2\\x88\\x921\\nk\\n+ bi\\x0e\\n,\\nft\\nvk = Sigmoid\\n\\r\\nW f xt\\nv + U f\\nm(v,k)ht\\xe2\\x88\\x921\\nk\\n+ b f \\x0e\\n,\\not\\nv = Sigmoid\\n\\r\\nWoxt\\nv +\\n\\x02\\nk\\xe2\\x88\\x88Nv\\nUo\\nm(v,k)ht\\xe2\\x88\\x921\\nk\\n+ bo\\x0e\\n,\\n(8.96)\\nut\\nv = tanh\\n\\r\\nWuxt\\nv +\\n\\x02\\nk\\xe2\\x88\\x88Nv\\nUu\\nm(v,k)ht\\xe2\\x88\\x921\\nk\\n+ bu\\x0e\\n,\\nct\\nv = it\\nv \\xe2\\x8a\\x99ut\\nv +\\n\\x02\\nk\\xe2\\x88\\x88Nv\\nft\\nvk \\xe2\\x8a\\x99ct\\xe2\\x88\\x921\\nk\\n,\\nht\\nv = ot\\nv \\xe2\\x8a\\x99tanh(ct\\nv),\\nwhere m(v, k) denotes the edge label between node v and k.\\nBesides, [74] proposes a Graph LSTM network to address the semantic object\\nparsing task. It uses the con\\xef\\xac\\x81dence-driven scheme to adaptively select the starting\\nnode and determine the node updating sequence. It follows the same idea of general-\\nizing the existing LSTMs into the graph-structured data but has a speci\\xef\\xac\\x81c updating\\nsequence while the methods we mentioned above are agnostic to the order of nodes.\\nSentence LSTM. Zhang et al. [152] proposes the Sentence LSTM (S-LSTM) for\\nimproving text encoding. It converts text into a graph and utilizes the Graph LSTM\\nto learn the representation. The S-LSTM shows strong representation power in many\\nNLP problems.\\n8.3.5\\nExtensions\\nIn this subsection, we will talk about some extensions of graph neural networks.\\n8.3.5.1\\nSkip Connection\\nMany applications unroll or stack the graph neural network layer aiming to achieve\\nbetter results as more layers (i.e., k layers) make each node aggregate more informa-\\ntion from neighbors k hops away. However, it has been observed in many experiments\\nthat deeper models could not improve the performance and deeper models could even\\n8.3 Graph Neural Networks\\n263\\nperform worse [59]. This is mainly because more layers could also propagate the\\nnoisy information from an exponentially increasing number of expanded neighbor-\\nhood members.\\nA straightforward method to address the problem, the residual network [45], can\\nbe found from the computer vision community. Nevertheless, even with residual\\nconnections, GCNs with more layers do not perform as well as the 2-layer GCN on\\nmany datasets [59].\\nHighway Network. Rahimi et al. [96] borrows ideas from the highway net-\\nwork [159] and uses layer-wise gates to build a Highway GCN. The input of each\\nlayer is multiplied by the gating weights and then summed with the output:\\nT (h(t)) = Sigmoid\\n\\x0f\\nW(t)h(t) + b(t)\\x10\\n,\\nh(t+1) = h(t+1) \\xe2\\x8a\\x99T (h(t)) + h(t) \\xe2\\x8a\\x99(1 \\xe2\\x88\\x92T (h(t))).\\n(8.97)\\nBy adding the highway gates, the performance peaks at four layers in a speci\\xef\\xac\\x81c\\nproblem discussed in [96]. The Column Network (CLN) proposed in [94] also utilizes\\nthe highway network. However, it has a different function to compute the gating\\nweights.\\nJump Knowledge Network. Xu et al. [134] studies properties and resulting limi-\\ntations of neighborhood aggregation schemes. It proposes the Jump Knowledge Net-\\nwork which could learn adaptive, structure-aware representations. The Jump Knowl-\\nedge Network selects from all of the intermediate representations (which\"jump\" to\\nthe last layer) for each node at the last layer, which enables the model to select effec-\\ntive neighborhood information for each node. Xu et al. [134] uses three approaches\\nof concatenation, max-pooling, and LSTM-attention in the experiments to aggre-\\ngate information. The Jump Knowledge Network performs well on the experiments\\nin social, bioinformatics, and citation networks. It can also be combined with models\\nlike Graph Convolutional Networks, GraphSAGE, and Graph Attention Networks to\\nimprove their performance.\\n8.3.5.2\\nHierarchical Pooling\\nIn the area of computer vision, a convolutional layer is usually followed by a pooling\\nlayer to get more general features. Similar to these pooling layers, much work focuses\\non designing hierarchical pooling layers on graphs. Complicated and large-scale\\ngraphs usually carry rich hierarchical structures that are of great importance for\\nnode-level and graph-level classi\\xef\\xac\\x81cation tasks.\\nTo explore such inner features, Edge-Conditioned Convolution (ECC) [106]\\ndesigns its pooling module with the recursively downsampling operation. The down-\\nsampling method is based on splitting the graph into two components by the sign of\\nthe largest eigenvector of the Laplacian.\\n264\\n8\\nNetwork Representation\\nDIFFPOOL [144] proposes a learnable hierarchical clustering module by training\\nan assignment matrix in each layer:\\nS(l) = Softmax(GNNl,pool(A(l), V(l))),\\n(8.98)\\nwhere V(l) is node features and A(l) is coarsened adjacency matrix of layer l.\\n8.3.5.3\\nNeighborhood Sampling\\nThe original graph convolutional neural network has several drawbacks. Speci\\xef\\xac\\x81cally,\\nGCN requires the full graph Laplacian, which is computationally consuming for large\\ngraphs. Furthermore, the embedding of a node at layer L is computed recursively by\\nthe embeddings of all its neighbors at layer L \\xe2\\x88\\x921. Therefore, the receptive \\xef\\xac\\x81eld of a\\nsingle node grows exponentially with respect to the number of layers, so computing\\ngradient for a single node costs a lot. Finally, GCN is trained independently for a\\n\\xef\\xac\\x81xed graph, which lacks the ability for inductive learning.\\nGraphSAGE [41] is a comprehensive improvement of the original GCN. To\\nsolve the problems mentioned above, GraphSAGE replaced full graph Laplacian\\nwith learnable aggregation functions, which are crucial to perform message passing\\nand generalize to unseen nodes. As shown in Eq.8.86, they \\xef\\xac\\x81rst aggregate neighbor-\\nhood embeddings, concatenate with target node\\xe2\\x80\\x99s embedding, then propagate to the\\nnext layer. With learned aggregation and propagation functions, GraphSAGE could\\ngenerate embeddings for unseen nodes. Also, GraphSAGE uses neighbor sampling\\nto alleviate the receptive \\xef\\xac\\x81eld expansion.\\nPinSage [143] proposes importance-based sampling method. By simulating ran-\\ndom walks starting from target nodes, this approach chooses the top T nodes with\\nthe highest normalized visit counts.\\nFastGCN [16] further improves the sampling algorithm. Instead of sampling\\nneighbors for each node, FastGCN directly samples the receptive \\xef\\xac\\x81eld for each\\nlayer. FastGCN uses importance sampling, which the important factor is calculated\\nas below:\\nq(v) \\xe2\\x88\\x9d\\n1\\n|Nv|\\n\\x02\\nu\\xe2\\x88\\x88Nv\\n1\\n|Nu|.\\n(8.99)\\nAdapt. In contrast to \\xef\\xac\\x81xed sampling methods above, [51] introduces a parameter-\\nized and trainable sampler to perform layer-wise sampling conditioned on the former\\nlayer. Furthermore, this adaptive sampler could \\xef\\xac\\x81nd optimal sampling importance\\nand reduce variance simultaneously.\\n8.3 Graph Neural Networks\\n265\\n8.3.5.4\\nVarious Graph Types\\nIn the original GNN [101], the input graph consists of nodes with label information\\nand undirected edges, which is the simplest graph format. However, there are many\\nvariants of graphs in the world. In the following, we will introduce some methods\\ndesigned to model different kinds of graphs.\\nDirected Graphs. The \\xef\\xac\\x81rst variant of the graph is directed graphs. Undirected\\nedge which can be treated as two directed edges shows that there is a relation between\\ntwo nodes. However, directed edges can bring more information than undirected\\nedges. For example, in a knowledge graph where the edge starts from the head entity\\nand ends at the tail entity, the head entity is the parent class of the tail entity, which\\nsuggests we should treat the information propagation process from parent classes\\nand child classes differently. DGP [55] uses two kinds of the weight matrix, Wp\\nand Wc, to incorporate more precise structural information. The propagation rule is\\nshown as follows:\\nH(t) = f (D\\xe2\\x88\\x921\\np Ap f (D\\xe2\\x88\\x921\\nc AcH(t\\xe2\\x88\\x921)Wc)Wp),\\n(8.100)\\nwhere D\\xe2\\x88\\x921\\np Ap, D\\xe2\\x88\\x921\\nc Ac are the normalized adjacency matrix for parents and children,\\nrespectively.\\nHeterogeneous Graphs. The second variant of the graph is a heterogeneous\\ngraph, where there are several kinds of nodes. The simplest way to process the\\nheterogeneous graph is to convert the type of each node to a one-hot feature vector\\nwhich is concatenated with the original feature.\\nWhat\\xe2\\x80\\x99s more, GraphInception [151] introduces the concept of metapath into the\\npropagation on the heterogeneous graph. With metapath, we can group the neighbors\\naccording to their node types and distances. For each neighbor group, GraphInception\\ntreats it as a subgraph in a homogeneous graph to do propagation and concatenates\\nthe propagation results from different homogeneous graphs to do a collective node\\nrepresentation.Recently,[128]proposestheHeterogeneousgraphAttentionNetwork\\n(HAN) which utilizes node-level and semantic-level attention. And the model has\\nthe ability to consider node importance and metapaths simultaneously.\\nGraphs with Edge Information. In another variant of graph, each edge has\\nadditional information like the weight or the type of the edge. We list two ways to\\nhandle this kind of graphs:\\nFirstly, we can convert the graph to a bipartite graph where the original edges\\nalso become nodes and one original edge is split into two new edges which means\\nthere are two new edges between the edge node and begin/end nodes. The encoder\\nof G2S [7] uses the following aggregation function for neighbors:\\nh(t)\\nv\\n= f\\n\\x15\\n1\\n|Nv|\\n\\x02\\nu\\xe2\\x88\\x88Nv\\nWr\\n\\x0f\\nr(t)\\nv \\xe2\\x8a\\x99h(t\\xe2\\x88\\x921)\\nu\\n\\x10\\n+ br\\n\\x16\\n,\\n(8.101)\\nwhere Wr and br are the propagation parameters for different types of edges\\n(relations).\\n266\\n8\\nNetwork Representation\\nSecondly, we can adapt different weight matrices for the propagation of different\\nkinds of edges. When the number of relations is huge, r-GCN [102] introduces two\\nkinds of regularization to reduce the number of parameters for modeling amounts of\\nrelations: basis- and block-diagonal-decomposition. With the basis decomposition,\\neach Wr is de\\xef\\xac\\x81ned as follows:\\nWr =\\nB\\n\\x02\\nb=1\\n\\xce\\xb1rbMb.\\n(8.102)\\nHere each Wr is a linear combination of basis transformations Mb \\xe2\\x88\\x88Rdin\\xc3\\x97dout\\nwith coef\\xef\\xac\\x81cients \\xce\\xb1rb. In the block-diagonal decomposition, r-GCN de\\xef\\xac\\x81nes each Wr\\nthrough the direct sum over a set of low-dimensional matrices, which needs more\\nparameters than the \\xef\\xac\\x81rst one.\\nDynamic Graphs. Another variant of the graph is dynamic graph, which has\\na static graph structure and dynamic input signals. To capture both kinds of infor-\\nmation, DCRNN [71] and STGCN [147] \\xef\\xac\\x81rst collect spatial information by GNNs,\\nthen feed the outputs into a sequence model like sequence-to-sequence model or\\nCNNs. Differently, Structural-RNN [53] and ST-GCN [135] collect spatial and tem-\\nporal messages at the same time. They extend static graph structure with temporal\\nconnections so they can apply traditional GNNs on the extended graphs.\\n8.3.6\\nApplications\\nGraph neural networks have been explored in a wide range of problem domains across\\nsupervised, semi-supervised, unsupervised, and reinforcement learning settings. In\\nthis section, we simply divide the applications into three scenarios: (1) Structural\\nscenarios where the data has explicit relational structure, such as physical systems,\\nmolecular structures, and knowledge graphs; (2) Nonstructural scenarios where the\\nrelational structure is not explicit include image, text, etc; (3) Other application\\nscenarios such as generative models and combinatorial optimization problems. Note\\nthat we only list several representative applications instead of providing an exhaustive\\nlist. We further give some examples of GNNs in the task of fact veri\\xef\\xac\\x81cation and\\nrelation extraction. Figure8.14 illustrates some application scenarios of graph neural\\nnetworks.\\n8.3.6.1\\nStructural Scenarios\\nIn the following, we will introduce GNN\\xe2\\x80\\x99s applications in structural scenarios, where\\nthe data are naturally performed in the graph structure. For example, GNNs are\\nwidely being used in social network prediction [41, 59], traf\\xef\\xac\\x81c prediction [25, 96],\\nrecommender systems [120, 143], and graph representation [144]. Speci\\xef\\xac\\x81cally, we\\n8.3 Graph Neural Networks\\n267\\nFig. 8.14 Application scenarios of graph neural network [155]\\nare discussing how to model real-world physical systems with object-relationship\\ngraphs, how to predict the chemical properties of molecules and biological interaction\\nproperties of proteins and the applications of GNNs on knowledge graphs.\\nPhysics. Modeling real-world physical systems is one of the most fundamental\\naspects of understanding human intelligence. By representing objects as nodes and\\nrelations as edges, we can perform GNN-based reasoning about objects, relations,\\nand physics in a simpli\\xef\\xac\\x81ed but effective way.\\nBattaglia et al. [6] proposes Interaction Networks to make predictions and infer-\\nencesaboutvariousphysicalsystems.Objectsandrelationsare\\xef\\xac\\x81rstfedintothemodel\\nas input. Then the model considers the interactions and physical dynamics to predict\\nnew states. They separately model relation-centric and object-centric models, mak-\\ning it easier to generalize across different systems. In CommNet [107], interactions\\nare not modeled explicitly. Instead, an interaction vector is obtained by averaging all\\nother agents\\xe2\\x80\\x99 hidden vectors. VAIN [49] further introduced attentional methods into\\nthe agent interaction process, which preserves both the complexity advantages and\\ncomputational ef\\xef\\xac\\x81ciency as well.\\n268\\n8\\nNetwork Representation\\nVisual Interaction Networks [132] can make predictions from pixels. It learns a\\nstate code from two consecutive input frames for each object. Then, after adding\\ntheir interaction effect by an Interaction Net block, the state decoder converts state\\ncodes to the next step\\xe2\\x80\\x99s state.\\nSanchez-Gonzalez et al. [99] proposes a Graph Network based model which could\\neither perform state prediction or inductive inference. The inference model takes\\npartially observed information as input and constructs a hidden graph for implicit\\nsystem classi\\xef\\xac\\x81cation.\\nMolecular Fingerprints. Molecular \\xef\\xac\\x81ngerprints are feature vectors representing\\nmolecules, which are important in computer-aided drug design. Traditional molecu-\\nlar \\xef\\xac\\x81ngerprint discovering relies on heuristic methods which are hand-crafted. And\\nGNNs can provide more \\xef\\xac\\x82exible approaches for better \\xef\\xac\\x81ngerprints.\\nDuvenaud et al. [31] propose neural graph \\xef\\xac\\x81ngerprints (Neural FPs) that calculate\\nsubstructure feature vectors via GCN and sum to get overall representation. The\\naggregation function is introduced in Eq.8.83.\\nKearnes et al. [56] further explicitly models atom and atom pairs independently\\nto emphasize atom interactions. It introduces edge representation e(t)\\nuv instead of\\naggregation function, i.e., h(t)\\nNv = \\x04\\nu\\xe2\\x88\\x88Nv e(t)\\nuv. The node update function is\\nh(t+1)\\nv\\n= ReLU(W1[ReLU(W0h(t)\\nu ); h(t)\\nNv]),\\n(8.103)\\nwhile the edge update function is\\ne(t+1)\\nuv\\n= ReLU(W4[ReLU(W2e(t)\\nuv); ReLU(W3[h(t)\\nv ; h(t)\\nu ])]).\\n(8.104)\\nProtein Interface Prediction. Fout et al. [33] focuses on the task named protein\\ninterface prediction, which is a challenging problem with critical applications in\\ndrug discovery and design. The proposed GCN-based method, respectively, learns\\nligand and receptor protein residue representation and merges them for pair-wise\\nclassi\\xef\\xac\\x81cation.\\nGNN can also be used in biomedical engineering. With Protein-Protein Inter-\\naction Network, [97] leverages graph convolution and relation network for breast\\ncancer subtype classi\\xef\\xac\\x81cation. Zitnik et al. [160] also suggest a GCN-based model\\nfor polypharmacy side effects prediction. Their work models the drug and protein\\ninteraction network and separately deals with edges in different types.\\nKnowledge Graph. Hamaguchi et al. [40] utilizes GNNs to solve the Out-Of-\\nKnowledge-Base (OOKB) entity problem in Knowledge Base Completion (KBC).\\nThe OOKB entities in [40] are directly connected to the existing entities thus the\\nembeddings of OOKB entities can be aggregated from the existing entities. The\\nmethod achieves satisfying performance both in the standard KBC setting and the\\nOOKB setting.\\nWang et al. [130] utilize GCNs to solve the cross-lingual knowledge graph align-\\nment problem. The model embeds entities from different languages into a uni\\xef\\xac\\x81ed\\nembedding space and aligns them based on the embedding similarity.\\n8.3 Graph Neural Networks\\n269\\n8.3.6.2\\nNonstructural Scenarios\\nIn this section we will talk about applications on nonstructural scenarios such as\\nimage, text, programming source code [1, 72], and multi-agent systems [49, 58,\\n107]. We will only give a detailed introduction to the \\xef\\xac\\x81rst two scenarios due to the\\nlength limit. Roughly, there are two ways to apply the graph neural networks on\\nnonstructural scenarios: (1) Incorporate structural information from other domains\\nto improve the performance, for example, using information from knowledge graphs\\nto alleviate the zero-shot problems in image tasks; (2) Infer or assume the relational\\nstructure in the scenario and then apply the model to solve the problems de\\xef\\xac\\x81ned on\\ngraphs, such as the method in [152] which models text as graphs.\\nImage classi\\xef\\xac\\x81cation. Image classi\\xef\\xac\\x81cation is a fundamental and essential task\\nin the \\xef\\xac\\x81eld of computer vision, which attracts much attention and has many famous\\ndatasets like ImageNet [62]. Recent progress in image classi\\xef\\xac\\x81cation bene\\xef\\xac\\x81ts from big\\ndata and the strong power of GPU computation, which allows us to train a classi\\xef\\xac\\x81er\\nwithout extracting structural information from images. However, zero-shot and few-\\nshot learning become more and more popular in the \\xef\\xac\\x81eld of image classi\\xef\\xac\\x81cation,\\nbecause most models can achieve similar performance with enough data. There are\\nseveral works leveraging graph neural networks to incorporate structural information\\nin image classi\\xef\\xac\\x81cation.\\nFirst, knowledge graphs can be used as extra information to guide zero-shot recog-\\nnition classi\\xef\\xac\\x81cation [55, 129]. Wang et al. [129] builds a knowledge graph where\\neach node corresponds to an object category and takes the word embeddings of nodes\\nas input for predicting the classi\\xef\\xac\\x81er of different categories. As the over-smoothing\\neffect happens with the deep depth of convolution architecture, the 6-layer GCN used\\nin [129] will wash out much useful information in the representation. To solve the\\nsmoothing problem in the propagation of GCN, [55] uses single-layer GCN with a\\nlarger neighborhood, which includes both one-hop and multi-hop nodes in the graph.\\nAnd it proved effective in building a zero-shot classi\\xef\\xac\\x81er with existing ones.\\nExcept for the knowledge graph, the similarity between images in the dataset is\\nalso helpful for few-shot learning [100]. Satorras and Estrach [100] propose to build\\na weighted fully connected image network based on the similarity and do message\\npassing in the graph for few-shot recognition. As most knowledge graphs are large\\nfor reasoning, [77] selects some related entities to build a subgraph based on the\\nresult of object detection and apply GGNN to the extracted graph for prediction.\\nBesides, [66] proposes to construct a new knowledge graph where the entities are all\\nthe categories. And, they de\\xef\\xac\\x81ned three types of label relations: super-subordinate,\\npositive correlation, and negative correlation and propagate the con\\xef\\xac\\x81dence of labels\\nin the graph directly.\\nVisual reasoning. Computer vision systems usually need to perform reasoning\\nby incorporating both spatial and semantic information. So it is natural to generate\\ngraphs for reasoning tasks.\\nA typical visual reasoning task is Visual Question Answering (VQA), [114],\\nrespectively, constructs image scene graph and question syntactic graph. Then it\\napplies GGNN to train the embeddings for predicting the \\xef\\xac\\x81nal answer. Despite spatial\\n270\\n8\\nNetwork Representation\\nconnections among objects, [87] builds the relational graphs conditioned on the\\nquestions. With knowledge graphs, [83, 131] can perform \\xef\\xac\\x81ner relation exploration\\nand a more interpretable reasoning process.\\nOther applications of visual reasoning include object detection, interaction detec-\\ntion, and region classi\\xef\\xac\\x81cation. In object detection [39, 50], GNNs are used to calcu-\\nlate RoI features; In interaction detection [53, 95], GNNs are message-passing tools\\nbetween human and objects; In region classi\\xef\\xac\\x81cation [18], GNNs perform reasoning\\non graphs which connects regions and classes.\\nText Classi\\xef\\xac\\x81cation. Text classi\\xef\\xac\\x81cation is an essential and classical problem in\\nnatural language processing. The classical GCN models [4, 28, 41, 47, 59, 82] and\\nGAT model [122] are applied to solve the problem, but they only use the structural\\ninformation between the documents and they do not use much text information.\\nPeng et al. [91] propose a graph-CNN-based deep learning model. It \\xef\\xac\\x81rst turns\\ntexts to graph-of-words, then the graph convolution operations in [347] are used\\non the word graph. Zhang et al. [152] propose the Sentence LSTM to encode text.\\nThe whole sentence is represented in a single state which contains a global sentence-\\nlevel state and several substates for individual words. It uses the global sentence-level\\nrepresentation for classi\\xef\\xac\\x81cation tasks.\\nThese methods either view a document or a sentence as a graph of word nodes\\nor rely on the document citation relation to construct the graph. Yao et al. [142]\\nregard the documents and words as nodes to construct the corpus graph (hence\\nheterogeneous graph) and uses the Text GCN to learn embeddings of words and\\ndocuments. Sentiment classi\\xef\\xac\\x81cation could also be regarded as a text classi\\xef\\xac\\x81cation\\nproblem and a Tree-LSTM approach is proposed by [109].\\nSequence Labeling. As each node in GNNs has its hidden state, we can utilize\\nthe hidden state to address the sequence labeling problem if we consider every word\\nin the sentence as a node. Zhang et al. [152] utilize the Sentence LSTM to label the\\nsequence. It has conducted experiments on POS-tagging and NER tasks and achieves\\npromising performance.\\nSemantic role labeling is another task of sequence labeling. Marcheggiani and\\nTitov [76] propose a Syntactic GCN to solve the problem. The Syntactic GCN which\\noperates on the direct graph with labeled edges is a special variant of the GCN [59].\\nIt uses edge-wise gates that enable the model to regulate the contribution of each\\ndependency edge. The Syntactic GCNs over syntactic dependency trees are used as\\nsentence encoders to learn latent feature representations of words in the sentence.\\nMarcheggiani and Titov [76] also reveal that GCNs and LSTMs are functionally\\ncomplementary in the task.\\n8.3.6.3\\nOther Scenarios\\nBesides structural and nonstructural scenarios, there are some other scenarios where\\ngraph neural networks play an important role. In this subsection, we will introduce\\ngenerative graph models and combinatorial optimization with GNNs.\\n8.3 Graph Neural Networks\\n271\\nGenerative Models. Generative models for real-world graphs have drawn signif-\\nicant attention for its essential applications, including modeling social interactions,\\ndiscovering new chemical structures, and constructing knowledge graphs. As deep\\nlearning methods have a powerful ability to learn the implicit distribution of graphs,\\nthere is a surge in neural graph generative models recently.\\nNetGAN [104] is one of the \\xef\\xac\\x81rst works to build a neural graph generative model,\\nwhich generates graphs via random walks. It transformed the problem of graph\\ngeneration to the problem of walk generation, which takes the random walks from a\\nspeci\\xef\\xac\\x81c graph as input and trains a walk generative model using GAN architecture.\\nWhile the generated graph preserves essential topological properties of the original\\ngraph, the number of nodes is unable to change in the generating process, which is\\nas same as the original graph. GraphRNN [146] generate the adjacency matrix of a\\ngraph by generating the adjacency vector of each node step by step, which can output\\nrequired networks having different numbers of nodes.\\nInstead of generating adjacency matrix sequentially, MolGAN [27] predict a\\ndiscrete graph structure (the adjacency matrix) at once and utilizes a permutation-\\ninvariant discriminator to solve the node variant problem in the adjacency matrix.\\nBesides, it applies a reward network for RL-based optimization towards desired\\nchemical properties. What is more, [75] proposes constrained variational autoen-\\ncoders to ensure the semantic validity of generated graphs. Moreover, GCPN [145]\\nincorporates domain-speci\\xef\\xac\\x81c rules through reinforcement learning.\\nLi et al. [73] propose a model that generates edges and nodes sequentially and\\nutilize a graph neural network to extract the hidden state of the current graph, which\\nis used to decide the action in the next step during the sequential generative process.\\nCombinatorial optimization. Combinatorial optimization problems over graphs\\nare a set of NP-hard problems that attract much attention from scientists of all \\xef\\xac\\x81elds.\\nSome speci\\xef\\xac\\x81c problems like Traveling Salesman Problem (TSP) have got various\\nheuristic solutions. Recently, using a deep neural network for solving such problems\\nhas been a hotspot, and some of the solutions further leverage graph neural networks\\nbecause of their graph structure.\\nBello et al. [9] \\xef\\xac\\x81rst propose a deep learning approach to tackle TSP. Their method\\nconsistsoftwoparts:aPointerNetwork[123]forparameterizingrewardsandapolicy\\ngradient [108] module for training. This work has been proved to be comparable with\\ntraditional approaches. However, Pointer Networks are designed for sequential data\\nlike texts, while order-invariant encoders are more appropriate for such work.\\nKhalil et al. [57] and Kool and Welling [61] improve the above method by includ-\\ning graph neural networks. The former work \\xef\\xac\\x81rst obtains the node embeddings from\\nstructure2vec [26] then feeds them into a Q-learning module for making decisions.\\nThe latter one builds an attention-based encoder-decoder system. By replacing the\\nreinforcement learning module with an attention-based decoder, it is more ef\\xef\\xac\\x81cient\\nfor training. These work achieved better performance than previous algorithms,\\nwhich proved the representation power of graph neural networks.\\nNowaketal.[88]focusonQuadraticAssignmentProblem,i.e.,measuringthesim-\\nilarity of two graphs. The GNN-based model learns node embeddings for each graph\\nindependently and matches them using an attention mechanism. Even in situations\\n272\\n8\\nNetwork Representation\\nwhere traditional relaxation-based methods may perform not well, this model still\\nshows satisfying performance.\\n8.3.6.4\\nExample: GNNs for Fact Veri\\xef\\xac\\x81cation\\nDue to the rapid development of Information Extraction (IE), huge volumes of data\\nhave been extracted. How to automatically verify the data becomes a vital problem\\nfor various data-driven applications, e.g., knowledge graph completion [126] and\\nopen domain question answering [15]. Hence, many recent research efforts have\\nbeen devoted to Fact Veri\\xef\\xac\\x81cation (FV), which aims to verify given claims with the\\nevidence retrieved from plain text. More speci\\xef\\xac\\x81cally, given a claim, an FV system\\nis asked to label it as \\xe2\\x80\\x9cSUPPORTED\\xe2\\x80\\x9d, \\xe2\\x80\\x9cREFUTED\\xe2\\x80\\x9d, or \\xe2\\x80\\x9cNOT ENOUGH INFO\\xe2\\x80\\x9d,\\nwhich indicates that the evidence can support, refute, or is not suf\\xef\\xac\\x81cient for the claim.\\nAn example of the FV task is shown in Table8.5.\\nExisting FV methods formulate FV as a Natural Language Inference (NLI) [3]\\ntask. However, they utilize simple evidence combination methods such as concate-\\nnating the evidence or just dealing with each evidence-claim pair. These methods are\\nunable to grasp suf\\xef\\xac\\x81cient relational and logical information among the evidence. In\\nfact, many claims require to simultaneously integrate and reason over several pieces\\nof evidence for veri\\xef\\xac\\x81cation. As shown in Table8.5, for this particular example, we\\ncannot verify the given claim by checking any evidence in isolation. The claim can\\nbe veri\\xef\\xac\\x81ed only by understanding and reasoning over multiple evidence.\\nTable 8.5 A case of the claim that requires integrating multiple evidence to verify. The represen-\\ntation for evidence \\xe2\\x80\\x9c{DocName, LineNum}\\xe2\\x80\\x9d means the evidence is extracted from the document\\n\\xe2\\x80\\x9cDocName\\xe2\\x80\\x9d and of which the line number is LineNum\\nClaim:\\nAl Jardine is an American rhythm guitarist\\nTruth evidence:\\n{Al Jardine, 0}, {Al Jardine, 1}\\nRetrieved evidence:\\n{Al Jardine, 1}, {Al Jardine, 0}, {Al Jardine, 2}, {Al Jardine, 5}, {Jardine, 42}\\nEvidence:\\n(1) He is best known as the band\\xe2\\x80\\x99s rhythm guitarist, and for occasionally singing lead vocals\\non singles such as \\xe2\\x80\\x9cHelp Me, Rhonda\\xe2\\x80\\x9d (1965), \\xe2\\x80\\x9cThen I Kissed Her\\xe2\\x80\\x9d (1965), and \\xe2\\x80\\x9cCome Go with\\nMe\\xe2\\x80\\x9d (1978)\\n(2) Alan Charles Jardine (born September 3, 1942) is an American musician, singer and\\nsongwriter who co-founded the Beach Boys\\n(3) In 2010, Jardine released his debut solo studio album, A Postcard from California\\n(4) In 1988, Jardine was inducted into the Rock and Roll Hall of Fame as a member of the\\nBeach Boys\\n(5) Ray Jardine American rock climber, lightweight backpacker, inventor, author, and global\\nadventurer\\nLabel: SUPPORTED\\n8.3 Graph Neural Networks\\n273\\nTo integrate and reason over information from multiple pieces of evidence, [156]\\nproposes a graph-based evidence aggregating and reasoning (GEAR) framework.\\nSpeci\\xef\\xac\\x81cally, [156] \\xef\\xac\\x81rst builds a fully connected evidence graph and encourages\\ninformation propagation among the evidence. Then, GEAR aggregates the pieces\\nof evidence and adopts a classi\\xef\\xac\\x81er to decide whether the evidence can support,\\nrefute, or is not suf\\xef\\xac\\x81cient for the claim. Intuitively, by suf\\xef\\xac\\x81ciently exchanging and\\nreasoning over evidence information on the evidence graph, the proposed model can\\nmake the best of the information for verifying claims. For example, by delivering the\\ninformation \\xe2\\x80\\x9cLos Angeles County is the most populous county in the USA\\xe2\\x80\\x9d to \\xe2\\x80\\x9cthe\\nRodney King riots occurred in Los Angeles County\\xe2\\x80\\x9d through the evidence graph,\\nthe synthetic information can support \\xe2\\x80\\x9cThe Rodney King riots took place in the\\nmost populous county in the USA\\xe2\\x80\\x9d. Furthermore, we adopt an effective pretrained\\nlanguage representation model BERT [29] to better grasp both evidence and claim\\nsemantics.\\nZhou et al. [156] employ a three-step pipeline with components for document\\nretrieval, sentence selection, and claim veri\\xef\\xac\\x81cation to solve the task. In the document\\nretrieval and sentence selection stages, they simply follow the method from [44] since\\ntheir method has the highest score on evidence recall in the former FEVER-shared\\ntask. And they propose the GEAR framework in the \\xef\\xac\\x81nal claim veri\\xef\\xac\\x81cation stage.\\nThe full pipeline is illustrated in Fig.8.15.\\nGiven a claim and the retrieved evidence, GEAR \\xef\\xac\\x81rst utilizes a sentence encoder\\nto obtain representations for the claim and the evidence. Then it builds a fully con-\\nnected evidence graph and uses an Evidence Reasoning Network (ERNet) to prop-\\nagate information among evidence and reason over the graph. Finally, it utilizes an\\nevidence aggregator to infer the \\xef\\xac\\x81nal results.\\nSentence Encoder. Given an input sentence, GEAR employs BERT [29] as the\\nsentence encoder by extracting the \\xef\\xac\\x81nal hidden state of the [CLS] token as the\\nrepresentation, where [CLS] is the special classi\\xef\\xac\\x81cation token in BERT.\\nei = BERT (ei, c) ,\\nc = BERT (c) .\\n(8.105)\\nFig. 8.15 The pipeline used in [156]. The GEAR framework is illustrated in the claim veri\\xef\\xac\\x81cation\\nsection\\n274\\n8\\nNetwork Representation\\nEvidence Reasoning Network. To encourage the information propagation among\\nevidence, GEAR builds a fully connected evidence graph where each node indi-\\ncates a piece of evidence. It also adds self-loop to every node because each node\\nneeds the information from itself in the message propagation process. We use\\nht = {ht\\n1, ht\\n2, . . . , ht\\nN} to represent the hidden states of nodes at layer t. The ini-\\ntial hidden state of each evidence node h0\\ni is initialized by the evidence presentation:\\nh0\\ni = ei.\\nInspired by recent work on semi-supervised graph learning and relational rea-\\nsoning [59, 90, 122], Zhou et al. [156] propose an Evidence Reasoning Network\\n(ERNet) to propagate information among the evidence nodes. It \\xef\\xac\\x81rst uses an MLP\\nto compute the attention coef\\xef\\xac\\x81cients between a node i and its neighbor j ( j \\xe2\\x88\\x88Ni),\\nyi j = W(t\\xe2\\x88\\x921)\\n1\\n(ReLU(W(t\\xe2\\x88\\x921)\\n0\\n[h(t\\xe2\\x88\\x921)\\ni\\n; h(t\\xe2\\x88\\x921)\\nj\\n])),\\n(8.106)\\nwhere Ni denotes the set of neighbors of node i, W(t\\xe2\\x88\\x921)\\n0\\nand W(t\\xe2\\x88\\x921)\\n1\\nare weight\\nmatrices, and [\\xc2\\xb7; \\xc2\\xb7] denotes concatenation operation.\\nThen, it normalizes the coef\\xef\\xac\\x81cients using the softmax function\\n\\xce\\xb1i j = Softmax j(yi j) =\\nexp(yi j)\\n\\x04\\nk\\xe2\\x88\\x88Ni exp(yik).\\n(8.107)\\nFinally, the normalized attention coef\\xef\\xac\\x81cients are used to compute a linear com-\\nbination of the neighbor features and thus we obtain the features for node i at layer\\nt,\\nh(t)\\ni\\n=\\n\\x02\\nj\\xe2\\x88\\x88Ni\\n\\xce\\xb1i jh(t\\xe2\\x88\\x921)\\nj\\n.\\n(8.108)\\nBy stacking T layers of ERNet, [156] assumes that each evidence could grasp\\nenough information by communicating with other evidence.\\nEvidence Aggregator. Zhou et al. [156] employ an evidence aggregator to gather\\ninformation from different evidence nodes and obtain the \\xef\\xac\\x81nal hidden state o. The\\naggregator may utilize different aggregating strategies and [156] suggests three\\naggregators:\\nAttention Aggregator. Zhou et al. [156] use the representation of the claim c to\\nattend the hidden states of evidence and get the \\xef\\xac\\x81nal aggregated state o.\\ny j = W\\xe2\\x80\\xb2\\n1(ReLU(W\\xe2\\x80\\xb2\\n0[c; h\\xe2\\x8a\\xa4\\nj ])),\\n\\xce\\xb1 j = Softmax(y j) =\\nexp(y j)\\n\\x04N\\nk=1 exp(yk)\\n,\\no =\\nN\\n\\x02\\nk=1\\n\\xce\\xb1kh\\xe2\\x8a\\xa4\\nk .\\n(8.109)\\n8.3 Graph Neural Networks\\n275\\nMax Aggregator. The max aggregator performs the element-wise max operation\\namong hidden states.\\no = Max(h\\xe2\\x8a\\xa4\\n1 , h\\xe2\\x8a\\xa4\\n2 , . . . , h\\xe2\\x8a\\xa4\\nN).\\n(8.110)\\nMean Aggregator. The mean aggregator performs the element-wise mean opera-\\ntion among hidden states.\\no = Mean(h\\xe2\\x8a\\xa4\\n1 , h\\xe2\\x8a\\xa4\\n2 , . . . , h\\xe2\\x8a\\xa4\\nN).\\n(8.111)\\nOnce the \\xef\\xac\\x81nal state o is obtained, GEAR employs a one-layer MLP to get the \\xef\\xac\\x81nal\\nprediction l.\\nl = Softmax(ReLU(Wo + b)),\\n(8.112)\\nwhere W and b are parameters.\\nZhou et al. [156] conduct experiments on the large-scale benchmark dataset for\\nFact Extraction and VERi\\xef\\xac\\x81cation (FEVER) [115]. Experimental results show that\\nthe proposed framework outperforms recent state-of-the-art baseline systems. The\\nfurther case study indicates that the framework could better leverage multi-evidence\\ninformation and reason over the evidence for FV.\\n8.4\\nSummary\\nIn this chapter, we have introduced network representation learning, which turns\\nthe network structure information into the continuous vector space and make deep\\nlearning techniques possible on network data.\\nUnsupervised network representation learning comes \\xef\\xac\\x81rst during the development\\nof NRL. Spectral Clustering, DeepWalk, LINE, GraRep, and other methods utilize\\nthe network structure for vertex embedding learning. Afterward, TADW incorporates\\ntext information into NRL under the framework of matrix factorization. The NEU\\nalgorithm then moves one step forward and proposes a general method to improve\\nthe quality of any learned network embeddings. Other unsupervised methods also\\nconsider preserving speci\\xef\\xac\\x81c properties of the network topology, e.g., community and\\nasymmetry.\\nRecently, semi-supervised NRL algorithms have attracted much attention. This\\nkind of methods focus on a speci\\xef\\xac\\x81c task such as classi\\xef\\xac\\x81cation and use the labels of\\nthe training set to improve the quality of network embeddings. Node2vec, MMDW,\\nand many other methods including the family of Graph Neural Networks (GNNs)\\nare proposed for this end. Semi-supervised algorithms can achieve better results as\\nthey can take advantage of more information from the speci\\xef\\xac\\x81c task.\\nFor further understanding of network representation learning, you can also \\xef\\xac\\x81nd\\nmore related papers in this paper list https://github.com/thunlp/GNNPapers. There\\nare also some recommended surveys and books including the following:\\n276\\n8\\nNetwork Representation\\n\\xe2\\x80\\xa2 Cui et al. A survey on network embedding [24].\\n\\xe2\\x80\\xa2 Goyal and Ferrara. Graph embedding techniques, applications, and performance:\\nA survey [37].\\n\\xe2\\x80\\xa2 Zhang et al. Network representation learning: A survey [149].\\n\\xe2\\x80\\xa2 Wu et al. A comprehensive survey on graph neural networks [133].\\n\\xe2\\x80\\xa2 Zhou et al. Graph neural networks: A review of methods and applications [155].\\n\\xe2\\x80\\xa2 Zhang et al. Deep learning on graphs: A survey [154].\\nIn the future, for better network representation learning, some directions are\\nrequiring further efforts:\\n(1) More Complex and Realistic Networks. An intriguing direction would be\\nthe representation of learning on heterogeneous and dynamic networks where most\\nreal-world network data fall into this category. The vertices and edges in a heteroge-\\nneous network may belong to different types. Networks in real life are also highly\\ndynamic, e.g., the friendship between Facebook users may establish and disappear.\\nThese characteristics require the researchers to design speci\\xef\\xac\\x81c algorithms for them.\\nNetwork embedding learning on dynamic network structures is, therefore, an impor-\\ntant task. There have been some works proposed [14, 105] for much more complex\\nand realistic settings.\\n(2) Deeper Model Architectures. Conventional deep neural networks can stack\\nhundreds of layers to get better performance because the deeper structure has more\\nparameters and may improve the expressive power signi\\xef\\xac\\x81cantly. However, NRL and\\nGNNmodelsareusuallyshallow.Infact,mostofthemhavenomorethanthreelayers.\\nTaking GCN as an example, as experiments in [70] show, stacking multiple GCN\\nlayers will result in over-smoothing: the representations of all vertices will converge\\nto the same. Although some researchers have managed to tackle this problem [70,\\n125] to some extents, it remains to be a limitation of NRL. Designing deeper model\\narchitectures is an exciting challenge for future research, and will be a considerable\\ncontribution to the understanding of NRL.\\n(3) Scalability. Scalability determines whether an algorithm is able to be applied\\nto practical use. How to apply NRL methods in real-world web-scale scenarios such\\nas social networks or recommendation systems has been an essential problem for\\nmost network embedding algorithms. Scaling up NRL methods especially GNN\\nis dif\\xef\\xac\\x81cult because many core steps are computationally consuming in a big data\\nenvironment. For example, network data are not regular Euclidean, and each node\\nhas its own neighborhood structure. Therefore, batch tricks cannot be easily applied.\\nMoreover, computing graph Laplacian is also unfeasible when there are millions or\\neven billions of nodes and edges. Several works has proposed their solutions to this\\nproblem [143, 153, 157] and we are paying close attention to the progress.\\nReferences\\n277\\nReferences\\n1. Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent\\nprograms with graphs. In Proceedings of ICLR, 2018.\\n2. Reid Andersen, Fan Chung, and Kevin Lang. Local graph partitioning using pagerank vectors.\\nIn Proceedings of FOCS, 2006.\\n3. Gabor Angeli and Christopher D Manning. Naturalli: Natural logic inference for common\\nsense reasoning. In Proceedings of EMNLP, 2014.\\n4. James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Proceedings\\nof NeurIPS, 2016.\\n5. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\\njointly learning to align and translate. In Proceedings of ICLR, 2015.\\n6. Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction\\nnetworks for learning about objects, relations and physics. In Proceedings of NeurIPS, 2016.\\n7. Daniel Beck, Gholamreza Haffari, and Trevor Cohn. Graph-to-sequence learning using gated\\ngraph neural networks. In Proceedings of ACL, 2018.\\n8. Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embed-\\nding and clustering. In Proceedings of NeurIPS, volume 14, 2001.\\n9. Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural com-\\nbinatorial optimization with reinforcement learning. In Proceedings of ICLR, 2017.\\n10. Davide Boscaini, Jonathan Masci, Emanuele Rodol\\xc3\\xa0, and Michael Bronstein. Learning shape\\ncorrespondence with anisotropic convolutional neural networks. In Proceedings of NeurIPS,\\n2016.\\n11. Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Lecun. Spectral networks and locally\\nconnected networks on graphs. In Proceedings of ICLR, 2014.\\n12. Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of\\ngraph embedding: Problems, techniques, and applications. IEEE Transactions on Knowledge\\nand Data Engineering, 30(9):1616\\xe2\\x80\\x931637, 2018.\\n13. Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with\\nglobal structural information. In Proceedings of CIKM, 2015.\\n14. Shiyu Chang, Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C Aggarwal, and Thomas S Huang.\\nHeterogeneous network embedding via deep architectures. In Proceedings of SIGKDD, 2015.\\n15. Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer\\nopen-domain questions. In Proceedings of the ACL, 2017.\\n16. Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks\\nvia importance sampling. In Proceedings of ICLR, 2018.\\n17. Mo Chen, Qiong Yang, and Xiaoou Tang. Directed graph embedding. In Proceedings of IJCAI,\\n2007.\\n18. Xinlei Chen, Lijia Li, Li Feifei, and Abhinav Gupta. Iterative visual reasoning beyond con-\\nvolutions. In Proceedings of CVPR, 2018.\\n19. Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for\\nmachine reading. In Proceedings of EMNLP, 2016.\\n20. Kyunghyun Cho, Bart van Merri\\xc3\\xabnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\\nHolger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder\\xe2\\x80\\x93\\ndecoder for statistical machine translation. In Proceedings of EMNLP, 2014.\\n21. Yoon-Sik Cho, Greg Ver Steeg, and Aram Galstyan. Socially relevant venue clustering from\\ncheck-in data. In Proceedings of KDD Workshop, 2013.\\n22. Wojciech Chojnacki and Michael J Brooks. A note on the locally linear embedding algorithm.\\nInternational Journal of Pattern Recognition and Arti\\xef\\xac\\x81cial Intelligence, 23(08):1739\\xe2\\x80\\x931752,\\n2009.\\n23. Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Math-\\nematical Soc., 1997.\\n24. Peng Cui, Xiao Wang, Jian Pei, and Wenwu Zhu. A survey on network embedding. IEEE\\nTransactions on Knowledge and Data Engineering, 2018.\\n278\\n8\\nNetwork Representation\\n25. Zhiyong Cui, Kristian Henrickson, Ruimin Ke, and Yinhai Wang. Traf\\xef\\xac\\x81c graph convolutional\\nrecurrent neural network: A deep learning framework for network-scale traf\\xef\\xac\\x81c learning and\\nforecasting. IEEE Transactions on Intelligent Transportation Systems, 2019.\\n26. Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for\\nstructured data. In Proceedings of ICML, 2016.\\n27. Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular\\ngraphs. arXiv preprint arXiv:1805.11973, 2018.\\n28. Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural net-\\nworks on graphs with fast localized spectral \\xef\\xac\\x81ltering. In Proceedings of NeurIPS, 2016.\\n29. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019.\\n30. Giuseppe Di Battista, Peter Eades, Roberto Tamassia, and Ioannis G Tollis. Algorithms for\\ndrawing graphs: an annotated bibliography. Computational Geometry, 4(5):235\\xe2\\x80\\x93282, 1994.\\n31. David K Duvenaud, Dougal Maclaurin, Jorge Aguileraiparraguirre, Rafael Gomezbombarelli,\\nTimothy D Hirzel, Alan Aspuruguzik, and Ryan P Adams. Convolutional networks on graphs\\nfor learning molecular \\xef\\xac\\x81ngerprints. In Proceedings of NeurIPS, 2015.\\n32. Francois Fouss, Alain Pirotte, Jean-Michel Renders, and Marco Saerens. Random-walk com-\\nputation of similarities between nodes of a graph with application to collaborative recommen-\\ndation. IEEE Transactions on Knowledge and Data Engineering, 19(3):355\\xe2\\x80\\x93369, 2007.\\n33. Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using\\ngraph convolutional networks. In Proceedings of NeurIPS, 2017.\\n34. Thomas MJ Fruchterman and Edward M Reingold. Graph drawing by force-directed place-\\nment. Software: Practice and Experience, 21(11):1129\\xe2\\x80\\x931164, 1991.\\n35. Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolu-\\ntional networks. In Proceedings of SIGKDD, 2018.\\n36. Jonas Gehring, Michael Auli, David Grangier, and Yann N Dauphin. A convolutional encoder\\nmodel for neural machine translation. In Proceedings of ACL, 2017.\\n37. Palash Goyal and Emilio Ferrara. Graph embedding techniques, applications, and perfor-\\nmance: A survey. Knowledge-Based Systems, 151:78\\xe2\\x80\\x9394, 2018.\\n38. Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Pro-\\nceedings of SIGKDD, 2016.\\n39. Jiayuan Gu, Han Hu, Liwei Wang, Yichen Wei, and Jifeng Dai. Learning region features for\\nobject detection. In Proceedings of ECCV, 2018.\\n40. Takuo Hamaguchi, Hidekazu Oiwa, Masashi Shimbo, and Yuji Matsumoto. Knowledge trans-\\nfer for out-of-knowledge-base entities : A graph neural network approach. In Proceedings of\\nIJCAI, 2017.\\n41. Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large\\ngraphs. In Proceedings of NeurIPS, 2017.\\n42. William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Meth-\\nods and applications. IEEE Data(base) Engineering Bulletin, 40:52\\xe2\\x80\\x9374, 2017.\\n43. David K Hammond, Pierre Vandergheynst, and Remi Gribonval. Wavelets on graphs via\\nspectral graph theory. Applied and Computational Harmonic Analysis, 30(2):129\\xe2\\x80\\x93150, 2011.\\n44. Andreas Hanselowski, Hao Zhang, Zile Li, Daniil Sorokin, Benjamin Schiller, Claudia Schulz,\\nand Iryna Gurevych. Ukp-athene: Multi-sentence textual entailment for claim veri\\xef\\xac\\x81cation. In\\nProceedings of EMNLP, 2018.\\n45. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition. In Proceedings of CVPR, 2016.\\n46. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual\\nnetworks. In Proceedings of ECCV, 2016.\\n47. Mikael Henaff, Joan Bruna, and Yann Lecun. Deep convolutional networks on graph-\\nstructured data. arXiv preprint arXiv:1506.05163, 2015.\\n48. Sepp Hochreiter and J\\xc3\\xbcrgen Schmidhuber. Long short-term memory. Neural Computation,\\n9(8):1735\\xe2\\x80\\x931780, 1997.\\nReferences\\n279\\n49. Yedid Hoshen. Vain: Attentional multi-agent predictive modeling. In Proceedings of NeurIPS,\\n2017.\\n50. Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\\ndetection. In Proceedings of CVPR, 2018.\\n51. Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast\\ngraph representation learning. In Proceedings of NeurIPS, 2018.\\n52. Yann Jacob, Ludovic Denoyer, and Patrick Gallinari. Learning latent representations of nodes\\nfor classifying in heterogeneous social networks. In Proceedings of WSDM, 2014.\\n53. Ashesh Jain, Amir R Zamir, Silvio Savarese, and Ashutosh Saxena. Structural-rnn: Deep\\nlearning on spatio-temporal graphs. In Proceedings of CVPR, 2016.\\n54. Tomihisa Kamada and Satoru Kawai. An algorithm for drawing general undirected graphs.\\nInformation Processing Letters, 31(1):7\\xe2\\x80\\x9315, 1989.\\n55. Michael Kampffmeyer, Yinbo Chen, Xiaodan Liang, Hao Wang, Yujia Zhang, and Eric P\\nXing. Rethinking knowledge graph propagation for zero-shot learning. In Proceedings of\\nCVPR, 2019.\\n56. Steven Kearnes, Kevin McCloskey, Marc Berndl, Vijay Pande, and Patrick Riley. Molecu-\\nlar graph convolutions: moving beyond \\xef\\xac\\x81ngerprints. Journal of computer-aided molecular\\ndesign, 30(8):595\\xe2\\x80\\x93608, 2016.\\n57. Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial\\noptimization algorithms over graphs. In Proceedings of NeurIPS, 2017.\\n58. Thomas Kipf, Ethan Fetaya, Kuanchieh Wang, Max Welling, and Richard S Zemel. Neural\\nrelational inference for interacting systems. In Proceedings of ICML, 2018.\\n59. Thomas N Kipf and Max Welling. Semi-supervised classi\\xef\\xac\\x81cation with graph convolutional\\nnetworks. In Proceedings of ICLR, 2017.\\n60. Stephen G Kobourov. Spring embedders and force directed graph drawing algorithms. arXiv\\npreprint arXiv:1201.3011, 2012.\\n61. WWM Kool and M Welling. Attention solves your tsp. arXiv preprint arXiv:1803.08475,\\n2018.\\n62. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classi\\xef\\xac\\x81cation with deep\\nconvolutional neural networks. In Proceedings of NeurIPS, 2012.\\n63. Theodoros Lappas, Evimaria Terzi, Dimitrios Gunopulos, and Heikki Mannila. Finding effec-\\ntors in social networks. In Proceedings of SIGKDD, 2010.\\n64. Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436,\\n2015.\\n65. Yann LeCun, L\\xc3\\xa9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning\\napplied to document recognition. Proceedings of the IEEE, 1998.\\n66. Chungwei Lee, Wei Fang, Chihkuan Yeh, and Yuchiang Frank Wang. Multi-label zero-shot\\nlearning with structured knowledge graphs. In Proceedings of CVPR, 2018.\\n67. Jure Leskovec, Lada A Adamic, and Bernardo A Huberman. The dynamics of viral marketing.\\nACM Transactions on the Web (TWEB), 1(1):5, 2007.\\n68. Jure Leskovec, Lars Backstrom, and Jon Kleinberg. Meme-tracking and the dynamics of the\\nnews cycle. In Proceedings of SIGKDD, 2009.\\n69. Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In\\nProceedings of NeurIPS, 2014.\\n70. Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional net-\\nworks for semi-supervised learning. In Proceedings of AAAI, 2018.\\n71. Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural\\nnetwork: Data-driven traf\\xef\\xac\\x81c forecasting. In Proceedings of ICLR, 2018.\\n72. Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S Zemel. Gated graph sequence\\nneural networks. In Proceedings of ICLR, 2016.\\n73. Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep\\ngenerative models of graphs. In Proceedings of ICLR, 2018.\\n74. Xiaodan Liang, Xiaohui Shen, Jiashi Feng, Liang Lin, and Shuicheng Yan. Semantic object\\nparsing with graph lstm. In Proceedings of ECCV, 2016.\\n280\\n8\\nNetwork Representation\\n75. Tengfei Ma, Jie Chen, and Cao Xiao. Constrained generation of semantically valid graphs via\\nregularizing variational autoencoders. In Proceedings of NeurIPS, 2018.\\n76. Diego Marcheggiani and Ivan Titov. Encoding sentences with graph convolutional networks\\nfor semantic role labeling. In Proceedings of EMNLP, 2017.\\n77. Kenneth Marino, Ruslan Salakhutdinov, and Abhinav Gupta. The more you know: Using\\nknowledge graphs for image classi\\xef\\xac\\x81cation. In Proceedings of CVPR, 2017.\\n78. Jonathan Masci, Davide Boscaini, Michael Bronstein, and Pierre Vandergheynst. Geodesic\\nconvolutional neural networks on riemannian manifolds. In Proceedings of ICCV workshops,\\n2015.\\n79. Julian J McAuley and Jure Leskovec. Learning to discover social circles in ego networks. In\\nProceedings of NeurIPS, 2012.\\n80. T Mikolov and J Dean. Distributed representations of words and phrases and their composi-\\ntionality. Proceedings of NeurIPS, 2013.\\n81. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\\xef\\xac\\x81cient estimation of word\\nrepresentations in vector space. In Proceedings of ICLR, 2013.\\n82. Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and\\nMichael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model\\ncnns. In Proceedings of CVPR, 2017.\\n83. Medhini Narasimhan, Svetlana Lazebnik, and Alexander Gerhard Schwing. Out of the box:\\nReasoning with graph convolution nets for factual visual question answering. In Proceedings\\nof NeurIPS, 2018.\\n84. Mark EJ Newman. Finding community structure in networks using the eigenvectors of matri-\\nces. Physical Review E, 74(3):036104, 2006.\\n85. Mark EJ Newman. Modularity and community structure in networks. Proceedings of the\\nNational Academy of Sciences, 103(23):8577\\xe2\\x80\\x938582, 2006.\\n86. Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural\\nnetworks for graphs. In Proceedings of ICML, 2016.\\n87. Will Norcliffebrown, Stathis Vafeias, and Sarah Parisot. Learning conditioned graph structures\\nfor interpretable visual question answering. In Proceedings of NeurIPS, 2018.\\n88. Alex Nowak, Soledad Villar, Afonso S Bandeira, and Joan Bruna. Revised note on learning\\nquadratic assignment with graph neural networks. In Proceedings of IEEE DSW 2018, 2018.\\n89. Mingdong Ou, Peng Cui, Jian Pei, and Wenwu Zhu. Asymmetric transitivity preserving graph\\nembedding. In Proceedings of SIGKDD, 2016.\\n90. Rasmus Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In Proceedings\\nof NeurIPS, 2018.\\n91. Hao Peng, Jianxin Li, Yu He, Yaopeng Liu, Mengjiao Bao, Lihong Wang, Yangqiu Song,\\nand Qiang Yang. Large-scale hierarchical text classi\\xef\\xac\\x81cation with recursively regularized deep\\ngraph-cnn. In Proceedings of WWW, 2018.\\n92. Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wentau Yih. Cross-\\nsentence n-ary relation extraction with graph lstms. Transactions of the Association for Com-\\nputational Linguistics, 5(1):101\\xe2\\x80\\x93115, 2017.\\n93. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social rep-\\nresentations. In Proceedings of SIGKDD, 2014.\\n94. TrangPham,TruyenTran,DinhPhung,andSvethaVenkatesh.Columnnetworksforcollective\\nclassi\\xef\\xac\\x81cation. In Proceedings of AAAI, 2017.\\n95. Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen, and Songchun Zhu. Learning\\nhuman-object interactions by graph parsing neural networks. In Proceedings of ECCV, 2018.\\n96. Afshin Rahimi, Trevor Cohn, and Timothy Baldwin. Semi-supervised user geolocation via\\ngraph convolutional networks. In Proceedings of ACL, 2018.\\n97. Sungmin Rhee, Seokjun Seo, and Sun Kim. Hybrid approach of relation network and localized\\ngraph convolutional \\xef\\xac\\x81ltering for breast cancer subtype classi\\xef\\xac\\x81cation. In Proceedings of IJCAI,\\n2018.\\n98. Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear\\nembedding. science, 290(5500):2323\\xe2\\x80\\x932326, 2000.\\nReferences\\n281\\n99. Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Ried-\\nmiller, Raia Hadsell, and Peter Battaglia. Graph networks as learnable physics engines for\\ninference and control. In Proceedings of ICML, 2018.\\n100. Victor Garcia Satorras and Joan Bruna Estrach. Few-shot learning with graph neural networks.\\nIn Proceedings of ICLR, 2018.\\n101. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfar-\\ndini. The graph neural network model. IEEE TNN 2009, 20(1):61\\xe2\\x80\\x9380, 2009.\\n102. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and\\nMax Welling. Modeling relational data with graph convolutional networks. In Proceedings of\\nESWC, 2018.\\n103. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-\\nRad. Collective classi\\xef\\xac\\x81cation in network data. AI magazine, 29(3):93\\xe2\\x80\\x9393, 2008.\\n104. Oleksandr Shchur, Daniel Zugner, Aleksandar Bojchevski, and Stephan Gunnemann. Netgan:\\nGenerating graphs via random walks. In Proceedings of ICML, 2018.\\n105. Chuan Shi, Binbin Hu, Wayne Xin Zhao, and S Yu Philip. Heterogeneous information network\\nembedding for recommendation. IEEE Transactions on Knowledge and Data Engineering,\\n31(2):357\\xe2\\x80\\x93370, 2018.\\n106. Martin Simonovsky and Nikos Komodakis. Dynamic edge-conditioned \\xef\\xac\\x81lters in convolutional\\nneural networks on graphs. In Proceedings of CVPR, 2017.\\n107. Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backprop-\\nagation. In Proceedings of NeurIPS, 2016.\\n108. Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\\n2018.\\n109. Kai Sheng Tai, Richard Socher, and Christopher D. Manning. Improved semantic representa-\\ntions from tree-structured long short-term memory networks. In Proceedings of ACL, 2015.\\n110. Jian Tang, Meng Qu, and Qiaozhu Mei. Pte: Predictive text embedding through large-scale\\nheterogeneous text networks. In Proceedings of SIGKDD, 2015.\\n111. Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-\\nscale information network embedding. In Proceedings of WWW, 2015.\\n112. Lei Tang and Huan Liu. Relational learning via latent social dimensions. In Proceedings of\\nSIGKDD, 2009.\\n113. Lei Tang and Huan Liu. Leveraging social media networks for classi\\xef\\xac\\x81cation. Data Mining\\nand Knowledge Discovery, 23(3):447\\xe2\\x80\\x93478, 2011.\\n114. Damien Teney, Lingqiao Liu, and Anton Van Den Hengel. Graph-structured representations\\nfor visual question answering. In Proceedings of CVPR, 2017.\\n115. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a\\nlarge-scale dataset for fact extraction and veri\\xef\\xac\\x81cation. In Proceedings of NAACL-HLT, 2018.\\n116. Cunchao Tu, Hao Wang, Xiangkai Zeng, Zhiyuan Liu, and Maosong Sun. Community-\\nenhanced\\nnetwork\\nrepresentation\\nlearning\\nfor\\nnetwork\\nanalysis.\\narXiv\\npreprint\\narXiv:1611.06645, 2016.\\n117. Cunchao Tu, Xiangkai Zeng, Hao Wang, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun,\\nBo Zhang, and Leyu Lin. A uni\\xef\\xac\\x81ed framework for community detection and network rep-\\nresentation learning. IEEE Transactions on Knowledge and Data Engineering (TKDE),\\n31(6):1051\\xe2\\x80\\x931065, 2018.\\n118. Cunchao Tu, Weicheng Zhang, Zhiyuan Liu, and Maosong Sun. Max-margin deepwalk: Dis-\\ncriminative learning of network representation. In Proceedings of IJCAI, 2016.\\n119. Cunchao Tu, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Transnet: translation-based\\nnetwork representation learning for social relation extraction. In Proceedings of IJCAI, 2017.\\n120. Rianne van den Berg, Thomas N Kipf, and Max Welling. Graph convolutional matrix com-\\npletion. arXiv preprint arXiv:1706.02263, 2017.\\n121. Ashish Vaswani, Noam Shazeer, Niki Parmar, Llion Jones, Jakob Uszkoreit, Aidan N Gomez,\\nand Lukasz Kaiser. Attention is all you need. In Proceedings of NeurIPS, 2017.\\n122. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and\\nYoshua Bengio. Graph attention networks. In Proceedings of ICLR, 2018.\\n282\\n8\\nNetwork Representation\\n123. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Proceedings of\\nNeurIPS, 2015.\\n124. Jacco Wallinga and Peter Teunis. Different epidemic curves for severe acute respiratory syn-\\ndrome reveal similar impacts of control measures. American Journal of epidemiology, 2004.\\n125. Daixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings\\nof SIGKDD, 2016.\\n126. Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph embedding: A survey\\nof approaches and applications. TKDE, 29(12):2724\\xe2\\x80\\x932743, 2017.\\n127. Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, and Shiqiang Yang. Community\\npreserving network embedding. In Proceedings of AAAI, 2017.\\n128. Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Het-\\nerogeneous graph attention network. In Proceedings of WWW, 2019.\\n129. Xiaolong Wang, Yufei Ye, and Abhinav Gupta. Zero-shot recognition via semantic embed-\\ndings and knowledge graphs. In Proceedings of CVPR, 2018.\\n130. Zhichun Wang, Qingsong Lv, Xiaohan Lan, and Yu Zhang. Cross-lingual knowledge graph\\nalignment via graph convolutional networks. In Proceedings of EMNLP, 2018.\\n131. Zhouxia Wang, Tianshui Chen, Jimmy S J Ren, Weihao Yu, Hui Cheng, and Liang Lin. Deep\\nreasoning with knowledge graph for social relationship understanding. In Proceedings of\\nIJCAI, 2018.\\n132. Nicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu, and\\nAndrea Tacchetti. Visual interaction networks: Learning a physics simulator from video. In\\nProceedings of NeurIPS, 2017.\\n133. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu.\\nA comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019.\\n134. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Kenichi Kawarabayashi, and\\nStefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In\\nProceedings of ICML, 2018.\\n135. Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for\\nskeleton-based action recognition. In Proceedings of AAAI, 2018.\\n136. Cheng Yang, Zhiyuan Liu, Deli Zhao, Maosong Sun, and Edward Y Chang. Network repre-\\nsentation learning with rich text information. In Proceedings of IJCAI, 2015.\\n137. Cheng Yang, Maosong Sun, Zhiyuan Liu, and Cunchao Tu. Fast network embedding enhance-\\nment via high order proximity approximation. In Proceedings of IJCAI, 2017.\\n138. Cheng Yang, Maosong Sun, Wayne Xin Zhao, Zhiyuan Liu, and Edward Y Chang. A neural\\nnetwork approach to jointly modeling social networks and mobile trajectories. ACM Trans-\\nactions on Information Systems (TOIS), 35(4):36, 2017.\\n139. Cheng Yang, Jian Tang, Maosong Sun, Ganqu Cui, and Liu Zhiyuan. Multi-scale information\\ndiffusion prediction with reinforced recurrent networks. In Proceedings of IJCAI, 2019.\\n140. Jaewon Yang and Jure Leskovec. Overlapping community detection at scale: a nonnegative\\nmatrix factorization approach. In Proceedings of WSDM, 2013.\\n141. Jaewon Yang, Julian McAuley, and Jure Leskovec. Detecting cohesive and 2-mode commu-\\nnities indirected and undirected networks. In Proceedings of WSDM, 2014.\\n142. Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classi\\xef\\xac\\x81-\\ncation. In Proceedings of AAAI, 2019.\\n143. Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure\\nLeskovec. Graph convolutional neural networks for web-scale recommender systems. In Pro-\\nceedings of SIGKDD, 2018.\\n144. Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec.\\nHierarchical graph representation learning with differentiable pooling. In Proceedings of\\nNeurIPS, 2018.\\n145. Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay S Pande, and Jure Leskovec. Graph convolutional\\npolicy network for goal-directed molecular graph generation. In Proceedings of NeurIPS,\\n2018.\\nReferences\\n283\\n146. Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. Graphrnn: Gen-\\nerating realistic graphs with deep auto-regressive models. In Proceedings of ICML, 2018.\\n147. Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks: A\\ndeep learning framework for traf\\xef\\xac\\x81c forecasting. In Proceedings of ICLR, 2018.\\n148. Victoria Zayats and Mari Ostendorf. Conversation modeling on reddit using a graph-structured\\nlstm. Transactions of the Association for Computational Linguistics, 6:121\\xe2\\x80\\x93132, 2018.\\n149. Daokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang. Network representation learning:\\nA survey. IEEE transactions on Big Data, 2018.\\n150. Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit Yan Yeung. Gaan:\\nGated attention networks for learning on large and spatiotemporal graphs. In Proceedings of\\nUAI, 2018.\\n151. Yizhou Zhang, Yun Xiong, Xiangnan Kong, Shanshan Li, Jinhong Mi, and Yangyong Zhu.\\nDeep collective classi\\xef\\xac\\x81cation in heterogeneous information networks. In Proceedings of\\nWWW, 2018.\\n152. Yue Zhang, Qi Liu, and Linfeng Song. Sentence-state lstm for text representation. In Pro-\\nceedings of ACL, 2018.\\n153. Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Maosong Sun, Zhichong Fang, Bo Zhang, and\\nLeyu Lin. Cosine: Compressive network embedding on large-scale information networks.\\narXiv preprint arXiv:1812.08972, 2018.\\n154. Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs: A survey. arXiv preprint\\narXiv:1812.04202, 2018.\\n155. Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng\\nLi, and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv\\npreprint arXiv:1812.08434, 2018.\\n156. JieZhou,XuHan,ChengYang,ZhiyuanLiu,LifengWang,ChangchengLi,andMaosongSun.\\nGEAR: Graph-based evidence aggregating and reasoning for fact veri\\xef\\xac\\x81cation. In Proceedings\\nof ACL 2019, 2019.\\n157. Zhaocheng Zhu, Shizhen Xu, Jian Tang, and Meng Qu. Graphvite: A high-performance cpu-\\ngpu hybrid system for node embedding. In Proceedings of WWW, 2019.\\n158. Chenyi Zhuang and Qiang Ma. Dual graph convolutional networks for graph-based semi-\\nsupervised classi\\xef\\xac\\x81cation. In Proceedings of WWW, 2018.\\n159. Julian G Zilly, Rupesh Kumar Srivastava, Jan Koutnik, and Jurgen Schmidhuber. Recurrent\\nhighway networks. In Proceedings of ICML, 2016.\\n160. Marinka Zitnik, Monica Agrawal, and Jure Leskovec. Modeling polypharmacy side effects\\nwith graph convolutional networks. Intelligent Systems in Molecular Biology, 34(13):258814,\\n2018.\\n161. Zhou Jie, Cui Ganqu, Zhang Zhengyan, Yang Cheng, Liu Zhiyuan, Wang Lifeng, Li\\nChangcheng, and Sun Maosong. Graph neural networks: A review of methods and appli-\\ncations. arXiv preprint arXiv:1812.08434, 2018.\\n162. Liu Zhiyuan and Zhou Jie. Introduction to graph neural networks. Morgan & Claypool Pub-\\nlishers, 2020.\\n284\\n8\\nNetwork Representation\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 9\\nCross-Modal Representation\\nAbstract Cross-modal representation learning is an essential part of representation\\nlearning, which aims to learn latent semantic representations for modalities including\\ntexts,audio,images,videos,etc.Inthischapter,we\\xef\\xac\\x81rstintroducetypicalcross-modal\\nrepresentation models. After that, we review several real-world applications related\\nto cross-modal representation learning including image captioning, visual relation\\ndetection, and visual question answering.\\n9.1\\nIntroduction\\nAs introduced in Wikipedia, a modality is the classi\\xef\\xac\\x81cation of a single independent\\nchannel of sensory input/output between a computer and a human. To be more\\ngeneral, modalities are different means of information exchange between human\\nbeings and the real world. The classi\\xef\\xac\\x81cation is usually based on the form in which\\ninformation is presented to a human. Typical modalities in the real world include\\ntexts, audio, images, videos, etc.\\nCross-modal representation learning is an important part of representation learn-\\ning. In fact, arti\\xef\\xac\\x81cial intelligence is inherently a multi-modal task [30]. Human beings\\nare exposed to multi-modal information every day, and it is normal for us to integrate\\ninformation from different modalities and make comprehensive judgments. Further-\\nmore, different modalities are not independent, but they have correlations more or\\nless. For example, the judgment of a syllable is made by not only the sound we hear\\nbut also the movement of the lips and tongue of the speaker we see. An experiment\\nin [48] shows that a voiced /ba/ with a visual /ga/ is perceived by most people as\\na /da/. Another example is human beings\\xe2\\x80\\x99 ability to consider the 2D image and 3D\\nscan of the same object together and reconstruct its structure: correlations between\\nimage and scan can be found based on the fact that a discontinuity of depth in the\\nscan usually indicates a sharp line in the image [52]. Inspired by this, it is natural\\nfor us to consider the possibility of combining inputs from multi-modalities in our\\narti\\xef\\xac\\x81cial intelligence systems and generate cross-modal representation.\\nNgiam et al. [52] explore the probability of merging multi-modalities into one\\nlearning task. The authors divide a typical machine learning task into three stages:\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_9\\n285\\n286\\n9\\nCross-Modal Representation\\nfeature learning, supervised learning, and prediction. And they further propose four\\nkinds of learning settings for multi-modalities:\\n(1) Single-modal learning: all stages are all done on just one modality.\\n(2) Multi-modal fusion: all stages are all done with all modalities available.\\n(3) Cross-modal learning: in the feature learning stage, all modalities are avail-\\nable, but in supervised learning and prediction, only one modality is used.\\n(4) Shared representation learning: in the feature learning stage, all modalities\\nare available. In supervised learning, only one modality is used, and in prediction, a\\ndifferent modality is used.\\nExperiments show promising results for these multi-modal tasks. When more\\nmodalities are provided (such as multi-modal fusion, cross-modal learning, and\\nshared representation learning), the performance of the system is generally better.\\nIn the following part of this chapter, we will \\xef\\xac\\x81rst introduce cross-modal represen-\\ntation models, which are fundamental parts of cross-modal representation learning\\nin NLP. And then, we will introduce several critical applications, such as image\\ncaptioning, visual relationship detection, and visual question answering.\\n9.2\\nCross-Modal Representation\\nCross-modal representation learning aims to build embeddings using information\\nfrom multiple modalities. Existing cross-modal representation models involving text\\nmodality can be generally divided into two categories: (1) [30, 77] try to fuse infor-\\nmation from different modalities into uni\\xef\\xac\\x81ed embeddings (e.g., visually grounded\\nword representations). (2) Researchers also try to build embeddings for different\\nmodalities in a common semantic space, which allows the model to compute cross-\\nmodal similarity. Such cross-modal similarity can be further utilized for downstream\\ntasks, such as zero-shot recognition [5, 14, 18, 53, 65] and cross-media retrieval [23,\\n55]. In this section, we will introduce these two kinds of cross-modal representation\\nmodels, respectively.\\n9.2.1\\nVisual Word2vec\\nComputing word embeddings is a fundamental task in representation learning for\\nnatural language processing. Typical word embedding models (like Word2vec [49])\\nare trained on a text corpus. These models, while being extremely successful, cannot\\ndiscoverimplicitsemanticrelatednessbetweenwordsthatcouldbeexpressedinother\\nmodalities. Kottur et al. [30] provide an example: even though eat and stare at\\nseem are unrelated from text, images might show that when people are eating\\nsomething they would also tend to stare at it. This implies that considering\\nother modalities when constructing word embeddings may help capture more implicit\\nsemantic relatedness.\\n9.2 Cross-Modal Representation\\n287\\nTwo\\nducks\\nin\\nwater\\nswim\\nwalk\\nsit\\nCNN\\nFig. 9.1 The architecture for word embedding with global visual context\\nVision, being one of the most critical modalities, has attracted attention from\\nresearchers seeking to improve word representation. Several models that incorporate\\nvisual information and improve word embeddings with vision have been proposed.\\nWe introduce two typical word representation models incorporating visual informa-\\ntion in the following.\\n9.2.1.1\\nWord Embedding with Global Visual Context\\nXu et al. [77] propose a model that makes a natural attempt to incorporate visual\\nfeatures. It claims that in most word representation models, only local context infor-\\nmation (e.g., trying to predict a word using neighboring words and phrases) is con-\\nsidered. Global text information (e.g., the topic of the passage), on the other hand,\\nis often neglected. This model extends a simple local context model by using visual\\ninformation as global features (see Fig.9.1).\\nThe input of the model is an image I and a sequence describing it. It is based\\non a simple local context language model: when we consider a certain word wt in\\na sequence, its local feature is the average of embeddings of words in a window,\\ni.e., {wt\\xe2\\x88\\x92k, . . . , wt\\xe2\\x88\\x921, wt+1, . . . , wt+k}. The visual feature is computed directly from\\nthe image I using a CNN and then used as the global feature. The local feature and\\nthe global feature are then concatenated into a vector f. The predicted probability\\nof a word wt (in this blank) is the softmax normalized product of f and the word\\nembedding wt:\\nowt = wT\\nt f,\\n(9.1)\\nP(wt|wt\\xe2\\x88\\x92k, . . . , wt\\xe2\\x88\\x921,wt+1, . . . , wt+k; I) =\\nexp(owt)\\n\\x02\\ni exp(owi).\\n(9.2)\\nThe model is optimized by maximizing the average of log probability:\\n288\\n9\\nCross-Modal Representation\\nL = 1\\nT\\nT\\xe2\\x88\\x92k\\n\\x03\\nt=k\\nlog P(wt|wt\\xe2\\x88\\x92k, . . . , wt\\xe2\\x88\\x921, wt+1, . . . , wt+k; I).\\n(9.3)\\nThe classi\\xef\\xac\\x81cation error will be back-propagated to local text vector (i.e., word\\nembeddings), visual vector, and all model parameters. This accomplishes jointly\\nlearning for a set of word embeddings, a language model, and the model used for\\nvisual encoding.\\n9.2.1.2\\nWord Embedding with Abstract Visual Scene\\nKottur et al. [30] also propose a neural model to capture \\xef\\xac\\x81ne-grained semantics from\\nvisual information. Instead of focusing on literal pixels, the abstract scene behind\\nthe vision is considered. The model takes a pair of the visual scene and a related\\nword sequence (I, w) as input. At each training step, a window is used upon the\\nword sequence w, forming a subsequence Sw. All the words in Sw will be fed into the\\ninput layer using one-hot encoding, and therefore the dimension of the input layer\\nis |V |, which is also the size of the vocabulary. The words are then transformed into\\ntheir embeddings, and the hidden layer is the average of all these embeddings. The\\nsize of the hidden layer is NH, which is also the dimension of the word embeddings.\\nThe hidden layer and the output layer are connected by a full connection matrix of\\ndimension NH \\xe2\\x88\\x97NK and a softmax function. The output layer can be regarded as\\na probability distribution over a discrete-valued function g(\\xc2\\xb7) of the visual scene I\\n(details will be given in the following paragraph). The entire model is optimized by\\nminimizing the objective function:\\nL = \\xe2\\x88\\x92log P(g(w)|Sw).\\n(9.4)\\nThe most important part of the model is the function g(\\xc2\\xb7). It maps the visual scene\\nI into the set {1, 2, . . . , NK}, which indicates what kind of abstract scene it is. In\\npractice, it is learned of\\xef\\xac\\x82ine using K-means clustering, and each cluster represents\\nthe semantics of one kind of visual scenes, consequently, the word sequence w, which\\nis designed to be related to the scene.\\n9.2.2\\nCross-Modal Representation for Zero-Shot Recognition\\nLarge-scale datasets partially support the success of deep learning methods. Even\\nthough the scales of datasets continue to grow larger, and more categories are\\ninvolved, the annotation of datasets is expensive and time-consuming. For many\\ncategories, there are very limited or even no instances, which restricts the scalability\\nof recognition systems.\\n9.2 Cross-Modal Representation\\n289\\nZero-shot recognition is proposed to solve the problem as mentioned above, which\\naims to classify instances of categories that have not been seen during training. Many\\nworksproposetoutilizecross-modalrepresentationforzero-shotimageclassi\\xef\\xac\\x81cation\\n[5, 14, 18, 53, 65]. Speci\\xef\\xac\\x81cally, image representation and category representation\\nare embedded into a common semantic space, where similarities between image and\\ncategory representations can serve for further classi\\xef\\xac\\x81cation. For example, in such a\\ncommon semantic space, the embedding of an image of cat is expected to be closer\\nto the embedding of category cat than the embedding of category truck.\\n9.2.2.1\\nDeep Visual-Semantic Embedding\\nThe challenge of zero-shot learning lies in the absence of instances of unseen cat-\\negories, which makes it challenging to obtain well-performed classi\\xef\\xac\\x81ers of unseen\\ncategories. Frome et al. [18] present a model that utilizes both labeled images and\\ninformation from the large-scale plain text for zero-shot image classi\\xef\\xac\\x81cation. They\\ntry to leverage semantic information from word embeddings and transfer it to image\\nclassi\\xef\\xac\\x81cation systems.\\nTheir model is motivated by the fact that word embeddings incorporate semantic\\ninformation of concepts or categories, which can be potentially utilized as classi-\\n\\xef\\xac\\x81ers of corresponding categories. Similar categories cluster well in semantic space.\\nFor example, in word embedding space, the nearest neighbors of the term tiger\\nshark are similar kinds of sharks, such as bull shark, blacktip shark,\\nsandbar shark, and oceanic whitetip shark. In addition, bound-\\naries between different clusters are clear. The aforementioned properties indicate\\nthat word embeddings can be further utilized as classi\\xef\\xac\\x81ers for recognition systems.\\nSpeci\\xef\\xac\\x81cally, the model \\xef\\xac\\x81rst pretrains word embeddings using the Skip-gram text\\nmodel on large-scale Wikipedia articles. For visual feature extraction, the model pre-\\ntrains a deep convolutional neural network for 1, 000 object categories on ImageNet.\\nThe pretrained word embeddings and the convolutional neural network are used to\\ninitialize the proposed Deep Visual-Semantic Embedding model (DeViSE).\\nTo train the proposed model, they replace the softmax layer of the pretrained\\nconvolutional neural network with a linear projection layer. The model is trained to\\npredict the word embeddings of categories for images using a hinge ranking loss:\\nL (I, y) =\\n\\x03\\nj\\xcc\\xb8=y\\nmax[0, \\xce\\xb3 \\xe2\\x88\\x92wyMI + w jMI],\\n(9.5)\\nwhere wy and w j are the learned word embeddings of the positive label and sampled\\nnegative label, respectively, I denotes the feature of the image obtained from the\\nconvolutional neural network, M is the trainable parameters in linear projection\\nlayer, and \\xce\\xb3 is a hyperparameter in hinge ranking loss. Given an image, the objective\\nrequires the model to produce a higher score for the correct label than randomly\\nchosen labels, where the score is de\\xef\\xac\\x81ned as the dot product of the projected image\\nfeature and word embedding of terms.\\n290\\n9\\nCross-Modal Representation\\nAt test time, given a test image, the score of each possible category is obtained\\nusing the same approach during training. Note that a crucial difference at test time is\\nthat the classi\\xef\\xac\\x81ers (word embeddings) are expanded to all possible categories, includ-\\ning unseen categories. Thus the model is capable of predicting unseen categories.\\nExperiment results show that DeViSE can make zero-shot predictions with more\\nsemantically reasonable errors, which means that even if the prediction is not exactly\\ncorrect, it is semantically related to the ground truth class. However, a drawback is\\nthat although the model can utilize semantic information in word embeddings to\\nmake zero-shot image classi\\xef\\xac\\x81cation, using word embeddings as classi\\xef\\xac\\x81ers restricts\\nthe \\xef\\xac\\x82exibility of the model, which results in inferior performance in the original\\n1, 000 categories compared to the original softmax classi\\xef\\xac\\x81er.\\n9.2.2.2\\nConvex Combination of Semantic Embeddings\\nInspired by DeViSE, [53] proposes a model ConSE that tries to utilize semantic\\ninformation from word embeddings for zero-shot classi\\xef\\xac\\x81cation. A vital difference\\nto DeViSE is that they obtain the semantic embedding of test image using a convex\\ncombination of word embeddings of seen categories. The score of the corresponding\\ncategory determines the weights of the composing word embeddings.\\nSpeci\\xef\\xac\\x81cally, they train a deep convolutional neural network on seen categories.\\nAt test time, given a test image I (possibly from unseen categories), they obtain\\nthe top T con\\xef\\xac\\x81dent predictions of seen categories, where T is a hyperparameter.\\nThen the semantic embedding f (I) of I is determined by the convex combination\\nof semantic embeddings of the top T con\\xef\\xac\\x81dent categories, which can be formally\\nde\\xef\\xac\\x81ned as follows:\\nf (I) = 1\\nZ\\nT\\n\\x03\\nt=1\\nP( \\xcb\\x86y0(I, t)|I) \\xc2\\xb7 w( \\xcb\\x86y0(I, t)),\\n(9.6)\\nwhere \\xcb\\x86y0(I, t) is the tth most con\\xef\\xac\\x81dent training label for I, w( \\xcb\\x86y0(I, t)) is the semantic\\nembedding (word embedding) of \\xcb\\x86y0(I, t), and Z is a normalization factor given by\\nZ =\\nT\\n\\x03\\nt=1\\nP( \\xcb\\x86y0(I, t)|I).\\n(9.7)\\nAfter obtaining the semantic embedding f (I), the score of the category m is given\\nby the cosine similarity of f (I) and w(m).\\nThe motivation of ConSE is that they assume novel categories can be mod-\\neled as the convex combination of seen categories. If the model is highly con\\xef\\xac\\x81-\\ndent about a prediction, (i.e., P( \\xcb\\x86y0(I, 1)|I) \\xe2\\x89\\x881), the semantic embedding f (I) will\\nbe close to w( \\xcb\\x86y0(I, 1)). If the predictions are ambiguous, (e.g., P(tiger|I) =\\n0.5, P(lion|I) = 0.5), the semantic embedding f (I) will be between w(lion)\\nand w(tiger). And they expect the semantic embedding f (I) = 0.5w(lion) +\\n9.2 Cross-Modal Representation\\n291\\n0.5w(tiger) to be close to the semantic embedding w(liger) (a hybrid cross\\nbetween lions and tigers).\\nAlthough ConSE and DeViSE share many similarities, there are also some crucial\\ndifferences. DeViSE replaces the softmax layer of the pretrained visual model with a\\nprojection layer, while ConSE preserves the softmax layer. ConSE does not need to\\nbe further trained and uses a convex combination of semantic embeddings to perform\\nzero-shot classi\\xef\\xac\\x81cation at test time. Experiment results show that ConSE outperforms\\nDeViSE on unseen categories, indicating better generalization capability. However,\\nthe performance of ConSE on seen categories is not as competitive as DeViSE and\\nthe original softmax classi\\xef\\xac\\x81er.\\n9.2.2.3\\nCross-Modal Transfer\\nSocher et al. [65] present a cross-modal representation model for zero-shot recogni-\\ntion. In their model, all word vectors are initialized with pretrained 50-dimensional\\nword vectors and are kept \\xef\\xac\\x81xed during training. Each image is represented by a vector\\nI constructed by a deep convolutional neural network. They \\xef\\xac\\x81rst project an image\\ninto semantic word spaces by minimizing\\nL (\\xce\\x98) =\\n\\x03\\ny\\xe2\\x88\\x88Ys\\n\\x03\\nI (i)\\xe2\\x88\\x88X y\\n\\xe2\\x88\\xa5wy \\xe2\\x88\\x92\\xce\\xb8(2) f (\\xce\\xb8(1)I(i))\\xe2\\x88\\xa52,\\n(9.8)\\nwhere Ys denotes the set of images\\xe2\\x80\\x99 classes which can be seen in training data,\\nX y denotes the set of images\\xe2\\x80\\x99 vectors of class y, wy denotes the word vector of\\nclass y, and \\xce\\x98 = (\\xce\\xb8(1), \\xce\\xb8(2)) denotes parameters of the 2-layer neural network with\\nf (\\xc2\\xb7) = tanh(\\xc2\\xb7) as activation function.\\nThey observe that instances from unseen categories are usually outliers of the\\ncomplete data manifold. Following this observation, they \\xef\\xac\\x81rst classify an instance\\ninto seen and unseen categories via outlier detection methods. Then the instance is\\nclassi\\xef\\xac\\x81ed using corresponding classi\\xef\\xac\\x81ers.\\nFormally, they marginalize a binary random variable V \\xe2\\x88\\x88{s, u} which denotes\\nwhether an instance belongs to seen categories or unseen categories separately, which\\nmeans probability is given as\\nP(y|I) =\\n\\x03\\nV \\xe2\\x88\\x88{s,u}\\nP(y|V, I)P(V |I).\\n(9.9)\\nFor seen image classes, they simply use softmax classi\\xef\\xac\\x81er to determine P(y|s, I),\\nwhile for unseen classes, they assume an isometric Gaussian distribution around each\\nof the novel class word vectors and assign classes based on their likelihood. To detect\\nnovelty, they calculate a Local Outlier Probability by Gaussian error function.\\n292\\n9\\nCross-Modal Representation\\n9.2.3\\nCross-Modal Representation for Cross-Media Retrieval\\nLearningcross-modalrepresentationfromdifferentmodalitiesinacommonsemantic\\nspace allows one to easily compute cross-modal similarities, which can facilitate\\nmany important cross-modal tasks, such as cross-media retrieval. With the rapid\\ngrowth of multimedia data such as text, image, video, and audio on the Internet, the\\nneed to retrieve information across different modalities has become stronger. Cross-\\nmedia retrieval is an important task in the multimedia area, which aims to perform\\nretrieval across different modalities such as text and image. For example, a user may\\nsubmit an image of a white horse, and retrieve relevant information from different\\nmodalities, such as textual descriptions of horses, and vice versa.\\nA signi\\xef\\xac\\x81cant challenge of cross-modal retrieval is the domain discrepancies\\nbetween different modalities. Besides, for a speci\\xef\\xac\\x81c area of interest, cross-modal data\\ncan be insuf\\xef\\xac\\x81cient, which limits the performance of existing cross-modal retrieval\\nmethods. Many works have focused on the challenges as mentioned above in cross-\\nmodal retrieval [23, 24].\\n9.2.3.1\\nCross-Modal Hybrid Transfer Network\\nHuang et al. [24] present a framework that tries to relieve the cross-modal data\\nsparsity problem by transfer learning. They propose to leverage knowledge from\\na large-scale single-modal dataset to boost the model training on the small-scale\\ndataset. The massive auxiliary dataset is denoted as the source domain, and the\\nsmall-scale dataset of interest is denoted as the target domain. In their work, they\\nadopt ImageNet [12], a large-scale image database as the source domain.\\nFormally, a training set consists of data from source domain Src = {I p\\ns , y p\\ns }P\\np=1\\nand target domain Tartr = {(I j\\ns , t j\\ns ), y j\\ns }J\\nj=1, where (I, t) is the image/text pair with\\nlabel y. Similarly, a test set can be denoted as Tarte = {(I m\\ns , tm\\ns ), ym\\ns }M\\nm=1. The goal\\nof their model is to transfer knowledge from Src to boost the model performance on\\nTarte for cross-media retrieval.\\nTheir model consists of a modal-sharing transfer subnetwork and a layer-sharing\\ncorrelation subnetwork. In modal-sharing transfer subnetwork, they adopt the con-\\nvolutional layers of AlexNet [32] to extract image features for source and target\\ndomains, and use word vectors to obtain text features. The image and text features\\npass through two fully connected layers, where single-modal and cross-modal knowl-\\nedge transfer are performed.\\nSingle-modal knowledge transfer aims to transfer knowledge from images in the\\nsource domain to images in the target domain. The main challenge is the domain\\ndiscrepancy between the two image datasets. They propose to solve the domain\\ndiscrepancy problem by minimizing the Maximum Mean Discrepancy (MMD) of\\nimage modality between the source and target domains. MMD is calculated in a\\nlayer-wise style in the fully connected layers. By minimizing MMD in reproduced\\nkernel Hilbert space, the image representations from source and target domains are\\n9.2 Cross-Modal Representation\\n293\\nencouraged to have the same distribution, so knowledge from images in the source\\ndomain is expected to transfer to images in the target domain. Besides, the image\\nencoder in the source domain is also \\xef\\xac\\x81ne-tuned by optimizing softmax loss on labeled\\nimage instances.\\nCross-modal knowledge transfer aims to transfer knowledge between image and\\ntext in the target domain. Text and image representations from an annotated pair\\nin the target domain are encouraged to be close to each other by minimizing their\\nEuclidean distance. The cross-modal transfer loss of image and text representations\\nis also computed in a layer-wise style in the fully connected layers. The domain\\ndiscrepancy between image and text modalities is expected to be reduced in high-\\nlevel layers.\\nIn layer-sharing correlation subnetwork, representations from modal-sharing\\ntransfer subnetwork in the target domain are fed into shared fully connected layers\\nto obtain the \\xef\\xac\\x81nal common representation for both image and text. As the parameters\\nare shared between two modalities, the last two fully connected layers are expected\\nto capture the cross-modal correlation. Their model also utilizes label information\\nin the target domain by minimizing softmax loss on labeled image/text pairs. After\\nobtaining the \\xef\\xac\\x81nal common representations, cross-media retrieval can be achieved\\nby simply computing the nearest neighbors in semantic space.\\n9.2.3.2\\nDeep Cross-Media Knowledge Transfer\\nAs an extension of [23, 24] also focuses on dealing with domain discrepancy and\\ninsuf\\xef\\xac\\x81cient cross-modal data for cross-media retrieval in speci\\xef\\xac\\x81c areas, Huang and\\nPeng [23] present a framework that transfers knowledge from a large-scale cross-\\nmediadataset(sourcedomain)toboostthemodelperformanceonanothersmall-scale\\ncross-media dataset (target domain).\\nA crucial difference from [24] is that the dataset in the source domain also consists\\nof image/text pairs with label annotations instead of a single-modal setting in [24].\\nSince both domains contain image and text media types, domain discrepancy comes\\nfrom the media-level discrepancy in the same media type, and correlation-level dis-\\ncrepancy in image/text correlation patterns between different domains. They propose\\nto transfer intra-media semantic and inter-media correlation knowledge by jointly\\nreducing domain discrepancies on media-level and correlation-level.\\nToextractthedistributedfeaturesfordifferentmediatypes,theyadoptVGG19[63]\\nfor image encoder and Word CNN [29] for text encoder. The two domains have the\\nsame architecture but do not share parameters. The extracted image/text features\\npass through two fully connected layers, respectively, where the media-level transfer\\nis performed. Similar to [24], they reduce domain discrepancies within the same\\nmodalities by minimizing Maximum Mean Discrepancy (MMD) between the source\\nand target domains. The MMD is computed in a layer-wise style to transfer knowl-\\nedge within the same modalities. They also minimize Euclidean distance between\\nimage/text representations pairs in both source and target domains to preserve the\\nsemantic information across modalities.\\n294\\n9\\nCross-Modal Representation\\nCorrelation-level transfer aims to reduce domain discrepancy in image/text corre-\\nlation patterns in different domains. In two domains, both image and text representa-\\ntions share the last two fully connected layers to obtain the common representation\\nfor each domain. They optimize layer-wise MMD loss between the shared fully con-\\nnected layers in different domains for correlation-level knowledge transfer, which\\nencourages source and target domains to have the same image/text correlation pat-\\nterns. Finally, both domains are trained with label information of image/text pairs.\\nNote that the source domain and target domain do not necessarily share the same\\nlabel set.\\nIn addition, they propose a progressive transfer mechanism, which is a curricu-\\nlum learning method aiming to promote the robustness of the model training. This is\\nachieved by selecting easy samples for model training in the early period, and grad-\\nually increases the dif\\xef\\xac\\x81culty during the training. The dif\\xef\\xac\\x81culty of training samples\\nis measured according to the bidirectional cross-media retrieval consistency.\\n9.3\\nImage Captioning\\nImage captioning is the task of automatically generating natural language descrip-\\ntions for images. It is a fundamental task in arti\\xef\\xac\\x81cial intelligence, which connects\\nnatural language processing and computer vision. Compared with other computer\\nvision tasks, such as image classi\\xef\\xac\\x81cation and object detection, image captioning\\nis signi\\xef\\xac\\x81cantly harder for two reasons: \\xef\\xac\\x81rst, not only objects but also relationships\\nbetween them have to be detected; second, besides basic judgments and classi\\xef\\xac\\x81cation,\\nnatural language sentences have to be generated.\\nTraditional methods for image captioning are usually using retrieval models or\\ngeneration models, of which the ability to generalize is comparatively weaker com-\\npared with that of novel deep neural network models. In this section, we will introduce\\nseveral typical models of both genres in the following.\\n9.3.1\\nRetrieval Models for Image Captioning\\nThe primary pipeline of retrieval models is (1) represent images and/or sentences\\nusing special features; (2) for new images and/or sentences, search for probable\\ncandidates according to the similarity of features.\\nLinking words to images has a rich history, and [50] (a retrieval model) is the \\xef\\xac\\x81rst\\nimage annotation system. This paper tries to build a keyword assigning system for\\nimages from labeled data. The pipeline is as follows:\\n(1) Image segmentation. Every image is divided into several parts, using the\\nsimplest rectangular division. The reason for doing so is that an image is typically\\nannotated with multiple labels, each of which often corresponds to only a part of it.\\nSegmentation would help reduce noises in labeling.\\n9.3 Image Captioning\\n295\\n(2) Feature extraction. Features of every part of the image are extracted.\\n(3) Clustering. Feature vectors of image segments are divided into several clusters.\\nEach cluster accumulates word frequencies and thereby calculates word likelihood.\\nConcretely,\\nP(wi|c j) =\\nP(c j|wi)P(wi)\\n\\x02\\nk P(c j|wk)P(wk) = n ji\\nN j\\n,\\n(9.10)\\nwhere n ji is the number of times word wi appears in cluster j, and N j is the number of\\ntimes that all words appear in cluster j. The calculation is based on using frequencies\\nas probabilities.\\n(4)Inference.Foranewimage,themodeldividesitintosegments,extractsfeatures\\nfor every part, and \\xef\\xac\\x81nally, aggregates keywords assigned to every part to obtain the\\n\\xef\\xac\\x81nal prediction.\\nThe key idea of this model is image segmentation. Take a landscape picture, for\\ninstance, there are two parts: mountain and sky, and both parts will be annotated\\nwith both labels. However, if another picture has two parts mountain and river,\\nthe two mountain parts would hopefully be in the same cluster and discover that\\nthey share the same label mountain. In this way, labels can be assigned to the\\ncorrect part of the image, and noises could be alleviated.\\nAnother typical retrieval model is proposed by [17], which can assign a linking\\nscore between an image and a sentence. An intermediate space of meaning calculates\\nthis score of linking. The representation of the meaning space is a triple in the form of\\n\\xe2\\x9f\\xa8object, action, scene \\xe2\\x9f\\xa9. Each slot of the triple has a \\xef\\xac\\x81nite discrete candidate set. The\\nproblem of mapping images and sentences into the meaning space involves solving\\na Markov random \\xef\\xac\\x81eld.\\nDifferent from the previous model, this system can do not only image caption,\\nbut also do the inverse, that is, given a sentence, the model provides certain probable\\nassociated images. At the inference stage, the image (sentence) is \\xef\\xac\\x81rst mapped to the\\nintermediate meaning space, then we search in the pool for the sentence (image) that\\nhas the best matching score.\\nAfter that, researchers also proposed a lot of retrieval models which consider\\ndifferent kinds of characteristics of the images, such as [21, 28, 34].\\n9.3.2\\nGeneration Models for Image Captioning\\nDifferent from the retrieval-based model, the basic pipeline of generation models is\\n(1) use computer vision techniques to extract image features, (2) generate sentences\\nfrom these features using methods such as language models or sentence templates.\\nKulkarni et al. [33] propose a system that makes a tight connection between the\\nparticular image and the sentence generating process. The model uses visual detectors\\nto detect speci\\xef\\xac\\x81c objects, as well as attributes of a single object and relationships\\nbetween multiple objects. Then it constructs a conditional random \\xef\\xac\\x81eld to incorporate\\nunary image potentials and higher order text potentials and thereby predicts labels\\n296\\n9\\nCross-Modal Representation\\nfor the image. Labels predicted by conditional random \\xef\\xac\\x81elds (CRF) is arranged as a\\ntriple, e.g., \\xe2\\x9f\\xa8\\xe2\\x9f\\xa8white, cloud\\xe2\\x9f\\xa9, in, \\xe2\\x9f\\xa8blue, sky\\xe2\\x9f\\xa9\\xe2\\x9f\\xa9.\\nThen sentences are generated according to the labels. There are two ways to build\\na sentence based on the triple skeleton. (1) The \\xef\\xac\\x81rst is to use an n-gram language\\nmodel. For example, when trying to decide whether or not to put a glue word x\\nbetween a pair of meaningful words (which means they are inside the triple) a and b,\\nthe probabilities \\xcb\\x86p(axb) and \\xcb\\x86p(ab) are compared for the decision. \\xcb\\x86p is the standard\\nlength-normalized probability of the n-gram language model. (2) The second is to\\nuse a set of descriptive language templates, which alleviates the problem of grammar\\nmistakes in the language model.\\nFurther, [16] proposes a novel framework to explicitly represent the relationship\\nbetween image structure and its caption sentence\\xe2\\x80\\x99s structure. The method, Visual\\nDependency Representation, detects objects in the image, and detects the relationship\\nbetween these objects based on the proposed Visual Dependency Grammar, which\\nincludes eight typical relations like beside or above. Then the image can be\\narranged as a dependency graph, where nodes are objects and edges are relations. This\\nimage dependency graph can be aligned with the syntactic dependency representation\\nof the caption sentence. The paper further provides four templates to generating\\ndescriptive sentences from the extracted dependency representation.\\nBesides these two typical works, there are massive generation models for image\\ncaptioning, such as [15, 35, 78].\\n9.3.3\\nNeural Models for Image Captioning\\nIn [33], it was claimed in 2011 that in image captioning tasks: Natural language\\ngeneration still remains an open research problem. Most previous work is based on\\nretrieval and summarization. From 2015, inspired by advances in neural language\\nmodel and neural machine translation, a number of end-to-end neural image caption-\\ning models based on the encoder-decoder system have been proposed. These new\\nmodels signi\\xef\\xac\\x81cantly improve the ability to generate natural language descriptions.\\n9.3.3.1\\nThe Basic Model\\nTraditional machine translation models typically stitch many subtasks together, such\\nas individual word translation and reordering, to perform sentence and paragraph\\ntranslation. Recent neural machine translation models, such as [8], use a single\\nencoder-decoder model, which can be optimized by stochastic gradient descent con-\\nveniently.Thetaskofimagecaptioningisinherentlyanalogoustomachinetranslation\\nbecause it can also be regarded as a translation task, where the source \\xe2\\x80\\x9clanguage\\xe2\\x80\\x9d\\nis an image. The encoders and decoders used for machine translations are typically\\nRNNs, which is a natural selection for sequences of words. For image captioning,\\nCNN is chosen to be the encoder, and RNN is still used as the decoder.\\n9.3 Image Captioning\\n297\\nInput Image\\no\\no\\no\\np\\np\\np\\nw1\\nw2\\nEND\\nw0\\nw1\\nwN\\n\\xe2\\x80\\xa6\\xe2\\x80\\xa6\\nCNN\\nLSTM\\nSoftmax\\nCorrect Caption Sentence s={w1, w2, \\xe2\\x80\\xa6, wN}\\nFig. 9.2 The architecture of encoder-decoder framework for image captioning\\nVinyals et al. [70] is the most typical model which uses encoder-decoder for\\nimage captioning (see Fig.9.2). Concretely, a CNN model is used to encode the\\nimage into a \\xef\\xac\\x81x length vector, which is believed to contain the necessary information\\nfor captioning. With this vector, an RNN language model is used to generate natural\\nlanguage descriptions, and this is the decoder. Here, the decoder is similar to the\\nLSTM used for machine translation. The \\xef\\xac\\x81rst unit takes the image vector as the\\ninput vector, and the rest units take the previous word embedding as input. Each unit\\noutputs a vector o and passes a vector to the next unit. o is further fed into a softmax\\nlayer, whose output p is the probability of each word within the vocabulary. The\\nways to deal with these calculated probabilities are different in training and testing:\\nTraining. These probabilities p are used to calculate the likelihood of the provided\\ndescription sentences. Considering the nature of RNNs, it is easy to model the joint\\nprobability into conditional probabilities.\\nlog P(s|I) =\\nN\\n\\x03\\nt=0\\nlog P(wt|I, w0, . . . , wt\\xe2\\x88\\x921),\\n(9.11)\\nwhere s = {w1, w2, ..., wN} is the sentence and its words, w0 is a special START\\ntoken, and I is the image. Stochastic gradient descent can thereby be performed to\\noptimize the model.\\nTesting. There are multiple approaches to generate sentences given an image.\\nThe \\xef\\xac\\x81rst one is called Sampling. For each step, the single word with the highest\\nprobability in p is chosen, and used as the input of the next unit until the END\\ntoken is generated or a maximal length is reached. The second one is called Beam\\nSearch. For each step (now the length of sentences is t), k best sentences are kept.\\nEach of them generates several new sentences of length t + 1, and again, only k new\\n298\\n9\\nCross-Modal Representation\\nFig. 9.3 An example of image captioning with attention mechanism\\nsentences are kept. Beam Search provides a better approximation for\\ns\\xe2\\x88\\x97= arg max\\ns\\nlog P(s|I).\\n(9.12)\\n9.3.3.2\\nVariants of the Basic Model\\nThe research on image captioning tightly follows that on machine translation.\\nInspired by [6], which uses attention mechanism in machine translation, [76] intro-\\nduces visual attention into the encoder-decoder image captioning model.\\nThe major bottleneck of [70] is the fact that information from the image is shown\\nto the LSTM decoder only at the \\xef\\xac\\x81rst decoding unit, which actually requires the\\nencoder to squeeze all useful information into one \\xef\\xac\\x81xed-length vector. In contrast,\\n[76] does not require such compression. The CNN encoder does not produce one\\nvector for the entire image; instead, it produces L region vectors Ii, each of which is\\nthe representation of a part of the image. At every step of decoding, the inputs include\\nstandard LSTM inputs (i.e., output and hidden state of last step ot\\xe2\\x88\\x921 and ht\\xe2\\x88\\x921), and\\nan input vector z from the encoder. Here, z is the weighted sum of image vectors\\nIi: z = \\x02\\ni \\xce\\xb1iIi, where \\xce\\xb1i is the weight computed from Ii and ht\\xe2\\x88\\x921. Throughout\\nthe training process, the model learns to focus on parts of the image for generating\\nthe next word by producing larger weights \\xce\\xb1 on more relevant parts, as shown in\\nFig.9.3.1\\nWhile the above paper uses soft attention for the image, [27] makes explicit\\nalignment between image fragments and sentence fragments before generating a\\ndescription for the image. In the \\xef\\xac\\x81rst stage, the alignment stage, sentence and image\\nfragments are aligned by being mapped to a shared space. Concretely, sentence\\nfragments (i.e., n consecutive words) are encoded using a bidirectional LSTM into\\nthe embeddings s, and image fragments (i.e., part of the image, and also the entire\\nimage) are encoded using a CNN into the embeddings I. The similarity score between\\nimage I and sentence s is computed as\\n1The example is obtained from the implementation of Yunjey Choi (https://github.com/yunjey/\\nshow-attend-and-tell).\\n9.3 Image Captioning\\n299\\nsim(I, s) =\\n\\x03\\nt\\xe2\\x88\\x88gs\\nmaxi\\xe2\\x88\\x88gI (0, I\\xe2\\x8a\\xa4\\ni st),\\n(9.13)\\nwhere gs is the sentence fragment set of sentence s, and gI is the image fragment set\\nof image I. The alignment is then optimized by minimizing the ranking loss L for\\nboth sentences and images:\\nL =\\n\\x03\\nI\\n\\x04\\x03\\ns\\nmax(0, sim(I, s) \\xe2\\x88\\x92sim(I, I) + 1) +\\n\\x03\\ns\\nmax(0, sim(s, I) \\xe2\\x88\\x92sim(I, I) + 1)\\n\\x05\\n.\\n(9.14)\\nThe assumption for this alignment procedure is similar to [50] (see Sect.9.3.1): all\\ndescription sentences are regarded as (possibly noisy) labels for every image section\\nand are based on the massive training data, the model would hopefully be trained to\\nalign caption sentences to their corresponding image fragments. The second stage is\\nsimilar to the basic model in [70], but the alignment results are used to provide more\\nprecise training data.\\nAs mentioned above, [76] makes the decoder have the ability to focus attention\\non the different parts of the image for different words. However, there are some\\nnonvisual words in the decoding process. For example, words such as the and of\\nare more dependent on semantic information than visual information. Furthermore,\\nwords such as phone followed by cell or meter before near the parking\\nare usually generated by the language model. To avoid the gradient of a nonvisual\\nword decreasing the effectiveness of visual attention, in the process of generating\\ncaptions, [43] adopts an adaptive attention model with a visual sentinel. At each time\\nstep, the model needs to determine that it depends on an image region or a visual\\nsentinel.\\nAdaptive attention model [43] uses attention in the process of generating a word\\nrather than updating the LSTM state; it utilizes \\xe2\\x80\\x9cvisual sentinel\" vector xt and image\\nregion vectors Ii. Here, xt is produced by the inputs and states of LSTM at time step\\nt, while Ii is provided from CNN encoder. Then the adaptive context vector \\xcb\\x86ct is the\\nweighted sum of L image region vectors Ii and visual sentinel xt:\\n\\xcb\\x86ct =\\nL\\n\\x03\\ni\\n\\xce\\xb1iIi + \\xce\\xb1L+1xt,\\n(9.15)\\nwhere \\xce\\xb1i are the weights computed by Ii, xt, and the LSTM hidden state ht. We\\nhave \\x02L+1\\ni=1 \\xce\\xb1i = 1. Finally the probability of a word in vocabulary at time t can be\\ncalculated as a residual form:\\npt = Softmax(Wp(\\xcb\\x86ct + ht)),\\n(9.16)\\nwhere Wp is a learned weight parameter.\\nMany existing image captioning models with attention allocate attention over\\nimage\\xe2\\x80\\x99s regions, whose size is often 7 \\xc3\\x97 7 or 14 \\xc3\\x97 14 decided by the last pooling\\n300\\n9\\nCross-Modal Representation\\nResize\\nLatent Channel\\nActivation\\nActivated Region\\nImage\\nHt\\nct\\nI\\nFig. 9.4 An example of the activated region of a latent channel\\nlayer in CNN encoder. Anderson et al. [2] \\xef\\xac\\x81rst calculate attention at the level of\\nobjects. It \\xef\\xac\\x81rst employs Faster R-CNN [58] which is trained on ImageNet [60] and\\nGenome [31] to predict attribute class, such as an open oven, green bottle, \\xef\\xac\\x82oral\\ndress, and so on. After that, it applies attention over valid bounding boxes to get\\n\\xef\\xac\\x81ne-grained attention for helping the caption generation.\\nBesides, [11] rethinks the form of latent states in image captioning, which usu-\\nally compresses two-dimensional visual feature maps encoded by CNN to a one-\\ndimensional vector as the input of the language model. They \\xef\\xac\\x81nd that the language\\nmodel with 2D states can preserve the spatial locality, which can link the input visual\\ndomain and output linguistic domain observed by visualizing the transformation of\\nhidden states.\\nWord embeddings and hidden states in [11] are 3D tensors of size C \\xc3\\x97 H \\xc3\\x97 W,\\nwhich means C channels, each of size H \\xc3\\x97 W. The encoded features maps will\\nbe directly inputted to the 2D language model instead of going through an average\\npooling layer. In the 2D language model, the convolution operator takes the place of\\nmatrix multiplication in the 1D model, and mean pooling will be used to generate\\nthe output word probability distribution from 2D hidden states. Figure9.4 shows\\nactivated region of a latent channel at the tth step. When we set a threshold for the\\nactivated regions, it is revealed that the special channels are associated with speci\\xef\\xac\\x81c\\nnouns in the decoding process, which help get a better understanding of the process\\nof generating captions.\\nTraditional methods train the caption model by maximizing the likelihood of\\ntraining examples, which forms a gap between the optimization objective and evalu-\\nating metrics. To alleviate the problem, [59] uses reinforcement learning to directly\\nmaximize the CIDEr metric [69]. CIDEr re\\xef\\xac\\x82ects the diversity of generated cap-\\ntions by giving high weights to the low-frequency n-grams in the training set, which\\ndemonstrates that people prefer detailed captions rather than universal ones, like a\\nboy is playing a game. To encourage the distinctiveness of captions, [10]\\nadopts contrastive learning. Their model learns to discriminate the caption of a given\\nimage and the caption of an alike image by maximizing the difference between\\nground truth positive pair and mismatch negative pair. The experiment shows that\\ncontrastive learning increases the diversity of captions signi\\xef\\xac\\x81cantly.\\nFurthermore, automatic evaluation metrics, such as BLEU [54], METEOR [13],\\nROUGE [38], CIDEr [69], SPICE [1], and so on, may neglect some novel expres-\\nsions restrained by the ground truth captions. To better evaluate the naturalness and\\ndiversity of captions, [9] proposes a framework based on Conditional Generative\\nAdversarial Networks, whose generator tries to achieve a higher score in the evalua-\\n9.3 Image Captioning\\n301\\ntor, while the evaluator tries to distinguish between the generated caption and human\\ndescriptions for a given image, as well as between the given image and the mismatch\\ndescription. The user study shows that the trained generator can generate natural and\\ndiverse captions than the model trained by maximum likelihood estimate, while the\\ntrained evaluator is more consistent with human\\xe2\\x80\\x99s evaluation.\\nBesides the works we introduced above, there are also a mass of variants of the\\nbasic encoder-decoder model such as [20, 26, 40, 45, 51, 71, 73].\\n9.4\\nVisual Relationship Detection\\nVisual relationship detection is the task of detecting objects in an image and under-\\nstanding the relationship between them. While detecting the objects is always based\\non semantic segmentation or object detection methods, such as R-CNN, understand-\\ning the relationship is the key challenge of this task. While detecting visual relation\\nwith image information is intuitive and effective [25, 62, 84], leveraging information\\nfrom language can further boost the model performance [37, 41, 82].\\n9.4.1\\nVisual Relationship Detection with Language Priors\\nLu et al. [41] propose a model that uses language priors to enhance the performance\\non infrequent relationships for which suf\\xef\\xac\\x81cient training instances are hard to obtain\\nsolely from images. The overall architecture is shown in Fig.9.5.\\nThey \\xef\\xac\\x81rst train a CNN to calculate the unnormalized relations\\xe2\\x80\\x99 probability\\nobtained from visual inputs by\\nRCNN\\nLanguage Module     f(R,W)\\nhat-ride-horse\\nhorse-pull-hat\\nperson-ride-horse\\nperson-wear-horse\\nVisual Module     V(R,\\xce\\xb8)\\nperson-wear-hat\\nhorse-wear-hat\\nperson-ride-horse\\n0.76\\n0.11\\n0.81\\nCNN\\n...\\n...\\nFig. 9.5 The architecture of visual relationship detection with language prior\\n302\\n9\\nCross-Modal Representation\\nPV (R\\xe2\\x9f\\xa8i, j,k\\xe2\\x9f\\xa9, \\xce\\x98|\\xe2\\x9f\\xa8O1, O2\\xe2\\x9f\\xa9) = Pi(O1)(z\\xe2\\x8a\\xa4\\nk CNN(O1, O2) + sk)Pj(O2),\\n(9.17)\\nwhere Pi(O j) denotes the probability that bounding box O j is entity i, and\\nC N N(O1, O2) is the joint feature of box O1 with box O2. \\xce\\x98 = {zk, sk} is the set of\\nparameters.\\nBesides, language prior is considered in this model by calculating the unnormal-\\nized probability that the entity pair \\xe2\\x9f\\xa8i, j\\xe2\\x9f\\xa9has the relation k:\\nPf (R, W) = r\\xe2\\x8a\\xa4\\nk [wi; w j] + bk,\\n(9.18)\\nwhere wi and w j are the word embeddings of the text of subject and object, respec-\\ntively, rk is the learned relational embedding of the relation k.\\nGiven the probabilities of a relation from visual and textual inputs, respectively,\\nthe authors combine them into the integrated probability of a relation. The \\xef\\xac\\x81nal\\nprediction is the one with maximal integrated probability:\\nR\\xe2\\x88\\x97= max\\nR\\nPV (R\\xe2\\x9f\\xa8i, j,k\\xe2\\x9f\\xa9|\\xe2\\x9f\\xa8O1, O2\\xe2\\x9f\\xa9)Pf (R, W).\\n(9.19)\\nThe rank of the ground truth relationship R with bounding boxes O1 and O2 is\\nmaximized using the following rank loss function:\\nC(\\xce\\x98, W) =\\n\\x03\\n\\xe2\\x9f\\xa8O1,O2\\xe2\\x9f\\xa9,R\\nmax{1 \\xe2\\x88\\x92PV (R, \\xce\\x98|\\xe2\\x9f\\xa8O1, O2\\xe2\\x9f\\xa9)Pf (R, W)\\n+\\nmax\\n\\xe2\\x9f\\xa8O\\xe2\\x80\\xb2\\n1,O\\xe2\\x80\\xb2\\n2\\xe2\\x9f\\xa9\\xcc\\xb8=\\xe2\\x9f\\xa8O1,O2\\xe2\\x9f\\xa9,R\\xe2\\x80\\xb2\\xcc\\xb8=R PV (R\\xe2\\x80\\xb2, \\xce\\x98|\\xe2\\x9f\\xa8O\\xe2\\x80\\xb2\\n1, O\\xe2\\x80\\xb2\\n2\\xe2\\x9f\\xa9)Pf (R\\xe2\\x80\\xb2, W), 0}.\\n(9.20)\\nIn addition to the loss that optimizes the rank of the ground truth relationships,\\nthe authors also propose two regularization functions for language priors. The \\xef\\xac\\x81nal\\nloss function of this model is de\\xef\\xac\\x81ned as\\nL = C(\\xce\\x98, W) + \\xce\\xbb1L(W) + \\xce\\xbb2K(W).\\n(9.21)\\nK(W) is a variance function to make the similar relationships\\xe2\\x80\\x99 corresponding\\nf (\\xc2\\xb7) function closer:\\nK(W) = Var{[Pf (R, W) \\xe2\\x88\\x92Pf (R\\xe2\\x80\\xb2, W)]2\\nd(R, R\\xe2\\x80\\xb2)\\n}, \\xe2\\x88\\x80R, R\\xe2\\x80\\xb2,\\n(9.22)\\nwhere d(R, R\\xe2\\x80\\xb2) is the sum of the cosine distances (in Word2vec space) between the\\ntwo objects and the predicates of the two relationships R and R\\xe2\\x80\\xb2.\\nL(W) is a function to encourage less-frequent relation to have a lower f () score.\\nWhen R occurs more frequently than R\\xe2\\x80\\xb2, we have\\nL(W) =\\n\\x03\\nR,R\\xe2\\x80\\xb2\\nmax{Pf (R\\xe2\\x80\\xb2, W) \\xe2\\x88\\x92Pf (R, W) + 1, 0}.\\n(9.23)\\n9.4 Visual Relationship Detection\\n303\\nCNN\\nObject Detection Module\\nRelation Prediction Module\\nclasseme\\nlocation\\nvisual\\nperson\\nelephant\\nperson\\nperson\\nride\\ntaller\\nnext to\\nwith\\nelephant\\nperson\\nelephant\\npants\\nperson\\nelephant\\nperson\\npants\\nbox\\nBilinear\\nInterpolation\\nconv\\nfeat\\nScaling\\nScaling\\nws\\nFeature\\nExtraction\\nLayer\\nwo\\nSoftmax\\n...\\nbox\\n...\\nFig. 9.6 The architecture of VTransE model\\n(a) A scene.\\nTable\\nTablecloth\\nGoose\\nApple\\nPorcelain\\nR: On\\nR: On\\nR: Inside\\nR: Inside\\nR: ?  Answer: On\\n(b) The corresponding scene graph.\\nFig. 9.7 An illustration for scene graph generation\\n9.4.2\\nVisual Translation Embedding Network\\nInspired by recent progress in knowledge representation learning, [82] proposes\\nVTransE, a visual translation embedding network. Objects and the relationship\\nbetween objects are modeled as TransE [7] like vector translation. VTransE \\xef\\xac\\x81rst\\nprojects subject and object into the same space as relation translation vector r \\xe2\\x88\\x88Rr.\\nSubject and object could be denoted as xs, xo \\xe2\\x88\\x88RM in the feature space, where\\nM \\xe2\\x89\\xabr. Similar to TransE relationship, VTransE establishes a relationship as\\nWsxs + r \\xe2\\x88\\xbcWoxo,\\n(9.24)\\nwhere Ws and Wo are projection matrices. The overall architecture is shown in\\nFig.9.6.\\n9.4.3\\nScene Graph Generation\\nLi et al. [37] further formulate visual relation detection as a scene graph generation\\ntask, where nodes correspond to objects and directed edges correspond to visual\\nrelations between objects, as shown in Fig.9.7.\\nThis formulation allows [37] to leverage different levels of context information,\\nsuch as information from objects, phrases (i.e., \\xe2\\x9f\\xa8subject, predicate, object\\xe2\\x9f\\xa9triples),\\n304\\n9\\nCross-Modal Representation\\nc)\\nb)\\na)\\nFig. 9.8 Dynamical graph construction. a The input image. b Object (bottom), phrase (middle),\\nand caption region (top) proposals. c The graph modeling connections between proposals. Some of\\nthe phrase boxes are omitted\\nand region captions, to boost the performance of visual relation detection. Speci\\xef\\xac\\x81-\\ncally, [37] proposes to construct a graph that aligns these three levels of information\\nand perform feature re\\xef\\xac\\x81nement via message passing, as shown in Fig.9.8. By leverag-\\ning complementary information from different levels, the performances of different\\ntasks are expected to be mutually improved.\\nDynamic Graph Construction. Given an image, they \\xef\\xac\\x81rst generate three kinds\\nof proposals that correspond to three kinds of nodes in the proposed graph structure.\\nThe proposals include object proposals, phrase proposals, and region proposals. The\\nobject and region proposals are generated using Region Proposal Network (RPN)\\n[57] trained with ground truth bounding boxes. Given N object proposals, phrase\\nproposals are constructed based on N(N \\xe2\\x88\\x921) object pairs that fully connect the\\nobject proposals with direct edges, where each direct edge represents a potential\\nphrase between an object pair.\\nEach phrase proposal is connected to the corresponding subject and object with\\ntwo directed edges. A phrase proposal and a region proposal are connected if their\\noverlapexceedsacertainfraction(e.g.,0.7)ofthephraseproposal.Therearenodirect\\nconnections between objects and regions since they can be indirectly connected via\\nphrases.\\nFeature Re\\xef\\xac\\x81nement. After obtaining the graph structure of different levels of\\nnodes, they perform feature re\\xef\\xac\\x81nement by iterative message passing. The message\\npassing procedure is divided into three parallel stages, including object re\\xef\\xac\\x81nement,\\nphrase re\\xef\\xac\\x81nement, and region re\\xef\\xac\\x81nement.\\nIn object feature re\\xef\\xac\\x81nement, the object proposal feature is updated with gated\\nfeatures from adjacent phrases. Given an object i, the aggregated feature from regions\\nthat are linked to object i via subject-predicate edges \\xcb\\x86xp\\xe2\\x86\\x92s\\ni\\ncan be de\\xef\\xac\\x81ned as follows:\\n\\xcb\\x86xp\\xe2\\x86\\x92s\\ni\\n=\\n1\\n\\xe2\\x88\\xa5Ei,p\\xe2\\x88\\xa5\\n\\x03\\n(i, j)\\xe2\\x88\\x88Es,p\\nf\\xe2\\x9f\\xa8o,p\\xe2\\x9f\\xa9(x(o)\\ni , x(p)\\nj )x(p)\\nj ,\\n(9.25)\\n9.4 Visual Relationship Detection\\n305\\nwhere Es,p is the set of subject predicate connections, and Ei,p denotes the number of\\nphrases connected with the object i as the subject predicate pairs. f\\xe2\\x9f\\xa8o,p\\xe2\\x9f\\xa9is a learnable\\ngate function that controls the weights of information from different sources:\\nf\\xe2\\x9f\\xa8o,p\\xe2\\x9f\\xa9(x(o)\\ni , x(p)\\nj ) =\\nK\\n\\x03\\nk=1\\nSigmoid(\\xcf\\x89(k)\\n\\xe2\\x9f\\xa8o,p\\xe2\\x9f\\xa9\\xc2\\xb7 [x(o)\\ni ; x(p)\\nj ]),\\n(9.26)\\nwhere \\xcf\\x89(k)\\n\\xe2\\x9f\\xa8o,p\\xe2\\x9f\\xa9is a gate template used to calculate the importance of the information\\nfrom a subject-predicate edge and K is the number of templates. The aggregated\\nfeature from object-predicate edges \\xcb\\x86xp\\xe2\\x86\\x92o\\ni\\ncan be similarly computed.\\nAfter obtaining information \\xcb\\x86xp\\xe2\\x86\\x92s\\ni\\nand \\xcb\\x86xp\\xe2\\x86\\x92o\\ni\\nfrom adjacent phrases, the object\\nre\\xef\\xac\\x81nement at time step t can be de\\xef\\xac\\x81ned as follows:\\nx(o)\\ni,t+1 = x(o)\\ni,t + f (p\\xe2\\x86\\x92s)(\\xcb\\x86xp\\xe2\\x86\\x92s\\ni\\n) + f (p\\xe2\\x86\\x92o)(\\xcb\\x86xp\\xe2\\x86\\x92o\\ni\\n),\\n(9.27)\\nwhere f (\\xc2\\xb7) = WReLU(\\xc2\\xb7), W is a learnable parameter and not shared between\\nf (p\\xe2\\x86\\x92s)(\\xc2\\xb7) and f (p\\xe2\\x86\\x92o)(\\xc2\\xb7).\\nThe re\\xef\\xac\\x81nement scheme of phrases and regions is similar to objects. The only\\ndifference is the information sources: Phrase proposals receive information from\\nadjacent objects and regions, and region proposals receive information from phrases.\\nAfter feature re\\xef\\xac\\x81nement via iterative message passing, the feature of different\\nlevels of nodes can be used for corresponding tasks. Region features can be used\\nas the initial state of a language model to generate region captions. Phrase features\\ncan be used to predict visual relation between objects, which composes of the scene\\ngraph of the image.\\nIn comparison with scene graph generation methods that model the dependencies\\nbetween relation instances by attention mechanism or message passing, [47] decom-\\nposes the scene graph task into a mixture of two phases: extracting primary relations\\nfrom input, and completing the scene graph with reasoning. The authors propose a\\nHybrid Scene Graph generator (HRE) that combines these two phases in a uni\\xef\\xac\\x81ed\\nframework and generates scene graphs from scratch.\\nSpeci\\xef\\xac\\x81cally, HRE \\xef\\xac\\x81rst encodes the object pair into representations and then\\nemploys a neural relation extractor resolving primary relations from inputs and a dif-\\nferentiable inductive logic programming model that iteratively completes the scene\\ngraph. As shown in Fig.9.9, HRE contains two units, a pair selector and a relation\\npredictor, and runs in an iterative way.\\nAt each time step, the pair selector takes a look at all object pairs P\\xe2\\x88\\x92that have not\\nbeen associated with a relation and chooses the next pair of entities whose relation\\nis to be determined. The relation predictor utilizes the information contained in all\\npairs P+ whose relations have been determined, and the contextual information of\\nthe pair to make the prediction on the relation. The prediction result is then added to\\nP+ and bene\\xef\\xac\\x81ts future predictions.\\nTo encode object pair into representations, HRE extends the union box encoder\\nproposed by [41] by adding the object features (what are the objects) and their\\n306\\n9\\nCross-Modal Representation\\nFig. 9.9 Framework of HRE that detects primary relations from inputs and iteratively completes\\nthe scene graph via inductive logic programming\\nFig. 9.10 Object pair encoder of HRE\\nlocations (where are the objects) into the object pair representation, as shown in\\nFig.9.10.\\nRelation Predictor. The relation predictor is composed of two modules: a neural\\nmodule predicting the relations between entities based on the given context (i.e., a\\nvisual image) and a differentiable inductive logic module performing reasoning on\\nP+. Both modules predict the relation score between a pair of objects individually.\\nThe relation scores from the two modules are \\xef\\xac\\x81nally integrated by multiplication.\\nPair Selector. The selector works as the predictor\\xe2\\x80\\x99s collaborator with the goal\\nto \\xef\\xac\\x81gure out the next relation which should be determined. Ideally, the choice p\\xe2\\x88\\x97\\nmade by the selector should satisfy the condition that all relations that will affect\\nthe predictor\\xe2\\x80\\x99s prediction on p\\xe2\\x88\\x97should be sent to the predictor ahead of p\\xe2\\x88\\x97. HRE\\nimplements the pair selector as a greedy selector which always chooses the entity\\npair from P\\xe2\\x88\\x92to be added to P+ as the entity pair of which the relation predictor is\\nmost con\\xef\\xac\\x81dent in its prediction.\\n9.4 Visual Relationship Detection\\n307\\nIt is worth noting that the task of scene graph generation resembles document-\\nlevel relation extraction in many aspects. Both tasks seek to extract structured graphs\\nconsisting of entities and relations. Also, they need to model the complex dependen-\\ncies between entities and relations in a rich context. We believe both tasks are worthy\\nto explore for future research.\\n9.5\\nVisual Question Answering\\nVisual Question Answering (VQA) aims to answer natural language questions about\\nan image, and can be seen as a single turn of dialogue about a picture. In this section,\\nwe will introduce widely used VQA datasets and several typical VQA models.\\n9.5.1\\nVQA and VQA Datasets\\nVQA was \\xef\\xac\\x81rst proposed in [46]. They \\xef\\xac\\x81rst propose a single-world approach by\\nmodeling the probability of an answer a given question q and a world w by\\nP(a|q, w) =\\n\\x03\\nz\\nP(a|z, w)P(z|q),\\n(9.28)\\nwhere z is a latent variable associated with the question and the world w is a represen-\\ntation of the image. They further extend the single-world approach to a multi-world\\napproach by marginalizing over different segments s of the given image. The prob-\\nability of an answer a given question q and a world w is given by\\nP(a|q, s) =\\n\\x03\\nw\\n\\x03\\nz\\nP(a|w, z)P(w|s)P(z|q).\\n(9.29)\\nThey also release the \\xef\\xac\\x81rst dataset of VQA named as DAQUAR in their paper.\\nBesides DAQUAR, researchers also release a lot of VQA datasets with vari-\\nous characteristics. The most widely used dataset was released in [4], where the\\nauthors provided cases and experimental evidence to demonstrate that to answer\\nthese questions, a human or an algorithm should use features of the image and exter-\\nnal knowledge. Figure9.11 shows examples of VQA dataset released in [4]. It is also\\ndemonstrated that this problem cannot be solved by converting images to captions\\nand answering questions according to captions. Experiment results show that the\\nperformance of vanilla methods is still far from human.\\nIn fact, there are also other existing datasets for Visual QA such as Visual7W\\n[85], Visual Madlibs [80], COCO-QA [56], and FM-IQA [19].\\n308\\n9\\nCross-Modal Representation\\nQuestion: Why are the men jumping?\\nAnswer: to catch frisbee\\nQuestion: Is the water still?\\nAnswer: no\\nQuestion: What is the kid doing?\\nAnswer: skateboarding\\nQuestion: What is hanging on the wall \\nabove the headboard?\\nAnswer: pictures\\nFig. 9.11 Examples of VQA dataset\\n9.5.2\\nVQA Models\\nBesides, [4, 46] further investigate approaches to solve speci\\xef\\xac\\x81c types of questions in\\nVQA. Moreover, [83] proposes an approach to solve \\xe2\\x80\\x9cYES/NO\\xe2\\x80\\x9d questions. Note that\\nthe model is an ensemble model of two similar models: Q-model and Tuple-model,\\nthe difference between which will be described later. The overall approach can be\\ndivided into two steps: (1) Language Parsing and (2) Visual Veri\\xef\\xac\\x81cation. In the former\\nstep, they extract \\xe2\\x9f\\xa8P, R, S\\xe2\\x9f\\xa9tuples from questions \\xef\\xac\\x81rst by parsing it and assigning an\\nentity to each word. Then they summarize the parsed sentences through removing\\n\\xe2\\x80\\x9cstop words\\xe2\\x80\\x9d, auxiliary verbs, and all words before a nominal subject or passive\\nnominal subject, and further split the summary into PRS arguments according to the\\npart of speech of phrases. The difference between Q-Model and Tuple-model is that\\nthe Q-model is the one used in their previous work [4], embedding the question into\\na dense 256-dim vector by LSTM, while Tuple-model is to convert \\xe2\\x9f\\xa8P, R, S\\xe2\\x9f\\xa9tuples\\ninto 256-dim embeddings by MLP. As for the Visual Veri\\xef\\xac\\x81cation step, they use the\\nsame feature of images as in [39] which was encoded into the dense 256-dim vector\\n9.5 Visual Question Answering\\n309\\nby an inner-product layer followed by a tanh layer. These two vectors are passed\\nthrough an MLP to produce the \\xef\\xac\\x81nal output (\\xe2\\x80\\x9cYes\\xe2\\x80\\x9d or \\xe2\\x80\\x9cNo\\xe2\\x80\\x9d).\\nMoreover, [61] proposes a method to calculate attention \\xce\\xb1 j by the set of image\\nfeatures I = (I1, I2, . . . , IK) and the question embedding q by\\n\\xce\\xb1 j = (W1I j + b1)\\xe2\\x8a\\xa4(W2q + b2),\\n(9.30)\\nwhere W1,W2,b1,b2 are trainable parameters.\\nAttention based techniques are quite ef\\xef\\xac\\x81cient for \\xef\\xac\\x81ltering noises that are irrelevant\\nto the question. However, some questions are only related to some small regions,\\nwhich encourages researchers to use stacked attention to further \\xef\\xac\\x81ltering noises. We\\nrefer readers to Fig.1b in [79] for an example of stacked attention.\\nYang et al. [79] further extend the attention-based model used in [61], which\\nemploys LSTMs to predict the answer. They take the question as input and attend\\nto different regions in the image to obtain additional input. The key idea is to grad-\\nually \\xef\\xac\\x81lter out noises and pinpoint the regions that are highly relevant to the answer\\nby reasoning through multiple stacked attention layers progressively. The stacked\\nattention could be calculated by stacking:\\nhk\\nA = tanh(Wk\\n1I \\xe2\\x8a\\x95(Wk\\n2uk\\xe2\\x88\\x921 + bk\\nA)).\\n(9.31)\\nNote that we denote the addition of a matrix and a vector by \\xe2\\x8a\\x95. The addition\\nbetween a matrix and a vector is performed by adding each column of the matrix by\\nthe vector. u is a re\\xef\\xac\\x81ned query vector that combines information from the question\\nand image regions. u0 (i.e, u from the \\xef\\xac\\x81rst attention layer with k = 0) could be\\ninitialized as the feature vector of the question. hk\\nA is then used to compute pk\\nI, which\\ncorresponds to the attention probability of each image region,\\npk\\nI = Softmax(Wk\\n3hk\\nA + bk\\nP).\\n(9.32)\\nuk could be iterated by\\n\\xcb\\x9cI\\nk =\\n\\x03\\ni\\npk\\ni Ii,\\n(9.33)\\nuk = uk\\xe2\\x88\\x921 + \\xcb\\x9cI\\nk.\\n(9.34)\\nThat is, in every layer, the model progressively uses the combined question and\\nimage vector uk\\xe2\\x88\\x921 as the query vector for attending the image region to obtain the\\nnew query.\\nThe above models attend only on images, but questions should also be attended.\\n[44] calculates co-attention by\\nZ = tanh(Q\\xe2\\x8a\\xa4WI),\\n(9.35)\\n310\\n9\\nCross-Modal Representation\\nWhat color on the stop light is lit up\\nWhat\\ncolor\\nWhat\\nthe \\nstop\\nlight\\ncolor\\ncolor\\nQuestion:What color on the stop light is lit up?\\nco-attention\\nimage\\nAnswer:green\\nstop\\nthe stop light\\ncolor stop light lit\\nstop\\nlight\\nlight\\n...\\n...\\n...\\n...\\nFig. 9.12 The architecture of hierarchical co-attention model\\nRegions\\nProposals\\nMulti-label CNN\\nCap 1: a dog laying on the floor with a\\nbird next to it and a cat behind them,\\non the other side of a sliding glass door.\\nCap 2: a brown and black dog laying\\non a floor next to a bird.\\nCap 3: the dog, cat, and bird are all on\\nthe floor in the room.\\n\\xe2\\x80\\xa6..\\nCaption\\nInternal Representation\\n\\xe2\\x80\\xa6\\nAverage\\nPooling\\nTop 5\\nAttributes\\nSPARQL\\nDBpedia\\nThe dog is a furry, carnivorous\\nmember of the canidae family,\\nmammal class. The cat is a small,\\nusually furry, domesticated, and\\ncarnivorous mammal. Birds, Aves\\nclass, are a group of endothermic\\nvertebrates,\\ncharacterised\\nby\\nfeathers, a beak with no teeth.\\nPlants, also called green plants ,\\nare multicellular eukaryotes of\\nthe\\xe2\\x80\\xa6.\\nDoc2Vec\\nExternal Knowledge\\n...\\n)\\n)\\n)\\n...\\n)\\nHow\\nmany\\n?\\n...\\nThere\\nThere\\nare\\nare\\ntwo\\nmammals\\nEND\\n...\\n. . .\\n1\\n\\xe2\\x88\\x99\\nminimizing cost function\\nGeneration\\nh0\\nh1\\nhn\\nhn+1\\nhn+l-1\\nLSTM\\nCap 1\\nCap 2\\nCap 5\\nFig. 9.13 The architecture of VQA incorporating external knowledge bases\\nwhere Zi j represents the af\\xef\\xac\\x81nity of the ith word and jth region. Figure9.12 shows\\nthe hierarchical co-attention model.\\nAnother intuitive approach is to use external knowledge from knowledge bases,\\nwhich will help us better explain the implicit information hiding behind the image.\\nSuch an approach was proposed in [75], which \\xef\\xac\\x81rst encodes the image into cap-\\ntions and vectors representing different attributes of the image to retrieve documents\\nabout a different part of the images from knowledge bases. Documents are encoded\\nthrough doc2vec [36]. The representation of captions, attributes, and documents are\\ntransformed and concatenated to form the initial vector of an LSTM, which is trained\\nin Seq2seq fashion. Details of the model are shown in Fig.9.13.\\nNeural Module Network is a framework for constructing deep networks with a\\ndynamic computational structure, which was \\xef\\xac\\x81rst proposed in [3]. In such a frame-\\nwork, every input is associated with a layout that provides a template for assembling\\nan instance-speci\\xef\\xac\\x81c network from a collection of shallow network fragments called\\n9.5 Visual Question Answering\\n311\\nWhere is\\nthe dog?\\nparser\\nLSTM\\ncouch\\nLayout\\ncount\\nwhere\\ncolor\\nstanding\\ng\\no\\nd\\nta\\nc\\nCNN\\n...\\n...\\nFig. 9.14 The architecture of the neural module network model\\nmodules. The proposed method processes the input question through two separate\\nways: (1) parsing and laying out several modules, and (2) encoding by an LSTM.\\nThe corresponding picture is processed by the modules laid out according to the\\nquestion, the types of which are prede\\xef\\xac\\x81ned, find, transform, combine,\\ndescribe, and measure. The authors de\\xef\\xac\\x81ned find to be a transformation from\\nImage to Attention map, transform to be a mapping from one Attention to\\nanother, combine to be a combination of two Attention, describe to be a\\ndescription relying on Image and Attention, and Measure to be a measure only\\nrelying on Attention. The model is shown in Fig.9.14.\\nA key drawback of [3] is that it relies on the parser to generate modules. [22]\\nproposes an end-to-end model to generate a sequence of Reverse-Polish expression\\nto describe the module network, as shown in Fig.9.15. And the overall architecture\\nis shown in Fig.9.16.\\nGraph Neural Networks (GNNs) have also been applied to VQA tasks. [68] tries\\nto build graphs about both the scene and the question. The authors described a deep\\nneural network to take advantage of such a structured representation. As shown in\\nFig.9.17, the GNN-based VQA model could capture the relationships between words\\nand objects.\\n9.6\\nSummary\\nIn this chapter, we \\xef\\xac\\x81rst introduce the concept of cross-modal representation learn-\\ning. Cross-modal learning is essential since many real-world tasks require the ability\\nto understand the information from different modalities, such as text and image.\\nNext, we introduce the concept of cross-modal representation learning, which aims\\nto exploit the links and enable better utilization of information from different modali-\\n312\\n9\\nCross-Modal Representation\\nFig. 9.15 The architecture of Reverse-Polish expression and corresponding module network model\\nHow many other things are of the\\nsame size as the green matte ball\\nQuestion encoder(RNN)\\nQuestion features\\nLayout predicion\\n(reverse Polish notation)\\nfind()\\nrelocate(_)\\nHow many other things are \\nof the same size as the\\ngreen matte ball?\\nHow many other things are\\nof the same size as the\\ngreen matte ball?\\nQuestion attentions\\nAnswer\\ncount\\nrelocate\\nrelocate\\ne the same size as th\\ne green matte ball?\\nImage encoder(CNN)\\nImage features\\n4\\nModule\\nnetwork\\nNetwork builder\\ncount(_)\\nLayout policy (RNN)\\nFig. 9.16 The architecture of end-to-end module network model\\nFig. 9.17 The architecture of GNN-based VQA models\\n9.6 Summary\\n313\\nties. And we overview existing cross-modal representation learning methods for sev-\\neral representative cross-modal tasks, including zero-shot recognition, cross-media\\nretrieval, image captioning, and visual question answering. These cross-modal learn-\\ning methods either try to fuse information from different modalities into uni\\xef\\xac\\x81ed\\nembeddings, or try to build embeddings for different modalities in a common seman-\\ntic space, allowing the model to compute cross-modal similarity. Cross-modal repre-\\nsentation learning is drawing more and more attention and can serve as a promising\\nconnection between different research areas.\\nFor further understanding of cross-modal representation learning, there are also\\nsome recommended surveys and books including:\\n\\xe2\\x80\\xa2 Skocaj et al., Cross-modal learning [64].\\n\\xe2\\x80\\xa2 Spence, Crossmodal correspondences: A tutorial review [66].\\n\\xe2\\x80\\xa2 Wang et al., A comprehensive survey on cross-modal retrieval [72].\\nIn the future, for better cross-modal representation learning, some directions are\\nrequiring further efforts:\\n(1) Fine-grained Cross-modal Grounding. Cross-modal grounding is a funda-\\nmental ability in solving cross-modal tasks, which aims to align semantic units in\\ndifferent modalities. For example, visual grounding aims to ground textual symbols\\n(e.g., words or phrases) into visual objects or regions. Many existing works [27, 74,\\n76] have been devoted to cross-modal grounding, which mainly focuses on coarse-\\ngrained semantic unit grounding (e.g., grounding of sentences and images). Better\\n\\xef\\xac\\x81ne-grained cross-modal grounding (e.g., grounding of words and objects) could\\npromote the development of a broad variety of cross-modal tasks.\\n(2) Cross-modal Reasoning. In addition to recognizing and grounding semantic\\nunits in different modalities, understanding and inferring the relationship between\\nsemantic units are also crucial to cross-modal tasks. Many existing works [37, 41,\\n82] have investigated detecting visual relation between objects. However, most visual\\nrelations in existing visual relation detection datasets do not require complex reason-\\ning. Some works [81] have made preliminary attempts on cross-modal commonsense\\nreasoning. Inferring the latent semantic relationships in cross-modal context is crit-\\nical for cross-modal understanding and modeling.\\n(3) Utilizing Unsupervised Cross-modal Data. Most current cross-modal learn-\\ning approaches rely on human-annotated datasets. The scale of such supervised\\ndatasets is usually limited, which also limits the capability of data-hungry neural\\nmodels. With the rapid development of the World Wide Web, cross-modal data on\\nthe Web have become larger and larger. Some existing works [42, 67] have lever-\\naged unsupervised cross-modal data for representation learning. They \\xef\\xac\\x81rst pretrained\\ncross-modal models on large-scale image-caption pairs, and then \\xef\\xac\\x81ne-tuned the mod-\\nels on those downstream tasks, which shows signi\\xef\\xac\\x81cant improvement in a broad\\nvariety of cross-modal tasks. It is thus promising to better leverage the vast amount\\nof unsupervised cross-modal data for representation learning.\\n314\\n9\\nCross-Modal Representation\\nReferences\\n1. Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. SPICE: semantic\\npropositional image caption evaluation. In Proceedings of ECCV, 2016.\\n2. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould,\\nand Lei Zhang. Bottom-up and top-down attention for image captioning and visual question\\nanswering. In Proceedings of CVPR, 2018.\\n3. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks.\\nIn Proceedings of CVPR, pages 39\\xe2\\x80\\x9348, 2016.\\n4. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence\\nZitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of ICCV, 2015.\\n5. Lei Jimmy Ba, Kevin Swersky, Sanja Fidler, and Ruslan Salakhutdinov. Predicting deep zero-\\nshot convolutional neural networks using textual descriptions. In Proceedings of ICCV, 2015.\\n6. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\\njointly learning to align and translate. In Proceedings of ICLR, 2015.\\n7. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana\\nYakhnenko. Translating embeddings for modeling multi-relational data. In Proceedings of\\nNeurIPS, 2013.\\n8. Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\\nHolger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder\\xe2\\x80\\x93\\ndecoder for statistical machine translation. In Proceedings of EMNLP, 2014.\\n9. Bo Dai, Sanja Fidler, Raquel Urtasun, and Dahua Lin. Towards diverse and natural image\\ndescriptions via a conditional GAN. In Proceedings of ICCV, 2017.\\n10. Bo Dai and Dahua Lin. Contrastive learning for image captioning. In Proceedings of NeurIPS,\\n2017.\\n11. Bo Dai, Deming Ye, and Dahua Lin. Rethinking the form of latent states in image captioning.\\nIn Proceedings of ECCV, 2018.\\n12. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\\nhierarchical image database. In Proceedings of CVPR, 2009.\\n13. Michael J. Denkowski and Alon Lavie. Meteor universal: Language speci\\xef\\xac\\x81c translation eval-\\nuation for any target language. In Proceedings of ACL, 2014.\\n14. Mohamed Elhoseiny, Babak Saleh, and Ahmed Elgammal. Write a classi\\xef\\xac\\x81er: Zero-shot learn-\\ning using purely textual descriptions. In Proceedings of ICCV, 2013.\\n15. Desmond Elliott and Arjen de Vries. Describing images using inferred visual dependency\\nrepresentations. In Proceedings of ACL, 2015.\\n16. DesmondElliottandFrankKeller.Imagedescriptionusingvisualdependencyrepresentations.\\nIn Proceedings of EMNLP, 2013.\\n17. Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian,\\nJulia Hockenmaier, and David Forsyth. Every picture tells a story: Generating sentences from\\nimages. In Proceedings of ECCV, 2010.\\n18. Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al.\\nDevise: A deep visual-semantic embedding model. In Proceedings of NeurIPS, 2013.\\n19. Haoyuan Gao, Junhua Mao, Jie Zhou, Zhiheng Huang, Lei Wang, and Wei Xu. Are you\\ntalking to a machine? dataset and methods for multilingual image question. In Proceedings\\nof NeurIPS, 2015.\\n20. Jiuxiang Gu, Jianfei Cai, Gang Wang, and Tsuhan Chen. Stack-captioning: Coarse-to-\\xef\\xac\\x81ne\\nlearning for image captioning. In Proceedings of AAAI, 2018.\\n21. Micah Hodosh, Peter Young, and Julia Hockenmaier. Framing image description as a ranking\\ntask: Data, models and evaluation metrics. Journal of Machine Learning Research, 47:853\\xe2\\x80\\x93\\n899, 2013.\\n22. Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning\\nto reason: End-to-end module networks for visual question answering. In Proceedings of\\nICCV, 2017.\\nReferences\\n315\\n23. Xin Huang and Yuxin Peng. Deep cross-media knowledge transfer. In Proceedings of CVPR,\\n2018.\\n24. Xin Huang, Yuxin Peng, and Mingkuan Yuan. Cross-modal common representation learning\\nby hybrid transfer network. In Proceedings of IJCAI, 2017.\\n25. Zhaoyin Jia, Andrew Gallagher, Ashutosh Saxena, and Tsuhan Chen. 3d-based reasoning\\nwith blocks, support, and stability. In Proceedings of ICCV, 2013.\\n26. Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization\\nnetworks for dense captioning. In Proceedings of CVPR, 2016.\\n27. Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image\\ndescriptions. In Proceedings of CVPR, 2015.\\n28. Andrej Karpathy, Armand Joulin, and Fei Fei F Li. Deep fragment embeddings for bidirec-\\ntional image sentence mapping. In Proceedings of NeurIPS, 2014.\\n29. Yoon Kim. Convolutional neural networks for sentence classi\\xef\\xac\\x81cation. In Proceedings of\\nEMNLP, 2014.\\n30. Satwik Kottur, Ramakrishna Vedantam, Jos\\xc3\\xa9 MF Moura, and Devi Parikh. Visual word2vec\\n(vis-w2v): Learning visually grounded word embeddings using abstract scenes. In Proceed-\\nings of CVPR, 2016.\\n31. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz,\\nStephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and\\nLi Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image\\nannotations. International Journal of Computer Vision, 123(1):32\\xe2\\x80\\x9373, 2017.\\n32. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classi\\xef\\xac\\x81cation with deep\\nconvolutional neural networks. In Proceedings of NeurIPS, 2012.\\n33. Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and\\nTamara L Berg. Baby talk: Understanding and generating image descriptions. In Proceedings\\nof CVPR, 2011.\\n34. Polina Kuznetsova, Vicente Ordonez, Alexander C Berg, Tamara L Berg, and Yejin Choi.\\nCollective generation of natural image descriptions. In Proceedings of ACL, 2012.\\n35. Polina Kuznetsova, Vicente Ordonez, Tamara L Berg, and Yejin Choi. Treetalk: Composi-\\ntion and compression of trees for image descriptions. Transactions of the Association for\\nComputational Linguistics, 2(10):351\\xe2\\x80\\x93362, 2014.\\n36. Quoc V Le and Tomas Mikolov. Distributed representations of sentences and documents. In\\nProceedings of ICML, 2014.\\n37. Yikang Li, Wanli Ouyang, Bolei Zhou, Kun Wang, and Xiaogang Wang. Scene graph gener-\\nation from objects, phrases and region captions. In Proceedings of ICCV, 2017.\\n38. Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summarization\\nBranches Out, 2004.\\n39. Xiao Lin and Devi Parikh. Don\\xe2\\x80\\x99t just listen, use your imagination: Leveraging visual common\\nsense for non-visual tasks. In Proceedings of CVPR, 2015.\\n40. Chenxi Liu, Junhua Mao, Fei Sha, and Alan L. Yuille. Attention correctness in neural image\\ncaptioning. In Proceedings of AAAI, 2017.\\n41. Cewu Lu, Ranjay Krishna, Michael Bernstein, and Li Fei-Fei. Visual relationship detection\\nwith language priors. In Proceedings of ECCV, 2016.\\n42. Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visi-\\nolinguistic representations for vision-and-language tasks. In Proceedings of NeurIPS, 2019.\\n43. Jiasen Lu, Caiming Xiong, Devi Parikh, and Richard Socher. Knowing when to look: Adaptive\\nattention via a visual sentinel for image captioning. In Proceedings of CVPR, 2017.\\n44. Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-\\nattention for visual question answering. In Proceedings of NeurIPS, 2016.\\n45. Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In Proceedings of\\nCVPR, 2018.\\n46. Mateusz Malinowski and Mario Fritz. A multi-world approach to question answering about\\nreal-world scenes based on uncertain input. In Proceedings of NeurIPS, 2014.\\n316\\n9\\nCross-Modal Representation\\n47. Jiayuan Mao, Yuan Yao, Stefan Heinrich, Tobias Hinz, Cornelius Weber, Stefan Wermter,\\nZhiyuan Liu, and Maosong Sun. Bootstrapping knowledge graphs from images and text.\\nFrontiers in Neurorobotics, 13:93, 2019.\\n48. Harry McGurk and John MacDonald. Hearing lips and seeing voices. Nature, 1976.\\n49. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\\xef\\xac\\x81cient estimation of word\\nrepresentations in vector space. In Proceedings of ICLR, 2013.\\n50. Yasuhide Mori, Hironobu Takahashi, and Ryuichi Oka. Image-to-word transformation based\\non dividing and vector quantizing images with words. In Proceedings of WMISR, 1999.\\n51. Jonghwan Mun, Minsu Cho, and Bohyung Han. Text-guided attention model for image cap-\\ntioning. In Proceedings of AAAI, 2017.\\n52. Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng.\\nMultimodal deep learning. In Proceedings of ICML, 2011.\\n53. Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea\\nFrome, Greg S Corrado, and Jeffrey Dean. Zero-shot learning by convex combination of\\nsemantic embeddings. In Proceedings of ICLR, 2014.\\n54. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\\nevaluation of machine translation. In Proceedings of ACL, 2002.\\n55. Yuxin Peng, Xin Huang, and Yunzhen Zhao. An overview of cross-media retrieval: Concepts,\\nmethodologies, benchmarks and challenges. IEEE Transactions on Circuits and Systems for\\nVideo Technology, 2017.\\n56. Mengye Ren, Ryan Kiros, and Richard Zemel. Exploring models and data for image question\\nanswering. In Proceedings of NeurIPS, 2015.\\n57. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\\nobject detection with region proposal networks. In Proceedings of NeurIPS, 2015.\\n58. Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time\\nobject detection with region proposal networks. In Proceedings of NeurIPS, 2015.\\n59. Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jarret Ross, and Vaibhava Goel. Self-\\ncritical sequence training for image captioning. In Proceedings of CVPR, 2017.\\n60. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\\nHuang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-\\nFei Li. Imagenet large scale visual recognition challenge. International Journal of Computer\\nVision, 115(3):211\\xe2\\x80\\x93252, 2015.\\n61. Kevin J Shih, Saurabh Singh, and Derek Hoiem. Where to look: Focus regions for visual\\nquestion answering. In Proceedings of CVPR, 2016.\\n62. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and\\nsupport inference from rgbd images. In Proceedings of ECCV, 2012.\\n63. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\\nimage recognition. arXiv:1409.1556, 2014.\\n64. Danijel Skocaj, Ales Leonardis, and Geert-Jan M. Kruijff. Cross-Modal Learning, pp. 861\\xe2\\x80\\x93\\n864. Boston, MA, 2012.\\n65. Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. Zero-shot learning\\nthrough cross-modal transfer. In Proceedings of NeurIPS, 2013.\\n66. Charles Spence. Crossmodal correspondences: A tutorial review. Attention, Perception, &\\nPsychophysics, 73(4):971\\xe2\\x80\\x93995, 2011.\\n67. Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A\\njoint model for video and language representation learning. In Proceedings of ICCV, 2019.\\n68. Damien Teney, Lingqiao Liu, and Anton Van Den Hengel. Graph-structured representations\\nfor visual question answering. In Proceedings of CVPR, 2017.\\n69. Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based\\nimage description evaluation. In Proceedings of CVPR, 2015.\\n70. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural\\nimage caption generator. In Proceedings of CVPR, 2015.\\n71. Cheng Wang, Haojin Yang, Christian Bartz, and Christoph Meinel. Image captioning with\\ndeep bidirectional lstms. In Proceedings of ACMMM, 2016.\\nReferences\\n317\\n72. Kaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, and Liang Wang. A comprehensive survey on\\ncross-modal retrieval. arXiv:1607.06215, 2016.\\n73. Yufei Wang, Zhe Lin, Xiaohui Shen, Scott Cohen, and Garrison W. Cottrell. Skeleton key:\\nImage captioning by skeleton-attribute decomposition. In Proceedings of CVPR, 2017.\\n74. Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun, and Wei-Ying Ma.\\nUni\\xef\\xac\\x81ed visual-semantic embeddings: Bridging vision and language with structured meaning\\nrepresentations. In Proceedings of CVPR, 2019.\\n75. Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den Hengel. Ask me\\nanything: Free-form visual question answering based on knowledge from external sources.\\nIn Proceedings of CVPR, 2016.\\n76. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,\\nRich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with\\nvisual attention. In Proceedings of ICML, 2015.\\n77. Ran Xu, Jiasen Lu, Caiming Xiong, Zhi Yang, and Jason J Corso. Improving word represen-\\ntations via global visual context. In Proceedings of NeurIPS Workshop, 2014.\\n78. YezhouYang,ChingLikTeo,HalDaum\\xc3\\xa9III,andYiannisAloimonos.Corpus-guidedsentence\\ngeneration of natural images. In Proceedings of EMNLP, 2011.\\n79. Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention net-\\nworks for image question answering. In Proceedings of CVPR, 2016.\\n80. Licheng Yu, Eunbyung Park, Alexander C Berg, and Tamara L Berg. Visual madlibs: Fill in\\nthe blank description generation and question answering. In Proceedings of ICCV, 2015.\\n81. Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition:\\nVisual commonsense reasoning. In Proceedings of CVPR, 2019.\\n82. HanwangZhang,ZawlinKyaw,Shih-FuChang,andTat-SengChua.Visualtranslationembed-\\nding network for visual relation detection. In Proceedings of CVPR, 2017.\\n83. Peng Zhang, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Yin and\\nyang: Balancing and answering binary visual questions. In Proceedings of CVPR, 2016.\\n84. Bo Zheng, Yibiao Zhao, Joey Yu, Katsushi Ikeuchi, and Song-Chun Zhu. Scene understanding\\nby reasoning stability and safety. International Journal of Computer Vision, 112(2):221\\xe2\\x80\\x93238,\\n2015.\\n85. Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question\\nanswering in images. In Proceedings of CVPR, 2016.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 10\\nResources\\nAbstract Deep learning has been shown as a powerful method for a variety of\\narti\\xef\\xac\\x81cial intelligence tasks, including some critical tasks in NLP. However, training a\\ndeep neural network is usually a very time-intensive process and requires lots of code\\nto build related models. To alleviate these issues, some deep learning frameworks\\nhave been developed and released, which incorporate some existing and necessary\\narithmetic operators for neural network constructions. And these frameworks exploit\\nhardware features such as multi-core CPUs and many-core GPUs to shorten the\\ntraining time. Each framework has its advantages and disadvantages. In this chapter,\\nwe aim to exhibit features and running performance of these frameworks so that\\nusers can select an appropriate framework for their usage.\\n10.1\\nOpen-Source Frameworks for Deep Learning\\nIn this section, we will introduce several typical open-source frameworks for deep\\nlearning including Caffe, Theano, TensorFlow, Torch, PyTorch, Keras, and MXNet.\\nIn fact, as the rapid development of the deep learning community, these open-source\\nframeworks are updating every day, and therefore the information in this section\\nmay not be up to date. In fact, this section mainly focuses on introducing the special\\nfeatures of these frameworks and lets the readers have a preliminary understanding\\nof them. To know the latest features of these deep learning frameworks, please refer\\nto their of\\xef\\xac\\x81cial sites.\\n10.1.1\\nCaffe\\nCaffe1 is a well-known framework and is widely used for computer vision tasks. It\\nwas created by Yangqing Jia and developed by Berkeley AI Research (BAIR). Caffe\\nuses a layer-wise approach to make building models become easy, and it is also\\n1http://caffe.berkeleyvision.org/.\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_10\\n319\\n320\\n10\\nResources\\nconvenient to \\xef\\xac\\x81ne-tune the existing neural networks without writing too much code\\nvia its simple interfaces. The underlying designs of Caffe are for the fast construction\\nof convolutional neural networks, which make it ef\\xef\\xac\\x81cient and effective.\\nOn the other hand, as normal pictures often have a \\xef\\xac\\x81xed size, the interfaces of\\nCaffe are \\xef\\xac\\x81xed and hard to be extended. It is thus dif\\xef\\xac\\x81cult to use Caffe for other\\ntasks with a variable input length, such as text, sound, or other time-series data.\\nRecurrent neural networks are also not well supported by Caffe. Although users can\\neasily build an existing network architecture with the layer-wise framework, it is not\\n\\xef\\xac\\x82exible when dealing with big and complex networks. If users want to design a new\\nlayer, the users need to use C/C++ and CUDA for the underlying coding of the new\\nlayer.\\n10.1.2\\nTheano\\nTheano2 is the typical framework developed to use symbolic tensor graphs for model\\nspeci\\xef\\xac\\x81cation. Any neural networks or other machine-learning models can be repre-\\nsented as symbolic tensor graphs. Forward, backward, and gradient updates can be\\ncalculated based on the \\xef\\xac\\x82ow between tensors. Hence, Theano provides more \\xef\\xac\\x82exibil-\\nity than Caffe using a layer-wise approach to build models. In Caffe, to de\\xef\\xac\\x81ne a new\\nlayer that is not already in the existing repository of layers is complicated, which\\nneeds to implement its forward, backward, and gradient update functions before. In\\nTheano, you only need to use basic operators to de\\xef\\xac\\x81ne the customized layer following\\nthe order of operations.\\nTheano is a platform and is easy to con\\xef\\xac\\x81gure as compared with other frameworks.\\nAnd some high-level frameworks are built on top of Theano such as Keras, which\\nfurther makes Theano easier to use. Theano supports cross-platform con\\xef\\xac\\x81guration\\nwell, which means it works on not only Linux but also Windows. Because of this,\\nmany researchers and engineers use Theano to build their models and then release\\nthese projects. Rich open resources based on Theano attract some more users.\\nThough Theano uses Python syntax to de\\xef\\xac\\x81ne symbolic tensor graphs, its graph\\nprocessor will compile the graphs into high-performance C++ or CUDA code for\\ncomputing. Owing to this, Theano can run very fast and make programmers code\\nmode simply. Only one de\\xef\\xac\\x81ciency is that the compilation process is slow and needs\\nsome time. If a neural network does not need to be trained for several days, it is\\nnot a good idea to select Theano. Compiling too often in Theano is maddening and\\nannoying. As a comparison, the later framework like TensorFlow uses the compiled\\npackage for the symbolic tensor operations, which seems a little more relaxing.\\nTheano has some other serious disadvantages. Theano cannot support many-core\\nGPUs very well, which makes it hard to train big neural networks. Besides the\\ncompilation process, importing Theano is also slow. When you run your code in\\nTheano, you will be stuck for a long time with a precon\\xef\\xac\\x81gured device. If you want to\\nimprove and contribute to Theano itself, this will also be maddening and annoying.\\n2http://www.deeplearning.net/software/theano/.\\n10.1 Open-Source Frameworks for Deep Learning\\n321\\nIn fact, Theano is no longer maintained, but it is still worth introducing as a landmark\\nwork in the history of deep learning frameworks, which inspires many subsequent\\nframeworks.\\n10.1.3\\nTensorFlow\\nTensorFlow3 is mainly developed and used by Google based on the experience on\\nTheano and DistBelief [1]. TensorFlow and Theano are in fact quite similar to some\\nextent. Both of them allow building a symbolic graph of the neural network archi-\\ntecture via the Python interface. Different from Theano, TensorFlow allows imple-\\nmenting new operations or machine-learning algorithms using C/C++ and Java. With\\nbuilding symbolic graphs, the auto-gradient can be easily used to train complicated\\nmodels. Hence, TensorFlow is more than a deep learning framework. Its \\xef\\xac\\x82exibil-\\nity enables it to solve various complex computing problems such as reinforcement\\nlearning.\\nIn TensorFlow, both code development and deployment is fast and convenient.\\nTrained models can be deployed quickly on a variety of devices, including servers\\nand mobile devices, without the need to implement a separate model setting code\\nor load Python/LuaJIT interpreter. Caffe also allows easy deployment of models.\\nHowever, Caffe has trouble running on devices without a GPU, which is a prevalent\\nsituation of smartphones. TensorFlow supports model decoding using ARM/NEON\\ninstructions and does not need too many operations to choose training devices.\\nTensorBoard of TensorFlow provides a platform for visualization of the model\\narchitectures, which is beautiful and also useful. By visualizing the symbolic graph, it\\nis not dif\\xef\\xac\\x81cult to \\xef\\xac\\x81nd bugs in the source code. To debug models on other deep learning\\nframeworks is relatively bothering. TensorBoard can also log and generate real-time\\nvisualization of variables during training, which is a pleasant way to monitor the\\ntraining process.\\nThough customizing operations in TensorFlow is convenient, it usually changes a\\nlot of function interfaces in every new release which is challenging for developers to\\nkeep their code compatible with different TensorFlow versions. And mastering Ten-\\nsorFlow is also not easy. As TensorFlow 2.0 has been released recently, TensorFlow\\nmay gradually handle these issues in the predictable future.\\n10.1.4\\nTorch\\nTorch4 is a computational framework mainly developed and used by Facebook and\\nTwitter. Torch provides an API written in Lua to support the implementation of some\\n3https://www.tensor\\xef\\xac\\x82ow.org/.\\n4http://torch.ch/.\\n322\\n10\\nResources\\nmachine-learning algorithms, especially convolutional neural networks. A temporal\\nconvolutional layer implemented by Torch can have a variable input length, which\\nis extremely useful for NLP tasks and not designed in Theano and TensorFlow.\\nTorch also contains the 3D convolutional layer, which can be easily used in video\\nrecognition tasks. Besides its various \\xef\\xac\\x82exible convolutional layers, Torch is light and\\nspeedy. The above reasons attract lots of researchers in universities and companies\\nto customize their own deep learning platforms.\\nHowever, the negative aspects of Torch are also apparent. Though Torch is pow-\\nerful, it is not designed to be widely accessible to the Python-based academic com-\\nmunity. And there are not any other interfaces but Lua. Lua is a multi-paradigm\\nscripting language, which was developed in Brazil in the early 1990s and is not a\\npopular mainstream programming language. Hence, it needs some time to learn Lua\\nbefore you use Torch to construct models. Different from convolutional neural net-\\nworks, there is no of\\xef\\xac\\x81cial support for recurrent neural networks. There are some open\\nresources about recurrent neural networks implemented by Torch, but they are not yet\\nintegrated to the main repository. And it is dif\\xef\\xac\\x81cult to distinguish the effectiveness\\nof these implementations.\\nSimilar to Caffe, Torch is not a framework based on symbolic tensor graphs, it\\nalso uses the layer-wise approach. This means that your models in Torch are a graph\\nof layers and not a graph of mathematical functions. The mechanism is convenient\\nto build a network whose layers are stable and hierarchical. If you want to design a\\nnew connection layer or change an existing neural model, you need lots of code to\\nimplement new layers with full forward, backward, and gradient update functions.\\nHowever, those frameworks based on symbolic tensor graphs, such as Theano and\\nTensorFlow, give more \\xef\\xac\\x82exibility to do this. In fact, these issues are handled as\\nPyTorch has been released, which we will introduce then.\\n10.1.5\\nPyTorch\\nPyTorch5 is a Python package built over Torch, developed by Facebook and other\\ncompanies. However, it is not just an interface, and PyTorch has amounts of improve-\\nments over Torch. The most important one is that PyTorch can use a symbolic graph\\nto de\\xef\\xac\\x81ne neural networks, and then use automatic differentiation following the graph\\nto automate the computation of backward passes in neural networks. Meanwhile,\\nPyTorch maintains some characteristics of the layer-wise approach in Torch, which\\nmeans coding with PyTorch is easy. Moreover, PyTorch has minimal framework over-\\nhead and custom memory allocators for the GPU, which means PyTorch is faster and\\nmemory-ef\\xef\\xac\\x81cient than Torch.\\n5http://pytorch.org/.\\n10.1 Open-Source Frameworks for Deep Learning\\n323\\nCompared with other deep learning frameworks, PyTorch has two main advan-\\ntages. First, most frameworks like TensorFlow are based on static computational\\ngraphs (de\\xef\\xac\\x81ne-and-run), while PyTorch uses dynamic computational graphs (de\\xef\\xac\\x81ne-\\nby-run). What it means is that with dynamic computational graphs, you can change\\nthe network architecture based on the data \\xef\\xac\\x82owing through the network. There is a\\nway to do something similar in TensorFlow, but your static computational graphs\\nmust contain all possible branches in advance, which will limit the performance.\\nSecond, PyTorch is built to be deeply integrated into Python and has a seamless\\nconnection with other popular Python packages, such as Numpy, Spicy, and Cython.\\nThus, it is easy to extend your model when needed.\\nAfter Facebook released it, PyTorch has drawn considerable attention from the\\ndeep learning community, and many past Torch users switch to this new package. For\\nnow, PyTorch already has a thriving community which contributes to its increasing\\npopularity among researchers. It is no exaggeration to say that PyTorch is one of the\\nmost popular frameworks at present.\\n10.1.6\\nKeras\\nKeras6 is a top-design deep learning framework that is based on Theano and Tensor-\\nFlow. Interestingly, Keras sits atop Theano and TensorFlow, however, its interfaces\\nare similar to Torch. To use Keras needs Python code, and there are lots of detailed\\ndocuments and examples for a quick start. There is also a very active community\\nof developers, and they make Keras fastly updated. Hence, it is a very fast-growing\\nframework.\\nBecause Theano and TensorFlow are the backends of Keras, disadvantages of\\nKeras are most similar to Theano and TensorFlow. With TensorFlow as the backend,\\nit will run even slower than the pure TensorFlow code. Because it is a high-level\\nframework, to customize a new neural layer is not easy, though you can easily use\\nexisting layers under Keras. The package is too advanced, and it hides too many\\ntraining parameters. You cannot touch and change all details of your own models\\nunless you use Theano, TensorFlow, or PyTorch.\\n10.1.7\\nMXNet\\nMXNet7 is an effective and ef\\xef\\xac\\x81cient open-source machine-learning framework,\\nmainly pushed by Amazon. It supports APIs with multiple languages, including\\nC++, Python, R, Scala, Julia, Perl, MATLAB, and JavaScript, some of which can be\\nadopted for Amazon Web Services. Some interfaces of MXNet are also reserved for\\n6https://keras.io/.\\n7http://mxnet.io/.\\n324\\n10\\nResources\\nfuture mobile devices, just like TensorFlow. MXNet is built on a dynamic dependency\\nscheduler that automatically parallelizes both symbolic and imperative operations on\\nthe \\xef\\xac\\x82y. A graph optimization layer on top of that makes symbolic execution fast and\\nmemory ef\\xef\\xac\\x81cient. The MXNet library is portable and lightweight, and it scales to\\nmultiple GPUs and multiple machines. The main problem of MXNet is the lack of\\ndetailed and well-organized documentation. The user groups are also smaller than\\nother frameworks, especially as compared with TensorFlow and PyTorch. It is more\\nchallenging to grasp MXNet for newbies. The MXNet is developing fastly, and these\\nproblems may be solved in the future.\\n10.2\\nOpen Resources for Word Representation\\n10.2.1\\nWord2Vec\\nWord2vec8 is a widely used toolkit for word representation learning, which provides\\nan effective and ef\\xef\\xac\\x81cient implementation of the continuous bag-of-words and Skip-\\ngram architectures. The word representations learned by Word2vec can be used in\\nmany natural language processing \\xef\\xac\\x81elds. Empirically, To use pretrained word vectors\\nas the model inputs can be a good way to enhance model performances.\\nWord2vec takes free text corpus as input and constructs the vocabulary list from\\nthe training data. Then it uses simple predictive models based on neural networks\\nto learn the language model, which encode the co-occurrence information between\\nwords into the resulting word representations.\\nTheresultingrepresentationsshowcaseinterestinglinearsubstructuresoftheword\\nvector space. The Euclidean distance (or cosine similarity) between two-word vectors\\nprovides an effective method for measuring the linguistic or semantic similarity of\\nthe corresponding words. Sometimes, the nearest neighbors, according to this metric,\\nreveal rare but relevant words that lie outside an average human\\xe2\\x80\\x99s vocabulary.\\nWords frequently appearing together in the text will have representations with\\nclose distance within the embedding space. Word2vec also provides a tool to \\xef\\xac\\x81nd the\\nclosest words for a user-speci\\xef\\xac\\x81ed word via the learned representations and distances\\nbetween representation embeddings.\\n10.2.2\\nGloVe\\nGloVe9 is a widely used toolkit, which supports an unsupervised learning method for\\nword representation learning. Similar to Word2vec, GloVe also trains on text corpus\\n8https://code.google.com/archive/p/word2vec/.\\n9https://nlp.stanford.edu/projects/glove/.\\n10.2 Open Resources for Word Representation\\n325\\nand captures the aggregated global word-word co-occurrence information for word\\nembeddings. However, GloVe uses count-based models instead of predictive models,\\nwhich are different from Word2vec.\\nThe GloVe model \\xef\\xac\\x81rst builds a global word-word co-occurrence matrix, which\\ncan show how frequently words co-occur with one another in a given text. Then\\nword representations are trained on the nonzero entries of the matrix. To construct\\nthis matrix requires the entire corpus traversal for the statistics collection. For large\\ncorpora, this pass can be computationally expensive, but it is a one-time up-front\\ncost. Subsequent training iterations are much faster because the number of nonzero\\nmatrix entries is typically much smaller than the total number of words in the corpus.\\n10.3\\nOpen Resources for Knowledge Graph Representation\\n10.3.1\\nOpenKE\\nOpenKE10 [2] is an open-source toolkit for Knowledge Embedding (KE), which pro-\\nvides a uni\\xef\\xac\\x81ed framework and various fundamental KE models. OpenKE prioritizes\\noperational ef\\xef\\xac\\x81ciency to support quick model validation and large-scale knowledge\\nrepresentation learning. Meanwhile, OpenKE maintains suf\\xef\\xac\\x81cient modularity and\\nextensibility to incorporate new models easily. Besides the toolkit, the embeddings\\nof some existing large-scale knowledge graphs pretrained by OpenKE are also avail-\\nable. The toolkit, documentation, and pretrained embeddings are all released on\\nhttp://openke.thunlp.org/.\\nAs compared to other implementations, OpenKE has \\xef\\xac\\x81ve advantages. First,\\nOpenKE has implemented nine classical knowledge embedding algorithms, includ-\\ning RESCAL, TransE, TransH, TransR, TransD, ComplEx, DistMult, HolE, and\\nAnalogy, which are veri\\xef\\xac\\x81ed effective and stable. Second, OpenKE shows high per-\\nformance due to memory optimization, multi-threading acceleration, and GPU learn-\\ning. OpenKE supports multiple computing devices and provides interfaces to control\\nCPU/GPU modes. Third, system encapsulation makes OpenKE easy to train and test\\nKE models. Users just need to set hyperparameters via interfaces of the platform to\\nconstruct KE models. Fourth, it is easy to construct new KE models. All speci\\xef\\xac\\x81c\\nmodels are implemented by inheriting the base class by designing their own scoring\\nfunctions and loss functions. Fifth, besides the toolkit, OpenKE also provides the\\nembeddings of some existing large-scale knowledge graphs pretrained by OpenKE,\\nwhich can be directly applied for many applications, including information retrieval,\\npersonalized recommendation, and question answering.\\n10https://github.com/thunlp/OpenKE.\\n326\\n10\\nResources\\n10.3.2\\nScikit-Kge\\nScikit-kge11 is an open-source Python library for knowledge representation learn-\\ning. The library supports different building blocks to train and develop models for\\nknowledge graph embeddings. The primary purpose of Scikit-kge is to compute the\\nembeddings of knowledge graphs for the method HolE; meanwhile, it also provides\\nsome other methods. Besides HolE, RESCAL, TransE, TransR, and ER-MLP can\\nalso be trained in Scikit-kge. The library contains some parameter update methods,\\nnot only the basic SGD but also AdaGrad. It also implements different negative\\nsampling strategies to select negative samples.\\n10.4\\nOpen Resources for Network Representation\\n10.4.1\\nOpenNE\\nOpenNE12 is an open-source standard NE/NRL (Network Representation Learning)\\ntraining and testing framework. It uni\\xef\\xac\\x81es the input and output interfaces of different\\nNE models and provides scalable options for each model. Moreover, typical NE\\nmodels under this framework are based on TensorFlow, which enables these models\\nto be trained with GPUs. The implemented or modi\\xef\\xac\\x81ed models include DeepWalk,\\nLINE, node2vec, GraRep, TADW, GCN, HOPE, GF, SDNE, and LE. The framework\\nalso provides classi\\xef\\xac\\x81cation and embedding visualization modules for evaluating the\\nresult of NRL.\\n10.4.2\\nGEM\\nGEM (Graph Embedding Methods)13 is a Python package that offers a general frame-\\nwork for graph embedding methods. It implements many state-of-the-art embedding\\ntechniques including Locally Linear Embedding, Laplacian Eigenmaps, Graph Fac-\\ntorization, High-Order Proximity preserved Embedding (HOPE), Structural Deep\\nNetwork Embedding (SDNE), and node2vec. Furthermore, the framework imple-\\nments several functions to evaluate the quality of the obtained embeddings including\\ngraph reconstruction, link prediction, visualization, and node classi\\xef\\xac\\x81cation. For faster\\nexecution, C++ backend is integrated using Boost for supported methods.\\n11https://github.com/mnick/scikit-kge.\\n12https://github.com/thunlp/OpenNE.\\n13https://github.com/palash1992/GEM.\\n10.4 Open Resources for Network Representation\\n327\\n10.4.3\\nGraphVite\\nGraphVite14 is a general and high-performance graph embedding system for various\\napplications including node embedding, knowledge graph embedding, and graph\\nhigh-dimensional data visualization.\\nGraphVite provides a complete pipeline for users to implement and evaluate graph\\nembedding models. For reproducibility, the system integrates several commonly\\nused models and benchmarks, and you can also develop your own models with the\\n\\xef\\xac\\x82exible interface. Additionally, for semantic tasks, GraphVite releases a bunch of\\npretrained knowledge graph embedding models to enhance language understanding.\\nThere are two core advantages of GraphVite over other toolkits: fast and large-scale\\ntraining. GraphVite accelerates graph embedding with multiple CPUs and GPUs.\\nIt takes around one minute to learn node embeddings for graphs with one million\\nnodes. Moreover, GraphVite is designed to be scalable. Even with limited memory,\\nGraphVite can process node embedding task on billion-scale graphs.\\n10.4.4\\nCogDL\\nCogDL15 is another graph representation learning toolkit that allows researchers\\nand developers to easily train and evaluate baseline or custom models for node\\nclassi\\xef\\xac\\x81cation, link prediction, and other tasks on graphs. It provides implementations\\nof many popular models, including non-GNN models and GNN-based ones.\\nCogDL bene\\xef\\xac\\x81ts from several unique techniques. First, utilizing sparse matrix\\noperation, CogDL is capable of performing fast network embedding on large-scale\\nnetworks. Second, CogDL has the ability to deal with different types of graph struc-\\ntures attributed, multiplex, and heterogeneous networks. Third, CogDL supports\\nparallel training. With different seeds and different models, CogDL performs train-\\ning on multiple GPUs and reports the result table automatically. Finally, CogDL is\\nextendable. New datasets, models, and tasks can be added without dif\\xef\\xac\\x81culty.\\n10.5\\nOpen Resources for Relation Extraction\\n10.5.1\\nOpenNRE\\nOpenNRE16 [3] is an open-source framework for neural relation extraction, which\\naims to easily build relation extraction (RE) models.\\n14https://graphvite.io/.\\n15http://keg.cs.tsinghua.edu.cn/cogdl/index.html.\\n16https://github.com/thunlp/OpenNRE.\\n328\\n10\\nResources\\nCompared with other implementations, OpenNRE has four advantages. First,\\nOpenNRE has implemented various state-of-the-art RE models, including atten-\\ntion mechanism, adversarial learning, and reinforcement learning. Second, Open-\\nNRE enjoys great system encapsulation. It divides the pipeline of relation extraction\\ninto four parts, namely, embedding, encoder, selector (for distant supervision), and\\nclassi\\xef\\xac\\x81er. For each part, it has implemented several methods. System encapsulation\\nmakes it easy to train and test models by changing hyperparameters or appoint model\\narchitectures by using Python arguments. Third, OpenNRE is extendable. Users can\\nconstruct new RE models by choosing speci\\xef\\xac\\x81c blocks provided in four parts as men-\\ntioned above and combining them freely, with only a few lines of codes. Fourth, the\\nframework has implemented multi-GPU learning, which is ef\\xef\\xac\\x81cient.\\nReferences\\n1. Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc\\xe2\\x80\\x99aurelio\\nRanzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. In\\nAdvances in neural information processing systems, pages 1223\\xe2\\x80\\x931231, 2012.\\n2. Xu Han, Shulin Cao, Xin Lv, Yankai Lin, Zhiyuan Liu, Maosong Sun, and Juanzi Li. OpenKE:\\nAn open toolkit for knowledge embedding. In Proceedings of EMNLP: System Demonstrations,\\npages 139\\xe2\\x80\\x93144, 2018.\\n3. Xu Han, Tianyu Gao, Yuan Yao, Deming Ye, Zhiyuan Liu, and Maosong Sun. OpenNRE: An\\nopen and extensible toolkit for neural relation extraction. In Proceedings of EMNLP: System\\nDemonstrations, pages 169\\xe2\\x80\\x93174, 2019.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nChapter 11\\nOutlook\\nAbstract The aforementioned representation learning models and methods have\\nshowntheireffectivenessinvariousNLPscenariosandtasks.Withtherapidgrowthof\\ndatascalesandthedevelopmentofcomputationdevices,therearealsonewchallenges\\nand opportunities for next-stage researches of deep learning techniques. In the last\\nchapter, we will look into the future directions of representation learning techniques\\nfor NLP. To be more speci\\xef\\xac\\x81c, we will consider the following directions including\\nusing more unsupervised data, utilizing few labeled data, employing deeper neural\\narchitectures, improving model interpretability and fusing the advantages of other\\nareas.\\n11.1\\nIntroduction\\nWe have used ten chapters to introduce the advances of representation learning\\nfor NLP, covering both multi-grained language entries including words, phrases,\\nsentences and documents, and closely related objects including world knowledge,\\nsememe knowledge, networks, and cross-modal data. Those mentioned models and\\nmethods of representation learning for NLP have shown their effectiveness in various\\nNLP scenarios and tasks.\\nAs shown by the unsatisfactory performance of most NLP systems in open\\ndomains, and recent great advances of pre-trained language models, representation\\nlearning for NLP is far from perfect. With the rapid growth of data scales and the\\ndevelopment of computation devices, we are facing new challenges and opportunities\\nfor next-stage researches of representation learning and deep learning techniques.\\nIn this last chapter, we will look into the future research and exploration directions\\nof representation learning techniques for NLP. Since we have summarized the future\\nwork of each individual part in the summary section of each previous chapter, here\\nwe focus on discussing the general and important issues that should be addressed by\\nrepresentation learning for NLP.\\n\\xc2\\xa9 The Author(s) 2020\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_11\\n329\\n330\\n11\\nOutlook\\nFor general representation learning for NLP, we conclude the following direc-\\ntions, including using more unsupervised data, utilizing a few labeled data, employ-\\ning deeper neural architectures, improving model interpretability, and fusing the\\nadvances from other areas.\\n11.2\\nUsing More Unsupervised Data\\nThe rapid development of Internet technology and the popularization of information\\ndigitization have brought massive text data for NLP researches and applications.\\nFor example, the whole corpus of Wikipedia already contains more than 50 million\\narticles (including 6 million articles in English)1 and is growing rapidly every day\\ncontributed by collaborative work all over the world. The amount of user-generated\\ncontent onmanysocial platforms suchas Twitter, Weibo, andFacebookalsoincreases\\nquicklybybillionsofusers.Itisworthconsideringthesemassivetextdataforlearning\\nbetter NLP models. However, due to the expensive cost of expert annotations, it is\\nimpossible to label such massive amounts of data for speci\\xef\\xac\\x81c NLP tasks.\\nHence, an essential direction of NLP is how to take better advantages of unla-\\nbeled data for ef\\xef\\xac\\x81cient unsupervised representation learning. Though without labeled\\nannotations, unsupervised data can help initialize the randomized neural network\\nparameters and thus improve the performances of those downstream NLP tasks.\\nThis line of work usually employs a pipeline strategy: \\xef\\xac\\x81rst, pretrain the model\\nparameters and then \\xef\\xac\\x81ne-tune these parameters in speci\\xef\\xac\\x81c downstream NLP tasks.\\nRecurrent language model [7], word embeddings [6], and pre-trained language mod-\\nels (PLM) such as BERT [3], all utilize unsupervised plain text to pretrain neural\\nparameters and then bene\\xef\\xac\\x81t downstream supervised tasks via \\xef\\xac\\x81ne-tuning.\\nCurrent state-of-the-art PLM models still can only learn from limited plain text\\ndue to limited learning ef\\xef\\xac\\x81ciency and computation power. Moreover, there are various\\ntypes of large-scale data online with abundant informative signals and labels, such as\\nHTMLtags, anchor text, keywords, document meta-information, andother structured\\nand semi-structured data. How to take full advantage of the large-scale Web text data\\nhas not been extensively studied. In the future, with better computation devices (e.g.,\\nGPUs) and data resources, we are expected to develop more advanced methods to\\nutilize more unsupervised data.\\n11.3\\nUtilizing Fewer Labeled Data\\nAs NLP technologies become more powerful, people can explore more complicated\\nand \\xef\\xac\\x81ne-grained problems. Taking text classi\\xef\\xac\\x81cation as an example, early work tar-\\ngeted on \\xef\\xac\\x82at classi\\xef\\xac\\x81cation with limited categories, and now researchers are more\\n1http://en.wikipedia.org/wiki.\\n11.3 Utilizing Fewer Labeled Data\\n331\\ninterested in classi\\xef\\xac\\x81cation with hierarchical structure and a large number of classes.\\nHowever, when a problem gets more complicated, it requires more knowledge from\\nexperts to annotate training instances for \\xef\\xac\\x81ne-grained tasks and increases the cost of\\ndata labeling.\\nTherefore, we expect the models or systems can be developed ef\\xef\\xac\\x81ciently with\\n(very) few labeled data. When each class has only one or a few labeled instances, the\\nproblem becomes a one/few-shot learning problem. The few-shot learning problem is\\nderivedfromcomputervisionandhasalsobeenstudiedinNLPrecently.Forexample,\\nresearchers have explored few-shot relation extraction [5] where each relation has a\\nfew labeled instances, and low-resource machine translation [11] where the size of\\nthe parallel corpus is limited.\\nA promising approach to few-shot learning is to compare the semantic similarity\\nbetween the test instance and those labeled ones (i.e., the support set), and then make\\nthe prediction. The idea is similar to k-nearest neighbor classi\\xef\\xac\\x81cation (kNN) [10].\\nSince the key is to represent the semantic meanings of each instance for measuring\\ntheir semantic similarity, it has been veri\\xef\\xac\\x81ed that language models pretrained on\\nunsupervised data and \\xef\\xac\\x81ne-tuned on the target few-shot domain are very effective\\nfor few-shot learning.\\nAnother approach to few-shot learning is to transfer the models from some related\\ndomains into the target domain with the few-shot problem [2]. This is usually named\\nas transfer learning or domain adaptation. For these methods, representation learning\\ncan also help the transfer or adaptation process by learning joint representations of\\nboth domains.\\nIn the future, one may go beyond the abovementioned frameworks and design\\nmore appropriate methods according to the characteristics of NLP tasks and prob-\\nlems. The goal is to develop effective NLP methods with as less annotated data in the\\ntarget domain as possible, by better utilizing unsupervised data that are much cheaper\\nto get from the Web and existing supervised data from other domains. The explo-\\nration of the few-shot learning problem in NLP will help us develop data-ef\\xef\\xac\\x81cient\\nmethods for language learning.\\n11.4\\nEmploying Deeper Neural Architectures\\nAs the amount of available text data rapidly increases, the size of the training corpus\\nfor NLP tasks grows as well. With more training data, a natural way to boost model\\nperformances is to employ deeper neural architectures for modeling. Intuitively,\\ndeeper neural models that have more sophisticated architecture and parameters can\\nbetter \\xef\\xac\\x81t the increasing data. Another motivation for using deeper architectures for\\nmodeling comes from the development of computation devices (e.g., GPUs). Cur-\\nrent state-of-the-art methods are usually a compromise between ef\\xef\\xac\\x81ciency and effec-\\ntiveness. As the computation devices operate faster, the time/space complexities of\\ncomplicated models become acceptable, which motivate researchers to design more\\n332\\n11\\nOutlook\\ncomplex but effective models. To summarize, employing deeper neural architectures\\nwould be one of the de\\xef\\xac\\x81nite orientations for representation learning in NLP.\\nVery deep neural network architectures have been widely used in computer vision.\\nFor example, the well-known VGG [8] network which was proposed in the famous\\nImageNet contest has 16 layers of convolutional and fully connected layers. In NLP,\\nthe depths of neural architectures were relatively shallow until the Transformer [9]\\nstructure was proposed. Speci\\xef\\xac\\x81cally, as compared with word embedding [6] which is\\nbased on shallow models, the state-of-the-art pre-trained language model BERT [3]\\ncan be regarded as a giant model that stacks 12 self-attention layers and each layer has\\n8 attention heads. BERT has demonstrated its effectiveness in a number of NLP tasks.\\nBesides the well-designed model architecture and training objectives, the success of\\nBERT also bene\\xef\\xac\\x81ts from TPUs which is one of the most powerful devices for parallel\\ncomputations. In contrast, it may take months or years for a single CPU to \\xef\\xac\\x81nish the\\ntraining process of BERT. When these computation devices go popular, we can expect\\nmore deep neural architectures to be developed for NLP as well.\\n11.5\\nImproving Model Interpretability\\nModel transparency and interpretability are hot topics in arti\\xef\\xac\\x81cial intelligence and\\nmachine learning. Human interpretable predictions are very important for decision-\\ncritical applications related to ethics, privacy, and safety. However, neural network\\nmodels or deep learning techniques are short of model transparency for human inter-\\npretable predictions and thus are often treated as black boxes.\\nMost NLP techniques based on neural networks and distributed representation are\\nalso hard to be interpreted except for the attention mechanism where the attention\\nweights can be interpreted as the importance of corresponding inputs. For the sake of\\nemploying representation learning techniques for decision-critical applications, there\\nis a need to improve model interpretability and transparency of current representation\\nlearning and neural network models.\\nA recent survey [1] classi\\xef\\xac\\x81es interpretable machine learning methods into two\\nmaincategories:interpretablemodelsandpost-hocexplainabilitytechniques.Models\\nthat are understandable by themselves are called interpretable models. For example,\\nlinear models, decision trees, and rule-based systems are such transparent models.\\nHowever, in most cases, we have to probe into the model by a second one for expla-\\nnations, namely post-hoc explainability techniques. In NLP, there have been some\\nresearches to visualize neural models such as neural machine translation [4] for\\ninterpretable explanations. However, the understanding of most neural-based mod-\\nels remains unsolved. We are looking forward to more studies on improving model\\ninterpretability to facilitate the extensive use of representation learning methods for\\nNLP.\\n11.6 Fusing the Advances from Other Areas\\n333\\n11.6\\nFusing the Advances from Other Areas\\nDuring the development of deep learning techniques, mutual learning between dif-\\nferent research areas has never stopped.\\nFor example, Word2vec aims to learn word embeddings from large-scale text cor-\\npus published in 2013 and can be regarded as a milestone of representation learning\\nfor NLP. In 2014, the idea of Word2vec was adopted for learning node embeddings in\\na network/graph by treating random walks over the network as sentences, named as\\nDeepWalk; the analogical reasoning phenomenon learned by Word2vec, i.e., king \\xe2\\x88\\x92\\nman = queen \\xe2\\x88\\x92woman also inspired the representation learning of world knowledge,\\nnamed as TransE. Meanwhile, graph convolutional networks were \\xef\\xac\\x81rst proposed for\\nsemi-supervised graph learning in 2016, and have been widely applied on many NLP\\ntasks such as relation extraction and text classi\\xef\\xac\\x81cation recently. Another example is\\nthe Transformer model which was proposed for neural machine translation at \\xef\\xac\\x81rst\\nand then transferred to computer vision, data mining, and many other areas.\\nThe fusion also appears between two quite distant disciplines. We should recall\\nagain that, the idea of distributed representation proposed in the 1980s is inspired by\\ntheneuralcomputationschemeofhumansandotheranimals.Ittakesabout40yearsto\\nsee the development of distributed representation and deep learning come to fruition.\\nIn fact, many ideas such as convolution in CNN and the attention mechanism are\\ninspired by the computation scheme of human cognition.\\nTherefore, an intriguing direction of representation learning for NLP is to fuse\\nthe advances from other areas, including not only those closely related areas in AI\\nsuch as machine learning, computer vision, and data mining, but also those distant\\nareas to some extent such as linguistics, brain science, psychology, and sociology.\\nThis line of work requires researchers to have suf\\xef\\xac\\x81cient knowledge of other \\xef\\xac\\x81elds.\\nReferences\\n1. Alejandro Barredo Arrieta, Natalia D\\xc3\\xadaz-Rodr\\xc3\\xadguez, Javier Del Ser, Adrien Bennetot, Siham\\nTabik, Alberto Barbado, Salvador Garc\\xc3\\xada, Sergio Gil-L\\xc3\\xb3pez, Daniel Molina, Richard Ben-\\njamins, et al. Explainable arti\\xef\\xac\\x81cial intelligence (xai): Concepts, taxonomies, opportunities and\\nchallenges toward responsible ai. Information Fusion, 58:82\\xe2\\x80\\x93115, 2020.\\n2. Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A\\ncloser look at few-shot classi\\xef\\xac\\x81cation. In Proceedings of ICLR, 2019.\\n3. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\\ndeep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019.\\n4. Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. Visualizing and understanding\\nneural machine translation. In Proceedings of ACL, 2017.\\n5. Xu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao, Zhiyuan Liu, and Maosong Sun.\\nFewRel: A large-scale supervised few-shot relation classi\\xef\\xac\\x81cation dataset with state-of-the-art\\nevaluation. In Proceedings of EMNLP, 2018.\\n6. T Mikolov and J Dean. Distributed representations of words and phrases and their composi-\\ntionality. Proceedings of NeurIPS, 2013.\\n7. Tomas Mikolov, Martin Kara\\xef\\xac\\x81\\xc3\\xa1t, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recur-\\nrent neural network based language model. In Proceedings of InterSpeech, 2010.\\n334\\n11\\nOutlook\\n8. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\\nimage recognition. arXiv preprint arXiv:1409.1556, 2014.\\n9. Ashish Vaswani, Noam Shazeer, Niki Parmar, Llion Jones, Jakob Uszkoreit, Aidan N Gomez,\\nand Lukasz Kaiser. Attention is all you need. In Proceedings of NeurIPS, 2017.\\n10. Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Laurens van der Maaten. Sim-\\npleshot: Revisiting nearest-neighbor classi\\xef\\xac\\x81cation for few-shot learning. arXiv preprint\\narXiv:1911.04623, 2019.\\n11. Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. Transfer learning for low-resource\\nneural machine translation. In Proceedings of EMNLP, 2016.\\nOpen Access This chapter is licensed under the terms of the Creative Commons Attribution 4.0\\nInternational License (http://creativecommons.org/licenses/by/4.0/), which permits use, sharing,\\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate\\ncredit to the original author(s) and the source, provide a link to the Creative Commons license and\\nindicate if changes were made.\\nThe images or other third party material in this chapter are included in the chapter\\xe2\\x80\\x99s Creative\\nCommons license, unless indicated otherwise in a credit line to the material. If material is not\\nincluded in the chapter\\xe2\\x80\\x99s Creative Commons license and your intended use is not permitted by\\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from\\nthe copyright holder.\\nCorrection to: Representation Learning\\nfor Natural Language Processing\\nCorrection to:\\nZ. Liu et al., Representation Learning for Natural Language\\nProcessing, https://doi.org/10.1007/978-981-15-5573-2\\nIn the original version of the book, the following belated correction has been incor-\\nporated in both Preface and Chapter 8. In the Preface, a new reference citation has\\nbeen added in Chapter 8.\\nThe original version of Chapter 8 was revised with a new paragraph replacing the\\n\\xef\\xac\\x81rst paragraph in section 8.3.\\nThe chapter and the book have been updated with the changes.\\nThe updated version of this chapter can be found at\\nhttps://doi.org/10.1007/978-981-15-5573-2,\\nhttps://doi.org/10.1007/978-981-15-5573-2_8\\n\\xc2\\xa9 The Author(s) 2023\\nZ. Liu et al., Representation Learning for Natural Language Processing,\\nhttps://doi.org/10.1007/978-981-15-5573-2_12\\nC1\\n\\x0c'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pathname = '/home/ngoni97/Documents/Python Programming/Natural Language Processing/Representation Learning for Natural Language Processing.pdf'\n",
    "get_doc(pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240580aa-7d11-48ee-aa51-da273045366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_reader_data_collector_class import PdfDataCollector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c799bd-b7cc-4ddb-afb7-fe2393cf9953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'representation learning for natural language processing zhiyuan liu  yankai lin  maosong sunrepresentation learning for natural languageprocessingzhiyuan liu yankai lin maosong sunrepresentation learningfor natural languageprocessing123zhiyuan liutsinghua universitybeijing chinamaosong sundepartment of computerscience and technologytsinghua universitybeijing chinayankai linpattern recognition centertencent wechatbeijing chinaisbn 978 981 15 5572 5isbn 978 981 15 5573 2ebookhttpsdoiorg101007978 981 15 5573 2 the editors if applicable and the authors 2020 corrected publication 2023 this book is an openaccess publicationopen access this book is licensed under the terms of the creative commons attribution 40international license httpcreativecommonsorglicensesby40 which permits use sharing adap tation distribution and reproduction in any medium or format as long as you give appropriate credit tothe original authors and the source provide a link to the creative commons license and indicate ifchanges were madethe images or other third party material in this book are included in the books creative commonslicense unless indicated otherwise in a credit line to the material if material is not included in the bookscreative commons license and your intended use is not permitted by statutory regulation or exceeds thepermitted use you will need to obtain permission directly from the copyright holderthe use of general descriptive names registered names trademarks service marks etc in this publi cation does not imply even in the absence of a specic statement that such names are exempt from therelevant protective laws and regulations and therefore free for general usethe publisher the authors and the editors are safe to assume that the advice and information in thisbook are believed to be true and accurate at the date of publication neither the publisher nor theauthors or the editors give a warranty express or implied with respect to the material contained herein orfor any errors or omissions that may have been made the publisher remains neutral with regard tojurisdictional claims in published maps and institutional afliationsthis springer imprint is published by the registered company springer nature singapore pte ltdthe registered company address is 152 beach road 21 0104 gateway east singapore 189721singaporeprefacein traditional natural language processing nlp systems language entries such aswords and phrases are taken as distinct symbols various classic ideas and methodssuch as n gram and bag of words models were proposed and have been widelyused until now in many industrial applications all these methods take words as theminimum units for semantic representation which are either used to further esti mate the conditional probabilities of next words given previous words eg n gram or used to represent semantic meanings of text eg bag of words modelseven when people nd it is necessary to model word meanings they either man ually build some linguistic knowledge bases such as wordnet or use context wordsto represent the meaning of a target word ie distributional representation allthese semantic representation methods are still based on symbolswith the development of nlp techniques for years it is realized that manyissues in nlp are caused by the symbol based semantic representation first thesymbol based representation always suffers from the data sparsity problem takestatistical nlp methods such as n gram with large scale corpora for example dueto the intrinsic power law distribution of words the performance will decay dra matically for those few shot words even many smoothing methods have beendeveloped to calibrate the estimated probabilities about them moreover there aremultiple grained entries in natural languages from words phrases sentences todocuments it is difcult to nd a unied symbol set to represent the semanticmeanings for all of them simultaneously meanwhile in many nlp tasks it isrequired to measure semantic relatedness between these language entries at differentlevelsforexamplewehavetomeasuresemanticrelatednessbetweenwordsphrases and documents in information retrieval due to the absence of aunied scheme for semantic representation there used to be distinct approachesproposed and explored for different tasks in nlp and it sometimes makes nlpdoes not look like a compatible communityas an alternative approach to symbol based representation distributed repre sentation was originally proposed by geoffrey e hinton in a technique report in1984 the report was then included in the well known two volume book paralleldistributed processing pdp that introduced neural networks to model humanvcognition and intelligence according to this report distributed representation isinspired by the neural computation scheme of humans and other animals and theessential idea is as followseach entity is represented by a pattern of activity distributed over many computing ele ments and each computing element is involved in representing many different entitiesit means that each entity is represented by multiple neurons and each neuroninvolves in the representation of many concepts this also indicates the meaning ofdistributed in distributed representation as opposed to distributed representationpeople used to assume one neuron only represents a specic concept or object egthere exists a single neuron that will only be activated when recognizing a person orobject such as hisher grandmother well known as the grandmother cell hypothesisor local representation we can see the straightforward connection between thegrandmother cell hypothesis and symbol based representationit was about 20 years after distributed representation was proposed neuralprobabilistic language model was proposed to model natural languages by yoshuabengio in 2003 in which words are represented as low dimensional and real valuedvectors based on the idea of distributed representation however it was until 2013that a simpler and more efcient framework word2vec was proposed to learn worddistributed representations from large scale corpora we come to the popularity ofdistributed representation and neural network techniques in nlp the performanceof almost all nlp tasks has been signicantly improved with the support of thedistributed representation scheme and the deep learning methodsthis book aims to review and present the recent advances of distributed repre sentation learning for nlp including why representation learning can improvenlp how representation learning takes part in various important topics of nlpand what challenges are still not well addressed by distributed representationbook organizationthis book is organized into 11 chapters with 3 parts the rst part of the book depictskey components in nlp and how representation learning works for them in thispart chap 1 rst introduces the basics of representation learning and why it isimportant for nlp then we give a comprehensive review of representation learningtechniques on multiple grained entries in nlp including word representationchap 2 phrase representation as known as compositional semantics chap 3sentence representation chap 4 and document representation chap 5the second part presents representation learning for those components closelyrelated to nlp these components include sememe knowledge that describes thecommonsense knowledge of words as human concepts world knowledge alsoknown as knowledge graphs that organizes relational facts between entities in thereal world various network data such as social networks document networks andviprefacecross modal data that connects natural languages to other modalities such as visualdata a deep understanding of natural languages requires these complex compo nents as a rich context therefore we provide an extensive introduction to thesecomponents ie sememe knowledge representation chap 6 world knowledgerepresentation chap 7 network representation chap 8 and cross modal rep resentation chap 9in the third part we will further provide some widely used open resourcetools on representation learning techniques chap 10 and nally outlook theremaining challenges and future research directions of representation learning fornlp chap 11although the book is about representation learning for nlp those theories andalgorithms can be also applied in other related domains such as machine learningsocial network analysis semantic web information retrieval data mining andcomputational biologynote that some parts of this book are based on our previous published orpre printed papers including 1 11 in chap 2 32 in chap 3 10 5 29 inchap 4 12 7 in chap 5 17 14 24 30 6 16 2 15 in chap 6 9 8 13 2122 23 3 4 31 in chap 7 and 25 19 18 20 26 27 33 28 34 in chap 8book coverthe book cover shows an oracle bone divided into three parts corresponding tothree revolutionized stages of cognition and representation in human historythe left part shows oracle scripts the earliest known form of chinese writingcharacters used on oracle bones in the late 1200 bc it is used to represent theemergence of human languages especially writing systems we consider this as therst representation revolution for human beings about the worldthe upper right part shows the digitalized representation of information andsignals after the invention of electronic computers in the 1940s big data can beefciently represented and processed in computer programs this can be regardedas the second representation revolution for human beings about the worldthe bottom right part shows the distributed representation in articial neuralnetworks originally proposed in the 1980s as the representation basis of deeplearning it has extensively revolutionized many elds in articial intelligenceincluding natural language processing computer vision and speech recognitionever since the 2010s we consider this as the third representation revolution aboutthe world this book focuses on the theory methods and applications of distributedrepresentation learning in natural language processingprefaceviiprerequisitesthis book is designed for advanced undergraduate and graduate students post doctoral fellows researchers lecturers and industrial engineers as well as anyoneinterested in representation learning and nlp we expect the readers to have someprior knowledge in probability linear algebra and machine learning we rec ommend the readers who are specically interested in nlp to read the rst partchaps 15 which should be read sequentially the second and third parts can beread in selected order according to readers interestscontact informationwe welcome any feedback corrections and suggestions on the book which maybe sent to liuzytsinghuaeducn the readers can also nd updates about the bookfrom the personal homepage httpnlpcsaitsinghuaeducnlzybeijing chinazhiyuan liuyankai linmarch 2020maosong sunreferences1 xinxiong chen lei xu zhiyuan liu maosong sun and huanbo luan joint learning ofcharacter and word embeddings in proceedings of ijcai 20152 yihong gu jun yan hao zhu zhiyuan liu ruobing xie maosong sun fen lin and leyulin language modeling with sparse product of sememe experts in proceedings of emnlppages 46424651 20183 xu han zhiyuan liu and maosong sun joint representation learning of text and knowledgefor knowledge graph completion arxiv preprint arxiv161104125 20164 xu han zhiyuan liu and maosong sun neural knowledge acquisition via mutual attentionbetween knowledge graph and text in proceedings of aaai pages 48324839 20185 xu han hao zhu pengfei yu ziyun wang yuan yao zhiyuan liu and maosong sunfewrel a large scale supervised few shot relation classication dataset with state of the artevaluation in proceedings of emnlp 20186 huiming jin hao zhu zhiyuan liu ruobing xie maosong sun fen lin and leyu linincorporating chinese characters of words for lexical sememe prediction in proceedings ofacl 20187 yankai lin haozhe ji zhiyuan liu and maosong sun denoising distantly supervisedopen domain question answering in proceedings of acl 20188 yankai lin zhiyuan liu huanbo luan maosong sun siwei rao and song liu modelingrelation paths for representation learning of knowledge bases in proceedings of emnlp2015viiipreface9 yankai lin zhiyuan liu maosong sun yang liu and xuan zhu learning entity andrelation embeddings for knowledge graph completion in proceedings of aaai 201510 yankai lin shiqi shen zhiyuan liu huanbo luan and maosong sun neural relationextraction with selective attention over instances in proceedings of acl 201611 yang liu zhiyuan liu tat seng chua and maosong sun topical word embeddings inproceedings of aaai 201512 zhenghao liu chenyan xiong maosong sun and zhiyuan liu entity duet neural rankingunderstanding the role of knowledge graph semantics in neural information retrieval inproceedings of acl 201813 zhiyuan liu maosong sun yankai lin and ruobing xie knowledge representationlearning a review jcrd 532247261 201614 yilin niu ruobing xie zhiyuan liu and maosong sun improved word representationlearning with sememes in proceedings of acl 201715 fanchao qi junjie huang chenghao yang zhiyuan liu xiao chen qun liu and maosongsun modeling semantic compositionality with sememe knowledge in proceedings of acl201916 fanchao qi yankai lin maosong sun hao zhu ruobing xie and zhiyuan liucross lingual lexical sememe prediction in proceedings of emnlp 201817 maosong sun and xinxiong chen embedding for words and word senses based on humanannotated knowledge base a case study on hownet journal of chinese informationprocessing 3016 201618 cunchao tu hao wang xiangkai zeng zhiyuan liu and maosong sun community enhancednetworkrepresentationlearningfornetworkanalysisarxivpreprintarxiv161106645 201619 cunchao tu weicheng zhang zhiyuan liu and maosong sun max margin deepwalkdiscriminative learning of network representation in proceedings of ijcai 201620 cunchao tu zhengyan zhang zhiyuan liu and maosong sun transnet translation basednetwork representation learning for social relation extraction in proceedings of ijcai 201721 ruobingxiezhiyuanliutat sengchuahuanboluanandmaosongsunimage embodied knowledge representation learning in proceedings of ijcai 201622 ruobing xie zhiyuan liu jia jia huanbo luan and maosong sun representation learningof knowledge graphs with entity descriptions in proceedings of aaai 201623 ruobing xie zhiyuan liu and maosong sun representation learning of knowledge graphswith hierarchical types in proceedings of ijcai 201624 ruobing xie xingchi yuan zhiyuan liu and maosong sun lexical sememe prediction viaword embeddings and matrix factorization in proceedings of ijcai 201725 cheng yang zhiyuan liu deli zhao maosong sun and edward y chang network rep resentation learning with rich text information in proceedings of ijcai 201526 cheng yang maosong sun zhiyuan liu and cunchao tu fast network embeddingenhancement via high order proximity approximation in proceedings of ijcai 201727 cheng yang maosong sun wayne xin zhao zhiyuan liu and edward y chang a neuralnetwork approach to jointly modeling social networks and mobile trajectories acmtransactions on information systems tois 35436 201728 cheng yang jian tang maosong sun ganqu cui and liu zhiyuan multi scale informationdiffusion prediction with reinforced recurrent networks in proceedings of ijcai 201929 yuan yao deming ye peng li xu han yankai lin zhenghao liu zhiyuan liu lixinhuang jie zhou and maosong sun docred a large scale document level relationextraction dataset in proceedings of acl 201930 xiangkai zeng cheng yang cunchao tu zhiyuan liu and maosong sun chinese liwclexicon expansion via hierarchical classication of word embeddings with sememe attentionin proceedings of aaai 201831 zhengyan zhang xu han zhiyuan liu xin jiang maosong sun and qun liu ernieenhanced language representation with informative entities in proceedings of acl 2019prefaceix32 yu zhao zhiyuan liu and maosong sun phrase type sensitive tensor indexing model forsemantic composition in proceedings of aaai 201533 jie zhou xu han cheng yang zhiyuan liu lifeng wang changcheng li and maosongsun gear graph based evidence aggregating and reasoning for fact verication inproceedings of acl 2019 2019the original version of the book frontmatter has been revised with the addition ofreference citations in the preface the corrected version of frontmatter can be foundat httpsdoiorg101007978 981 15 5573 2 12xprefaceacknowledgementsthe authors are very grateful to the contributions of our students and researchcollaborators who have prepared initial drafts of some chapters or have given uscomments suggestions and corrections we list main contributors for preparinginitial drafts of each chapter as follows chapter 1 tianyu gao zhiyuan liu chapter 2 lei xu yankai lin chapter 3 yankai lin yang liu chapter 4 yankai lin zhengyan zhang cunchao tu hongyin luo chapter 5 jiawei wu yankai lin zhenghao liu haozhe ji chapter 6 fanchao qi chenghao yang chapter 7 ruobing xie xu han chapter 8 cheng yang jie zhou zhengyan zhang chapter 9 ji xin yuan yao deming ye hao zhu chapter 10 xu han zhengyan zhang cheng yang chapter 11 cheng yang zhiyuan liufor the whole book we thank chaojun xiao and zhengyan zhang for drawingmodel gures thank chaojun xiao for unifying the styles of gures and tables inthe book thank shengding hu for making the notation table and unifying thenotations across chapters thank jingcheng yuzhi and chaojun xiao for organizingthe format of reference thank jingcheng yuzhi jiaju du haozhe ji sicongouyang and ayana for the rst round proofreading and thank weize chen ganqucui bowen dong tianyu gao xu han zhenghao liu fanchao qi guangxuanxiao cheng yang yuan yao shi yu yuan zang zhengyan zhang haoxi zhongand jie zhou for the second round proofreading we also thank cuncun zhao fordesigning the book coverin this book there is a specic chapter talking about sememe knowledge rep resentation many works in this chapter are carried out by our research group theseworks have received great encouragement from the inventor of hownetmr zhendong dong who died at 82 on february 28 2019 hownet is the greatxilinguistic and commonsense knowledge base composed by mr dong for about 30years at the end of his life he and his son mr qiang dong decided to collaboratewith us and released the open source version of hownet openhownet as apioneer of machine translation in china mr zhendong dong devoted his wholelife to natural language processing he will be missed by all of us foreverwe thank our colleagues and friends yang liu and juanzi li at tsinghuauniversity and peng li at tencent wechat who offered close and frequent dis cussions which substantially improved this book we also want to express ourspecial thanks to prof bo zhang his insights to deep learning and representationlearning and sincere encouragements to our research of representation learning onnlp have greatly stimulated us to move forward with more condence andpassionwe proposed the plan of this book in 2015 after discussing it with the springersenior editor dr celine lanlan chang as the rst of the time of preparing atechnical book we were not expecting it took so long to nish this book we thankceline for providing insightful comments and incredible patience to the preparationof this book we are also grateful to springers assistant editor jane li foroffering invaluable help during manuscript preparationfinally we give our appreciations to our organizations department of computerscience and technology at tsinghua university institute for articial intelligenceat tsinghua university beijing academy of articial intelligence baai chineseinformation processing society of china and tencent wechat who have providedoutstanding environment supports and facilities for preparing this bookthis book is supported by the natural science foundation of china nsfc andthe german research foundation dfg in project crossmodal learning nsfc61621136008dfg trr 169xiiacknowledgementscontents1representation learning and nlp                         111motivation                                       112why representation learning is important for nlp         313basic ideas of representation learning                  314development of representation learning for nlp          415learning approaches to representation learning for nlp    716applications of representation learning for nlp           817the organization of this book                        9references                                            102word representation                                   1321introduction                                      1322one hot word representation                         1423distributed word representation                       14231brown cluster                              15232latent semantic analysis                      17233word2vec                                 18234glove                                    2124contextualized word representation                    2225extensions                                       23251word representation theories                  24252multi prototype word representation             25253multisource word representation                26254multilingual word representation                30255task specic word representation               32256time specic word representation               3326evaluation                                       33261word similarityrelatedness                    34262word analogy                              36xiii27summary                                        36references                                            383compositional semantics                                4331introduction                                      4332semantic space                                   45321vector space                               45322matrix vector space                          4533binary composition                                46331additive model                             47332multiplicative model                         4934n ary composition                                51341recurrent neural network                     52342recursive neural network                     53343convolutional neural network                  5535summary                                        55references                                            564sentence representation                                 5941introduction                                      5942one hot sentence representation                      6043probabilistic language model                         6144neural language model                             62441feedforward neural network language model      62442convolutional neural network language model     63443recurrent neural network language model        63444transformer language model                   64445extensions                                 6945applications                                      72451text classication                           73452relation extraction                           7446summary                                        84references                                            855retracted chapter document representation          9151introduction                                      9152one hot document representation                     9253topic model                                     93531latent dirichlet allocation                     94532extensions                                 9754distributed document representation                    100541paragraph vector                            100542neural document representation                102xivcontents55applications                                      107551neural information retrieval                    107552question answering                          11056summary                                        119references                                            1206sememe knowledge representation                        12561introduction                                      125611linguistic knowledge graphs                   12662sememe knowledge representation                     128621simple sememe aggregation model              129622sememe attention over context model            129623sememe attention over target model             13163applications                                      132631sememe guided word representation             132632sememe guided semantic compositionalitymodeling                                  134633sememe guided language modeling             139634sememe prediction                           142635other sememe guided applications              15364summary                                        157references                                            1587world knowledge representation                          16371introduction                                      163711world knowledge graphs                     16472knowledge graph representation                      166721notations                                  168722transe                                    168723extensions of transe                         172724other models                               18273multisource knowledge graph representation             189731knowledge graph representation with texts        190732knowledge graph representation with types       192733knowledge graph representation with images      194734knowledge graph representation with logic rules   19574applications                                      196741knowledge graph completion                  197742knowledge guided entity typing                199743knowledge guided information retrieval          201744knowledge guided language models             205745other knowledge guided applications            20875summary                                        210references                                            211contentsxv8network representation                                 21781introduction                                      21782network representation                             219821spectral clustering based methods               219822deepwalk                                 223823matrix factorization based methods              230824structural deep network methods                232825extensions                                 234826applications                                24783graph neural networks                             252831motivations                                253832graph convolutional networks                  254833graph attention networks                     259834graph recurrent networks                     260835extensions                                 262836applications                                26684summary                                        275references                                            2779cross modal representation                              28591introduction                                      28592cross modal representation                          286921visual word2vec                            286922cross modal representation for zero shotrecognition                                288923cross modal representation for cross mediaretrieval                                  29293image captioning                                  294931retrieval models for image captioning            294932generation models for image captioning          295933neural models for image captioning              29694visual relationship detection                         301941visual relationship detection with language priors   301942visual translation embedding network            303943scene graph generation                       30395visual question answering                           307951vqa and vqa datasets                      307952vqa models                               30896summary                                        311references                                            314xvicontents10resources                                            319101open source frameworks for deep learning              3191011caffe                                     3191012theano                                   3201013tensorflow                                3211014torch                                    3211015pytorch                                   3221016keras                                     3231017mxnet                                   323102open resources for word representation                3241021word2vec                                 3241022glove                                    324103open resources for knowledge graph representation       3251031openke                                  3251032scikit kge                                 326104open resources for network representation              3261041openne                                  3261042gem                                     3261043graphvite                                 3271044cogdl                                   327105open resources for relation extraction                  3271051opennre                                 327references                                            32811outlook                                              329111introduction                                      329112using more unsupervised data                        330113utilizing fewer labeled data                         330114employing deeper neural architectures                  331115improving model interpretability                       332116fusing the advances from other areas                  333references                                            333correction to representation learning for natural languageprocessing                                               c1contentsxviiacronymsacnnanisotropic convolutional neural networkaiarticial intelligenceaucarea under the receiver operating characteristic curvebertbidirectional encoder representations from transformersbfsbreadth first searchbidafbi directional attention flowbrnnbidirectional recurrent neural networkcbowcontinuous bag of wordsccdclmcontext to context document context language modelciderconsensus based image description evaluationclncolumn networkclspcross lingual lexical sememe predictioncnnconvolutional neural networkcnrlcommunity enhanced network representation learningcoco qacommon objects in context question answeringconseconvex combination of semantic embeddingsconv knrmconvolutional kernel based neural ranking modelcspcharacter enhanced sememe predictioncwecharacter based word embeddingsdcnndiffusion convolutional neural networkdevisedeep visual semantic embedding modeldfsdepth first searchdgcndual graph convolutional networkdgedirected graph embeddingdkrldescription embodied knowledge graph representation learningdrmmdeep relevance matching modeldssmdeep structured semantic modeleccedge conditioned convolutionernieenhanced language representation model with informativeentitiesxixfm iqafreestyle multilingual image question answeringgaangated attention networkgatgraph attention networksgcngraph convolutional networkgcnngeodesic convolutional neural networkgeargraph based evidence aggregating and reasoninggenqagenerative question answering modelggnngated graph neural networkgloveglobal vectors for word representationgnngraph neural networksgrngraph recurrent networkgrugated recurrent unithanheterogeneous graph attention networkhmmhidden markov modelhopehigh order proximity preserved embeddingsidfinverse document frequencyieinformation extractionikrlimage embodied knowledge graph representation learningirinformation retrievalkalmknowledge augmented language modelkbknowledge basekbcknowledge base completionkgknowledge graphklkullback leiblerknetknowledge guided attention neural entity typingk nrmkernel based neural ranking modelkrknowledge representationlbsnlocation based social networkldalatent dirichlet allocationliwclinguistic inquiry and word countllelocally linear embeddinglmlanguage modellsalatent semantic analysislshmlatent space heterogeneous modellstmlong short term memorymapmean average precisionmeteormetric for evaluation of translation with explicit orderingmmdmaximum mean discrepancymmdwmax margin deepwalkm nmfmodularized nonnegative matrix factorizationmovmfmixture of von mises fisher distributionsmrfmarkov random fieldmslemean square log transformed errormstminimum spanning treemv rnnmatrix vector recursive neural networkxxacronymsneunetwork embedding updatenklmneural knowledge language modelnlinatural language inferencenlpnatural language processingnreneural relation extractionookbout of knowledge basepcnnpiece wise convolution neural networkplsiprobabilistic latent semantic indexingpmipoint wise mutual informationpospart of speechppmipositive point wise mutual informationptepredictive text embeddingpv dbowparagraph vector with distributed bag of wordspv dmparagraph vector with distributed memoryqaquestion answeringrbfrestricted boltzmann machinercrelation classicationr cnnregion based convolutional neural networkrdfresource description frameworkrerelation extractionrmseroot mean squared errorrnnrecurrent neural networkrntnrecursive neural tensor networkrpnregion proposal networksacsememe attention over context modelsatsememe attention over target modelscsemantic compositionalityscassemantic compositionality with aggregated sememescmsasemantic compositionality with mutual sememe attentionsdlmsememe driven language modelsdnestructural deep network embeddingsse wrlsememe encoded word representation learningsgdstochastic gradient descentsgnsskip gram with negative sampling models lstmsentence long short term memoryspasesememe prediction with aggregated sememe embeddingsspcsesememe prediction with character and sememe embeddingsspicesemantic propositional image caption evaluationspsesememe prediction with sememe embeddingsspwcfsememe prediction with word to character filteringspwesememe prediction with word embeddingsssasimple sememe aggregation modelsswesentiment specic word embeddingssvdsingular value decompositionsvmsupport vector machineacronymsxxitadwtext associated deepwalktfterm frequencytf idfterm frequencyinverse document frequencytkrltype embodied knowledge graph representation learningtsptraveling salesman problemtwetopical word embeddingsvqavisual question answeringvsmvector space modelwrlword representation learningwsdword sense disambiguationyagoyet another great ontologyxxiiacronymssymbols and notationstokyoword exampleconvolution operatordened aselement wise multiplication hadamard productinducesproportional topsummation operatorminminimizemaxmaximizemax poolingarg minkthe parameter that minimizes a functionarg maxkthe parameter that maximizes a functionsimsimilarityexpexponential functionattattention functionavgaverage functionf1 scoref1 scorepmipair wise mutual informationrelurelu activation functionsigmoidsigmoid functionsoftmaxsoftmax functionvvocabulary setwwword word embedding vectoreword embedding matrixrrelation setrrrelation relation embedding vectoraanswer to questionqquerywmuweight matrixbdbias vectormimjmatrixs ith row matrixs jth columnxxiiiatmttranspose of a vector or a matrixtrtrace of a matrixmtensora b khyperparametersg f   u dfunctionsnanumber of occurrences of ad distance functions similarity functionp   p  probabilityi mutual informationhentropyo  time complexitydkl jjkl divergencelloss functionoobjective functioneenergy functionaattention scoreoptimal value of a variablefunction j  jvector length set sizek kaa  norm1indicator functionhparameter in a neural networkeexpectation of a random variable  positive sample negative samplecmarginlmean of normal distributionlmean vector of gaussian distributionrstandard error of normal distributionrcovariance matrix of gaussian distributioninn dimensional identity matrixnnormal distributionxxivsymbols and notationschapter 1representation learning and nlpabstract natural languages are typical unstructured information conventionalnatural language processing nlp heavily relies on feature engineering whichrequires careful design and considerable expertise representation learning aims tolearnrepresentationsofrawdataasusefulinformationforfurtherclassicationorpre diction this chapter presents a brief introduction to representation learning includ ing its motivation and basic idea and also reviews its history and recent advances inboth machine learning and nlp11motivationmachine learning addresses the problem of automatically learning computer pro grams from data a typical machine learning system consists of three components 5machine learning  representation  objective  optimization11that is to build an effective machine learning system we rst transform usefulinformation on raw data into internal representations such as feature vectors then bydesigning appropriate objective functions we can employ optimization algorithmsto nd the optimal parameter settings for the systemdata representation determines how much useful information can be extractedfrom raw data for further classication or prediction if there is more useful infor mation transformed from raw data to feature representations the performance ofclassication or prediction will tend to be better hence data representation is acrucial component to support effective machine learningconventional machine learning systems adopt careful feature engineering aspreprocessing to build feature representations from raw data feature engineeringneeds careful design and considerable expertise and a specic task usually requirescustomized feature engineering algorithms which makes feature engineering laborintensive time consuming and inexiblerepresentation learning aims to learn informative representations of objects fromraw data automatically the learned representations can be further fed as input to the authors 2020z liu et al representation learning for natural language processinghttpsdoiorg101007978 981 15 5573 2 1121representation learning and nlpmachine learning systems for prediction or classication in this way machine learn ing algorithms will be more exible and desirable while handling large scale andnoisy unstructured data such as speech images videos time series and textsdeep learning 9 is a typical approach for representation learning which hasrecently achieved great success in speech recognition computer vision and naturallanguage processing deep learning has two distinguishing features distributed representation deep learning algorithms typically represent eachobject with a low dimensional real valued dense vector which is named as dis tributed representation as compared to one hot representation in conventionalrepresentation schemes such as bag of words models distributed representationis able to represent data in a more compact and smoothing way as shown in fig 11and hence is more robust to address the sparsity issue in large scale data deep architecture deep learning algorithms usually learn a hierarchical deeparchitecture to represent objects known as multilayer neural networks the deeparchitecture is able to extract abstractive features of objects from raw data whichis regarded as an important reason for the great success of deep learning for speechrecognition and computer visioncurrently the improvements caused by deep learning for nlp may still not beso signicant as compared to speech and vision however deep learning for nlphas been able to signicantly reduce the work of feature engineering in nlp in themeantime of performance improvement hence many researchers are devoting todeveloping efcient algorithms on representation learning especially deep learningfor nlpin this chapter we will rst discuss why representation learning is important fornlp and introduce the basic ideas of representation learning afterward we willapple incsteve jobstim cookiphoneceofounderproductsteve jobstim cookiphoneapple incentitiesembeddingsfig 11 distributed representation of words and entities in human languages11 motivation3briey review the development history of representation learning for nlp introducetypical approaches of contemporary representation learning and summarize existingand potential applications of representation learning finally we will introduce thegeneral organization of this book12why representation learning is important for nlpnlpaimstobuildlinguistic specicprogramsformachinestounderstandlanguagesnatural language texts are typical unstructured data with multiple granularities mul tiple tasks and multiple domains which make nlp challenging to achieve satisfac tory performancemultiple granularities nlp concerns about multiple levels of language entriesincluding but not limited to characters words phrases sentences paragraphs anddocuments representation learning can help to represent the semantics of theselanguage entries in a unied semantic space and build complex semantic relationsamong these language entriesmultiple tasks there are various nlp tasks based on the same input for exam ple given a sentence we can perform multiple tasks such as word segmentationpart of speech tagging named entity recognition relation extraction and machinetranslation in this case it will be more efcient and robust to build a unied repre sentation space of inputs for multiple tasksmultiple domains natural language texts may be generated from multipledomains including but not limited to news articles scientic articles literary worksand online user generated content such as product reviews moreover we can alsoregard texts in different languages as multiple domains conventional nlp systemshave to design specic feature extraction algorithms for each domain according to itscharacteristics in contrast representation learning enables us to build representationsautomatically from large scale domain datain summary as shown in fig 12 representation learning can facilitate knowledgetransfer across multiple language entries multiple nlp tasks and multiple appli cation domains and signicantly improve the effectiveness and robustness of nlpperformance13basic ideas of representation learningin this book we focus on the distributed representation scheme ie embeddingand talk about recent advances of representation learning methods for multiple lan guage entries including words phrases sentences and documents and their closelyrelated objects including sememe based linguistic knowledge entity based worldknowledge networks and cross modal entries41representation learning and nlpnlp taskslanguage entriesknowledgenetworkdocumentsentencephrasewordnlp applicationssemantic analysissyntactic analysislexical analysisfig 12 distributed representation can provide unied semantic space for multi grained languageentries and for multiple nlp tasksby distributed representation learning all objects that we are interested in areprojected into a unied low dimensional semantic space as demonstrated in fig 11the geometric distance between two objects in the semantic space indicates theirsemantic relatedness the semantic meaning of an object is related to which objectsare close to it in other words it is the relative closeness with other objects that revealsan objects meaning rather than the absolute position14development of representation learning for nlpin this section we introduce the development of representation learning for nlpalso shown in fig 13 to study representation schemes in nlp words would be agood start since they are the minimum units in natural languages the easiest wayto represent a word in a computer readable way eg using a vector is one hotvector which has the dimension of the vocabulary size and assigns 1 to the wordscorresponding position and 0 to others it is apparent that one hot vectors hardlycontain any semantic information about words except simply distinguishing themfrom each otherone of the earliest ideas of word representation learning can date back to n grammodels 15 it is easy to understand when we want to predict the next word in asequence we usually look at some previous words and in the case of n gram theyare the previous n 1 words and if going through a large scale corpus we cancount and get a good probability estimation of each word under the condition of allcombinations of n 1 previous words these probabilities are useful for predictingwords in sequences and also form vector representations for words since they reectthe meanings of wordsthe idea of n gram models is coherent with the distributional hypothesis lin guistic items with similar distributions have similar meanings 7 in another phrasea word is characterized by the company it keeps 6 it became the fundamentalidea of many nlp models from word2vec to bert14 development of representation learning for nlp51948n gram model1954distributional hypothesisbag of wordsdistributed representation1986neural probabilisticlanguage model20032013word2vec2018pre trained language modelpredicts the next item in a sequence based on its previous n 1 itemsa word is characterized by the company it keepsrepresents a sentence or a document as the bag of its wordsrepresents items by a pattern of activation distributed over elementslearns a distributed representation of words for language modelinga simple and e cient distributed word representation used in many nlp modelscontextual word representation pipeline larger corpora and deeper neural architecturesfig 13 the timeline for the development of representation learning in nlp with the growingcomputing power and large scale text data distributed representation trained with neural networksand large corpora has become the mainstreamanother example of the distributional hypothesis is bag of words bow mod els 7 bow models regard a document as a bag of its words disregarding the ordersof these words in the document in this way the document can be represented as avocabulary size vector in which each word that has appeared in the document cor responds to a unique and nonzero dimension then a score can be further computedfor each word eg the numbers of occurrences to indicate the weights of thesewords in the document though very simple bow models work great in applica tions like spam ltering text classication and information retrieval proving thatthe distributions of words can serve as a good representation for textin the above cases each value in the representation clearly matches one entryeg word scores in bow models this one to one correspondence between con cepts and representation elements is called local representation or symbol basedrepresentation which is natural and simplein distributed representation on the other hand each entity or attribute isrepresented by a pattern of activation distributed over multiple elements and eachcomputing element is involved in representing multiple entities 11 distributed rep resentation has been proved to be more efcient because it usually has low dimen sions that can prevent the sparsity issue useful hidden properties can be learned fromlarge scale data and emerged in distributed representation the idea of distributedrepresentation was originally inspired by the neural computation scheme of humansand other animals it comes from neural networks activations of neurons and withthe great success of deep learning distributed representation has become the mostcommonly used approach for representation learningone of the pioneer practices of distributed representation in nlp is neural prob abilistic language model nplm 1 a language model is to predict the jointprobability of sequences of words n gram models are simple language modelsnplm rst assigns a distributed vector for each word then uses a neural networkto predict the next word by going through the training corpora nplm successfullylearns how to model the joint probability of sentences while brings word embed dings ie low dimensional word vectors as learned parameters in nplm though61representation learning and nlpword embedding modelpre training objectiveword embeddingtarget task modeltarget task objectiveword embeddingpre trained language modelpre training objectiveword embeddingtarget task objectivetarget task modelword embeddingword embeddingpre trained language modelfig 14 this gure shows how word embeddings and pre trained language models work in nlppipelines they both learn distributed representations for language entries eg words throughpretraining objectives and transfer them to target tasks furthermore pre trained language modelscan also transfer model parametersit is hard to tell what each element of a word embedding actually means the vectorsindeed encode semantic meanings about the words veried by the performance ofnplminspired by nplm there came many methods that embed words into distributedrepresentations and use the language modeling objective to optimize them as modelparameters famous examples include word2vec 12 glove 13 and fasttext 3though differing in detail these methods are all very efcient to train utilize large scale corpora and have been widely adopted as word embeddings in many nlpmodels word embeddings in the nlp pipeline map discrete words into informativelow dimensional vectors and help to shine a light on neural networks in comput ing and understanding languages it makes representation learning a critical part ofnatural language processingthe research on representation learning in nlp took a big leap when elmo14 and bert 4 came out besides using larger corpora more parameters andmore computing resources as compared to word2vec they also take complicatedcontext in text into consideration it means that instead of assigning each wordwith a xed vector elmo and bert use multilayer neural networks to calculatedynamic representations for the words based on their context which is especiallyuseful for the words with multiple meanings moreover bert starts a new fashionthough not originated from it of the pretrained ne tuning pipeline previouslyword embeddings are simply adopted as input representation but after bert itbecomes a common practice to keep using the same neural network structure such asbert in both pretraining and ne tuning which is taking the parameters of bertfor initialization and ne tuning the model on downstream tasks fig 14though not a big theoretical breakthrough bert like models also known aspre trained language models plm for they are pretrained through languagemodeling objective on large corpora have attracted wide attention in the nlp andmachine learning community for they have been so successful and achieved state of the art on almost every nlp benchmarks these models show what large scaledata and computing power can lead to and new research works on the topic of pre trained language models plms emerge rapidly probing experiments demonstratethat plms implicitly encode a variety of linguistic knowledge and patterns inside14 development of representation learning for nlp7their multilayer network parameters 8 10 all these signicant performances andinteresting analyses suggest that there are still a lot of open problems to explore inplms as the future of representation learning for nlpbased on the distributional hypothesis representation learning for nlp hasevolved from symbol based representation to distributed representation startingfrom word2vec word embeddings trained from large corpora have shown signicantpower in most nlp tasks recently emerged plms like bert take complicatedcontext into word representation and start a new trend of the pretraining ne tuningpipeline bringing nlp to a new level what will be the next big change in repre sentation learning for nlp we hope the contents of this book can give you someinspiration15learning approaches to representation learning fornlppeople have developed various effective and efcient approaches to learn semanticrepresentations for nlp here we list some typical approachesstatistical features as introduced before semantic representations for nlp inthe early stage often come from statistics instead of emerging from the optimizationprocess for example in n gram or bag of words models elements in the representa tion are usually frequencies or numbers of occurrences of the corresponding entriescounted in large scale corporahand craft features in certain nlp tasks syntactic and semantic features areuseful for solving the problem for example types of words and entities semanticroles and parse trees etc these linguistic features may be provided with the tasksor can be extracted by specic nlp systems in a long period before the wide useof distributed representation researchers used to devote lots of effort into designinguseful features and combining them as the inputs for nlp modelssupervised learning distributed representations emerge from the optimizationprocess of neural networks under supervised learning in the hidden layers of neu ral networks the different activation patterns of neurons represent different entitiesor attributes with a training objective usually a loss function for the target taskand supervised signals usually the gold standard labels for training instances of thetarget tasks the networks can learn better parameters via optimization eg gra dient descent with proper training the hidden states will become informative andgeneralized as good semantic representations of natural languagesfor example to train a neural network for a sentiment classication task the lossfunction is usually set as the cross entropy of the model predictions with respect tothe gold standard sentiment labels as supervision while optimizing the objectivethe loss gets smaller and the model performance gets better in the meantime thehidden states of the model gradually form good sentence representations by encodingthe necessary information for sentiment classication inside the continuous hiddenspace81representation learning and nlpself supervised learning in some cases we simply want to get good represen tations for certain elements so that these representations can be transferred to othertasks for example in most neural nlp models words in sentences are rst mappedto their corresponding word embeddings maybe from word2vec or glove beforesent to the networks however there are no human annotated labels for learningword embeddings to acquire the training objective necessary for neural networkswe need to generate labels intrinsically from existing data this is called self supervised learning one way for unsupervised learningfor example language modeling is a typical self supervised objective for itdoes not require any human annotations based on the distributional hypothesisusing the language modeling objective can lead to hidden representations that encodethe semantics of words you may have heard of a famous equation wking wman  wwoman  wqueen which demonstrates the analogical propertiesthat the word embeddings have possessed through self supervised learningwe can see another angle of self supervised learning in autoencoders it is also away to learn representations for a set of data typical autoencoders have a reductionencoding phase and a reconstruction decoding phase in the reduction phase anitem from the data is encoded into a low dimensional representation and in thereconstruction phase the model tries to reconstruct the item from the intermediaterepresentation here the training objective is the reconstruction loss derived fromthe data itself during the training process meaningful information is encoded andkept in the latent representation while noise signals are discardedself supervised learning has made a great success in nlp for the plain text itselfcontains abundant knowledge and patterns about languages and self supervisedlearning can fully utilize the existing large scale corpora nowadays it is still themost exciting research area of representation learning for natural languages andresearchers continue to put their efforts into this directionbesides many other machine learning approaches have also been explored inrepresentation learning for nlp such as adversarial training contrastive learningfew shot learning meta learning continual learning reinforcement learning et alhow to develop more effective and efcient approaches of representation learningfor nlp and to better take advantage of large scale and complicated corpora andcomputing power is still an important research topic16applications of representation learning for nlpin general there are two kinds of applications of representation learning for nlp inone case the semantic representation is trained in a pretraining task or designed byhuman experts and is transferred to the model for the target task word embeddingis an example of the application it is trained by using language modeling objectiveand is taken as inputs for other down stream nlp models in this book we will16 applications of representation learning for nlp9also introduce sememe knowledge representation and world knowledge representa tion which can also be integrated into some nlp systems as additional knowledgeaugmentation to enhance their performance in certain aspectsin other cases the semantic representation lies within the hidden states of the neu ral model and directly aims for better performance of target tasks as an end to endfashion for example many nlp tasks want to semantically compose sentence ordocument representation tasks like sentiment classication natural language infer ence and relation extraction require sentence representation and the tasks like ques tion answering need document representation as shown in the latter part of thebook many representation learning methods have been developed for sentences anddocuments and benet these nlp tasks17the organization of this bookwe start the book from word representation by giving a thorough introduction toword representation we hope the readers can grasp the basic ideas for representa tion learning for nlp based on that we further talk about how to compositionallyacquire the representation for higher level language components from sentences todocumentsas shown in fig 15 representation learning will be able to incorporate varioustypes of structural knowledge to support a deep understanding of natural languagesnamed as knowledge guided nlp hence we next introduce two forms of knowledgerepresentation that are closely related to nlp on the one hand sememe represen tation tries to encode linguistic and commonsense knowledge in natural languagessememe is dened as the minimum indivisible unit of semantic meaning 2 withthe help of sememe representation learning we can get more interpretable and morerobust nlp models on the other hand world knowledge representation studies howto encode world facts into continuous semantic space it can not only help withknowledge graph tasks but also benet knowledge guided nlp applicationsbesides the network is also a natural way to represent objects and their relation ships in the network representation section we study how to embed vertices andedges in a network and how these elements interact with each other through theapplications we further show how network representations can help nlp tasksanother interesting topic related to nlp is the cross modal representation whichstudies how to model unied semantic representations across different modalitieseg text audios images videos etc through this section we review severalcross modal problems along with representative modelsat the end of the book we introduce some useful resources to the readers includ ing deep learning frameworks and open source codes we also share some viewsabout the next big topics in representation learning for nlp we hope that theresources and the outlook can help our readers have a better understanding of thecontent of the book and inspire our readers about how representation learning innlp would further develop101representation learning and nlpdeep networkopen datasymbolembeddingknowledgeguidanceknowledgeextractionlearningunder standinggnnkrldeep learningknowledge graphfig 15 the architecture of knowledge guided nlpreferences1 yoshua bengio rjean ducharme pascal vincent and christian jauvin a neural probabilisticlanguage model journal of machine learning research 3feb11371155 20032 leonard bloomeld a set of postulates for the science of language language 2315316419263 piotr bojanowski edouard grave armand joulin and tomas mikolov enriching word vectorswith subword information transactions of the association for computational linguistics5135146 20174 jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training ofdeep bidirectional transformers for language understanding in proceedings of naacl 20195 pedro domingos a few useful things to know about machine learning communications ofthe acm 55107887 20126 john r firth a synopsis of linguistic theory 19301955 19577 zellig s harris distributional structure word 1023146162 19548 john hewitt and christopher d manning a structural probe for nding syntax in word repre sentations in proceedings of naacl hlt 20199 goodfellow ian yoshua bengio and aaron courville deep learning book in preparation formit press 201610 nelson f liu matt gardner yonatan belinkov matthew e peters and noah a smith lin guistic knowledge and transferability of contextual representations in proceedings of naacl hlt 2019references1111 james l mcclelland david e rumelhart pdp research group et al parallel distributedprocessing explorations in the microstructure of cognition 2216271 198612 t mikolov and j dean distributed representations of words and phrases and their composi tionality proceedings of neurips 201313 jeffrey pennington richard socher and christopher manning glove global vectors for wordrepresentation in proceedings of emnlp 201414 matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton leeand luke zettlemoyer deep contextualized word representations in proceedings of naacl hlt pages 22272237 201815 claude e shannon a mathematical theory of communication bell system technical journal273379423 1948open access this chapter is licensed under the terms of the creative commons attribution 40international license httpcreativecommonsorglicensesby40 which permits use sharingadaptation distribution and reproduction in any medium or format as long as you give appropriatecredit to the original authors and the source provide a link to the creative commons license andindicate if changes were madethe images or other third party material in this chapter are included in the chapters creativecommons license unless indicated otherwise in a credit line to the material if material is notincluded in the chapters creative commons license and your intended use is not permitted bystatutory regulation or exceeds the permitted use you will need to obtain permission directly fromthe copyright holderchapter 2word representationabstract word representation aiming to represent a word with a vector playsan essential role in nlp in this chapter we rst introduce several typical wordrepresentation learning methods including one hot representation and distributedrepresentation after that we present two widely used evaluation tasks for measuringthe quality of word embeddings finally we introduce the recent extensions for wordrepresentation learning models21introductionwords are usually considered as the smallest meaningful units of speech or writing inhuman languages high level structures in a language such as phrases and sentencesare further composed of words for human beings to understand a language it iscrucial to understand the meanings of words therefore it is essential to accuratelyrepresent words which could help models better understand categorize or generatetext in nlp tasksa word can be naturally represented as a sequence of several characters howeverit is very inefcient and ineffective only to use raw character sequences to representwords first the variable lengths of words make it hard to be processed and used inmachine learning methods moreover it is very sparse because only a tiny proportionof arrangements are meaningful for example english words are usually charactersequences which are composed of 120 characters in the english alphabet but mostof these character sequences such as aaaaa are meaninglessone hot representation is another natural approach to represent words whichassignsauniqueindextoeachworditisalsonotgoodenoughtorepresentwordswithone hot representation first one hot representation could not capture the seman tic relatedness among words second one hot representation is a high dimensionalsparse representation which is very inefcient third it is very inexible for one hotrepresentation to deal with new words which requires assigning new indexes for newwords and would change the dimensions of the representation the change may leadto some problems for existing nlp systems the authors 2020z liu et al representation learning for natural language processinghttpsdoiorg101007978 981 15 5573 2 213142word representationrecently distributed word representation approaches are proposed to address theproblem of one hot word representation the distributional hypothesis 23 30 thatlinguistic objects with similar distributions have similar meanings is the basis for dis tributed word representation learning based on the distributional hypothesis variousword representation models such as cbow and skip gram have been proposed andapplied in different areasin the remaining part of this chapter we start with one hot word representationfurther we introduce distributed word representation models including brown clus ter latent semantic analysis word2vec and glove in detail then we introducetwo typical evaluation tasks for word representation finally we discuss variousextensions of word representation models22one hot word representationin this section we will introduce one hot word representation in details given axed set of vocabulary v  w1 w2     wv  one very intuitive way to representa word w is to encode it with a v  dimensional vector w where each dimension ofw is either 0 or 1 only one dimension in w can be 1 while all the other dimensionsare 0 formally each dimension of w can be represented aswi 1 if w  wi0 otherwise21one hot word representation in essence maps each word to an index of thevocabulary which can be very efcient for storage and computation however itdoesnotcontainrichsemanticandsyntacticinformationofwordsthereforeone hotrepresentation cannot capture the relatedness among words the difference betweencat and dog is as much as the difference between cat and bed in one hot wordrepresentation besides one hot word representation embeds each word into a v  dimensional vector which can only work for a xed vocabulary therefore it isinexible to deal with new words in a real world scenario23distributed word representationrecently distributed word representation approaches are proposed to address theproblem of one hot word representation the distributional hypothesis 23 30 thatlinguistic objects with similar distributions have similar meanings is the basis forsemantic word representation learningbased on the distributional hypothesis brown cluster 9 groups words into hier archical clusters where words in the same cluster have similar meanings the cluster23 distributed word representation15label can roughly represent the similarity between words but it cannot preciselycompare words in the same group to address this issue distributed word represen tation1 aims to embed each word into a continuous real valued vector it is a denserepresentation and dense means that one concept is represented by more than onedimension of the vector and each dimension of the vector is involved in representingmultiple concepts due to its continuous characteristic distributed word represen tation can be easily applied in deep neural models for nlp tasks distributed wordrepresentation approaches such as word2vec and glove usually learn word vectorsfrom a large corpus based on the distributional hypothesis in this section we willintroduce several distributed word representation approaches in detail231brown clusterbrown cluster classies words into several clusters that have similar semantic mean ings detailedly brown cluster learns a binary tree from a large scale corpus inwhich the leaves of the tree indicate the words and the internal nodes of the treeindicate word hierarchical clusters this is a hard clustering method since each wordbelongs to exactly one groupthe idea of brown cluster to cluster the words comes from the n gram languagemodel a language model evaluates the probability of a sentence for examplethe sentence have a nice day should have a higher probability than a randomsequence of words using a k gram language model the probability of a sentences  w1 w2 w3     wn can be represented asps ni1pwiwi1ik22it is easy to estimate pwiwi1ik from a large corpus but the model has v k 1independent parameters which is a huge number for computers in the 1990s even if kis 2 the number of parameters is considerable moreover the estimation is inaccuratefor rare words to address these problems 9 proposes to group words into clustersand train a cluster level n gram language model rather than a word level model byassigning a cluster to each word the probability can be written asps ni1pcici1ikpwici231we emphasize that distributed representation and distributional representation are two completelydifferent aspects of representations a word representation method may belong to both categoriesdistributed representation indicates that the representation is a real valued vector while distri butional representation indicates that the meaning of a word is learned under the distributionalhypothesis162word representationwhere ci is the corresponding cluster of wi in cluster level language model thereare only ck 1  v  c independent parameters where c is the cluster setwhich is usually much smaller than the vocabulary v the quality of the cluster affects the performance of the language model given atraining text s for a 2 gram language model the quality of a mapping  from wordsto clusters is dened asq  1n log ps24 1nni1log pcici1  log pwici25let nw be the number of times word w appears in corpus s nw1w2 be the numberof times bigram w1w2 appears and nw be the number of times a cluster appearsthen the quality function q can be rewritten in a statistical way as followsq  1nni1log pcici1  log pwici26w1w2nw1w2nlog pw2w1pw2w227w1w2nw1w2nlog nw1w2nw1nw2nw228w1w2nw1w2nlog nw1w2nnw1nw2w1w2nw1w2nlog nw2n29c1c2nc1c2nlog nc1c2nnc1 nc2w2nw2nlog nw2n 210since pw  nwn  pc  ncn  and pc1c2 nc1c2n  the quality function can berewritten asq c1c2pc1c2 logpc1c2pc1pc2 wpw log pw211 i c hv 212where i c is the mutual information between clusters and hv  is the entropy ofthe word distribution which is a constant value therefore to optimize q equalsto optimize the mutual informationthere is no practical method to obtain optimum partitions nevertheless browncluster uses a greedy strategy to obtain a suboptimal result initially it assigns adistinct class for each word then it merges two classes with the least average mutualinformation after v  c mergences the partition is generated keeping the c23 distributed word representation17table 21 some clusters of brown clustercluster 1fridaymondaythursdaywednesdaytuesdaysaturdaycluster 2junemarchjulyapriljanuarydecembercluster 3watergascoalliquidacidsandcluster 4greatbigvastsuddenmeresheercluster 5manwomanboygirllawyerdoctorcluster 6americanindianeuropeanjapanesegermanafricanclusters we can continuously perform c 1 mergences to get a binary tree withcertain care in implementation the complexity of this algorithm is ov 3we show some clusters in table 21 from the table we can nd that each clusterrelates to a sense in the natural language the words in the same cluster tend toexpress similar meanings or could be used exchangeably232latent semantic analysislatent semantic analysis lsa is a family of strategies derived from vector spacemodels which could capture word semantics much better lsa aims to explore latentfactors for words and documents by matrix factorization to improve the estimationof word similarities reference 14 applies singular value decomposition svdon the word document matrix and exploits uncorrelated factors for both words anddocuments the svd of word document matrix m yields three matrices e  andd such thatm  ed213where  is the diagonal matrix of singular values of m each row vector wi inmatrix e corresponds to word wi and each row vector di in matrix d correspondsto document di then the similarity between two words could besimwi w j  mimj  wi2w j214here the number of singular values k included in  is a hyperparameter thatneeds to be tuned with a reasonable amount of the largest singular values used lsacould capture much useful information in the word document matrix and provide asmoothing effect that prevents large variancewith a relatively small k once the matrices e  and d are computed measuringword similarity could be very efcient because there are often fewer nonzero dimen sions in word vectors however the computation of e and d can be costly becausefull svd on a n  m matrix takes ominm2n mn2 time while the parallelizationof svd is not trivial182word representationanother algorithm for lsa is random indexing 34 55 it overcomes the dif culty of svd based lsa by avoiding costly preprocessing of a huge word documentmatrix in random indexing each document is assigned with a randomly generatedhigh dimensional sparse ternary vector called index vector then for each wordin the document the index vector is added to the words vector the index vectorsare supposed to be orthogonal or nearly orthogonal this algorithm is simple andscalable which is easy to parallelize and implemented incrementally moreover itsperformance is comparable with the svd based lsa according to 55233word2vecgoogles word2vec2 toolkit was released in 2013 it can efciently learn word vectorsfrom a large corpus the toolkit has two models including continuous bag of words cbow and skip gram based on the assumption that the meaning of aword can be learned from its context cbow optimizes the embeddings so that theycan predict a target word given its context words skip gram on the contrary learnsthe embeddings that can predict the context words given a target word in this sectionwe will introduce these two models in detail2331continuous bag of wordscbow predicts the center word given a window of context figure21 shows theidea of cbow with a window of 5 wordsformally cbow predicts wi according to its contexts aspwiw j jil ji  softmaxm jil jiw j215where pwiw j jil jiistheprobabilityofwordwi givenitscontextsl isthesizeof training contexts m is the weight matrix in rv m v indicates the vocabularyand m is the dimension of the word vectorthe cbow model is optimized by minimizing the sum of negative log probabil itiesl  ilog pwiw j jil ji216here the window size l is a hyperparameter to be tuned a larger window sizemay lead to a higher accuracy as well as the more expense of the training time2httpscodegooglecomarchivepword2vec23 distributed word representation19classifierword matrixaverageconcatenatewiwi 1wi1wi2wwwwi 2wfig 21 the architecture of cbow model2332skip gramon the contrary to cbow skip gram predicts the context given the center wordfigure22 shows the modelformally given a word wi skip gram predicts its context aspw jwi  softmaxmwi j i l j  i217where pw jwi is the probability of context word w j given wi and m is the weightmatrix the loss function is dened similar to cbow asl  ij jil jipw jwi218word matrixclassifierwi 2wi 1wi1wi2wiwfig 22 the architecture of skip gram model202word representation2333hierarchical softmax and negative samplingto train cbow or skip gram directly is very time consuming the most time consuming part is the softmax layer the conventional softmax layer needs to obtainthe scores of all words even though only one word is used in computing the lossfunction an intuitive idea to improve efciency is to get a reasonable but muchfaster approximation of that word here we will introduce two typical approxima tion methods which are included in the toolkit including hierarchical softmax andnegative sampling we explain these two methods using cbow as an examplethe idea of hierarchical softmax is to build hierarchical classes for all wordsand to estimate the probability of a word by estimating the conditional probabilityof its corresponding hierarchical class figure23 gives an example each internalnode of the tree indicates a hierarchical class and has a feature vector while eachleaf node of the tree indicates a word in this example the probability of wordthe is p0  p01 while the probability of cat is p0  p00  p001 the conditionalprobability is computed by the feature vector of each node and the context vectorfor examplep0 expw0  wcexpw0  wc  expw1  wc219p1  1 p0220where wc is the context vector w0 and w1 are the feature vectorshierarchical softmax generates the hierarchical classes according to the wordfrequency ie a huffman tree by the approximation it can compute the probabilityof each word much faster and the complexity of calculating the probability of eachword is olog v negative sampling is more straightforward to calculate the probability of a wordnegative sampling directly samples k words as negative samples according to theword frequency then it computes a softmax over the k  1 words to approximatethe probability of the target wordfig 23 an illustration ofhierarchical softmax1dogcattheis1123 distributed word representation21table 22 co occurrence probabilities and the ratio of probabilities for target words ice andsteam with context word solid gas water and fashionprobability and ratiok  solidk  gask  waterk  f ashionpkice19e 466e 53e 317e 5pksteam22e 578e 422e 318e 5pkicepksteam8985e 2136096234glovemethods like skip gram and cbow are shallow window based methods thesemethods scan a context window across the entire corpus which fails to take advantageof some global information global vectors for word representation glove on thecontrary can capture corpus statistics directlyas shown in table 22 the meaning of a word can be learned from the co occurrence matrix the ratio of co occurrence probabilities can be especially usefulin the example the meaning of ice and water can be examined by studying theratio of their co occurrence probabilities with various probe words for words relatedto ice but not steam for example solid the ratio psolidicepsolidsteam will be large similarly gas is related to steam but not ice sopgasicepgassteam will be small for words that are relevant or irrel evant to both words the ratio is close to 1based on this idea glove modelsfwi w j wk  pikpjk221where w rd are separate context word vectors and pi j is the probability of wordj to be in the context of word i formallypi j  ni jni222where ni j is the number of occurrences of word j in the context of word i andni  k nik is the number of times any word appears in the context of word jf is supposed to encode the information presented in the ratio pikpjk in theword vector space to keep the inherently linear structure f should only depend onthe difference of two target wordsfwi w j wk  pikpjk223the arguments of f are vectors while the right side of the equation is a scalar toavoid f obfuscating the linear structure a dot product is used222word representationfwi w jwk pikpjk224the model keeps the invariance under relabeling the target word and context wordit requires f to be a homomorphism between the groups r  and r0  thesolution is f  exp thenwi wk  log nik log ni225to keep exchange symmetry log ni is eliminated by adding biases bi and bk themodel becomeswi wk  bi  bk  log nik226which is signicantly simpler than eq221the loss function is dened asl v i j1f ni jwi w j  bi  b j log ni j227where f  is a weighting functionf x xxmaxif x  xmax1otherwise22824contextualized word representationin natural language the meaning of an individual word usually relates to its contextin a sentence for example the central bank has slashed its forecast for economicgrowth this year from 41 to 26 more recently on a blazing summer day he took me backto one of the den sites in a slumping bank above thesouth saskatchewan riverinthesetwosentencesalthoughtheword bankisalwaysthesametheirmeaningsare different however most of the traditional word embeddings cbow skip gramglove etc cannot well understand the different nuances of the meanings of wordswith the different surrounding texts the reason is that these models only learn aunique representation for each word and therefore it is impossible for these modelsto capture how the meanings of words change based on their surrounding contextsto address this issue 48 proposes elmo which uses a deep bidirectional lstmmodel to build word representations elmo could represent each word depending24 contextualized word representation23on the entire context in which it is used more specically rather than having a look up table of word embedding matrix elmo converts words into low dimensionalvectors on the y by feeding the word and its surrounding text into a deep neuralnetwork elmo utilizes a bidirectional language model to conduct word representa tion formally given a sequence of n words w1 w2     wn a forward languagemodel lm the details of language model are in sect 4 models the probability ofthe sequence by predicting the probability of each word tk according to the historicalcontextpw1 w2     wn nk1pwk  w1 w2     wk1229the forward lm in elmo is a multilayer lstm and the jth layer of the lstm based forward lm will generate the context dependent word representation h lmk jfor the word wk the backward lm is similar to the forward lm the only differenceis that it reverses the input word sequence to wn wn1     w1 and predicts eachword according to the future contextpw1 w2     wn nk1pwk  wk1 wk2     wn230as the same as the forward lm the jth backward lm layer generates the repre sentations h lmk j for the word wkelmogeneratesatask specicwordrepresentationwhichcombinesalllayerrep resentations of the bidirectional lm formally it computes a task specic weightingof all bidirectional lm layerswk  tasklj0staskjhlmk j 231where stask are softmax normalized weights and task is the weight of the entire wordvector for the task25extensionsbesides those very popular toolkits such as word2vec and glove various worksare focusing on different aspects of word representation contributing to numerousextensions these extensions usually focus on the following directions242word representation251word representation theorieswith the success of word representation researchers begin to explore the theories ofword representation some works attempt to give more theoretical analysis to provethe reasonability of existing tricks on word representation learning 39 45 whilesome works try to discuss the new learning methods 26 61reasonability word2vec and other similar tools are empirical methods of wordrepresentationlearningmanytricksareproposedin43tolearntherepresentationofwords from a large corpus efciently for example negative sampling consideringthe effectiveness of these methods a more theoretical analysis should be done toprove the reasonability of these tricks reference 39 gives some theoretical analysisof these tricks they formalize the skip gram model with negative sampling asan implicit matrix factorization process the skip gram model generates a wordembedding matrix e and a context matrix c the size of the word embedding matrixe is v   m each row of context matrix c is a context words m dimensional vectorthe training process of skip gram is an implicit factorization of m  ec c is notexplicitly considered in word2vec this work further analyzes that the matrix m ismi j  wi  c j  pmiwi c j log k232where k is the number of negative samples pmiw c is the point wise mutualinformationpmiw c  logpw cpwpc233the shifted pmi matrix can directly be used to compare the similarity of wordsanother intuitive idea is to factorize the shifted pmi matrix directly reference 39evaluates the performance of using the svd matrix factorization method on theimplicit matrix m matrix factorization achieves signicantly better objective valuewhen the embedding size is smaller than 500 dimensions and the number of negativesamples is 1 with more negative samples and higher embedding dimensions skip gram with negative sampling gets better objective value this is because when thenumber of zeros increases in m and svd prefers to factorize a matrix with mini mum values with 1000 dimensional embeddings and different numbers of negativesamples in 1 5 15 svd achieves slightly better performance on word analogy andword similarity in contrast skip gram with negative sampling achieves 2 betterperformance on syntactical analogyinterpretability most existing distributional word representation methods couldgenerate a dense real valued vector for each word however the word embeddingsobtained by these models are hard to be interpreted reference 26 introduces non negative and sparsity embeddings where the models are interpretable and eachdimension indicates a unique concept this method factorizes the corpus statisticsmatrix x rv d into a word embedding matrix e rv m and a documentstatistics matrix d rmd its training objective is25 extensions25arg mined12v i1xi eid2  ei1st didi 1 1 i mei j 0 1 i v  1 j m234by iteratively optimizing e and d via gradient descent this model can learn non negative and sparse embeddings for words since the embeddings are sparse andnonnegative words with the highest scores in each dimension show high similaritywhich can be viewed as a concept of this dimension to further improve the embed dings this work also proposes phrasal level constraints into the loss function withnew constraints it could achieve both interpretability and compositionality252multi prototype word representationusing only one single vector to represent a word is problematic due to the ambiguityof words a single vector cannot represent multiple meanings of a word well becauseit may lead to semantic confusion among the different senses of this wordthe multi prototype vector space model 51 is proposed to better represent dif ferent meanings of a word in multi prototype vector space model a mixture ofvon mises fisher distributions movmf clustering method with rst order unigramcontexts 5 is used to cluster different meanings of a word formally it assigns adifferent word representation wix to the same word x in each different cluster iwhen the multi prototype embedding is used the similarity between two words x yis computed straightforwardly if contexts of words are not available the similaritybetween two words is dened asavgsimx y  1k 2ki1kj1swix w jy235maxsimx y max1i jk swix w jy236where k is a hyperparameter indicating the number of the clusters and s is a simi larity function of two vectors such as cosine similarity when contexts are availablethe similarity can be computed more precisely asavgsimcx y  1k 2ki1kj1scxiscy jswix w jy237maxsimcx y  s wx wy238262word representationwherescxi  swic wixisthelikelihoodofcontextc belongingtoclusteriandwx  warg max1ik scxix is the maximum likelihood cluster for x in context c withmulti prototype embeddings the accuracy on the word similarity task is signicantlyimproved but the performance is still sensitive to the number of clustersalthough the multi prototype embedding method can effectively cluster differentmeaningsofawordviaitscontextstheclusteringisofineandthenumberofclustersis xed and needs to be predened it is difcult for a model to select an appropriateamount of meanings for different words to adapt to new senses new words or newdataandtoalignthesenseswithprototypestoaddresstheseproblems12proposesa unied model for word sense representation and word sense disambiguation thismodel uses available knowledge bases such as wordnet 46 to determine the sensesof a word each word and each sense had a single vector and are trained jointly thismodel can learn representations of both words and senses and two simple methodsare proposed to do disambiguation using the word and sense vectors253multisource word representationthereis muchinformationabout words that canbeleveragedtoimprovethequalityofword representations we will introduce other kinds of word representation learningmethods utilizing multisource information2531word representation with internal informationthere is much information locating inside words which can be utilized to improvethe quality of word representations furtherusing character information many languages such as chinese and japanesehave thousands of characters and the words in these languages are composed ofseveral characters characters in these languages have richer semantic informationcomparing with other languages containing only dozens of characters hence themeaning of a word can not only be learned from its contexts but also the compositionof characters driven by this intuitive idea 13 proposes a joint learning model forcharacter and word embeddings cwe in cwe a word is a composition of aword embedding and its character embeddings formallyx  w  1wici239where x is the representation of a word which is the composition of a word vector wand several character vectors ci and w is the number of characters in the word notethat this model can be integrated with various models such as skip gram cbowand glove25 extensions27further position based and cluster based methods are proposed to address thisissue that characters are highly ambiguous in position based approach each char acter is assigned three vectors which appear in begin middle and end of a wordrespectively since the meaning of a character varies when it appears in the differentpositions of a word this method can signicantly resolve the ambiguity problemhowever characters that appear in the same position may also have different mean ings in the cluster based method a character is assigned k different vectors for itsdifferent meanings in which a words context is used to determine which vector tobe usedby introducing character embeddings the representation of low frequency wordscan be signicantly improved besides this method can deal with new words whileother methods fail experiments show that the joint learning method can achieve bet ter performance on both word similarity and word analogy tasks by disambiguatingcharacters using the position based and cluster based method it can further improvethe performanceusing morphology information many languages such as english have richmorphology information and plenty of rare words most word representation modelsassign a distinct vector to each word ignoring the rich morphology information thisis a limitation because the afxes of a word can help infer the meaning of a wordand the morphology information of word is essential especially when facing rarecontextsto address this issue 8 proposes to represent a word as a bag of morphology n gramsthismodelsubstituteswordvectorsinskip gramwiththesumofmorphologyn gram vectors when creating the dictionary of n grams they select all n grams witha length greater or equal than 3 and smaller or equal than 6 to distinguish prexes andsufxes with other afxes they also add special characters to indicate the beginningand the end of a word this model is efcient and straightforward which achievesgood performance on word similarity and word analogy tasks especially when thetraining set is smallreference 41 further uses a bidirectional lstm to generate word representationby composing morphologies this model does not use a look up table to assign adistinct vector to each word like what those independent word embedding methodsare doing hence this model not only signicantly reduces the number of parametersbut also addresses some disadvantages of independent word embeddings moreoverthe embeddings of words in this model could affect each other2532word representation with external knowledgebesides internal information of words there is much external knowledge that couldhelp us learn the word representationsusing knowledge base some languages have rich internal information whereaspeople have also annotated lots of knowledge bases which can be used in wordrepresentation learning to constrain embeddings reference 62 introduces relationconstraints into the cbow model with these constraints the embeddings can not282word representationonly predict its contexts but also predict words with relations the objective is tomaximize the sum of log probability of all relations aso  1nni1wrwilog pwwi240where rwi indicates a set of words which have relation with wi then the jointobjective is dened aso  1nni1log pwiw j jil ji  nni1wrwilog pwwi241where  is a hyperparameter the external information helps to train a better wordrepresentation which shows signicant improvements on word similarity bench marksmoreover retrotting 19 introduces a post processing step which can introduceknowledge bases into word representation learning it is more modular than otherapproacheswhichconsiderknowledgebaseduringtrainingletthewordembeddingslearned by existing word representation approaches be e retrotting attempts to ndanother embedding space e which is close to e but considers the relations in theknowledge base formallyl iiwi wi2 i jri jwi w j2242where  and  are hyperparameters indicating the strength of the associations andr is a set of relations in the knowledge base the adapted embeddings e can beoptimized by several iterations of the following online updateswi  ji jr i j w j  iwi ji jr i j  i243where  is usually set to 1 and i j is degi1 deg is a nodes degree in a knowledgegraph with knowledge bases such as the paraphrase database 27 wordnet 46and framenet 3 this model can achieve consistent improvement on word similaritytasks but it also may signicantly reduce the performance on the analogy of syntacticrelations since this module is a post processing of word embeddings it is compatiblewith various distributed representation modelsin addition to the aforementioned synonym based knowledge bases there are alsosememe based knowledge bases in which the sememe is dened as the minimumsemantic unit of word meanings hownet 16 is one of such knowledge baseswhich annotates each chinese word with one or more relevant sememes general25 extensions29knowledge injecting methods could not apply to hownet as a result 47 proposesa specic model to introduce hownet into word representation learningbases on skip gram model 47 introduces sense and sememe embeddings torepresent target word wi more specically this model leverages context wordswhich are represented with original word embeddings as attention over multiplesenses of target word wi to obtain its new embeddingswi swi k1attswikswik244where swikdenotes the kth sense embedding of wi and swi is the sense set of withe attention term is as followsattswik expwc  swikswi n1 expwc  swin245where swikstands for the average of sememe embeddings x swik avgxsk andwc is the average of context word embeddings wc  avgw j j i l j  ithis model shows a substantial advance in both word similarity and analogytasks moreover the introduction of sense embeddings can also be used in wordsense disambiguationconsidering document information word embedding methods like skip gramsimply consider the context information within a window to learn word represen tation however the information in the whole document could help our word rep resentation learning topical word embeddings twe 42 introduces topic infor mation generated by latent dirichlet allocation lda to help distinguish differentmeanings of a word the model is dened to maximize the following average logprobabilityo  1nni1kckc0log pwicwi  log pwiczi 246where wi is the word embedding and zi is the topic embedding of wi each word wiis assigned a unique topic and each topic has a topic embedding the topical wordembedding model shows advantages of contextual word similarity and documentclassication taskshowever twe simply combines the lda with word embeddings and lacks statis tical foundations the lda topic model needs numerous documents to learn seman tically coherent topics reference 40 further proposes the topicvec model whichencodes words and topics in the same semantic space topicvec outperforms tweand other word embedding methods on text classication datasets it can learn coher ent topics on only one document which is not possible for other topic models302word representation2533word representation with hierarchical structurehuman knowledge is in a hierarchical structure recently many works also introducea hierarchical structure of texts into word representation learningdependency based word representation continuous word embeddings arecombinations of semantic and syntactic information however existing word repre sentation models depend solely on linear contexts and show more semantic infor mation than syntactic information to make the embeddings show more syntacticinformation the dependency based word embedding 38 uses the dependency basedcontext the dependency based embeddings are less topical and exhibit more func tional similarity than the original skip gram embeddings it takes the information ofdependency parsing tree into consideration when learning word representations thecontexts of a target word w are the modiers of this word ie m1r1     mkrkwhere ri is the type of the dependency relation between the head node and the mod ier when training the model optimizes the probability of dependency based con texts rather than neighboring contexts this model gains some improvements onword similarity benchmarks compared with skip gram experiments also show thatwords with syntactic similarity are more similar in the vector spacesemantic hierarchies because of the linear substructure of the vector spaceit is proven that word embeddings can make simple analogies for example thedifference between japan and tokyo is similar to the difference between chinaand beijing but it has trouble identifying hypernym hyponym relations sincethese relationships are complicated and do not necessarily have linear substructureto address this issue 25 tries to identify hypernym hyponym relationships usingword embeddings the basic idea is to learn a linear projection rather than simplyuse the embedding offset to represent the relationship the model optimizes theprojection asm arg minm1ni jmxi y j2247where xi and y j are hypernym and hyponym embeddingsto further increase the capability of the model they propose to rst cluster wordpairs into several groups and learn a linear projection for each group the linearprojection can help identify various hypernym hyponym relations254multilingual word representationthere are thousands of languages in the world in word level how to represent wordsfrom different languages in a unied vector space is an interesting problem thebilingual word embedding model 64 uses machine translation word alignments asconstraining translational evidence and embeds words of two languages into a singlevector space the basic idea is 1 to initialize each word according to its aligned25 extensions31words in another language and 2 to constrain the distance between two languagesduring the training using translation pairswhen learning bilingual word embeddings it rstly trains source word embed dings then they use aligned sentence pairs to count the co occurrence of source andtarget words the target word embeddings can be initialized asetinit ss1nts  1nt  s es248where es and etinit are the trained embeddings of the source word and the initialembedding of the target word respectively nts is the number of target words beingaligned with source word s is all the possible alignments of word t so nt  snormalizes the weights as a distribution during the training they jointly optimizethe word embedding objective as well as the bilingual constraint the constraint isdened aslcnen  een nencnecn2249where nencn is the normalized align countswhen given a lexicon of bilingual word pairs 44 proposes a simple model thatcan learn bilingual word embeddings in a unied space based on the distributionalgeometric similarities of word vectors of two languages this model learns a lineartransformation matrix t that transforms the vector space of source language to thatof the target language the training loss isl  tes et2250where et is the word vector matrix of aligned words in target languagehowever this model performs badly when the seed lexicon is small to tacklethis limitation some works introduce the idea of bootstrapping into bilingual wordrepresentation learning lets take 63 for example in this work in addition tomonolingual word embedding learning and bilingual word embedding alignmentbased on seed lexicon a new matching mechanism is introduced the main idea ofmatching is to nd the most probably matched source target word for each targetsource word and make their embeddings closer next we explain the target to source matching process formally and the source to target side is similarthe target to source matching loss function is dened aslt 2s  log pct es logmpct  mes251where ct  denotes the target corpus and m is a latent variable specifying the matchedsource word for each target word on independency assumption it has322word representationpct  meswt ict pwt i mesv t i1pwtiwsminwt i 252where nwt iis the number of wt ioccurrences in the target corpus by training usingviterbi em algorithm this method can improve bilingual word embeddings on itsown and address the limitation of a small seed lexicon255task specic word representationin recent years word representation learning has achieved great success and playeda crucial role in nlp tasks people nd that word representation learning of thegeneral eld is still a limitation in a specic task and begin to explore the learningof task specic word representation in this section we will take sentiment analysisas an exampleword representation for sentiment analysis most word representation meth ods capture syntactic and semantic information while ignoring sentiment of text thisis problematic because words with similar syntactic polarity but opposite sentimentpolarity obtain closed word vectors reference 58 proposes to learn sentiment specic word embeddings sswe by integrating the sentiment information anintuitive idea is to jointly optimize the sentiment classication model using wordembeddings as its feature and sswe minimizes the cross entropy loss to achievethis goal to better combine the unsupervised word embedding method and the super viseddiscriminativemodeltheyfurtherusethewordsinawindowratherthanawholesentence to classify sentiment polarity they propose the following ranking basedlosslrt  max0 1 1st f r0 t  1st f r1 t253where f r0  f r1 are the predicted positive and negative scores 1st is an indicatorfunction1st 1if t is positive1 if t is negative254this loss function only punishes the model when the model gives an incorrectresultto get massive training data they use distant supervision technology to gener ate sentiment labels for a document the increase of labeled data can improve thesentiment information in word embeddings on sentiment classication tasks senti ment embeddings outperform other strong baselines including svm and other wordembedding methods sswe also shows strong polarity consistency where the clos est words of a word are more likely to have the same sentiment polarity compared25 extensions33with existing word representation models this sentiment specic word embeddingmethod provides us a general way to learn task specic word embeddings which isto design a joint loss function and to generate massive labeled data automatically256time specic word representationthe meaning of a word changes during the time analyzing the changing meaningof a word is an exciting topic in both linguistic and nlp research with the riseof word embedding methods some works 29 35 use embeddings to analyze thechange of words meanings they separate corpus into bins with respect to yearsto train time specic word embeddings and compare embeddings of different timeseries to analyze the change of word semantics this method is intuitive but has someproblems dividing corpus into bins causes the data sparsity issue the objective ofword embedding methods is nonconvex so that different random initialization leadsto different results which makes comparing word embeddings difcult embeddingsof a word in different years are in different semantic spaces and cannot be compareddirectly most work indirectly compares the meanings of a word in a different timeby the changes of a words closest words in the semantic spaceto address these issues 4 proposes a dynamic skip gram model which connectsseveral bayesian skip gram models 6 using kalman lters 33 in this model theembeddings of words in different periods could affect each other for example a wordthat appears in the 1990s document can affect the embeddings of that word in the1980s and 2000s moreover it also trains the embedding in different periods by thewholecorpustoreducethesparsityissuethismodelalsoputsalltheembeddingsintothe same semantic space which is a signicant improvement against other methodsand makes word embeddings in different periods comparable therefore the changeof word embeddings in this model is continuous and smooth experimental resultsshow that the cosine distance between two words changes much more smoothly inthis model than those models which simply divide the corpus into bins26evaluationin recent years various methods to embed words into a vector space have beenproposed hence it is essential to evaluate different methods there are two gen eral evaluations of word embeddings including word similarity and word analogythey both aim to check if the word distribution is reasonable these two evaluationssometimes give different results for example cbow achieves better performanceon word similarity whereas skip gram outperforms cbow on word analogy there fore which method to choose depends on the high level application task specicword embedding methods are usually designed for specic high level tasks and342word representationachieve signicant improvement on these tasks compared with baselines such ascbow and skip gram however they only marginally outperform baselines on twogeneral evaluations261word similarityrelatednessthe dynamics of words are very complex and subtle there is no static nite set ofrelations that can describe all interactions between two words it is also not trivial fordownstream tasks to leverage different kinds of word relations a more practical wayis to assign a score to a pair of words to represent to what extent they are related thismeasurement is called word similarity when talking about the term word similaritythe precise meaning may vary a lot in different situations there are several kinds ofsimilarity that may be referred to in various literaturemorphological similarity many languages including english dene morphol ogy the same morpheme can have multiple surface forms according to the syntac tical function for example the word active is an adjective and activenessis its noun version the word activate is a verb and activation is its nounversion the morphology is an important dimension when considering the meaningand usage of words it denes some relations between words from a syntactical viewsome relations are used in the syntactic word relationship test set 43 includingadjectives to adverbs past tense and so on however in many higher level applica tions and tasks the words are often morphologically normalized by the base formthis process is also known as lemmatization one widely used technique is theporter stemming algorithm 49 this algorithm converts active activenessactivate and activation to the same root format activ by removing mor phological features the semantic meaning of words is more emphasizedsemantic similarity two words are semantically similar if they can expressthe same concept or sense like article and document one word may havedifferent senses and each of its synonyms is associated with one or more of its senseswordnet 46 is a lexical database that organizes the words as groups according to thesenses each group of words is called a synset which contains all synonymous wordssharing the same specic sense the words within the same synset are consideredsemantically similar words from two synsets that are linked by some certain relationsuch as hyponym are also considered semantically similar to some degree likebankriver and bankbankriveris the hyponym of banksemantic relatedness most modern literature that considers word similarityrefers to the semantic relatedness of words semantic relatedness is more generalthan semantic similarity words that are not semantically similar could still be relatedin many ways such as meronymy car and wheel or antonymy hot and coldsemantic relatedness often yields co occurrence but they are not equivalent thesyntactic structure could also yield co occurrence reference 10 argues that distri butional similarity is not an adequate proxy for semantic relatedness26 evaluation35table 23 datasets for evaluating word similarityrelatednessdatasetsimilarity typerg 65 52word similaritywordsim 353 22mixedwordsim 353 rel 1word relatednesswordsim 353 sim 1word similaritymturk 287 50word relatednesssimlex 999 31word similaritytoevaluatethewordrepresentationsystemintrinsicallythemostpopularapproachis to collect a set of word pairs and compute the correlation between human judg ment and system output so far many datasets are collected and made public somedatasets focus on the word similarity such as rg 65 52 and simlex 999 31other datasets concern word relatedness such as mturk 50 wordsim 353 22 isa very popular dataset for word representation evaluation but its annotation guidelinedoes not differentiate similarity and relatedness very clearly reference 1 conductsanother round of annotation based on wordsim 353 and generates two subsets onefor similarity and the other for relatedness some information about these datasets issummarized in table 23to evaluate the similarity of two distributed word vectors researchers usuallyselect cosine similarity as an evaluation metric the cosine similarity of word w andword v is dened assimw v w  vwv255when evaluating a word representation approach the similarity of each word pairis computed in advance using cosine similarity after that spearmans correlationcoefcient  is then used to evaluate the similarity between human annotator andword representation model as  1 6  d2in3 n 256where a higher spearmans correlation coefcient indicates they are more similarreference 10 describes a series of methods based on wordnet to evaluate thesimilarity of a pair of words after the comparison between the traditional wordnet based methods and distributed word representations 1 addresses that relatednessandsimilarityaretwodifferentconcernstheypointoutthatwordnet basedmethodsperform better on similarity than on relatedness while distributed word representa tion shows similar performance on both a series of distributed word representationsare compared on a wide variety of datasets in 56 the state of the art on bothsimilarity and relatedness is achieved by distributed representation without a doubt362word representationthis evaluation method is simple and straightforward however as stated in 20there are several problems with this evaluation since the datasets are small less than1000 word pairs in each dataset one system may yield many different scores ondifferent partitions testing on the whole dataset makes it easier to overt and hardto compute the statistical signicance moreover the performance of a system onthese datasets may not be very correlated to its performance on downstream tasksthe word similarity measurement can come in an alternative format the toeflsynonyms test in this test a cue word is given and the test is required to choose onefrom four words that are the synonym of the cue word the exciting part of this task isthat the performance of a system could be compared with human beings reference37 evaluates the system with the toefl synonyms test to address the knowledgeinquiring and representing of lsa the reported score is 644 which is very closeto the average rating of the human test takers on this test set with 80 queries 54reported a score of 720 reference 24 extends the original dataset with the helpof wordnet and generates a new dataset3 named wordnet based synonymy testcontaining thousands of queries262word analogybesides word similarity the word analogy task is an alternative way to measurehow well representations capture semantic meanings of words this task gives threewords w1 w2 and w3 then it requires the system to predict a word w4 such thatthe relation between w1 and w2 is the same as that between w3 and w4 this task isused since 43 45 to exploit the structural relationships among words here theword relations could be divided into two categories including semantic relationsand syntactic relations this is a relatively novel method for word representationevaluation but quickly becomes a standard evaluation metric since the dataset isreleased unlike the toefl synonyms test most words in this dataset are frequentacrossallkindsofthecorpusbutthefourthwordischosenfromthewholevocabularyinstead of four options this test favors distributed word representations because itemphasizes the structure of word spacethe comparison between different models on the word analogy task measured byaccuracy could be found in 7 56 57 6127summaryin this chapter we rst introduce word representation methods including one hotrepresentation and various distributed representation methods these classical meth ods are the important foundation of various nlp models and meanwhile present the3httpwwwcscmuedudaynewbst nanewstargz27 summary37major concepts and mechanisms of word representation learning for the reader nextconsidering classical word representation methods often suffer from the word poly semy we further introduce the effective contextualized word representation methodselmo to show the approach to capture complex word features across differentlinguistic contexts as word representation methods are widely utilized in variousdownstream tasks we then overview numerous extensions toward some representa tive directions and discuss how to adapt word representations for specic scenariosfinally we introduce several evaluation tasks of word representation including wordsimilarity and word analogy which are the basic experimental settings for research ing word representation methodsin the past decade learning methods and applications of word representationhave been studied in depth here we recommend some surveys and books on wordrepresentative learning for reading erk vector space models of word meaning and phrase meaning a survey 18 lai et al how to generate a good word embedding 36 camacho et al from word to sense embeddings a survey on vector represen tations of meaning 11 ruder et al a survey of cross lingual word embedding models 53 bakarov a survey of word embeddings evaluation methods 2in the future toward more effective word representation learning some directionsare requiring further efforts1 utilizing more knowledge current word representation learning models focuson representing words based on plain textual corpora in fact besides richsemantic information in text there are also various kinds of word related infor mation hidden in heterogeneous knowledge in the real world such as visualknowledge factual knowledge and commonsense knowledge some prelimi nary explorations have attempted 59 60 to utilize heterogeneous knowledgefor learning better word representations and these explorations indicate thatutilizing more knowledge is a promising direction toward enhancing word rep resentations there remain open problems for further explorations2 considering more contexts as shown in this chapter those word representa tion learning methods considering contexts can achieve more expressive wordembeddings which can grasp richer semantic information and further bene t downstream nlp tasks than classical distributed methods context awareword representations have been systematically veried for their effectivenessin existing works 32 48 and adopting those context aware word representa tions has also become a necessary and mainstream operation for various nlptasks after bert 15 has been proposed language models pretrained on large scale corpora have entered the public vision and their ne tuning models havealso achieved the state of the art performance on specic nlp tasks these newexplorations based on large scale textual corpora and pretrained ne tuning lan guage representation architectures indicate a promising direction to considermore contexts with more powerful representation architectures and we will dis cuss them more in the next chapter382word representation3 orienting finer granularity polysemy is a widespread phenomenon forwords hence it is essential and meaningful to consider the ner granulatedsemantic information than the words themselves as some linguistic knowledgebases have been developed such as synonym based knowledge bases word net 21 and sememe based knowledge bases hownet 17 we thus have waysto study the atomic semantics of words the current work on word representa tions learning is coarse grained and mainly focuses on shallow semantics of thewords themselves in text and ignores the rich semantic information inside thewords which is also an important resource for achieving better word embed dings reference 28 explores to inject ner granulated atomic semantics ofwords into word representations and performs much better language understand ing although these explorations are still preliminary orienting ner granularityof word representations is important in the next chapter we will also introducemore details in this partin the past decade learning methods and applications of distributed representationhave been studied in depth because of its efciency and effectiveness lots of task specic models have been proposed for various tasks word representation learninghas become a popular and important topic in nlp however word representationlearning is still challenging due to its ambiguity data sparsity and interpretability inrecent years word representation learning has been no longer studied in isolation butexplored together with sentence or document representation learning using pretrainedlanguage models readers are recommended to refer to the following chapters tofurther learn the integration of word representations in other scenariosreferences1 eneko agirre enrique alfonseca keith hall jana kravalova marius pasca and aitor soroaa study on similarity and relatedness using distributional and wordnet based approaches inproceedings of hlt naacl 20092 amirbakarovasurveyofwordembeddingsevaluationmethodsarxivpreprint arxiv180109536 20183 collin f baker charles j fillmore and john b lowe the berkeley framenet project inproceedings of acl 19984 robert bamler and stephan mandt dynamic word embeddings via skip gram ltering arxivpreprint arxiv170208359 20175 arindam banerjee inderjit s dhillon joydeep ghosh and suvrit sra clustering on the unithypersphere using von mises sher distributions journal of machine learning research 20056 oren barkan bayesian neural word embedding in proceedings of aaai 20177 marco baroni georgiana dinu and germn kruszewski dont count predict a systematiccomparison of context counting vs context predicting semantic vectors in proceedings ofacl 20148 piotr bojanowski edouard grave armand joulin and tomas mikolov enriching word vectorswith subword information transactions of the association for computational linguistics5135146 20179 peter f brown peter v desouza robert l mercer vincent j della pietra and jenifer c laiclass based n gram models of natural language computational linguistics 1844674791992references3910 alexanderbudanitskyandgraeme hirstevaluatingwordnet basedmeasuresoflexical seman tic relatedness computational linguistics 3211347 200611 jose camacho collados and mohammad taher pilehvar from word to sense embeddingsa survey on vector representations of meaning journal of articial intelligence research63743788 201812 xinxiong chen zhiyuan liu and maosong sun a unied model for word sense representationand disambiguation in proceedings of emnlp 201413 xinxiong chen lei xu zhiyuan liu maosong sun and huanbo luan joint learning ofcharacter and word embeddings in proceedings of ijcai 201514 scott c deerwester susan t dumais thomas k landauer george w furnas and richard aharshman indexing by latent semantic analysis japan analytical  scientic instrumentsshow 416391407 199015 jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training ofdeep bidirectional transformers for language understanding in proceedings of naacl 201916 zhendong dong and qiang dong hownet a hybrid language and knowledge resource inproceedings of nlp ke 200317 zhendong dong and qiang dong hownet and the computation of meaning with cd romworld scientic 200618 katrin erk vector space models of word meaning and phrase meaning a survey languageand linguistics compass 610635653 201219 manaal faruqui jesse dodge sujay kumar jauhar chris dyer eduard hovy and noah asmith retrotting word vectors to semantic lexicons in proceedings of naacl hlt 201520 manaalfaruquiyuliatsvetkovpushpendrerastogiandchrisdyerproblemswithevaluationof word embeddings using word similarity tasks in proceedings of repeval 201621 christiane fellbaum wordnet the encyclopedia of applied linguistics 201222 lev finkelstein evgeniy gabrilovich yossi matias ehud rivlin zach solan gadi wolfmanand eytan ruppin placing search in context the concept revisited in proceedings of www200123 john r firth a synopsis of linguistic theory 19301955 195724 dayne freitag matthias blume john byrnes edmond chow sadik kapadia richard rohwerand zhiqiang wang new experiments in distributional representations of synonymy in pro ceedings of conll 200525 ruiji fu jiang guo bing qin wanxiang che haifeng wang and ting liu learning semantichierarchies via word embeddings in proceedings of acl 201426 alona fyshe leila wehbe partha pratim talukdar brian murphy and tom m mitchell acompositional and interpretable semantic space in proceedings of hlt naacl 201527 juri ganitkevitch benjamin van durme and chris callison burch ppdb the paraphrasedatabase in proceedings of hlt naacl 201328 yihong gu jun yan hao zhu zhiyuan liu ruobing xie maosong sun fen lin and leyulin language modeling with sparse product of sememe experts in proceedings of emnlppages 46424651 201829 william l hamilton jure leskovec and dan jurafsky diachronic word embeddings revealstatistical laws of semantic change in proceedings of acl 201630 zellig s harris distributional structure word 1023146162 195431 felix hill roi reichart and anna korhonen simlex 999 evaluating semantic models withgenuine similarity estimation computational linguistics 201532 jeremy howard and sebastian ruder universal language model ne tuning for text classi cation in proceedings of acl pages 328339 201833 rudolph emil kalman et al a new approach to linear ltering and prediction problems journalof basic engineering 8213545 196034 pentti kanerva jan kristofersson and anders holst random indexing of text samples forlatent semantic analysis in proceedings of cogsci 200035 yoon kim yi i chiu kentaro hanaki darshan hegde and slav petrov temporal analysis oflanguage through neural language models in proceedings of the acl workshop 2014402word representation36 siwei lai kang liu shizhu he and jun zhao how to generate a good word embeddingieee intelligent systems 316514 201637 thomas k landauer and susan t dumais a solution to platos problem the latent seman tic analysis theory of acquisition induction and representation of knowledge psychologicalreview 1042211 199738 omer levy and yoav goldberg dependency based word embeddings in proceedings of acl201439 omer levy and yoav goldberg neural word embedding as implicit matrix factorization inproceedings of neurips 201440 shaohua li tat seng chua jun zhu and chunyan miao generative topic embedding acontinuous representation of documents in proceedings of acl 201641 wang ling chris dyer alan w black isabel trancoso ramn fermandez silvio amir luismarujo and tiago lus finding function in form compositional character models for openvocabulary word representation in proceedings of emnlp 201542 yang liu zhiyuan liu tat seng chua and maosong sun topical word embeddings inproceedings of aaai 201543 tomas mikolov kai chen greg corrado and jeffrey dean efcient estimation of wordrepresentations in vector space in proceedings of iclr 201344 tomas mikolov quoc v le and ilya sutskever exploiting similarities among languages formachine translation arxiv preprint arxiv13094168 201345 tomas mikolov wen tau yih and geoffrey zweig linguistic regularities in continuous spaceword representations in proceedings of hlt naacl 201346 george a miller wordnet a lexical database for english communications of the acm38113941 199547 yilin niu ruobing xie zhiyuan liu and maosong sun improved word representation learn ing with sememes in proceedings of acl 201748 matthew peters mark neumann mohit iyyer matt gardner christopher clark kenton leeand luke zettlemoyer deep contextualized word representations in proceedings of naacl hlt pages 22272237 201849 martin f porter an algorithm for sufx stripping program 143130137 198050 kira radinsky eugene agichtein evgeniy gabrilovich and shaul markovitch a word at atime computing word relatedness using temporal semantic analysis in proceedings of www201151 joseph reisinger and raymond j mooney multi prototype vector space models of word mean ing in proceedings of hlt naacl 201052 herbert rubenstein and john b goodenough contextual correlates of synonymy communi cations of the acm 810627633 196553 sebastian ruder ivan vulic and anders sgaard a survey of cross lingual word embeddingmodels journal of articial intelligence research 65569631 201954 magnus sahlgren vector based semantic analysis representing word meanings based onrandom labels in proceedings of workshop on skac 200155 magnus sahlgren an introduction to random indexing in proceedings of tke 200556 tobias schnabel igor labutov david mimno and thorsten joachims evaluation methods forunsupervised word embeddings in proceedings of emnlp 201557 fei sun jiafeng guo yanyan lan jun xu and xueqi cheng learning word representationsby jointly modeling syntagmatic and paradigmatic relations in proceedings of acl 201558 duyu tang furu wei nan yang ming zhou ting liu and bing qin learning sentiment specic word embedding for twitter sentiment classication in proceedings of acl 201459 kristinatoutanovadanqichenpatrickpantelhoifungpoonpallavichoudhuryandmichaelgamon representing text for joint embedding of text and knowledge bases in proceedings ofthe emnlp pages 14991509 201560 zhen wang jianwen zhang jianlin feng and zheng chen knowledge graph and text jointlyembedding in proceedings of emnlp pages 15911601 2014references4161 daniyogatamamanaalfaruquichrisdyerandnoahasmithlearningwordrepresentationswith hierarchical sparse coding in proceedings of icml 201562 mo yu and mark dredze improving lexical embeddings with semantic knowledge in pro ceedings of acl 201463 meng zhang haoruo peng yang liu huan bo luan and maosong sun bilingual lexiconinduction from non parallel data with minimal supervision in proceedings of aaai 201764 will y zou richard socher daniel m cer and christopher d manning bilingual wordembeddings for phrase based machine translation in proceedings of emnlp 2013open access this chapter is licensed under the terms of the creative commons attribution 40international license httpcreativecommonsorglicensesby40 which permits use sharingadaptation distribution and reproduction in any medium or format as long as you give appropriatecredit to the original authors and the source provide a link to the creative commons license andindicate if changes were madethe images or other third party material in this chapter are included in the chapters creativecommons license unless indicated otherwise in a credit line to the material if material is notincluded in the chapters creative commons license and your intended use is not permitted bystatutory regulation or exceeds the permitted use you will need to obtain permission directly fromthe copyright holderchapter 3compositional semanticsabstract many important applications in nlp elds rely on understanding morecomplex language units such as phrases sentences and documents beyond wordstherefore compositional semantics has remained a core task in nlp in this chapterwe rst introduce various models for binary semantic composition including additivemodels and multiplicative models after that we present various typical models forn ary semantic composition including recurrent neural network recursive neuralnetwork and convolutional neural network31introductionfrom the previous chapter following the distributed hypothesis one could projectthe semantic meaning of a word into a low dimensional real valued vector accordingto its context information which is named as word vectors here comes a furtherproblem how to compress a higher semantic unit into a vector or other kinds ofmathematical representations like a matrix or a tensor in other words using repre sentation learning to model a semantic composition function remains an unsolvedbut surging research topic recentlycompositionality enables natural languages to construct complex semantic mean ings from the combinations of simpler semantic elements this property is oftencaptured with the following principle the semantic meaning of a whole is a functionof the semantic meanings of its several parts therefore the semantic meanings ofcomplex structures will depend on how their semantic elements combinehere we express the composition of two semantic units which are denoted asu and v respectively and the most intuitive way to dene the joint representationcould be formulated as followsp  f u v31where p corresponds to the representation of the joint semantic unit u v it shouldbe noted that here u and v could denote words phrases sentences paragraphs oreven higher level semantic units the authors 2020z liu et al representation learning for natural language processinghttpsdoiorg101007978 981 15 5573 2 343443compositional semanticshowever given the representations of two semantic constituents it is not enoughto derive their joint embeddings with the lack of syntactic information for instancealthough the phrase machine learning and learning machine have thesame vocabulary they contain different meanings machine learning refers toa research eld in articial intelligence while learning machine means somespecic learning algorithms this phenomenon stresses the importance of syntacticand order information in a compositional sentence reference 12 takes the role ofsyntactic and order information into consideration and suggests a further renementof the above principle the meaning of a whole is a function of the meaning of itsseveral parts and the way they are syntactically combined therefore the compositionfunction in eq 31 is redened to combine the syntactic relationship rule r betweenthe semantic units u and vp  f u v r32where r denotes the syntactic relationship rule between two constituent semanticunitsunfortunately even this formulation may not be fully adequate therefore 7claims that the meaning of a whole is greater than the meanings of its severalparts it implies that people may suffer from the problem of constructing com plex meanings rather than simply understanding the meanings of several parts andtheir syntactic relations in real language composition in different contexts thesame sentence could have different meanings which means that some sentencesare hard to understand without any background information for example thesentence tom and jerry is one of the most popular comediesin that style needs two main backgrounds firstly tom and jerry isa special noun phrase or knowledge entity which indicates a cartoon comedy ratherthantwoordinarypeopletheotherpriorknowledgeshouldbethat stylewhichneeds further explanation in the previous sentences hence a full understanding ofthe compositional semantics needs to take existing knowledge into account herethe argument k is added into the composition function incorporating knowledgeinformation as a prior in the compositional processp  f u v r k 33where k represents the background knowledgereference 4 claims that we should ask for the meaning of a word in isolation butonly in the context of a statement that is the meaning of a whole is constructed fromits parts and the meanings of the parts are meanwhile derived from the whole more over compositionality is a matter of degree rather than a binary notion linguisticstructures range from fully compositional eg black hair to partly compositionalsyntactically xed expressions eg take advantage in which the constituents canstill be assigned separate meanings and non compositional idioms eg kick thebucket or multi word expressions eg by and large whose meaning cannot bedistributed across their constituents 1131 introduction45from the above three equations formulating composition function it could beconcluded that composition could be viewed as a specic binary operation butbeyond this the syntactic message could help to indicate a particular approach whilebackground knowledge helps to explain some obscure words or specic context dependent entities such as pronouns beyond binary compositional operations onecould build the sentence level composition by applying binary composition oper ations recursively in this chapter we will rst explain some sorts of basic binarycomposition functions in both the semantic vector space and matrix vector spaceafter we will climb up to more complex composition scenarios and introduce severalapproaches to model sentence level composition32semantic space321vector spacein general the central task in semantic representation is projecting words from anabstract semantic space to a mathematical low dimensional space as introduced inthe previous chapters to make the transformation reasonable the purpose is to main tain the word similarity in this new projected space in other words the more similarthe words are the closer their vectors should be for instance we hope the wordvectors wbook and wmagazine are close while the word vectors wapple andwcomputer are far away in this chapter we will introduce several widely usedtypical semantic vector space including one hot representation distributed represen tation and distributional representation322matrix vector spacedespite the wide use of semantic vector spaces an alternative semantic space isproposed to be a more powerful and general compositional semantic frameworkdifferent from conventional vector spaces matrix vector semantic space utilizes amatrix to represent the word meaning rather than a skinny vector the motivationbehind this is when modeling the semantic meaning under a specic context one iswondering not only what is the meaning of each word but also the holistic meaningof the whole sentence thus we concern about the semantic transformation betweenadjacent words inside each sentence however the semantic vector space could notcharacterize the semantic transformation of one word on the others explicitlydriven by the idea of modeling semantic transformation some researchers haveproposed to use a matrix to represent the transformation operation of one word on theothers different from those vector space models it could incorporate some structuralinformation like the word order and syntax composition463compositional semantics33binary compositionthe goal is to construct vector representations for phrases sentences paragraphsand documents without loss of generality we assume that each constituent of aphrase sentence paragraph or document is embedded into a vector which willbe subsequently combined in some way to generate a representation vector for thephrase sentence paragraph or document1in this section we focus on binary composition we will take phrases consistingof a head and a modier or complement as an example if we cannot model the binarycomposition or phrase representation there is little hope that we can construct morecomplex compositional representations for sentences or even documents thereforegiven a phrase such as machine learning and the vectors u and v representing theconstituents machine and learning respectively we aim to produce a represen tation vector p of the whole phrase let the hypothetical vectors for machine andlearning be 0 3 1 5 2 and 1 4 2 2 0 respectively this simplied seman tic space will serve to illustrate examples of the composition functions which weconsider in this sectionthe fundamental problem of semantic composition modeling in representing atwo word phrase is designing a primitive composition function as a binary operatorbased on this function one could apply it on a word sequence recursively and derivesentence level composition here a word sequence could be any level of the seman tic units such as a phrase a sentence a paragraph a knowledge entity or even adocumentfrom the previous section one of the basic formulae is to formulate semanticcomposition f in the following equationp  f u v r k 34where u v denote the representations of the constituent parts in this semantic unitp denotes the joint representation r indicates the relationship while k indicatesthe necessary background knowledge the expression denes a wide class of com position functions for easier discussion we give some appropriate constraints tonarrow the space of our considering function first we will ignore the backgroundknowledge k to explore what can be achieved without any utilization of backgroundor world knowledge second for the consideration of the syntactic relation r wecan proceed by investigating only one relation at a time and then we can removeany explicit dependence on r which allows us to explore any possible distinct com position function for various syntactic relations that is we simplify the formulap  f u v by simply ignoring the background knowledge and relationship1note that the problem of combining semantic vectors of small units to make a representation for amulti word sequence is different from the problem of incorporating information about multi wordcontexts into a distributional representation for a single target word33 binary composition47in recent years modeling the binary composition function is a well studied butstill challenging problem there are mainly two perspectives toward this questionincluding the additive model and the multiplicative model331additive modelthe additive model has a constraint in which it assumes that p u and v lie in thesame semantic space this essentially means that all syntactic types have the samedimension one of the simplest ways is to directly use the sum to represent the jointrepresentationp  u  v35according to eq 35 the sum of the two vectors representing machine andlearning would be wmachine  wlearning  1 7 3 7 2 it assumes thatthe composition of different constituents is a symmetric function of them in otherwords it does not consider the order of constituents although having lots of draw backs such as lack of the ability to model word orders and absence from backgroundsyntactic or knowledge information this approach still provides a relatively strongbaseline 9to overcome the word order issue one easy variant is applying a weighted suminstead of uniform weights this is to say the composition has the following formp  u  v36where  and  correspond to different weights for two vectors under this setting twosequences u v and v u have different representations which is consistent withreal language phenomena for example machine learning and learning machinehave different meanings which requires different representations in this setting wecould give greater emphasis to heads than other constituents as an example if weset  to 03 and  to 07 the 03  wmachine  0 09 03 15 06 and 07 wlearning  07 28 14 14 0andmachinelearningisrepresentedbytheiraddition 03  wmachine  07  wlearning  07 36 17 29 06however this model could not consider prior knowledge and syntax informationto incorporate prior information into the additive model one method combinesnearest neighborhood semantics into composition derivingp  u  v ki1ni37where n1 n2     nk denote all semantic neighbors of v therefore this methodcould ensemble all synonyms of the component as a smoothing factor into com position function which reduces the variance of language for example if in483compositional semanticsthe composition of machine and learning the chosen neighbor is optimiz ing with woptimizing  1 5 3 2 1 then this leads to the situation thatthe representation of machine learning becomes wmachine  wlearning woptimizing  2 12 6 9 3since the joint representations of one additive model still lie in the same semanticspace with their original component vectors it is natural to conduct cosine similarityto measure their semantic relationships thus under a naive additive model we havethe following similarity equationsp w p  wp wu  vwu  vw38uu  vsu w vu  vsv w39where w denotes any other word in the vocabulary and s indicates the similarityfunction from derivation ahead it could be concluded that this composition functioncomposes both magnitude and directions of two component vectors in other words ifonevectordominatesthemagnitudeitwillalsodominatethesimilarityfurthermorewe havep u  vu v310this lemma suggests that the semantic unit with a deeper rooted parsing tree coulddetermine the joint representation when combining with a shallow unit because thedeeper the semantic unit is the larger the magnitude it hasmoreover incorporating geometry insight we can observe that the additive modelbuilds a more solid understanding of semantic composition supposing that our com ponent vectors are u and v the additive model aims to project them to x and y wherex follows the direction of u while y is orthogonal to u the following gure couldclearly illustrate this issue fig31fig 31 an illustration ofthe additive modelabcdyuxv33 binary composition49from the gure the vector x and the vector y could be represented asx  u  vu  u  uy  v x  v u  vu  u  u311then using the linear combination of these two new vectors x y yields a newadditive modelp  x  y312  u  vu  u  u  v u  vu  u  u313    u  vu  u  u  v314furthermore using cosine similarity measurement the relationship could be writ ten as followssp w   su w  sv w315from similarity measurement derivation it is indicated that with this projectionmethod the composition similarity could be viewed as a linear combination of thesimilarities of two components which means that combining semantic units withdifferent semantic depths the deeper one will not dominate the representation332multiplicative modelthough the additive model achieves great success in semantic composition the sim plication it adopted may be too restrictive because it assumes all words phrasessentences and documents are substantially similar enough to be represented in a uni ed semantic space different from the additive model which regards composition asa simple linear transformation the multiplicative model aims to make higher orderinteraction among all models from this perspective the most intuitive approachtried to apply the pair wise product as a composition function approximation in thismethod the composition function is shown as the followingp  u v316where pi  ui  vi which implies each dimension of the output only depends onthe corresponding dimension of two input vectors however similar to the simplestadditive model this model is also suffering from the lack of the ability to model wordorder and the absence from background syntactic or knowledge information503compositional semanticsin the additive model we have p  u  v to alleviate the word order issuenote that here  and  are two scalars which could be easily changed to two matricestherefore the composition function could be represented asp  w  u  w  v317where w and w are matrices which determine the importance of u and v to pwith this expression the composition could be more expressive and exible althoughmuch harder to traingeneralizing multiplicative model ahead another approach is to utilize tensors asmultiplicative descriptors and the composition function could be viewed asp  w  uv318where w denotes a 3 order tensor ie the formula above could be written aspk  i j wi jk  ui  v j hence this model makes that each element of p could beinuenced by all elements of both u and v with a relationship of linear combinationby assigning each i j a unique weightstarting from this simple but general baseline some researchers proposed tomake the function not symmetric to consider word order in the sequence payingmore attention to the rst element the composition function could bep  w  uuv319where w denotes a 4 order tensor this method could be understood as replacinglinear transformation of u and v to a quadratic in u asymmetrically so this is a variantof the tensor multiplicative compositional modeldifferent from expanding a simple multiplicative model to complex ones otherkinds of approaches are proposed to reduce the parameter space with the reductionof parameter size people could make compositions much more efcient rather thanhave an on3 time complexity in the tensor based model thus some compressiontechniques could be applied in the original tensor model one representative instanceis the circular convolution model which could be shown asp  u v320where represents the circular convolution operation with the following denitionpi ju j  vij321if we assign each pair with unique weights the composition function will bepi jwi j  u j  vij32233 binary composition51note that the circular convolution model could be viewed as a special instance ofa tensor based composition model if we write the circular convolution in the tensorform we have wi jk  0 where k  i  j thus the parameter number could bereduced from n3 to n2 while maintaining the interactions between each pair ofdimensions in the input vectorsboth in the additive and multiplicative models the basic condition is all compo nents lie in the same semantic space as the output nevertheless different modelingtypes of words in different semantic spaces could bring us a different perspectivefor instance given u v the multiplicative model could be reformulated asp  w  u  v  u  v323this implies that each left unit could be treated as an operation on the repre sentation of the right one in other words each remaining unit could be formulatedas a transformation matrix while the right one should be represented as a seman tic vector this argument could be meaningful especially for some kinds of phrasecompositions reference 2 argues that for adj noun phrases the joint semanticinformation could be viewed as the conjunction of the semantic meanings of twocomponents given a phrase red car its semantic meaning is the conjunction ofall red things and all different kinds of cars thus red could be formulated as anoperator on the vector of car deriving the new semantic vector which expressedthe meaning of red car these observations lead to another genre of semanticcompositional modeling semantic matrix composition space34n ary compositionin real world nlp tasks the input is usually a sequence of multiple words rather thanjust a pair of words therefore besides designing a suitable binary compositionaloperator the order to apply binary operations is also important in this section wewill introduce three mainstream strategies in n ary composition by taking languagemodeling as an exampleto illustrate the language modeling task more clearly the composition problemto model a sentence or even a document could be formulated asgiven a sentencedocument consisting of a word sequence w0 w1 w2     wnwe aim to design following functions to obtain the joint semantic representation ofthe whole sentencedocument1 a semantic representation method like semantic vector space or compositionalmatrix space2 a binary compositional operation function f u v like we introduced in the pre vious sections here the input u and v denote the representations of two constitutesemantic units while the output is also the representation in the same space523compositional semantics3 a sequential order to apply the binary function in step 2 to describe in detailwe could use a bracket to identify the order to apply the composition functionfor instance we could use w1 w2 w3 to represent the sequential order frombeginning to endin this section we will introduce several systematic strategies to model sentencesemantics by describing the solutions for the three problems above we will classifythe methods by word level order sequential order recursive order following parsingtrees and convolution order341recurrent neural networkto design orders to apply binary compositional functions the most intuitive methodis utilizing sequentiality namely the sequence order should be sn  sn1 wnwhere sn1 is the order of the rst n 1 words motivated by this thought the neuralnetwork model used is the recurrent neural network rnnan rnn applies the composition function sequentially and derives the represen tations of hidden semantic units based on these hidden semantic units we coulduse them on some specic nlp tasks like sentiment analysis or text classicationalso note that the basic rnn only utilizes the sequential information from head totail of a sentencedocument to improve its representation ability the rnn couldbe enhanced as bi directional rnn by considering sequential and reverse sequentialinformationafter deciding sequential order to model sentence level semantics the next ques tion is determining the binary composition functions in detail supposing that htdenotes the representation of the rst t words and wt represents the tth word thegeneral composition could be formulated asht  f ht1 xt324where f is a well designed binary composition functionfrom the denition of the rnn the composition function could be formulated asfollowsht  tanhw1ht1  w2wt325where w1 and w2 are two weighted matriceswe could see that here we use a matrix weighted summation to represent binarysemantic compositionp  wu  wv326lstm since the raw rnn only utilizes the simple tangent function it is hardto obtain the long term dependency of a long sentencedocument reference 5reinvents long short term memory lstm networks to strengthen the ability to34 n ary composition53model long term semantic dependency in rnn in detail the composition function ofthe lstm allows information from previous layers to ow directly to their followinglayers the composition function could be dened asft  sigmoidwhf ht1  wxf xt  b f 327it  sigmoidwhi ht1  wxi xt  bi328ot  sigmoidwhoht1  wxoxt  bo329ct  tanhwhcht1  wxcxt  bc330ct  ft ct1  it ct331ht  ot ct332variants of lstm to simplify lstm and obtain more efcient algorithms3 proposes to utilize a simple but comparable rnn architecture named gatedrecurrent unit gru compared with lstm gru has fewer parameters whichbring higher efciency the composition function is showed aszt  sigmoidwhz ht1  wxz xt  bz333rt  sigmoidwhr ht1  wxr xt  br334ht  tanhwhrt ht1  wxhxt  bh335ht  1 zt ht1  zt ht336342recursive neural networkbesides the recurrent neural network another strategy to apply binary compositionalfunction follows a parsing tree instead of sequential word order based on this philos ophy 15 proposes a recursive neural network to model different levels of semanticunits in this subsection we will introduce some algorithms following the recursiveparsing tree with different binary compositional functionssince all the recursive neural networks are binary trees the basic problem we needto consider is how to derive the representation of the father component on the treegiven its two children semantic components reference 15 proposes a recursivematrix vector model mv rnn which captures constituent parsing tree structureinformation by assigning a matrix vector representation for each constituent thevector captures the meaning of the constituent itself and the matrix represents howit modies the meaning of the word it combines with suppose we have two childrencomponents a b and their father component p the composition can be formulatedas follows543compositional semanticsp  fveca b  gw1baab337p  fmatrixa b  w2ab338where a b p are the embedding vectors for each component and a b p are thematrices w1 is a matrix that maps the transformed words into another semanticspace the element wise function g is an activation function and w2 is a matrix thatmaps the two matrices into one combined matrix p with the same dimension thewhole process is illustrated in fig 32 and then mv rnn selects the highest nodeof the path in the parse tree between the two target entities to represent the inputsentencein fact the composition operation used in the above recursive network is similarto an rnn unit introduced in the previous subsection and the rnn unit here canbe replaced by lstm units or gru units reference 16 proposes two types oftree structured lstms including the child sum tree lstm and the n ary tree lstm to capture constituent or dependency parsing tree structure information forthe child sum tree lstm given a tree let ct denote the children set of the nodet its transition equations are dened as followsveryfbaabbaabbeautifulgirlaabbccfig 32 the architecture of the matrix vector recursive encoder34 n ary composition55ht kcthk339it  sigmoidwiwt  ui ht  bi340ftk  sigmoidw f wt  u f hk  b f  k ct341ot  sigmoidwowt  uo ht  bo342ut  tanhwuwt  uu ht  bu343ct  it ut kctftk ct1344ht  ot tanhct345the n ary tree lstm has similar transition equations as the child sum tree lstm the only difference is that it limits the tree structures to have at most nbranches343convolutional neural networkreference 6 proposes to embed an input sentence using a convolutional neuralnetwork cnn which extracts local features by a convolution layer and combinesall local features via a max pooling operation to obtain a xed sized vector for theinput sentenceformally the convolution operation is dened as a matrix multiplication betweena sequence of vectors a convolution matrix w and a bias vector b with a slidingwindow let us dene the vector qi as the concatenation of the subsequence of inputrepresentations in the ith window we haveh j  maxi f wqi  b j346where f indicates a nonlinear function such as sigmoid or tangent function and hindicates the nal representation of the sentence35summaryin this chapter we rst introduce the semantic space for compositional semanticsafterwards we take phrase representation as an example to introduce various modelsfor binary semantic composition including additive models and multiplicative mod els finally we introduce typical models for n ary semantic composition includingrecurrent neural network recursive neural network and convolutional neural net work compositional semantics allows languages to construct complex meaningsfrom the combinations of simpler elements and its binary semantic composition563compositional semanticsand n ary semantic composition is the foundation of multiple nlp tasks includingsentence representation document representation relational path representation etcwe will give a detailed introduction to these scenarios in the following chaptersfor further understanding of compositional semantics there are also some rec ommended surveys and books pelletier et al the principle of semantic compositionality 13 jeff et al composition in distributional models of semantics 10for better modeling compositional semantics some directions require furtherefforts in the future1 neurobiology inspired compositional semantics what is the neurobiologyfor dealing with compositional semantics in human language recently 14nds that the human combinatory system is related to rapidly peaking activityin the left anterior temporal lobe and later engagement of the medial prefrontalcortex the analysis of how language builds meaning and lays out directionsin neurobiological research may bring some instructive reference for modelingcompositional semantics in representation learning it is valuable to design novelcompositional forms inspired by recent neurobiological advances2 combination of symbolic and distributed representation human languageis inherently a discrete symbolic representation of knowledge however werepresent the semantics of discrete symbols with distributeddistributional rep resentations when dealing with natural language in deep learning recently thereare some approaches such as neural module networks 1 and neural symbolicmachine 8 attempting to consider discrete symbols in neural networks howto take advantage of these symbolic neural models to represent the compositionof semantics is an open problem to be exploredreferences1 jacob andreas marcus rohrbach trevor darrell and dan klein neural module networks inproceedings of cvpr pages 3948 20162 marcobaroniandrobertozamparellinounsarevectorsadjectivesarematricesrepresentingadjective noun constructions in semantic space in proceedings of emnlp 20103 junyoung chung caglar gulcehre kyunghyun cho and yoshua bengio gated feedbackrecurrent neural networks in proceedings of icml 20154 gottlob frege die grundlagen derarithmetik eine logisch mathematische untersuchung uberden begrijfder zahl breslau koebner 18845 sepp hochreiter and jrgen schmidhuber long short term memory neural computation9817351780 19976 nal kalchbrenner edward grefenstette and phil blunsom a convolutional neural networkfor modelling sentences in proceedings of acl 20147 george lakoff linguistic gestalts in proceedings of ilgisa 19778 chen liang jonathan berant quoc le kenneth forbus and ni lao neural symbolicmachines learning semantic parsers on freebase with weak supervision in proceedings ofacl pages 2333 2017references579 jeffmitchell andmirella lapatavector basedmodelsofsemantic compositionin proceedingsof acl 200810 jeff mitchell and mirella lapata composition in distributional models of semantics cognitivescience 34813881429 201011 geoffrey nunberg ivan a sag and thomas wasow idioms language pages 491538 199412 barbara partee lexical semantics and compositionality an invitation to cognitive sciencelanguage 1311360 199513 francis jeffry pelletier the principle of semantic compositionality topoi 1311124 199414 liina pylkknen the neural basis of combinatory syntax and semantics science36664616266 201915 richard socher brody huval christopher d manning and andrew y ng semantic compo sitionality through recursive matrix vector spaces in proceedings of emnlp 201216 kai sheng tai richard socher and christopher d manning improved semantic representa tions from tree structured long short term memory networks in proceedings of acl 2015open access this chapter is licensed under the terms of the creative commons attribution 40international license httpcreativecommonsorglicensesby40 which permits use sharingadaptation distribution and reproduction in any medium or format as long as you give appropriatecredit to the original authors and the source provide a link to the creative commons license andindicate if changes were madethe images or other third party material in this chapter are included in the chapters creativecommons license unless indicated otherwise in a credit line to the material if material is notincluded in the chapters creative commons license and your intended use is not permitted bystatutory regulation or exceeds the permitted use you will need to obtain permission directly fromthe copyright holderchapter 4sentence representationabstract sentence is an important linguistic unit of natural language sentence rep resentation has remained as a core task in natural language processing because manyimportant applications in related elds lie on understanding sentences for examplesummarization machine translation sentiment analysis and dialogue system sen tence representation aims to encode the semantic information into a real valued rep resentation vector which will be utilized in further sentence classication or match ing tasks with large scale text data available on the internet and recent advanceson deep neural networks researchers tend to employ neural networks eg con volutional neural networks and recurrent neural networks to learn low dimensionalsentence representations and achieve great progress on relevant tasks in this chapterwe rst introduce the one hot representation for sentences and the n gram sentencerepresentation ie probabilistic language model then we extensively introduceneural based models for sentence modeling including feedforward neural networkconvolutional neural network recurrent neural network and the latest transformerand pre trained language models finally we introduce several typical applicationsof sentence representations41introductionnatural language sentences consist of words or phrases follow grammatical rulesand convey complete semantic information compared with words and phrases sen tences have more complex structures including both sequential and hierarchicalstructures which are essential for understanding sentences in nlp how to rep resent sentences is critical for related applications such as sentence classicationsentiment analysis sentence matching and so onbefore deep learning took off sentences were usually represented as one hot vec tors or tf idf vectors following the assumption of bag of words in this case asentence is represented as a vocabulary sized vector in which each element repre sents the importance of a specic word either term frequency or tf idf to thesentence however this method confronts two issues firstly the dimension of suchrepresentation vectors is usually up to thousands or millions thus they usually facesparsity problem and bring in computational efciency problem secondly such a the authors 2020z liu et al representation learning for natural language processinghttpsdoiorg101007978 981 15 5573 2 459604sentence representationrepresentation method follows the bag of words assumption and ignores the sequen tial and structural information which can be crucial for understanding the semanticmeanings of sentencesinspired by recent advances of deep learning models in computer vision andspeech researchers proposed to model sentences with deep neural networks such asconvolutional neural network recurrent neural network and so on compared withconventional word frequency based sentence representations deep neural networkscan capture the internal structures of sentences eg sequential and dependencyinformation through convolutional or recurrent operations thus neural network based sentence representations have achieved great success in sentence modelingand nlp tasks42one hot sentence representationone hot representation is the most simple and straightforward method for word rep resentation tasks this method represents each word with a xed length binary vectorspecically for a vocabulary v  w1 w2     wv  the one hot representation ofword w is w  0     0 1 0     0 based on the one hot word representation andthe vocabulary it can be extended to represent a sentence s  w1 w2     wl ass lk1wi41where l indicates the length of the sentence s the sentence representation s is thesum of the one hot representations of n words within the sentence ie each elementin s represents the term frequency tf of the corresponding wordmoreover researchers usually take the importance of different words into consid eration rather than treat all the words equally for example the function words suchas a an and the usually appear in different sentences and reserve little mean ings therefore the inverse document frequency idf is employed to measure theimportance of wi in v as followsidfwi  log ddfwi42where d is the number of all documents in the corpus d and dfwi represents thedocument frequency df of wiwith the importance of each word the sentences are represented more preciselyas followss  s idf43where is the element wise producthere s is the tf idf representation of the sentence s43 probabilistic language model6143probabilistic language modelone hot sentence representation usually neglects the structure information in a sen tence to address this issue researchers propose probabilistic language model whichtreats n grams rather than words as the basic components an n gram means a subse quence of words in a context window of length n and probabilistic language modeldenes the probability of a sentence s  w1 w2     wl asps li1pwiwi1144actually model indicated in eq44 is not practicable due to its enormous param eter space in practice we simplify the model and set an n sized context windowassuming that the probability of word wi only depends on win1    wi1 morespecically an n gram language model predicts word wi in the sentence s basedon its previous n 1 words therefore the simplied probability of a sentence isformalized asps li1pwiwi1in145where the probability of selecting the word wi can be calculated from n gram modelfrequency countspwiwi1in1  pwiin1pwi1in146typically the conditional probabilities in n gram language models are not cal culated directly from the frequency counts since it suffers severe problems whenconfronted with any n grams that have not explicitly been seen before thereforeresearchers proposed several types of smoothing approaches which assign some ofthe total probability mass to unseen words or n grams such as add one smoothinggood turing discounting or back off modelsn gram model is a typical probabilistic language model for predicting the nextword in an n gram sequence which follows the markov assumption that the proba bility of the target word only relies on the previous n 1 words the idea is employedby most of current sentence modeling methods n gram language model is used asan approximation of the true underlying language model this assumption is crucialbecause it massively simplies the problem of learning the parameters of languagemodels from data recent works on word representation learning 3 40 43 aremainly based on the n gram language model624sentence representation44neural language modelalthough smoothing approaches could alleviate the sparse problem in the probabilis tic language model it still performs poorly for those unseen or uncommon words andn grams moreover since probabilistic language models are constructed on largerand larger texts the number of unique words the vocabulary increases and thenumber of possible sequences of words increases exponentially with the size of thevocabulary causing a data sparsity problem thus statistics are needed to estimateprobabilities accuratelyto address this issue researchers propose neural language models which usecontinuousrepresentationsorembeddingsofwordsandneuralnetworkstomaketheirpredictions in which embeddings in the continuous space help to alleviate the curseof dimensionality in language modeling and neural networks avoid this problem byrepresenting words in a distributed way as nonlinear combinations of weights in aneural net 2 an alternate description is that a neural network approximates thelanguage function the neural net architecture might be feedforward or recurrentand while the former is simpler the latter is more commonsimilar to probabilistic language models neural language models are constructedand trained as probabilistic classiers that learn to predict a probability distributionps li1pwiwi1147where the conditional probability of the selecting word wi can be calculated byvarious kinds of neural networks such as feedforward neural networks recurrentneural networks and so on in the following sections we will introduce these neurallanguage models in detail441feedforward neural network language modelthe goal of neural network language model is to estimate the conditional probabil ity pwiw1     wi1 however the feedforward neural network fnn lacks aneffective way to represent the long term historical context therefore it adopts theidea of n gram language models to approximate the conditional probability whichassumes that each word in a word sequence more statistically depends on thosewords closer to it and only n 1 context words are used to calculate the conditionalprobability ie pwiwi11 pwiwi1in1theoverallarchitectureofthefnnlanguagemodelisproposedby3toevaluatethe conditional probability of the word wi it rst projects its n 1 context relatedwords to their word vector representations x  win1     wi1 and then feedsthem into an fnn which can be generally represented as44 neural language model63y  m f wx  b  d48where w is a weighted matrix to transform word vectors to hidden representationsm is a weighted matrix for the connections between the hidden layer and the outputlayer and b d are bias vectors and then the conditional probability of the word wican be calculated aspwiwi1in expywij expy j49442convolutional neural network language modelthe convolutional neural network cnn is the family of neural network modelsthat features a type of layer known as the convolutional layer this layer can extractfeatures by a learnable lter or kernel at the different positions of an input phamet al 47 propose the cnn language model to enhance the fnn language modelthe proposed cnn network is produced by injecting a convolutional layer after theword input representation x  win     wi1 formally the convolutional layerinvolves a sliding window of the input vectors centered on each word vector using aparameter matrix wc which can be generally represented asy  mmaxwcx410where max indicates a max pooling layer the architecture of cnn is shown infig41moreover 12 also introduces a convolutional neural network for language mod eling with a novel gating mechanism443recurrent neural network language modelto address the lack of ability for modeling long term dependency in the fnn lan guage model 41 proposes a recurrent neural network rnn language modelwhich applies rnn in language modeling rnns are fundamentally different fromfnns in the sense that they operate on not only an input space but also an internalstate space and the internal state space enables the representation of sequentiallyextended dependencies therefore the rnn language model can deal with thosesentences of arbitrary length at every time step its input is the vector of its previousword instead of the concatenation of vectors of its n previous words and the infor mation of all other previous words can be taken into account by its internal stateformally the rnn language model can be dened as644sentence representationfig 41 the architecture ofcnnwc non linear layermaxpoolingconvolutionlayerinputrepresentationtanhhi  f w1hi1  w2wi  b411y  mhi1  d412where w1 w2 m are weighted matrices and b d are bias vectors here the rnnunit can also be implemented by lstm or gru the architecture of rnn is shownin fig42recently researchers make some comparisons among neural network languagemodels with different architectures on both small and large corpora the experimentalresults show that generally the rnn language model outperforms the cnn languagemodel444transformer language modelin 2018 google proposed a pre trained language model plm called bert whichachieved state of the art results on a variety of nlp tasks at that time it was verybig news since then all the nlp researchers began to consider how plms canbenet their research tasks44 neural language model65tanhrnnunitrnnunitrnnunitrnnunittanhgru celltanh1 xthth0h1hnx0x1xnht 1ct 1xthtctxththt 1lstm cellfig 42 the architecture of rnnin this section we will rst introduce the transformer architecture and then talkabout bert and other plms in detail4441transformertransformer 65 is a nonrecurrent encoder decoder architecture with a series ofattention based blocks for the encoder there are 6 layers and each layer is composedof a multi head attention sublayer and a position wise feedforward sublayer andthere is a residual connection between sublayers the architecture of the transformeris as shown in fig43there are several attention heads in the multi head attention sublayer a headrepresents a scaled dot product attention structure which takes the query matrix qthe key matrix k and the value matrix v as the inputs and the output is computedbyattentionq k v  softmaxqktdkv413where dk is the dimension of query matrixthe multi head attention sublayer linearly projects the input hidden states hseveral times into the query matrix the key matrix and the value matrix for h headsthe dimensions of the query key and value vectors are dk dk and dv respectively664sentence representationinputsoutputsshifted rightn feedforwardadd  normmulti headattentionadd  normmaskedmulti headattentionadd  normfeedforwardadd  normmulti headattentionadd  normn positionalencodingpositionalencodinglinearsoftmaxoutputprobabilitiesfig 43 the architecture of transformerthe multi head attention sublayer could be formulated asmultiheadh  head1 head2     headhwo414where headi  attentionhwqi  hwki  hwvi  and wqi  wki and wvi are linearprojections wo is also a linear projection for the output here the fully connectedposition wise feedforward sublayer contains two linear transformations with reluactivation44 neural language model67ffnx  w2 max0 w1x  b1  b2415transformer is better than rnns for modeling the long term dependency whereall tokens will be equally considered during the attention operation the transformerwas proposed to solve the problem of machine translation since transformer has avery powerful ability to model sequential data it becomes the most popular backboneof nlp applications4442transformer based plmneural models can learn large amounts of language knowledge from language mod eling since the language knowledge covers the demands of many downstreamnlp tasks and provides powerful representations of words and sentences someresearchers found that knowledge can be transferred to other nlp tasks easily thetransferred models are called pre trained language models plmslanguage modeling is the most basic and most important nlp task it contains avariety of knowledge for language understanding such as linguistic knowledge andfactual knowledge for example the model needs to decide whether it should addan article before a noun this requires linguistic knowledge about articles anotherexample is the question of what is the following word after trump is the presidentof the answer is america which requires factual knowledge since languagemodeling is very complex the models can learn a lot from this taskon the other hand language modeling only requires plain text without any humanannotation with this feature the models can learn complex nlp abilities from a verylarge scale corpus since deep learning needs large amounts of data and languagemodeling can make full use of all texts in the world plms signicantly benet thedevelopment of nlp researchinspired by the success of the transformer gpt 50 and bert 14 begin toadopt the transformer as the backbone of the pre trained language models gpt andbert are the most representative transformer based pre trained language modelsplms since they achieved state of the art performance on various nlp tasksnearly all plms after them are based on the transformer in this subsection we willtalk about gpt and bert in more detailgpt is the rst work to pretrain a plm based on the transformer the train ing procedure of gpt 50 contains two classic stages generative pretraining anddiscriminative ne tuningin the pretraining stage the input of the model is a large scale unlabeled corpusdenoted as u  u1 u2     un the pretraining stage aims to optimize a languagemodel the learning objective over the corpus is to maximize a conditional likelihoodin a xed size windowl1u  ilog puiuik     ui1 416684sentence representationwhere k represents the size of the window the conditional likelihood p is modeledby a neural network with parameters for a supervised dataset  the input is a sequence of words s  w1 w2  wland the output is a label y the pretraining stage provides an advantageous startpoint of parameters that can be used to initialize subsequent supervised tasks atthis occasion the objective is a discriminative task that maximizes the conditionalpossibility distributionl2 sylog pyw1     wl417where pyw1     wl is modeled by a k layer transformer after the input tokenspass through the pretrained gpt a hidden vector of the nal layer hkl will be pro duced to obtain the output distribution a linear transformation layer is added whichhas the same size as the number of labelspyw1     wm  softmaxwyhkl 418the nal training objective is combined with a language modeling l1 for bettergeneralizationl   l2   l1419where  is a weight hyperparameterbert 14 is a milestone work in the eld of plm bert achieved signicantempirical results on 17 different nlp tasks including squad outperform humanbeing glue 77 point absolute improvement multinli 46 point absoluteimprovement etc compared to gpt bert uses a bidirectional deep transformeras the model backbone as illustrated in fig44 bert contains pretraining andne tuning stagesin the pretraining stage two objectives are designed masked language modelmlm and next sentence prediction nsp 1 for mlm tokens are randomlypre trainingunlabeled sentence a and b pairmasked sentence amasked sentence bclstok1toknseptok1tokmeclse1enesepe1emeclse1enesepe1emnspmask lmmask lmbertfine tuningmnlinerquestion answer pairquestionparagraphclstok1toknseptok1tokmeclse1enesepe1emeclse1enesepe1embertsquadstartend spanfig 44 the pretraining and ne tuning stages for bert44 neural language model69masked with a special token mask the training objective is to predict the maskedtokens based on the contexts compared with the standard unidirectional conditionallanguage model which can only be trained in one direction mlm aims to train a deepbidirectional representation model this task is inspired by cloze 64 2 the objec tive of nsp is to capture relationships between sentences for some sentence baseddownstream tasks such as natural language inference nli and question answeringqa in this task a binary classier is trained to predict whether the sentence isthe next sentence for the current this task effectively captures the deep relationshipbetween sentences exploring semantic information from a different levelafter pretraining bert can capture various language knowledge for downstreamsupervised tasks by modifying inputs and outputs bert can be ne tuned for anynlp tasks which contain the applications with the input of single text or text pairsthe input consists of sentence a and sentence b which can represent 1 sentencepairs in paraphrase 2 hypothesis premise pairs in entailment 3 question passagepairs in qa and 4 text for text classication task or sequence tagging for theoutput bert can produce the token level representation for each token which isused to sequence tagging task or question answering besides the special tokencls in bert is fed into the classication layer for sequence classication4443plm familypre trained language models have rapid progress after bert we summarize sev eral important directions of plms and show some representative models and theirrelationship in fig45here is a brief introduction of the plms after bert firstly there are somevariants of bert for better general language representation such as roberta 38and xlnet 70 these models mainly focus on the improvement of pretraining taskssecondly some people work on pretrained generation models such as mass 57and unilm 15 these models achieve promising results on the generation tasksinstead of the natural language understanding nlu tasks used by bert thirdlythesentencepairformatofbertinspiredworksonthecross lingualandcross modalelds xlm 8 vilbert 39 and videobert 59 are the important works in thisdirection lastly there are some works 46 81 that explore to incorporate externalknowledge into plms since some low frequency knowledge cannot be efcientlylearned by plms445extensions4451importance samplinginspired by the contrastive divergence model 4 proposes to adopt importance sam pling to accelerate the training of neural language models they rst normalize the704sentence representationfig 45 the pre trained language model familyoutputs of neural network language model and view neural network language modelsas a special case of energy based probability models as followingpwiwi1in expywij expy j420the key idea of importance sampling is to approximate the mean of log likelihoodgradient of the loss function of neural network language model by sampling severalimportant words instead of calculating the explicit gradient here the log likelihoodgradient of the loss function of neural network language model can be generallyrepresented aspwiwi1in ywi v j1pw jwi1iny j yi  ewkpyk421where  indicates all parameters of the neural network language model here thelog likelihood gradient of the loss function consists of two parts including positivegradient for target word wi and negative gradient for all words w j ie ewip y j here the second part can be approximated by sampling important words followingthe probability distribution p44 neural language model71ewkpykwkv 1v yk 422where v  is the word set sampled under phowever since we cannot obtain probability distribution p in advance it is impos sible to sample important words following the probability distribution p thereforeimportance sampling adopts a monte carlo scheme which uses an existing proposaldistribution q to approximate p and then we haveewkpyk1v wlv yl pwlwi1inqwl423where v  is the word set sampled under q moreover the sample size of impor tance sampling approach should be increased as training processes in order to avoiddivergence which aims to ensure its effective sample size ss wlv  rl2wlv  r2l424where rl is further dened asrl pwlwi1inqwlw jv  pw jwi1inqw j4254452word classicationbesides important sampling researchers 7 22 also propose class based languagemodel which adopts word classication to improve the performance and speed of alanguage model in class based language model all words are assigned to a uniqueclass and the conditional probability of a word given its context can be decomposedinto the probability of the words class given its previous words and the probabilityof the word given its class and history which is formally dened aspwiwi1in cwicpwicwipcwiwi1in426where c indicates the set of all classes and cwi indicates the class of word wimoreover 44 proposes a hierarchical neural network language model whichextends word classication to a hierarchical binary clustering of words in a languagemodel instead of simply assigning each word with a unique class it rst buildsa hierarchical binary tree of words according to the word similarity obtained fromwordnetnextitassignsauniquebitvectorcwi  c1wi c2wi     clwifor724sentence representationeach word which indicates the hierarchical classes of them and then the conditionalprobability of each word can be dened aspwiwi1in lj0pc jwic1wi c2wi     c j1wi wi1in427the hierarchical neural network language model can achieve ok log k speedup as compared to a standard language model however the experimental results of44 show that although the hierarchical neural network language model achievesan impressive speed up for modeling sentences it has worse performance than thestandard language model the reason is perhaps that the introduction of hierarchicalarchitecture or word classes imposes a negative inuence on the word classicationby neural network language models4453cachingcaching is also one of the important extensions in language model a type of cache based language model assumes that each word in recent context is more likely toappear again 58 hence the conditional probability of a word can be calculated bythe information from history and cachingpwiwi1in  pswiwi1in  1 pcwiwi1in428where pswiwi1in indicates the conditional probability generated by standard lan guage and pcwiwi1in indicates the conditional probability generated by cachingand  is a constantanother cache based language model is also used to speed up the rnn languagemodeling 27 the main idea of this approach is to store the outputs and states oflanguage models for future predictions given the same contextual history45applicationsin this section we will introduce two typical sentence level nlp applications includ ing text classication and relation extraction as well as how to utilize sentence rep resentation for these applications45 applications73451text classicationtext classication is a typical nlp application and has lots of important real worldtasks such as parsing and semantic analysis therefore it has attracted the interest ofmany researchers the conventional text classication models eg the lda 6 andtree kernel 48 models focus on capturing more contextual information and correctword order by extracting more useful and distinct features but still expose a fewissues eg data sparseness which has the signicant impact on the classicationaccuracy recently with the development of deep learning in the various elds ofarticial intelligence neural models have been introduced into the text classicationeld due to their abilities of text representation learning in this section we willintroduce the two typical tasks of text classication including sentence classicationand sentiment classication4511sentence classicationsentence classication aims to assign a sentence an appropriate category which is abasic task of the text classication applicationconsidering the effectiveness of the cnn models in capturing sentence semanticmeanings 31 rst proposes to utilize the cnn models trained on the top of pre trained word embeddings to classify sentences which achieved promising results onseveral sentence classication datasets then 30 introduces a dynamic cnn modelto model the semantic meanings of sentences this model handles sentences of vary ing lengths and uses dynamic max pooling over linear sequences which could helpthe model capture both short range and long range semantic relations in sentencesfurthermore 9 proposes a novel cnn based model named as very deep cnnwhich operates directly at the character level it shows that those deeper models havebetter results on sentence classication and can capture the hierarchical informationfrom scattered characters to whole sentences yin and schtze 74 also proposemv cnn which utilizes multiple types of pretrained word embeddings and extractsfeatures from multi granular phrases with variable sized convolutional layers toaddress the drawbacks of mv cnn such as model complexity and the requirementfor the same dimension of embeddings 80 proposes a novel model called mg cnnto capture multiple features from multiple sets of embeddings that are concatenatedat the penultimate layer zhang et al 79 present ra cnn to jointly exploit labelson documents and their constituent sentences which can estimate the probabilitythat a given sentence is informative and then scales the contribution of each sentenceto aggregate a document representation in proportion to the estimatesthe rnn model which aims to capture the sequential information of sentencesis also widely used in sentence classication lai et al 32 propose a neural net work for text classication which applies a recurrent structure to capture contextualinformation moreover 37 introduces a multitask learning framework based on thernn to jointly learn across multiple sentence classication tasks which employs744sentence representationthree different mechanisms of sharing information to model sentences with both task specic and shared layers yang et al 71 introduce word level and sentence levelattention mechanisms into an rnn based model as well as a hierarchical structureto capture the hierarchical information of documents for sentence classication4512sentiment classicationsentiment classication is a special task of the sentence classication applicationwhose objective is to classify the sentimental polarities of opinions a piece of textcontains eg favorable or unfavorable positive or negative this task appeals thenlp community since it has lots of potential downstream applications such as moviereview suggestionssimilar to text classication the sentence representation based on neural modelshas also been widely explored for sentiment classication glorot et al 20 use astacked denoising autoencoder in sentiment classication for the rst time thena series of recursive neural network models based on the recursive tree structureof sentences are conducted to learn sentence representations for sentiment classi cation including the recursive autoencoder rae 55 matrix vector recursiveneural network mv rnn 54 and recursive neural tensor network rntn 56besides 29 adopts a cnn to learn sentence representations and achieves promisingperformance in sentiment classicationthe rnn models also benet sentiment classication as they are able to capturethe sequential information li et al 35 and tai et al 62 investigate a tree structuredlstmmodelontextclassicationtherearealsosomehierarchicalmodelsproposedto deal with document level sentiment classication 5 63 which generate seman tic representations at different levels eg phrase sentence or document withina document moreover the attention mechanism is also introduced into sentimentclassication which aims to select important words from a sentence or importantsentences from a document 71452relation extractionto enrich existing kgs researchers have devoted many efforts to automatically nd ing novel relational facts in text therefore relation extraction re which aimsat extracting relational facts according to semantic information in plain text hasbecome a crucial nlp application as re is also an important downstream applica tion of sentence representation we will respectively introduce the techniques andextensions to show how to utilize sentence representation for different re scenariosconsidering neural networks have become the backbone of the recent nlp researchwe mainly focus on neural re nre models in this section45 applications75fig 46 an example of sentence level relation extraction4521sentence level nresentence level nre aims at predicting the semantic relations between the givenentity or nominal pair in a sentence as shown in fig46 given the input sentences which consists of n words s  w1 w2     wn and its corresponding entity paire1 and e2 as input sentence level nre wants to obtain the conditional probabilityprs e1 e2 of relation r r r via a neural network which can be formalizedasprs e1 e2  prs e1 e2 429where  is all parameters of the neural network and r is a relation in the relation setra basic form of sentence level nre consists of three components a an inputencoder to give a representation for each input word b a sentence encoder whichcomputes either a single vector or a sequence of vectors to represent the originalsentence and c a relation classier which calculates the conditional probabilitydistribution of all relationsinput encoder first a sentence level nre system projects the discrete wordsof the source sentence into a continuous vector space and obtains the input repre sentation w  w1 w2     wm of the source sentence1 word embeddings word embeddings aim to transform words into distributedrepresentations to capture the syntactic and semantic meanings of the wordsin the sentence s every word wi is represented by a real valued vector wordrepresentations are encoded by column vectors in an embedding matrix e rdav  where v is a xed sized vocabulary although word embeddings arethe most common way to represent input words there are also efforts made toutilize more complicated information of input sentences for re2 position embeddings in re the words close to the target entities are usuallyinformative to determine the relation between the entities therefore positionembeddings are used to help models keep track of how close each word is tothe head or tail entities it is dened as the combination of the relative distancesfrom the current word to the head or tail entities for example in the sentencebill gates is the founder of microsoft the relative distance764sentence representationfrom the word founder to the head entity bill gates is 3 and the tailentity microsoft is 2 besides word position embeddings more linguisticfeatures are also considered in addition to the word embeddings to enrich thelinguistic features of the input sentence3 part of speech pos tag embeddings pos tag embeddings are to represent thelexical information of the target word in the sentence because word embeddingsare obtained from a large scale general corpus the general information theycontain may not be in accordance with the meaning in a specic sentence henceit is necessary to align each word with its linguistic information considering itsspecic context eg noun and verb formally each word wi is encoded by thecorresponding column vector in an embedding matrix ep rd pv p where d pis the dimension of embedding vector and v p indicates a xed sized pos tagvocabulary4 wordnet hypernym embeddings wordnet hypernym embeddings aim to takeadvantages of the prior knowledge of hypernym to help re models whengiven the hypernym information of each word in wordnet eg nounfood andverbmotion it is easier to build the connections between different but concep tually similar words formally each word wi is encoded by the correspondingcolumn vector in an embedding matrix eh rdhv h where dh is the dimensionof embedding vector and v h indicates a xed sized hypernym vocabularyfor each word the nre models often concatenate some of the above four featureembeddings as their input embeddings therefore the feature embeddings of allwords are concatenated and denoted as a nal input sequence w  w1 w2     wmwhere wi rd d is the total dimension of all feature embeddings concatenated foreach wordsentence encoder the sentence encoder is the core for sentence representationwhich encodes input representations into either a single vector or a sequence ofvectors x to represent sentences we will introduce the different sentence encodersin the following1 convolutional neural network encoder zeng et al 76 propose to encodeinput sentences using a cnn model which extracts local features by a convolutionallayer and combines all local features via a max pooling operation to obtain a xed sized vector for the input sentence formally a convolutional layer is dened as anoperation on a vector sequence wp  cnnw430where cnn indicates the convolution operation inside the convolutional layerand the ith element of the sentence vector x can be calculated as followsxi  f maxpi431where f is a nonlinear function applied at the output such as the hyperbolic tangentfunction45 applications77further pcnn 75 which is a variation of cnn adopts a piece wise max pooling operation all hidden vectors p1 p2    are divided into three segmentsby the head and tail entities the max pooling operation is performed over the threesegments separately and the x is the concatenation of the pooling results over thethree segments2 recurrent neural network encoder zhang and wang 78 propose to embedinput sentences using an rnn model which can learn the temporal features formallyeach input word representation is put into recurrent layers step by step for each stepi the network takes the ith word representation vector wi and the output of theprevious i 1 steps hi1 as inputhi  rnnwi hi1432where rnn indicates the transform function inside the rnn cell which can be thelstm units or the gru units mentioned beforethe conventional rnn models typically deal with text sequences from start toend and build the hidden state of each word only considering its preceding wordsit has been veried that the hidden state considering its following words is moreeffective hence the bi directional rnn brnn 52 is adopted to learn hiddenstates using both preceding and following wordssimilar to the previous cnn models in re the rnn model combines the outputvectors of the recurrent layer as local features and then uses a max pooling operationto extract the global feature which forms the representation of the whole inputsentence the max pooling layer could be formulated asx j  maxihi j433besides max pooling word attention can also combine all local feature vectorstogether the attention mechanism 1 learns attention weights on each step sup posing h  h1 h2     hm is the matrix consisting of all output vectors producedby the recurrent layer the feature vector of the whole sentence x is formed by aweighted sum of these output vectors  softmaxstanhh434x  h435where s is a trainable query vectorbesides42proposesamodelthatcapturesinformationfrombothwordsequenceand tree structured dependency by stacking bidirectional path based lstm rnnsie bottom up and top down more specically it focuses on the shortest pathbetween the two target entities in the dependency tree and utilizes the stacked layersto encode the shortest path for the whole sentence representation in fact somepreliminary work 69 has shown that these paths are useful in re and various784sentence representationrecursive neural models are also proposed for this next we will introduce theserecursive models in detail3 recursive neural network encoder the recursive encoder aims to extractfeatures from the information of syntactic parsing trees considering the syntacticinformation is benecial for extracting relations from sentences generally theseencoders treat the tree structure inside syntactic parsing trees as a strategy of com position as well as a direction to combine each word featuresocher et al 54 propose a recursive matrix vector model mv rnn whichcaptures the structure information by assigning a matrix vector representation foreach constituent of the constituents in parsing trees the vector captures the meaningof the constituent itself and the matrix represents how it modies the meaning of theword it combines with tai et al 62 further propose two types of tree structuredlstms including the child sum tree lstm and the n ary tree lstm to capturetree structure information for the child sum tree lstm given a tree let ctdenote the set of children of node t its transition equations are dened as followsht kcttlstmhk436where tlstm indicates a tree lstm cell which is simply modied from lstmcell the n ary tree lstm has similar transition equations as the child sum tree lstm the only difference is that it limits the tree structures to have at most nbranchesrelation classier when obtaining the representation x of the input sentencerelation classier calculates the conditional probability prx e1 e2 via a softmaxlayer as followsprx e1 e2  softmaxmx  b437where m indicates the relation matrix and b is a bias vector4522bag level nrealthough existing neural models have achieved great success for extracting novelrelational facts it always suffers the lack of training data to address this issueresearchers proposed a distant supervision assumption to generate training datavia aligning kgs and plain text automatically the intuition of distant supervisionassumption is that all sentences that contain two entities will express their relationsin kgs for example new york city of united states is a relationalfact in a kg distant supervision assumption will regard all sentences that containthese two entities as positive instances for the relation city of it offers a naturalway of utilizing information from multiple sentences bag level rather than a singlesentence sentence level to decide if a relation holds between two entitiestherefore bag level nre aims to predict the semantic relations between an entitypair using all involved sentences as shown in fig47 given the input sentence set45 applications79fig 47 an example of bag level relation extractions which consists of n sentences s  s1 s2     sn and its corresponding entitypair e1 and e2 as inputs bag level nre wants to obtain the conditional probabilityprs e1 e2 of relation r r r via a neural network which can be formalizedasprs e1 e2  prs e1 e2 438a basic form of bag level nre consists of four components a an input encodersimilar to sentence level nre b a sentence encoder similar to sentence level nrec a bag encoder which computes a vector representing all related sentences in a bagand d a relation classier similar to sentence level nre which takes bag vectorsas input instead of sentence vectors as the input encoder sentence encoder andrelation classier of bag level nre are similar to the ones of sentence level nrewe will thus mainly focus on introducing the bag encoder in detailbag encoder the bag encoder encodes all sentence vectors into a single vectors we will introduce the different bag encoders in the following1 random encoder it simply assumes that each sentence can express the relationbetween two target entities and randomly select one sentence to represent the bagformally the bag representation is dened ass  si i 1 2     n439where si indicates the sentence representation of si s and i is a random index2 max encoder as introduced above not all sentences containing two targetentities can express their relations for example the sentence new york cityis the premier gateway for legal immigration to theunited states does not express the relation city of hence in 75 theyfollow the at least one assumption which assumes that at least one sentence thatcontains these two target entities can express their relations and select the sentence804sentence representationwith the highest probability for the relation to represent the bag formally bag rep resentation is dened ass  si i  arg maxiprsi e1 e24403 average encoder both random encoder or max encoder use only one sentenceto represent the bag which ignores the rich information of different sentences toexploit the information of all sentences 36 believes that the representation s ofthe bag depends on all sentences representations each sentence representation sican give the relation information about two entities to a certain extent the averageencoder assumes that all sentences contribute equally to the representation of thebag it means the embedding s of the bag is the average of all the sentence vectorss i1n si4414 attentive encoder due to the wrong label issue brought by distant supervisionassumption inevitably the performance of average encoder will be inuenced bythose sentences that contain no relation information to address this issue 36further proposes to employ a selective attention to reduce those noisy sentencesformally the bag representation is dened as a weighted sum of sentence vectorss iisi442where i is dened asi expsi arj expxj ar443where a is a diagonal matrix and r is the representation vector of relation rrelation classier similar to sentence level nre when obtaining the bag rep resentation s relation classier also calculates the conditional probability prs e1e2 via a softmax layer as followsprs e1 e2  softmaxms  b444where m indicates the relation matrix and b is a bias vector4523extensionsrecently nre systems have achieved signicant improvements in both the super vised and distantly supervised scenarios however there are still many challenges inthe task of re and many researchers have been focusing on other aspects to improve45 applications81the performance of nre as well in this section we will introduce these extensionsin detailutilization of external information most existing nre systems stated aboveonly concentrate on the sentences which are extracted regardless of the rich externalinformation such as kgs this heterogeneous information could provide additionalknowledge from kg and is essential when extracting new relational factshan et al 24 propose a novel joint representation learning framework for knowl edge acquisition the key idea is that the joint model learns knowledge and textrepresentations within a unied semantic space via kg text alignments for the textpart the sentence with two entities mark twain and florida is regarded asthe input for a cnn encoder and the output of cnn is considered to be the latentrelation placeofbirth of this sentence for the kg part entity and relation rep resentations are learned via translation based methods the learned representationsof kg and text parts are aligned during training besides this preliminary attemptmany efforts have been devoted to this direction 25 28 51 67 68incorporating relational paths although existing nre systems have achievedpromising results they still suffer a major problem the models can only directlylearn from those sentences which contain both two target entities however thosesentences containing only one of the entities could also provide useful informationand help build inference chains for example if we know that a is the son of band b is the son of c we can infer that a is the grandson of cto utilize the information of both direct and indirect sentences 77 introducesa path based nre model that incorporates textual relational paths the model rstemploys a cnn encoder to embed the semantic meanings of sentences then themodel builds a relation path encoder which measures the probability of relationsgiven an inference chain in the text finally the model combines information fromboth direct sentences and relational paths and then predicts the condence of eachrelationship this work is the rst effort to consider the knowledge of relation pathin text for nre and there are also several methods later to consider the reasoningpath of sentence semantic meanings for re 11 19document level relation extraction in fact not all relational facts can beextracted by sentence level re ie a large number of relational facts are expressedin multiple sentences taking fig49 as an example multiple entities are mentionedin the document and exhibit complex interactions in order to identify the relationalfact riddarhuset country sweden one has to rst identify the fact thatriddarhuset is located in stockholm from sentence 4 then identify the factsstockholm is the capital of sweden and sweden is a country from sentence 1with the above facts we can nally infer that the sovereign state of riddarhusetis sweden this process requires reading and reasoning over multiple sentences ina document which is intuitively beyond the reach of sentence level re methodsaccording to the statistics on a human annotated corpus sampled from wikipediadocuments 72 at least 407 relational facts can only be extracted from multi ple sentences which is not negligible swampillai and stevenson 61 and vergaet al 66 also report similar observations therefore it is necessary to move re824sentence representationfig 48 an example of document level relation extractionforward from the sentence level to the document level figure48 is an example fordocument level refig 49 an example from docred 7245 applications83however existing datasets for document level re either only have a small num ber of manually annotated relations and entities 34 or exhibit noisy annotationsfrom distant supervision 45 49 or serve specic domains or approaches 33 toaddress this issue 72 constructs a large scale manually annotated and general purpose document level re dataset named as docred docred is constructedfrom wikipedia and wikidata and has two key features first docred contains132 375 entities and 56 354 relational facts annotated on 5 053 wikipedia docu ments which is the largest human annotated document level re dataset now sec ond over 40 of the relational facts in docred can only be extracted from multiplesentences this makes docred require reading multiple sentences in a documentto recognize entities and inferring their relations by synthesizing all information ofthe documentthe experimental results on docred show that the performance of existingsentence level re methods declines signicantly on docred indicating the taskdocument level re is more challenging than sentence level re and remains an openproblem it also relates to the document representation which will be introduced inthe next chapterfew shot relation extractionas we mentioned before the performance of the conventional re models 2376 heavily depend on time consuming and labor intensive annotated data whichmake themselves hard to generalize well although adopting distant supervisionis a primary approach to alleviate this problem the distantly supervised data alsoexhibits a long tail distribution where most relations have very limited instancesfurthermore distant supervision suffers the wrong labeling problem which makesit harder to classify long tail relations hence it is necessary to study training remodels with insufcient training instances figure410 is an example for few shotrefig 410 an example of few shot relation extraction844sentence representationtable 41 an example for a 3 way 2 shot scenario different colors indicate different entitiesunderline for head entity and emphasize for tail entitysupporting seta capital of1 london is the capital of the uk2 washington is the capital of the usab member of1 newton served as the president of the royalsociety2 leibniz was a member of the prussianacademy of sciencesc birth name1 samuel langhorne clemens better knownby his pen name mark twain was an americanwriter2 alexei maximovich peshkov primarilyknown as maxim gorky was a russian andsoviet writertest instancea or b or ceuler was elected a foreign member of theroyal swedish academy of sciencesfewrel 26 is a new large scale supervised few shot re dataset which requiresmodels capable of handling classication task with a handful of training instancesas shown in table41 beneting from the fewrel dataset there are some efforts toexploring few shot re 17 53 73 and achieve promising results yet few shot restill remains a challenging problem for further research 1846summaryin this chapter we introduce sentence representation learning sentence representa tion encodes the semantic information of a sentence into a real valued representationvector and can be utilized in further sentence classication or matching tasks firstwe introduce the one hot representation for sentences and probabilistic languagemodels secondly we extensively introduce several neural language models includ ing adopting the feedforward neural networks the convolutional neural networks therecurrent neural networks and the transformer for language models these neuralmodels can learn rich linguistic and semantic knowledge from language modelingbeneting from this the pre trained language models trained with large scale cor pora have achieved state of the art performance on various downstream nlp tasksby transferring the learned semantic knowledge from general corpora to the targettasks finally we introduce several typical applications of sentence representationincluding text classication and relation extraction46 summary85for further understanding of sentence representation learning and its applicationsthere are also some recommended surveys and books including yoav neural network methods for natural language processing 21 deng  liu deep learning in natural language processing 13in the future for better sentence representation some directions are requiringfurther efforts1 exploring advanced architectures the improvement of model architecturesis the key factor in the success of sentence representation from the feedforwardneural networks to the transformer people are designing more suitable neuralmodels for sequential inputs based on the transformer some researchers areworking on new nlp architectures for instance transformer xl 10 is pro posed to solve the problem of xed length context in the transformer sincethe transformer is the state of the art nlp architecture current works mainlyadopt attention mechanisms beyond these works is it possible to introducemore human cognitive mechanisms to neural models2 modeling long documents the representation of long documents is an impor tant extension of sentence representation there are some new challenges duringmodeling long documents such as discourse analysis and co reference resolu tion although some existing works already provide document level nlp taskseg docred 72 the model performance on these tasks is still much lowerthan the human performance we will also introduce the advances in documentrepresentation learning in the following chapter3 performing efcient representation although the combination of trans former and large scale data leads to very powerful sentence representation theserepresentation models require expensive computational cost which limits theapplications in downstream tasks some existing works explore to use modelcompression techniques for more efcient models these techniques includeknowledge distillation 60 parameter pruning 16 etc beyond these worksthere remain lots of unsolved problems for developing better representation mod els which can efciently learn from large scale data and provide effective vectorsin downstream tasksreferences1 dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointlylearning to align and translate in proceedings of iclr 20152 yoshua bengio neural net language models scholarpedia 313881 20083 yoshua bengio rjean ducharme pascal vincent and christian jauvin a neural probabilisticlanguage model journal of machine learning research 3feb11371155 20034 yoshua bengio jean sbastien sencal et al quick training of probabilistic neural nets byimportance sampling in proceedings of aistats 2003864sentence representation5 parminder bhatia yangfeng ji and jacob eisenstein better document level sentiment analysisfrom rst discourse parsing in proceedings of emnlp 20156 david m blei andrew y ng and michael i jordan latent dirichlet allocation journal ofmachine learning research 39931022 20037 peter f brown peter v desouza robert l mercer vincent j della pietra and jenifer c laiclass based n gram models of natural language computational linguistics 18446747919928 alexis conneau and guillaume lample cross lingual language model pretraining in pro ceedings of neurips 20199 alexis conneau holger schwenk loc barrault and yann lecun very deep convolutionalnetworks for text classication in proceedings of eacl volume 1 201710 zihangdaizhilinyangyimingyangjaimegcarbonellquocleandruslansalakhutdinovtransformer xl attentive language models beyond a xed length context in proceedings ofacl page 29782988 201911 rajarshi das arvind neelakantan david belanger and andrew mccallum chains of reason ing over entities relations and text using recurrent neural networks in proceedings of eaclpages 132141 201712 yann n dauphin angela fan michael auli and david grangier language modeling withgated convolutional networks in proceedings of icml 201713 li deng and yang liu deep learning in natural language processing springer 201814 jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training ofdeep bidirectional transformers for language understanding in proceedings of naacl 201915 li dong nan yang wenhui wang furu wei xiaodong liu yu wang jianfeng gao mingzhou and hsiao wuen hon unied language model pre training for natural language under standing and generation in proceedings of neurips 201916 angela fan edouard grave and armand joulin reducing transformer depth on demand withstructured dropout in proceedings of iclr 202017 tianyu gao xu han zhiyuan liu and maosong sun hybrid attention based prototypicalnetworks for noisy few shot relation classication in proceedings of aaai pages 64076414201918 tianyu gao xu han hao zhu zhiyuan liu peng li maosong sun and jie zhou fewrel 20towards more challenging few shot relation classication in proceedings of emnlp ijcnlppages 62516256 201919 michael glass alo gliozzo oktie hassanzadeh nandana mihindukulasooriya and gae tano rossiello inducing implicit relations from text using distantly supervised deep nets ininternational semantic web conference pages 3855 springer 201820 xavier glorot antoine bordes and yoshua bengio domain adaptation for large scale senti ment classication a deep learning approach in proceedings of icml 201121 yoav goldberg neural network methods for natural language processing synthesis lectureson human language technologies 1011309 201722 joshua goodman classes for fast maximum entropy training in proceedings of assp 200123 matthew r gormley mo yu and mark dredze improved relation extraction with feature richcompositional embedding models in proceedings of emnlp 201524 xu han zhiyuan liu and maosong sun joint representation learning of text and knowledgefor knowledge graph completion arxiv preprint arxiv161104125 201625 xu han zhiyuan liu and maosong sun neural knowledge acquisition via mutual attentionbetween knowledge graph and text in proceedings of aaai pages 48324839 201826 xu han hao zhu pengfei yu ziyun wang yuan yao zhiyuan liu and maosong sunfewrel a large scale supervised few shot relation classication dataset with state of the artevaluation in proceedings of emnlp 201827 zhiheng huang geoffrey zweig and benoit dumoulin cache based recurrent neural networklanguage model inference for rst pass speech recognition in proceedings of icassp 201428 guoliang ji kang liu shizhu he and jun zhao distant supervision for relation extractionwith sentence level attention and entity descriptions in proceedings of aaai pages 30603066 2017references8729 rie johnson and tong zhang effective use of word order for text categorization with convo lutional neural networks in proceedings of acl hlt 201530 nal kalchbrenner edward grefenstette and phil blunsom a convolutional neural networkfor modelling sentences in proceedings of acl 201431 yoon kim convolutional neural networks for sentence classication in proceedings ofemnlp 201432 siwei lai liheng xu kang liu and jun zhao recurrent convolutional neural networks fortext classication in proceedings of aaai 201533 omer levy minjoon seo eunsol choi and luke zettlemoyer zero shot relation extractionvia reading comprehension in proceedings of conll 201734 jiao li yueping sun robin j johnson daniela sciaky chih hsuan wei robert leamanallan peter davis carolyn j mattingly thomas c wiegers and zhiyong lu biocreative vcdr task corpus a resource for chemical disease relation extraction database pages 110201635 jiwei li minh thang luong dan jurafsky and eduard hovy when are tree structures nec essary for deep learning of representations in proceedings of emnlp 201536 yankai lin shiqi shen zhiyuan liu huanbo luan and maosong sun neural relation extrac tion with selective attention over instances in proceedings of acl 201637 pengfei liu xipeng qiu and xuanjing huang recurrent neural network for text classicationwith multi task learning in proceedings of ijcai 201638 yinhan liu myle ott naman goyal jingfei du mandar joshi danqi chen omer levy mikelewis luke zettlemoyer and veselin stoyanov roberta a robustly optimized bert pretrainingapproach arxiv preprint arxiv190711692 201939 jiasen lu dhruv batra devi parikh and stefan lee vilbert pretraining task agnostic visi olinguistic representations for vision and language tasks in proceedings of neurips 201940 tomas mikolov kai chen greg corrado and jeffrey dean efcient estimation of wordrepresentations in vector space in proceedings of iclr 201341 tomas mikolov martin karat lukas burget jan cernocky and sanjeev khudanpur recur rent neural network based language model in proceedings of interspeech 201042 makoto miwa and mohit bansal end to end relation extraction using lstms on sequences andtree structures in proceedings of acl 201643 andriy mnih and yee whye teh a fast and simple algorithm for training neural probabilisticlanguage models in proceedings of icml 201244 frederic morin and yoshua bengio hierarchical probabilistic neural network language modelin proceedings of aistats 200545 nanyun peng hoifung poon chris quirk kristina toutanova and wen tau yih cross sentence n ary relation extraction with graph lstms transactions of the association forcomputational linguistics 5101115 201746 matthew e peters mark neumann robert logan roy schwartz vidur joshi sameer singhand noah a smith knowledge enhanced contextual word representations in proceedings ofemnlp ijcnlp 201947 ngoc quan pham german kruszewski and gemma boleda convolutional neural networklanguage models in proceedings of emnlp 201648 matt post and shane bergsma explicit and implicit syntactic features for text classicationin proceedings of acl 201349 chris quirk and hoifung poon distant supervision for relation extraction beyond the sentenceboundary in proceedings of eacl 201750 alec radford karthik narasimhan tim salimans and ilya sutskever improving languageunderstanding by generative pre training url httpss3 us west 2amazonawscomopenai assetsresearchcoverslanguageunsupervisedlanguageunderstandingpaperpdf 201851 sebastian riedel limin yao andrew mccallum and benjamin m marlin relation extractionwith matrix factorization and universal schemas in proceedings of naacl hlt pages 74842013884sentence representation52 mike schuster and kuldip k paliwal bidirectional recurrent neural networks ieee transac tions on signal processing 451126732681 199753 livio baldini soares nicholas fitzgerald jeffrey ling and tom kwiatkowski matching theblanks distributional similarity for relation learning in proceedings of acl pages 28952905 201954 richard socher brody huval christopher d manning and andrew y ng semantic compo sitionality through recursive matrix vector spaces in proceedings of emnlp 201255 richard socher jeffrey pennington eric h huang andrew y ng and christopher d manningsemi supervised recursive autoencoders for predicting sentiment distributions in proceedingsof emnlp 201156 richard socher alex perelygin jean y wu jason chuang christopher d manning andrew yng and christopher potts recursive deep models for semantic compositionality over a senti ment treebank in proceedings of emnlp 201357 kaitao song xu tan tao qin jianfeng lu and tie yan liu mass masked sequence tosequence pre training for language generation in proceedings of icml 201958 daniel soutner zdenek loose ludek mller and ale prak neural network languagemodel with cache in proceedings of ictsd 201259 chen sun austin myers carl vondrick kevin murphy and cordelia schmid videobert ajoint model for video and language representation learning in proceedings of iccv 201960 siqi sun yu cheng zhe gan and jingjing liu patient knowledge distillation for bert modelcompression in proceedings of emnlp ijcnlp page 43144323 201961 kumutha swampillai and mark stevenson inter sentential relations in information extractioncorpora in proceedings of lrec 201062 kai sheng tai richard socher and christopher d manning improved semantic representa tions from tree structured long short term memory networks in proceedings of acl 201563 duyu tang bing qin and ting liu document modeling with gated recurrent neural networkfor sentiment classication in proceedings of emnlp 201564 wilson l taylor cloze procedure a new tool for measuring readability journalism bulletin304415433 195365 ashish vaswani noam shazeer niki parmar llion jones jakob uszkoreit aidan n gomezand lukasz kaiser attention is all you need in proceedings of neurips 201766 patrick verga emma strubell and andrew mccallum simultaneously self attending to allmentions for full abstract biological relation extraction in proceedings of naacl hlt 201867 zhen wang jianwen zhang jianlin feng and zheng chen knowledge graph and text jointlyembedding in proceedings of emnlp pages 15911601 201468 zhigang wang and juan zi li text enhanced representation learning for knowledge graph inproceedings of ijcai pages 12931299 201669 kun xu yansong feng songfang huang and dongyan zhao semantic relation classicationvia convolutional neural networks with simple negative sampling in proceedings of emnlp201570 zhilin yang zihang dai yiming yang jaime carbonell russ r salakhutdinov and quoc vle xlnet generalized autoregressive pretraining for language understanding in proceedingsof neurips 201971 zichao yang diyi yang chris dyer xiaodong he alex smola and eduard hovy hierarchicalattention networks for document classication in proceedings of naacl 201672 yuan yao deming ye peng li xu han yankai lin zhenghao liu zhiyuan liu lixin huangjie zhou and maosong sun docred a large scale document level relation extraction datasetin proceedings of acl 201973 zhi xiu ye and zhen hua ling multi level matching and aggregation network for few shotrelation classication in proceedings of acl pages 28722881 201974 wenpeng yin and hinrich schtze multichannel variable size convolution for sentence clas sication in proceedings of conll 201575 daojian zeng kang liu yubo chen and jun zhao distant supervision for relation extractionvia piecewise convolutional neural networks in proceedings of emnlp 2015references8976 daojian zeng kang liu siwei lai guangyou zhou and jun zhao relation classication viaconvolutional deep neural network in proceedings of coling 201477 wenyuan zeng yankai lin zhiyuan liu and maosong sun incorporating relation paths inneural relation extraction in proceedings of emnlp 201778 dongxu zhang and dong wang relation classication via recurrent neural network arxivpreprint arxiv150801006 201579 ye zhang iain marshall and byron c wallace rationale augmented convolutional neuralnetworks for text classication in proceedings of emnlp 201680 ye zhang stephen roller and byron c wallace mgnc cnn a simple approach to exploitingmultiple word embeddings for sentence classication in proceedings of naacl 201681 zhengyan zhang xu han zhiyuan liu xin jiang maosong sun and qun liu ernieenhanced language representation with informative entities in proceedings of acl 2019open access this chapter is licensed under the terms of the creative commons attribution 40international license httpcreativecommonsorglicensesby40 which permits use sharingadaptation distribution and reproduction in any medium or format as long as you give appropriatecredit to the original authors and the source provide a link to the creative commons license andindicate if changes were madethe images or other third party material in this chapter are included in the chapters creativecommons license unless indicated otherwise in a credit line to the material if material is notincluded in the chapters creative commons license and your intended use is not permitted bystatutory regulation or exceeds the permitted use you will need to obtain permission directly fromthe copyright holderchapter 5retracted chapter documentrepresentationthe authors have retracted this chapter because signicant portions of the text areduplicated from 1 and 2 the authors apologise to readers for this error all authorsagree with this retraction1 blei dm probabilistic topic models communications of the acm april 2012vol 55 no 4 pages 7784 101145213380621338262 le quoc and tomas mikolov distributed representations of sentences and doc uments proceedings of the 31st international conference on machine learningpmlr 32211881196 2014 the authors 2020 corrected publication 2023z liu et al representation learning for natural language processinghttpsdoiorg101007978 981 15 5573 2 591chapter 6sememe knowledge representationabstract linguistic knowledge graphs eg wordnet and hownet describe lin guistic knowledge in formal and structural language which can be easily incorpo rated in modern natural language processing systems in this chapter we focus onthe research about hownet we rst briey introduce the background and basicconcepts of hownet and sememe next we introduce the motivations of sememerepresentation learning and existing approaches at the end of this chapter we reviewimportant applications of sememe representation61introductionin the eld of natural language processing nlp words are generally the smallestobjects of study because they are considered as the smallest meaningful units thatcan stand by themselves of human languages however the meanings of wordscan be further divided into smaller parts for example the meaning of man can beconsidered as the combination of the meanings of human male and adult andthe meaning of boy is composed of the meanings of human male and childin linguistics the minimum indivisible units of meaning ie semantic units aredened as sememes 8 and some linguists believe that meanings of all the wordscan be composed of a limited closed set of sememeshowever sememes are implicit and as a result it is hard to intuitively dene the setof sememes and determine which sememes a word can have at a glance thereforesome researchers spend tens of years sifting sememes from all kinds of dictionariesand linguistic knowledge bases kbs and annotating words with these selectedsememes to construct sememe based linguistic kb wordnet and hownet 17 arethe two most famous ones of such kbs in this section we focus on the representationof linguistic knowledge in hownet the authors 2020z liu et al representation learning for natural language processinghttpsdoiorg101007978 981 15 5573 2 61251266sememe knowledge representation611linguistic knowledge graphs6111wordnetwordnet is a large lexical database for the english language and could also be viewedas a kg containing multi relational data it was rst started in 1985 and created underthe direction of george armitage miller a psychology professor in the cognitivescience laboratory of princeton university nowadays wordnet is becoming themost popular lexicon dictionary in the world that could be available through the webfor free and is widely used in nlp applications such as text analysis informationretrieval and relation extraction there is also a global wordnet association aimingto provide a public and noncommercial platform for wordnets of all languages inthe worldbased on meanings wordnet groups english nouns verbs adjectives andadverbs into synsets ie sets of cognitive synonyms which represent a distinctconcept each synset possesses a brief description and in most cases there are evensome short sentences functioning as examples illustrating the use of words in thissynset the conceptual semantic and lexical relations link the synsets and words themain relation among words is synonymy which indicates that the words share similarmeanings and could be replaced by others in some contexts while the main relationamong synsets is hyperonymyhyponymy ie the isa relation which indicatesthe relationship between a more general synset and a more specic synset there arealso hierarchical structures for verb synsets and the antonymy is describing the rela tion between adjectives with opposite meanings to sum up all wordnets 117 000synsets are linked to each other by a small number of conceptual relations6112hownethownet was initially designed and constructed by zhendong dong and his son qiangdong in the 1990s and it has been kept frequently updated since it was publishedin 1999the sememe set of hownet is determined by extracting analyzing mergingand ltering semantics of thousands of chinese characters and the sememe set canalso be adjusted or expanded in the subsequent process of annotating words eachsememe in hownet is represented by a word or phrase in chinese and english suchas human   and propername  hownet also builds a taxonomy for the sememes all the sememes of hownetcan be classied as one of the following types thing part attribute time spaceattribute value and event in addition to depict the semantics of words more pre cisely hownet incorporates relations between sememes which are called dynamicroles into the sememe annotations of wordsconsidering the polysemy hownet differentiates diverse senses of each wordin the sememe annotations and each sense is also expressed in both chinese and61 introduction127wordsensesememeapplecomputerbringbringspecificbrandspecificbrandcommunicatepatternvaluepatternvaluetoolfruitfruitableabletreereproduceapplecomputerapplephoneapplefruitappletreepatientpatientagentmodifiercoeventcoeventinstrumentpatientprodectscopescopemodifierfig 61 an example of word annotated with sememes in hownettable 61 statistics of hownettypecountsense229767distinct chinese word127266distinct english word104025sememe2187english an example of sememe annotation for a word is illustrated in fig61we can see from the gure that the word apple has four senses includingapplecomputer applephone applefruit and appletreeand each sense is the root node of a sememe tree where each pair of father and sonsememe nodes is multi relational additionally hownet annotates the pos tag foreach sense and adds sentiment category as well as some usage examples for certainsensesthe latest version of hownet was published in january 2019 and the statistics areshown in table61since hownet was published it has attracted wide attention people use hownetand sememe in various nlp tasks including word similarity computation 40 wordsense disambiguation 70 question classication 62 and sentiment analysis 1620 among these researches 40 is one of the most inuential works in which thesimilarity of given two words is computed by measuring the degree of resemblanceof their sememe treesrecent years also witnessed some works incorporating sememes into neural net work models reference 49 proposes a novel word representation learning modelnamed sst that reforms skip gram 43 by adding contextual attention to senses of1286sememe knowledge representationthe target word which are represented with combinations of corresponding sememesembeddings experimental results show that sst can not only improve the qualityof word embeddings but also learn satisfactory sense embeddings to do word sensedisambiguationreference 23 incorporates sememes into the decoding phase of language mod eling where sememes are predicted rst and then senses and words are predicted insuccession the proposed model shows enhancement in the perplexity of languagemodeling and the performance of the downstream task headline generationbesides hownet is also utilized in lexicon expansion 68 semantic rationalityevaluation 41 etcconsidering that human annotation is time consuming and labor intensive someworks attempt to employ machine learning methods to predict sememes for newwords automatically reference 66 proposes the task rstly and presents two sim ple but effective models spwe which is based on collaborative ltering and spsewhich is based on matrix factorization reference 30 further takes the internal infor mation of words into account when predicting sememes and achieves a considerableboost of performance and 38 takes advantage of denitions of words to predictsememes as for 56 they propose the task of cross lingual lexical sememe pre diction and present a bilingual word representation learning and alignment basedmodel which demonstrates effectiveness in predicting sememes for cross lingualwords62sememe knowledge representationwordrepresentationlearningwrlisafundamentalandcriticalstepinmanynlptasks such as language modeling 4 and neural machine translation 64 there havebeen a lot of researches for learning word representations among which word2vec43 achieves a nice balance between effectiveness and efciency in word2vec eachword corresponds to one single embedding ignoring the polysemy of most wordsto address this issue 29 introduces a multi prototype model for wrl conductingunsupervised word sense induction and embeddings according to context clustersreference 13 further utilizes the synset information in wordnet to instruct wordsense representation learningthese previous studies demonstrate that word sense disambiguation is criticalfor wrl and the sememe annotation of word senses in hownet can provide nec essary semantic regularization for these tasks 63 to explore its feasibility weintroduce the sememe encoded word representation learning se wrl modelwhich detects word senses and learns representations simultaneously more specif ically this framework regards each word sense as a combination of its sememesand iteratively performs word sense disambiguation according to their contexts andlearns representations of sememes senses and words by extending skip gram inword2vec 43 in this framework an attention based method is proposed to selectappropriate word senses according to contexts automatically to take full advantage62 sememe knowledge representation129of sememes we introduce three different learning and attention strategies ssa sacand sat for se wrl which will be described in the following paragraphs621simple sememe aggregation modelthe simple sememe aggregation model ssa is a straightforward idea based onskip gram model for each word ssa considers all sememes in all senses of theword together and represents the target word using the average of all its sememeembeddings formally we havew  1mswiswxsi jxwixsij 61which means the word embedding of w is composed by the average of all its sememeembeddings here sw is the sense set of w and xwiis the sememe set of the ithsense of w m stands for the overall number of sememes belonging to wthis model follows the assumption that the semantic meaning of a word is com posed of the semantic units ie sememes as compared to the conventional skip gram model since sememes are shared by multiple words this model can utilizesememe information to encode latent semantic correlations between words in thiscase similar words that share the same sememes may nally obtain similar repre sentations622sememe attention over context modelthe ssa model replaces the target word embedding with the aggregated sememeembeddings to encode sememe information into word representation learning how ever each word in the ssa model still has only one single representation in differentcontexts which cannot deal with the polysemy of most words it is intuitive that weshould construct distinct embeddings for a target word according to specic contextswith the favor of word sense annotation in hownetto address this issue the sememe attention over context model sac is pro posed sac utilizes the attention scheme to automatically select appropriate sensesfor context words according to the target word that is sac conducts word sensedisambiguation for context words to learn better representations of target words thestructure of the sac model is shown in fig62more specically sac utilizes the original word embedding for target word wand uses sememe embeddings to represent context word wc instead of the originalcontext word embeddings suppose a word typically demonstrates some specicsenses in one sentence here the target word embedding is employed as attention1306sememe knowledge representationsememesensecontextwords1s2s3attattattwt 2wt 1wt1wt2wtfig 62 the architecture of sac modelto select the most appropriate senses to make up the context word embeddings thecontext word embedding wc can be formalized as followswc swcj1attswcjswcj62where swcjstands for the jth sense embedding of wc and attswcj represents theattention score of the jth sense with respect to the target word w dened as followsattswcj expw  swcjswck1expw  swck63note that when calculating attention the average of sememe embeddings is usedto represent each sense swcjswcj1xwcjxwcjk1xs jk64the attention strategy assumes that the more relevant a context word sense embed ding is to the target word w the more this sense should be considered when buildingcontext word embeddings with the favor of attention scheme each context word62 sememe knowledge representation131sememesensecontextwords1s2s3attattattwt 2wt 1wt1wt2wtcontextualembeddingfig 63 the architecture of sat modelcan be represented as a particular distribution over its sense this can be regarded assoft wsd and it helps learn better word representations623sememe attention over target modelthe sememe attention over context model can exibly select appropriate sensesand sememes for context words according to the target word the process can alsobe applied to select appropriate senses for the target word by taking context wordsas attention hence the sememe attention over target model sat is proposedwhich is shown in fig63different from the sac model sat learns the original word embeddings forcontext words and sememe embeddings for target words then sat applies contextwords to perform attention over multiple senses of the target word w to build theembedding of w formalized as followsw swj1attswjswj 65and the context based attention is dened as follows1326sememe knowledge representationattswj expwc  swj swk1 expwc  swk 66where the average of sememe embeddings swjis also used to represent each senseswj here wc is the context embedding consisting of a constrained window of wordembeddings in cwi we havewc 12k kik kik wkk  i67note that since in experiments the sense selection of the target word is found tobe only dependent on more limited context words for calculating attention hence asmaller k  is selected as compared to krecall that sac only uses one target word as attention to select senses of contextwords whereas sat uses several context words together as attention to select appro priate senses of target words hence sat is expected to conduct more reliable wsdand result in more accurate word representations which is explored in experiments63applicationsin the previous section we introduce hownet and sememe representation in factlinguistic knowledge graphs such as hownet contain rich information which couldeffectivelyhelpdownstreamapplicationsthereforeinthissectionwewillintroducethe major applications of sememe representation including sememe based wordrepresentation linguistic knowledge graph construction and language modeling631sememe guided word representationsememe guided word representation is intended for improving word embeddingsfor sememe prediction by introducing the information of sememe based linguistickbs of the source language qi et al 56 present two methods of the sememe guidedword representation6311relation based word representationa simple and intuitive method is to let words with similar sememe annotations tendto have similar word embeddings which is named as word relation based approachto begin with a synonym list is constructed from sememe based linguistic kbs63 applications133where words sharing a certain number of sememes are regarded as synonyms nextsynonyms are forced to have closer word embeddingsformally let wi be the original word embedding of wi and wi be its adjustedword embedding and let synwi denote the synonym set of word wi then the lossfunction is dened aslsememe wiviwi wi2 w jsynwii jwi w j268where  and  control the relative strengths of the two terms it should be notedthat the idea of forcing similar words to have close word embeddings is similarto the state of the art retrotting approach 19 however the retrotting approachcannot be applied here because sememe based linguistic kbs such as hownet cannotdirectly provide its needed synonym list6312sememe embedding based word representationsimple and effective as the word relation based approach is it cannot make fulluse of the information of sememe based linguistic kbs because it disregards thecomplicated relations between sememes and words as well as relations between dif ferent sememes to address this limitation the sememe embedding based approachis proposed which learns both sememe and word embeddings jointlyin this approach sememes are represented with distributed vectors as well andplace them into the same semantic space as words similar to spse 66 whichlearns sememe embeddings by decomposing the word sememe matrix and sememe sememe matrix the method utilizes sememe embeddings as regularizers to learnbetter word embeddings different from spse the model described in 56 does notuse pretrained word embeddings instead it learns word embeddings and sememeembeddings simultaneously more specically a word sememe matrix m can beextracted from hownet where mi j  1 indicates word wi is annotated with sememex j otherwise mi j  0 hence by factorizing m the loss function can be dened aslsememe wivx jxwi  x j  bs  bj mi j269where bi and bj are the biases of wi and x j and x denotes sememe setin this approach word and sememe embeddings are obtained in a unied seman tic space the sememe embeddings bear all the information about the relationshipsbetween words and sememes and they inject the information into word embed dings therefore the word embeddings are expected to be more suitable for sememeprediction1346sememe knowledge representation632sememe guided semantic compositionality modelingsemantic compositionality sc is dened as the linguistic phenomenon that themeaning of a syntactically complex unit is a function of meanings of the complexunits constituents and their combination rule 50 some linguists regard sc as thefundamental truth of semantics 51 in the eld of nlp sc has proved effective inmany tasks including language modeling 47 sentiment analysis 42 61 syntacticparsing 59 etcmost literature on sc pays attention to using vector based distributional mod els of semantics to learn representations of multiword expressions mwes ieembeddings of phrases or compounds reference 46 conducts a pioneering workwhich introduces a general framework to formulate this taskp  f w1 w2 r k 610where1 f is the compositionality function p denotes the embedding of an mwew1 and w2 represent the embeddings of the mwes two constituents r stands forthe combination rule and k refers to the additional knowledge which is needed toconstruct the semantics of the mwemost of the proposed approaches ignore r and k  centering on reforming com positionality function f 3 21 60 61 some try to integrate combination rule rinto sc models 7 35 65 71 a few works consider external knowledge k ref erence 72 tries to incorporate task specic knowledge into an lstm model forsentence level screference 55 proposes a novel sememe based method to model semantic com positionality they argue that sememes are benecial to modeling sc to verify thisthey rst design a simple sc degree scd measurement experiment and nd thatthe scds of mwes computed by simple sememe based formulae are highly corre lated with human judgment this result shows that sememes can nely depict mean ings of mwes and their constituents and capture the semantic relations betweenthe two sides moreover they propose two sememe incorporated sc models forlearning embeddings of mwes namely semantic compositionality with aggre gated sememe scas model and semantic compositionality with mutual sememeattention scmsa model when learning the embedding of an mwe the scasmodel concatenates the embeddings of the mwes constituents and their sememeswhile the scmsa model considers the mutual attention between a constituentssememes and the other constituent finally they integrate the combination rule ier in eq 610 into the two models their models achieve signicant performanceover the mwe similarity computation task and sememe prediction task comparedwith baseline methodsin this section we focus on the work conducted by 55 we will rst intro duce sememe based sc degree scd computation formulae and then expand theirsememe incorporated sc models1this formula only applies to two word mwes but can be easily extended to longer mwes63 applications1356321sememe based scd computation formulaealthough sc widely exists in mwes not every mwe is fully semantically com positional in fact different mwes show different degrees of sc reference 55believes that sememes can be used to measure scd convenientlyto this end based on the assumption that all the sememes of a word accuratelydepict the words meaning they intuitively design a set of scd computation formu lae which are consistent with the principle of scdthe formulae are illustrated in table62 they dene four scds denoted bynumbers 3 2 1 and 0 where larger numbers mean higher scds sp sw1 and sw2represent the sememe sets of an mwe its rst and second constituent respectivelyhere is a brief explanation for their scd computation formulae1 for scd 3 the sememe set of an mwe is identical to the union of the twoconstituents sememe sets which means the meaning of the mwe is exactly thesame as the combination of the constituents meanings therefore the mwe is fullysemantically compositional and should have the highest scd2 for scd 0 an mwe has totally different sememes from its constituents whichmeans the mwes meaning cannot be derived from its constituents meanings hencethe mwe is completely non compositional and its scd should be the lowest3 as for scd 2 the sememe set of an mwe is a proper subset of the union ofits constituents sememe sets which means the meanings of the constituents coverthe mwes meaning but cannot precisely infer the mwes meaning4 finally for scd 1 an mwe shares some sememes with its constituents butboth the mwe itself and its constituents have some unique sememesthere is an example for each scd in table62 including a chinese mwe itstwo constituents and their sememes26322evaluating scd computation formulaeto evaluate their sememe based scd computation formulae 55 constructs ahuman annotated scd dataset they ask several native speakers to label scds for500 chinese mwes where there are four degrees to choose before labeling anmwe they are shown the dictionary denitions of both the mwe and its con stituentseach mwe is labeled by 3 annotators and the average of the 3 scds given bythem is the mwes nal scdeventually they obtain a dataset containing 500 chinese mwes together withtheir human annotated scdsthen they evaluate the correlativity between scds of the mwes in the datasetcomputed by sememe based rules and those given by humans they nd pearsonscorrelation coefcient is up to 075 and spearmans rank correlation coefcient is2in chinese most mwes are words consisting of more than two characters which are actuallysingle morpheme words1366sememe knowledge representationtable 62 sememe based semantic compositionality degree computation formulae and examples bold sememes of constituents are shared with the constituentscorresponding mwe63 applications137074 these results manifest remarkable capability of sememes to compute scds ofmwes and provide a proof that sememes of a word can nely represent the wordsmeaning6323sememe incorporated sc modelsin this section we rst introduce two basic sememe incorporated sc models in detailnamely semantic compositionality with aggregated sememe scas and semanticcompositionality with mutual sememe attention scmsa scas model simplyconcatenates the embeddings of the mwes constituents and their sememes whilethe scmsa model takes account of the mutual attention between a constituentssememes and the other constituent then we describe how to integrate combinationrules into the two basic modelsincorporating sememes only following the notations in eq 610 for anmwe p  w1 w2 its embedding can be represented asp  f w1 w2 k 611where p w1 w2 rd and d is the dimension of embeddings k denotes thesememe knowledge here and we assume that we only know the sememes of w1and w2 considering that mwes are normally not in the sememe kbs x indicatesthe set of all the sememes and xw  x1  xxw x to signify the sememe setof w in addition x rd denotes the embedding of sememe x1 scas model the rst model we introduce is the scas model which isillustrated in fig64 the idea of the scas model is straightforward ie simplymweconstituentsememew1w1w2w2fig 64 the architecture of scas model1386sememe knowledge representationconcatenating word embedding of a constituent and the aggregation of its sememesembeddings formally we havew1 xixw1xiw2 x jxw2xj612where w1 and w2 represent the aggregated sememe embeddings of w1 and w2 respec tively then p can be obtained byp  tanhwcw1  w2w1  w2  bc613where wc rd2d is the composition matrix and bc rd is a bias vector2 scmsa modelthe scas model simply uses the sum of all the sememes embeddings of aconstituent as the external information however a constituents meaning may varywith the other constituent and accordingly the sememes of a constituent should havedifferent weights when the constituent is combined with different constituents thereis an example in the case studycorrespondingly we introduce the scmsa model fig65 which adopts themutual attention mechanism to dynamically endow sememes with weights formallywe havee1  tanhwaw1  baa2i exp si  e1x jxw2 exp x j  e1w2 xixw2a2ixi614where wa rdd is the weight matrix and ba rd is a bias vector similarly w1can be calculated then they still use eq 613 to obtain pintegrating combination rules reference 55 further integrates combinationrules into their sememe incorporated sc models in other wordsp  f w1 w2 k r615we can use totally different composition matrices for mwes with different com bination ruleswc  wrcr rs616where wrc rd2d and rs refers to combination rule set containing syntax rules ofmwes eg adjective noun and noun nounhowever there are many different combination rules and some rules have sparseinstances which are not enough to train the corresponding composition matrices63 applications139mweconstituentsememew1w1w2w2a21a22a23a11a12fig 65 the architecture of scmsa modelwith d  2d parameters in addition we believe that the composition matrix shouldcontain common compositionality information except the combination rule speciccompositionality information hence they let composition matrix wc be the sum ofa low rank matrix containing combination rule information and a matrix containingcommon compositionality informationwc  ur1ur2  wcc617where ur1 rddr  ur2 rdr2d and dr n is a hyperparameter and may varywith the combination rule and wcc rd2d633sememe guided language modelinglanguage modeling lm aims to measure the probability of a word sequencereecting its uency and likelihood as a feasible sentence in a human languagelanguage modeling is an essential component in a wide range of natural language1406sememe knowledge representationabconventional decodersememe driven decodersememepredictorcontextvectorworddistributionsememedistributionsensedistributionsensepredictorwordpredictorworddistributioncontextvectorfig 66 decoders of a conventional lm b sememe driven lmprocessing nlp tasks such as machine translation 9 10 speech recognition 34information retrieval 5 24 45 54 and document summarization 2 57a probabilistic language model calculates the conditional probability of the nextword given their contextual words which are typically learned from large scale textcorpora taking the simplest language model for example n gram estimates theconditional probabilities according to maximum likelihood over text corpora 31recent years have witnessed the advances of recurrent neural networks rnnsas the state of the art approach for language modeling 44 in which the context isrepresented as a low dimensional hidden state to predict the next word fig66those conventional language models including neural models typically assumewords as atomic symbols and model sequential patterns at the word level howeverthis assumption does not necessarily hold to some extent consider the followingexample sentence for which people want to predict the next word in the blankthe us trade deficit last year is initially estimated to be 40 billionpeople may rst realize a unit should be lled in then realize it should be acurrency unit based on the country this sentence is talking about the usone may conrm it should be an american currency unit and predict theword dollars here the unit currency and american which are basicsemantic units of the word dollars are also the sememes of the word dollarshoweverthisprocesshasnotbeenexplicitlytakenintoconsiderationbyconventionallanguage models that is in most cases words are atomic language units words arenot necessarily atomic semantic units for language modeling thus explicit modelingof sememes could improve both the performance and the interpretability of languagemodels however as far as we know a few efforts have been devoted to exploring theeffectiveness of sememes in language models especially neural language modelsit is nontrivial for neural language models to incorporate discrete sememe knowl edge as it is not compatible with continuous representations in neural models inthis part sememe driven language model sdlm is proposed to leverage lexi cal sememe knowledge in order to predict the next word sdlm utilizes a novel63 applications141sememe sense word generation process 1 first sdlm estimates sememes dis tribution according to the context 2 regarding these sememes as experts sdlmemploys a sparse product of expert method to select the most probable senses 3finally sdlm calculates the distribution of words by marginalizing out the distri bution of sensessdlm is composed of three modules in series sememe predictor sense pre dictor and word predictor fig66 the sememe predictor rst takes the contextvector as input and assigns a weight to each sememe then each sememe is regardedas an expert and makes predictions about the probability distribution over a set ofsenses in the sense predictor finally the probability of each word is obtained in theword predictorsememe predictor the sememe predictor takes the context vector g rh1as input and assigns a weight to each sememe assume that given the contextw1 w2     wt1 the events that word wt contains sememe xk k 1 2     kare independent since the sememe is the minimum semantic unit and there is nosemantic overlap between any two different sememes for simplicity the superscriptt is ignored the sememe predictor is designed as a linear decoder with the sig moid activation function therefore pk the probability that the next word containssememe xk is formulated aspk  pxkg  sigmoidg  vk  bk618where vk rh1 bk r are trainable parameters and sigmoid denotes the sig moid activation functionsense predictor and word predictor the architecture of the sense predictor ismotivated by product of experts poe 25 each sememe is regarded as an expertthat only makes predictions on the senses connected with it let sxk denote theset of senses that contain sememe xk the kth expert different from conventionalneural language models which directly use the inner product of the context vectorg rh1 and the output embedding w rh2 for word w to generate the score foreach word sense predictor uses kg w to calculate the score given by expertxk and a bilinear function parameterized with a matrix uk rh1h2 is chosen as astraight implementation of k kg w  gukw619the score of sense s provided by sememe expert xk can be written as kg stherefore pxksg the probability of sense s given by expert xk is formulated aspxksg expqkckskg sssxk  expqkckskg s620where cks is a normalization constant because sense s is not connected to all expertsthe connections are sparse with approximately n edges   5 here we can1426sememe knowledge representationchoose either cks  1xs left normalization or cks  1xssxk sym metric normalizationin the sense predictor qk can be viewed as a gate which controls the magnitude ofthe term ckskg s thus controlling the atness of the sense distribution providedby sememe expert xk consider the extreme case when pk 0 the prediction willconverge to the discrete uniform distribution intuitively it means that the sememeexpert will refuse to provide any useful information when it is not likely to be relatedto the next wordfinally the predictions can be summarized on sense s by taking the product ofthe probabilities given by relevant experts and then normalize the result that is tosay psg the probability of sense s satisespsg xkxspxksg621using eqs619 and 620 psg can be formulated aspsg expxkxs qkcksgukss expxkxs qkcksguks622it should be emphasized that all the supervision information provided by hownetis embodied in the connections between the sememe experts and the senses if themodel wants to assign a high probability to sense s it must assign a high probability tosome of its relevant sememes if the model wants to assign a low probability to senses it can assign a low probability to its relevant sememes moreover the predictionmade by sememe expert xk has its own tendency because of its own k  besidesthe sparsity of connections between experts and senses is also determined by hownetitselfas illustrated in fig67 in the word predictor pwg the probability of wordw is calculated by summing up probabilities of corresponding s given by the sensepredictor that ispwg sswpsg623634sememe predictionthe manual construction of hownet is actually time consuming and labor intensiveeg hownet has been built for more than 10 years by several linguistic expertshowever as the development of communications and techniques new words andphrases are emerging the semantic meanings of existing words are also dynamicallyevolving in this case sustained manual annotation and updates are becoming muchmore overwhelmed moreover due to the high complexity of sememe ontology and63 applications143090102lstmlstmlstmlstm010302pwordpsensesememeexpertscontextvectorfig 67 the architecture of sdlm modelword meanings it is also challenging to maintain annotation consistency amongexperts when they collaboratively annotate lexical sememesto address the issues of inexibility and inconsistency of manual annotation theautomatic lexical sememe prediction task is proposed which is expected to assistexpert annotation and reduce manual workload note that for simplicity most worksintroduced in this part do not consider the complicated hierarchies of word sememesand simply group all annotated sememes of each word as the sememe set for learningand predictionthe basic idea of sememe prediction is that those words of similar semantic mean ings may share overlapped sememes hence the key challenge of sememe predictionis how to represent semantic meanings of words and sememes to model the semanticrelatedness between them in this part we will focus on introducing the sememe pre diction word accomplished by xie et al 66 in their work they propose to modelthe semantics of words and sememes using distributed representation learning 26distributed representation learning aims to encode objects into a low dimensionalsemantic space which has shown its impressive capability of modeling semantics ofhuman languages eg word embeddings 43 have been widely studied and utilizedin various tasks of nlpas shown in previous work 43 it is effective to measure word similarities usingcosine similarity or euclidean distance of their word embeddings learned from alarge scale text corpus hence a straightforward method for sememe prediction is1446sememe knowledge representationthat given an unlabeled word we nd its most related words in hownet accordingto their word embeddings and recommend the annotated sememes of these relatedwords to the given word the method is intrinsically similar to collaborative ltering58 in recommendation systems capable of capturing semantic relatedness betweenwords and sememes based on their annotation co occurrencesword embeddings can also be learned with techniques of matrix factorization37 inspired by the successful practice of matrix factorization for personalizedrecommendation 36 a new model which factorizes the word sememe matrix fromhownet and obtains sememe embeddings is proposed in this way the relatedness ofwords and sememes can be measured directly using dot products of their embeddingsaccording to which we could recommend the most related sememes to an unlabeledwordthe two methods are named as sememe prediction with word embeddingsspwe and with sememe embeddings spsespase respectively6341sememe prediction with word embeddingsgiven an unlabeled word it is straightforward to recommend sememes according toits most related words assuming that similar words should have similar sememesthis idea is similar to collaborative ltering in the personalized recommendation forin the scenario of sememe prediction words can be regarded as users and sememesas the itemsproducts to be recommended inspired by this sememe predictionwith word embeddings spwe model is proposed which uses similarities of wordembeddings to judge user distancesformally the score function px jw of sememes x j given a word w is denedaspx jw wivcosw wimi jcri624where cosw wi is the cosine similarity between word embeddings of w and wipretrained by glove mi j indicates the annotation of sememe x j on word wi wheremi j  1 indicates the word wi which has the sememe x j in hownet and otherwisehas not higher the score function px jw is more possible the word w should berecommended with x jdiffering from classical collaborative ltering in recommendation systems onlythe most similar words should be concentrated when predicting sememes for newwords since irrelevant words have totally different sememes which may be noisesfor sememe prediction to address this problem a declined condence factor cri isassigned for each word wi whereri is the descend rank of word similarity cosw wiand c 0 1 is a hyperparameter in this way only a few top words that are similarto w have strong inuences on predicting sememesspwe only uses word embeddings for word similarities and is simple and effec tive for sememe prediction it is because differing from the noisy and incompleteuser item matrix in most recommender systems hownet is carefully annotated by63 applications145human experts and thus the word sememe matrix is with high condence thereforethe word sememe matrix can be condently applied to collaboratively recommendreliable sememes based on similar words6342sememe prediction with sememe embeddingssememe prediction with word embeddings model follows the assumption that thesememes of a word can be predicted according to its related words sememes how ever simply considering sememes as discrete labels may inevitably neglect the latentrelations between sememes to take the latent relations of sememes into consider ation sememe prediction with sememe embeddings spse model is proposedwhich projects both words and sememes into the same semantic vector space learn ing sememe embeddings according to the co occurrences of words and sememes inhownetsimilar to glove 53 which decomposes co occurrence matrix of words to learnword embeddings sememe embeddings can be learned by factorizing word sememematrix and sememe sememe matrix simultaneously these two matrices are bothconstructed from hownet as for word embeddings similar to spwe spse usesword embeddings pretrained from a large scale corpus and xes them during fac torizing of the word sememe matrix with matrix factorization both sememe andword embeddings can be encoded into the same low dimensional semantic spaceand then computed the cosine similarity between normalized embeddings of wordsand sememes for sememe predictionmore specically similar to m a sememe sememe matrix c can also be extractedwhere c jk is dened as point wise mutual information that c jk  pmix j xk toindicate the correlations between two sememes x j and xk note that by factorizingc two distinct embeddings for each sememe s will be obtained denoted as x and xrespectively the loss function of learning sememe embeddings is dened as followsl wiwx jxwi  x j nx j  bi  bj mi j2  x jxkxx j  xk c jk2625where bi and bj denote the bias of wi and x j these two parts correspond to thelosses of factorizing matrices m and c adjusted by the hyperparameter  sincethe sememe embeddings are shared by both factorizations our spse model enablesjointly encoding both words and sememes into a unied semantic spacesince each word is typically annotated with 25 sememes in hownet most ele ments in the word sememe matrix are zeros if all zero elements and nonzero ele ments are treated equally during factorization the performance will be much worseto address this issue different factorization strategies are assigned for zero andnonzero elements for each zero element the model chooses to factorize them witha small probability like 05 and otherwise the model chooses to ignore while fornonzero elements the model always chooses to factorize them with the help of thisstrategy the model can pay more attention to those annotated word sememe pairs1466sememe knowledge representationin spse sememe embeddings are learned accompanying with word embeddingsvia matrix factorization into the unied low dimensional semantic space matrixfactorization has been veried as an effective approach in the personalized recom mendation because it can accurately model relatedness between users and items andis highly robust to noises in user item matrices using this model we can exiblycompute semantic relatedness of words and sememes which provides us an effec tive tool to manipulate and manage sememes including but not limited to sememeprediction6343sememe prediction with aggregated sememe embeddingsinspired by the characteristics of sememes we assume that the word embeddings aresemantically composed of sememe embeddings in the word sememe joint space wecan simply implement semantic composition as additive operations that each wordembedding is expected to be the sum of its all sememes embeddings following thisassumption sememe prediction with aggregated sememe embeddings spasemodel is proposed spase is also based on matrix factorization and is formallydenoted aswi x jxwimi jx j626where xwi is the sememe set of the word wi and mi j represents the weight ofsememe x j for word wi which only has value on nonzero elements of word sememelabeled matrix m to learn sememe embeddings we attempt to decompose the wordembedding matrix v into m and sememe embedding matrix x with pretrained wordembeddings xed during training which could also be written as v  mxthe contribution of spase is that it complies with the denition of sememesin hownet that sememes are the semantic components of words in spase eachsememe can be regarded as a tiny semantic unit and all words can be representedby composing several semantic units ie sememes which make up an interestingsemantic regularity however spase is difcult to train because word embeddingsare xed and the number of words is much larger than the number of sememes in thecase of modeling complex semantic compositions of sememes into words the rep resentation capability of spase may be strongly constrained by limited parametersof sememe embeddings and excessive simplication of additive assumption6344lexical sememe prediction with internal informationin the previous section we introduce the automatic lexical sememe prediction pro posed by xie et al 66 these methods ignore the internal information within wordseg the characters in chinese words which is also signicant for word understand ing especially for words which are of low frequency or do not appear in the corpus63 applications147wors embeddingironexternal informationinternal informationhostofrelate todomaincraftsmanironsmithironsmithhumanoccupationmetalindustrialwors embeddingwordsensesememefig 68 sememes of the wordironsmith in hownet where occupation humanand industrial can be inferred by both external contexts and internal characters informationwhile metal is well captured only by the internal information within the characterironat all in this section we introduce the work of jin et al 30 which takes chineseas an example and explores methods of taking full advantage of both external andinternal information of words for sememe predictionin chinese words are composed of one or multiple characters and most charactershavecorrespondingsemanticmeanings as shownby67 morethan90of chinesecharacters in modern chinese corpora are morphemes chinese words can be dividedinto single morpheme words and compound words where compound words accountfor a dominant proportion the meanings of compound words are closely relatedto their internal characters as shown in fig 68 taking a compound wordironsmith for instance it consists of two chinese charactersironandcraftsman and the semantic meaning ofcan be inferred from thecombinationof its twocharacters iron craftsman ironsmith evenforsome single morpheme words their semantic meanings may also be deduced fromtheir characters for example both characters of the single morpheme wordhover represent the meaning of hover or linger therefore it is intuitive totake the internal character information into consideration for sememe predictionreference 30 proposes a novel framework for character enhanced sememeprediction csp which leverages both internal character information and externalcontext for sememe prediction csp predicts the sememe candidates for a target wordfrom its word embedding and the corresponding character embeddings specicallyfollowing spwe and spse as introduced by 66 to model external informationsememe prediction with word to character filtering spwcf and sememe pre diction with character and sememe embeddings spcse are proposed to modelinternal character informationsememe prediction with word to character filtering inspired by collabora tive ltering 58 jin et al 30 propose to recommend sememes for an unlabeledword according to its similar words based on internal information and words areconsidered as similar if they contain the same characters at the same positions1486sememe knowledge representationfig 69 an example of theposition of characters in awordbeginendmiddlein chinese the meaning of a character may vary according to its position withina word 14 three positions within a word are considered begin middle andend for example as shown in fig 69 the character at the begin position of thewordrailway station isfire whilevehicle andstation are at the middle and end position respectively the characterusually means station when it is at the end position while it usually meansstand at the begin position like instandstandingguard andstand upformally for a word w  c1c2cw we dene bw  c1 mw c2  cw1 ew  cw andppx jc wiwcpwi mi jwiwcpwi xwi627that represents the score of a sememe x j given a character c and a position p wherep may be b m or e m is the same matrix used in spwe finally the scorefunction px jw of sememe x j given a word w is dened aspx jw pbmecpwppx jc628spwcf is a simple and efcient method it performs well because compositionalsemantics are pervasive in chinese compound words which makes it straightforwardand effective to nd similar words according to common characterssememe prediction with character and sememe embeddings spcse themethod sememe prediction with word to character filtering spwcf can effec tively recommend the sememes that have strong correlations with characters how ever just like spwe it ignores the relations between sememes hence inspiredby spse sememe prediction with character and sememe embeddings spcse isproposed to take the relations between sememes into account in spcse the modelinstead learns the sememe embeddings based on internal character information thencomputes the semantic distance between sememes and words for predictioninspired by glove 53 and spse matrix factorization is adopted in spcse bydecomposing the word sememe matrix and the sememe sememe matrix simultane ously instead of using pretrained word embeddings in spse pretrained characterembeddings are used in spcse since the ambiguity of characters is stronger thanthat of words multiple embeddings are learned for each character 14 the most rep resentative character and its embedding are selected to represent the word meaningbecause low frequency characters are much rare than those low frequency words63 applications149fig 610 an example of adopting multiple prototype character embeddings the numbers are thecosine distances the semememetal is the closest to one embedding ofironand even low frequency words are usually composed of common characters it isfeasible to use pretrained character embeddings to represent rare words during fac torizing of the word sememe matrix the character embeddings are xedne is set as the number of embeddings for each character and each character chas ne embeddings c1  cne given a word w and a sememe x the embedding ofa character of w closest to the sememe embedding by cosine distance is selected asthe representation of the word w as shown in fig 610 specically given a wordw  c1cw and a sememe x j we denekr arg minkr1 coscrk xj  xj629where kand rindicate the indices of the character and its embedding closest tothe sememe x j in the semantic space with the same word sememe matrix m andsememe sememe correlation matrix c in spse the sememe embeddings are learnedwith the loss functionl wiwx jxcrkxj  xj bck bj mi j2   x jxqxxj  xq c jq2 630where xj and xj are the sememe embeddings for sememe x j and crkis the embeddingof the character that is the closest to sememe x j within wi note that as the charactersand the words are not embedded into the same semantic space new sememe embed dings are learned instead of using those learned in spse hence different notationsare used for the sake of distinction bck and bj denote the biases of ck and x j and is the hyperparameter adjusting the two parts finally the score function of wordw  c1cw is dened aspx jw crkxj  xj6311506sememe knowledge representationspwespsespwcfspcselegendhigh frequency wordslow frequency wordswordexternalinternalcspfig 611 an illustration of model ensembling in sememe predictionmodel ensembling spwcfspcse and spwespse take different sourcesof information as input which means that they have different characteristicsspwcfspcse only have access to internal information while spwespse canonly make use of external information on the other hand just like the differencebetween spwe and spse spwcf originates from collaborative ltering whereasspcse uses matrix factorization all of those methods have in common that they tendto recommend the sememes of similar words but they diverge in their interpretationof similarthereforetoobtainbetterpredictionperformanceitisnecessarytocombinethesemodelswedenotetheensembleofspwcfandspcseastheinternalmodelandtheensemble of spwe and spse as the external model the ensemble of the internaland the external models is the novel framework csp in practice for words withreliable word embeddings ie high frequency words we can use the integration ofthe internal and the external models for words with extremely low frequencies eghaving no reliable word embeddings we can just use the internal model and ignorethe external model because the external information is noisy in this case figure611shows model ensembling in different scenarios for the sake of comparison we usethe integration of spwcf spcse spwe and spse as csp in all experimentsand two models are integrated by simple weighted addition6345cross lingual sememe predictionmost languages do not have sememe based linguistic kbs such as hownet whichprevents us from understanding and utilizing human languages to a greater extenttherefore it is important to build sememe based linguistic kbs for various lan guagesto address the issue of the high labor cost of manual annotation qi et al 56propose a new task cross lingual lexical sememe prediction clsp which aims to63 applications151automatically predict lexical sememes for words in other languages there are twocritical challenges for clsp1 there is not a consistent one to one match between words in different lan guages for example english word beautiful can refer to chinese words of eitheror hence we cannot simply translate hownet into another languageand how to recognize the semantic meaning of a word in other languages becomesa critical problem2 since there is a gap between the semantic meanings of words and sememeswe need to build semantic representations for words and sememes to capture thesemantic relatedness between themto tackle these challenges qi et al 56 propose a novel model for clsp whichaims to transfer sememe based linguistic kbs from source language to target lan guage their model contains three modules 1 monolingual word embedding learn ing which is intended for learning semantic representations of words for source andtarget languages respectively 2 cross lingual word embedding alignment whichaims to bridge the gap between the semantic representations of words in two lan guages 3 sememe based word embedding learning whose objective is to incorpo rate sememe information into word representationsthey take chinese as source language and english as the target language toshow the effectiveness of their model experimental results show that the proposedmodel could effectively predict lexical sememes for words with different frequenciesin other languages and their model has consistent improvements on two auxiliaryexperiments including bilingual lexicon induction and monolingual word similaritycomputation by jointly learning the representations of sememes words in source andtarget languagesthe model consists of three parts monolingual word representation learningcross lingual word embedding alignment and sememe based word representationlearning hence they dene the objective function of our method corresponding tothe three partsl  lmono  lcross  lsememe632here the monolingual term lmono is designed for learning monolingual wordembeddings from nonparallel corpora for source and target languages respectivelythe cross lingual term lcross aims to align cross lingual word embeddings in aunied semantic space and lsememe can draw sememe information into word rep resentation learning and conduce to better word embeddings for sememe predictionin the following paragraphs we will introduce the three parts in detailmonolingualwordrepresentationmonolingualwordrepresentationisrespon sible for explaining regularities in monolingual corpora of source and target lan guages since the two corpora are nonparallel lmono comprises two monolingualsubmodels that are independent of each otherlmono  l smono  l tmono633where the superscripts s and t denote source and target languages respectively1526sememe knowledge representationas a common practice the well established skip gram model is chosen to obtainmonolingual word embeddings the skip gram model is aimed at maximizing thepredictive probability of context words conditioned on the centered word formallytaking the source side for example given a training word sequence ws1     wsnskip gram model intends to minimizel smono  nkck1kkkk0log pwsckwsc 634where k is the size of the sliding window pwsckwsc  stands for the predictive prob ability of one of the context words conditioned on the centered word wsc  formalizedby the following softmax functionpwsckwsc  expwsck  wsc wss v s expwss  wsc 635in which v s indicates the word vocabulary of source language l tmono can be formu lated similarlycross lingual word embedding alignment cross lingual word embeddingalignment aims to build a unied semantic space for the words in source and targetlanguages inspired by 69 the cross lingual word embeddings are aligned withsignals of a seed lexicon and self matchingformally lcross is composed of two terms including alignment by seed lexiconlseed and alignment by matching lmatchlcross  slseed  mlmatch636where s and m are hyperparameters for controlling relative weightings of the twoterms1 alignment by seed lexiconthe seed lexicon term lseed encourages word embeddings of translation pairs ina seed lexicon d to be close which can be achieved via an l2 regularizerlseed wss wtt dwss wtt 2637in which wss and wtt indicate the words in source and target languages in the seedlexicon respectively2 alignment by matching mechanismas for the matching process it is found on the assumption that each target wordshould be matched to a single source word or a special empty word and vice versathe goal of the matching process is to nd the matched source target word for each63 applications153target source word and maximize the matching probabilities for all the matchedword pairs the loss of this part can be formulated aslmatch  l t2smatch  l s2tmatch638where l t 2smatch is the term for target to source matching and l s2tmatch is the term forsource to target matchingnext a detailed explanation of target to source matching is given and the source to target matching is dened in the same way a latent variable mt 0 1     v st  1 2     v t  is rst introduced for each target word wtt  where v s and v t indicate the vocabulary size of source and target languages respectively here mtspecies the index of the source word that wtt matches with and mt  0 signies theempty word is matched then we have m  m1 m2     mv t  and can formalizethe target to source matching terml t 2smatch  log pc t c s  logmpc t  mc s639where c t and c s denote the target and source corpus respectively here they simplyassume that the matching processes of target words are independent of each othertherefore we havepc t  mc s wt c tpwt  mc s v t t1pwtt wsmtcwtt 640where wsmt is the source word that wtt matches with and cwtt  is the number oftimes wtt occurs in the target corpus635other sememe guided applications6351chinese liwc lexicon expansionlinguistic inquiry and word count liwc 52 has been widely used for comput erized text analysis in social science not only can liwc be used to analyze textfor classication and prediction but it has also been used to examine the underlyingpsychological states of a writer or speaker in the beginning liwc was developedto address content analytic issues in experimental psychology nowadays there isan increasing number of applications across elds such as computational linguistics22 demographics 48 health diagnostics 11 and social relationship 32chinese is the most spoken language in the world but we cannot use the originalliwc to analyze chinese text fortunately chinese liwc 28 has been released1546sememe knowledge representationto ll the vacancy in this part we mainly focus on chinese liwc and using liwcto stand for chinese liwc if not otherwise speciedwhile liwc has been used in a variety of elds its lexicon only contains less than7000 words this is insufcient because according to 39 there are at least 56008common words in chinese moreover liwc lexicon does not consider emergingwords and phrases on the internet therefore it is reasonable and necessary toexpand the liwc lexicon so that it is more accurate and comprehensive for sci entic research one way to expand liwc lexicon is to annotate the new wordsmanually however it is too time consuming and often requires language expertiseto add new words hence expanding liwc lexicon automatically is proposedin liwc lexicon words are labeled with different categories and categories forma certain hierarchy therefore hierarchical classication algorithms can be naturallyapplied to liwc lexicon reference 15 proposes hierarchical svm support vec tor machine which is a modied version of svm based on the hierarchical problemdecomposition approach in 6 the authors presented a novel algorithm which canbe used on both tree  and directed acyclic graph dag structured hierarchiessome recent works 12 33 attempted to use neural networks in the hierarchicalclassicationhowever these methods are often too generic without considering the specialproperties of words and liwc lexicon many words and phrases have multiplemeaningsandaretherebyclassiedintomultipleleafcategoriesthisisoftenreferredto as polysemy additionally many categories in liwc are ne grained thus makingit more difcult to distinguish them to address these issues we introduce severalmodels to incorporate sememe information when expanding the lexicon which willbe discussed after the introduction of the basic modelbasic decoder for hierarchical classication first we introduce the basicmodel for chinese liwc lexicon expansion the well known sequence to sequencedecoder 64 is exploited for hierarchical classication the original sequence to sequence decoder is often trained to predict the next word wt with consideration ofall the previously predicted words w1     wt1 this is a useful feature since animportant difference between at multilabel classication and hierarchical classi cation is that there are explicit connections among hierarchical labels this propertyis utilized by transforming hierarchical labels into a sequence let y denote the labelset and  y y denote the parent relationship where y is the parent node ofy y given a word w its labels form a tree structure hierarchy we then chooseeach path from the root node to the leaf node and transform it into a sequencey1 y2     yl where yi  yi1 i 2 l and l is the number of levels inthe hierarchy in this way when the model predicts a label yi it takes into consid eration the probability of parent label sequence y1   yi1 formally the decoderdenes a probability over the label sequencepy1 y2     yl li1pyiy1     yi1 w64163 applications155a common approach for decoder is to use lstm 27 so that each conditionalprobability is computed aspyiy1     yi1 w  gyi1 si  oi tanhhi642wherehi  fi hi1  ii hihi  tanhwhhi1 yi1  bhoi  sigmoidwohi1 yi1  bozi  sigmoidwzhi1 yi1  bzfi  sigmoidw f hi1 yi1  b f 643where is an element wise multiplication and hi is the ith hidden state of the rnnwh wo wz w f are weights and bh bo bz b f are biases oi zi and fi are knownas output gate layer input gate layer and forget gate layer respectivelyto take advantage of word embeddings the initial state h0  w is dened wherew represents the embedding of the word in other words the word embeddings areapplied as the initial state of the decoderspecically the inputs of our model are word embeddings and label embeddingsfirst raw words are transformed into word embeddings by an embedding matrix e rv dw where dw is the word embedding dimension then at each time step labelembeddings y are fed to the model which is obtained by a label embedding matrixy rydy where dy is the label embedding dimension here word embeddings arepretrained and xed during traininggenerally speaking the decoder is expected to decode word labels hierarchicallybased on word embeddings at each time step the decoder will predict the currentlabel depending on previously predicted labelshierarchical decoder with sememe attention the basic decoder uses wordembeddings as the initial state then predicts word labels hierarchically as sequenceshowever each word in the basic decoder model has only one representation thisis insufcient because many words are polysemous and many categories are ne grained in the liwc lexicon it is difcult to handle these properties using a singlereal valued vector therefore zeng et al 68 propose to incorporate sememe infor mationbecause different sememes represent different meanings of a word they shouldhave different weights when predicting word labels moreover we believe that thesame sememe should have different weights in different categories take the wordapex in fig612 for example the sememe location should have a relativelyhigher weight when the decoder chooses among the subclasses of relative whenchoosing among the subclasses of personalconcerns location should havea lower weight because it represents a relatively irrelevant sense vertex1566sememe knowledge representation21sense  acmesense  vertexapexboundarylocationentityangulardotmostgreaterthannormalhostmodifierbelongmodifierdegreefig 612 example word apex and its senses and sememes in hownet annotationto achieve these goals the utilization of attention mechanism 1 is proposedto incorporate sememe information when decoding the word label sequence thestructure of the model is illustrated in fig613similar to the basic decoder approach word embeddings are applied as the initialstate of the decoder the primary difference is that the conditional probability isdened aspyiy1     yi1 w ci  gyi1 ci hi644where ci is known as context vector the context vector ci depends on a set of sememeembeddings x1     xn acquired by a sememe embedding matrix x rsdswhere ds is the sememe embedding dimensionto be more specic the context vector ci is computed as a weighted sum of thesememe embedding x jci nj1i jx j645the weight i j of each sememe embedding x j is dened asi j expv  tanhw1yi1  w2x jnk1 expv  tanhw1yi1  w2xk64663 applications157wordembeddingsememeembeddingsememeembeddingsememeembeddingsoftmaxlabel 1goeoslabel 11label 111label 1label 11label 111fig 613 the architecture of sememe attention decoder with word embeddings as the initial statewhere v ra is a trainable parameter w1 rady and w2 rads are weightmatrices and a is the number of hidden units in attention modelintuitively at each time step the decoder chooses which sememes to pay atten tion to when predicting the current word label in this way different sememes canhave different weights and the same sememe can have different weights in differ ent categories with the support of sememe attention the decoder can differentiatemultiple meanings in a word and the ne grained categories and thus can expand amore accurate and comprehensive lexicon64summaryin this chapter we rst give an introduction to the most well known sememe knowl edge base hownet which uses about 2 000 predened sememes to annotate over100 000 chinese and english words and phrases different from other linguisticknowledge bases like wordnet hownet is based on the minimum semantics unitssememes and captures the compositional relations between sememes and wordsto learn the representations of sememe knowledge we elaborate on three modelsnamely simple sememe aggregation model ssa sememe attention over contextmodel sac and sememe attention over target model sat these models notonly learn the representations of sememes but also help improve the representationsofwordsnextwedescribesomeapplicationsofsememeknowledgeincludingword1586sememe knowledge representationrepresentation semantic composition and language modeling we also detail how toautomatically predict sememes for both monolingual and cross lingual unannotatedwordsfor further learning of sememe knowledge based nlp you can read the bookwritten by the authors of hownet 18 you can also nd more related papers inthis paper list httpsgithubcomthunlpscpapers you can use the open source apiopenhownet httpsgithubcomthunlpopenhownet to access hownet datain the future there are some research directions worth exploring1 utilizing structures of sememe annotations the sememe annotations inhownet are hierarchical and sememes annotated to a word are actually organizedas a tree however existing studies still do not utilize the structural information ofsememes instead in current methods sememes are simply regarded as semanticlabels in fact the structures of sememes also incorporate abundant semantic infor mation and will be helpful to the deep understanding of lexical semantics besidesexisting sememe prediction studies also predict unstructured sememes only and itis an interesting task to conduct structured sememe predictions2 leveraging sememes in low data regimes one of the most important andtypical characteristics of sememes is that limited sememes can represent unlimitedsemantics which can play an important and positive role in tackling the low dataregimes in word representation learning the representations of low frequency wordscan be improved by their sememes which have been well learned with the high frequency words they annotate we believe sememes will be benecial to otherlow data regimes eg low resource language nlp tasks3buildingsememeknowledgebasesforotherlanguagesoriginalhownetannotates sememes for only two languages chinese and english as far as weknow there are not sememe knowledge bases like hownet in other languages sincehownet and its sememe knowledge have been veried helpful for better understand ing human languages it will be of great signicance to annotate sememes for wordsand phrases in other languages in the section we have described a study on cross lingual sememe prediction and we think it is promising to make efforts toward thisdirectionreferences1 dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation by jointlylearning to align and translate in proceedings of iclr 20152 michele banko vibhu o mittal and michael j witbrock headline generation based on sta tistical translation in proceedings of acl 20003 marcobaroniandrobertozamparellinounsarevectorsadjectivesarematricesrepresentingadjective noun constructions in semantic space in proceedings of emnlp 20104 yoshua bengio rjean ducharme pascal vincent and christian jauvin a neural probabilisticlanguage model journal of machine learning research 3feb11371155 20035 adam berger and john lafferty information retrieval as statistical translation in proceedingsof sigir 1999references1596 wei bi and james t kwok multi label classication on tree and dag structured hierarchiesin proceedings of icml 20117 william blacoe and mirella lapata a comparison of vector based representations for semanticcomposition in proceedings of emnlp conll 20128 leonard bloomeld a set of postulates for the science of language language 2315316419269 thorsten brants ashok c popat peng xu franz j och and jeffrey dean large languagemodels in machine translation in proceedings of emnlp 200710 peter f brown john cocke stephen a della pietra vincent j della pietra fredrick jelinekjohn d lafferty robert l mercer and paul s roossin a statistical approach to machinetranslation computational linguistics 1627985 199011 wilma bucci and bernard maskit building a weighted dictionary for referential activity com puting attitude and affect in text pages 4960 200512 ricardo cerri rodrigo c barros and andr cplf de carvalho hierarchical multi labelclassication using local neural networks journal of computer and system sciences 8013956 201413 xinxiong chen zhiyuan liu and maosong sun a unied model for word sense representationand disambiguation in proceedings of emnlp 201414 xinxiong chen lei xu zhiyuan liu maosong sun and huanbo luan joint learning ofcharacter and word embeddings in proceedings of ijcai 201515 yangchi chen melba m crawford and joydeep ghosh integrating support vector machinesin a hierarchical output space decomposition framework in proceedings of igarss 200416 lei dang and lei zhang method of discriminant for chinese sentence sentiment orientationbased on hownet application research of computers 443 201017 zhendong dong and qiang dong hownet a hybrid language and knowledge resource inproceedings of nlp ke 200318 zhendong dong and qiang dong hownet and the computation of meaning with cd romworld scientic 200619 manaal faruqui jesse dodge sujay kumar jauhar chris dyer eduard hovy and noah asmith retrotting word vectors to semantic lexicons in proceedings of naacl hlt 201520 xianghua fu guo liu yanyan guo and zhiqiang wang multi aspect sentiment analysis forchinese online social reviews based on topic modeling and hownet lexicon knowledge basedsystems 37186195 201321 edward grefenstette and mehrnoosh sadrzadeh experimental support for a categorical com positional distributional model of meaning in proceedings of emnlp 201122 justin grimmer and brandon m stewart text as data the promise and pitfalls of automaticcontent analysis methods for political texts political analysis 213267297 201323 yihong gu jun yan hao zhu zhiyuan liu ruobing xie maosong sun fen lin and leyulin language modeling with sparse product of sememe experts in proceedings of emnlppages 46424651 201824 djoerd hiemstra a linguistically motivated probabilistic model of information retrieval inproceedings of tpdl 199825 g e hinton products of experts in proceedings of icann 199926 geoffrey e hinton learning distributed representations of concepts in proceedings of cogsci198627 sepp hochreiter and jrgen schmidhuber long short term memory neural computation9817351780 199728 chin lan huang ck chung natalie hui yi cheng lin yi tai seih wc chen and jw pen nebaker the development of the chinese linguistic inquiry and word count dictionary chinesejournal of psychology 542185201 201229 eric h huang richard socher christopher d manning and andrew y ng improving wordrepresentations via global context and multiple word prototypes in proceedings of acl 201230 huiming jin hao zhu zhiyuan liu ruobing xie maosong sun fen lin and leyu linincorporating chinese characters of words for lexical sememe prediction in proceedings ofacl 20181606sememe knowledge representation31 dan jurafsky speech  language processing 200032 ewa kacewicz james w pennebaker matthew davis moongee jeon and arthur c graesserpronoun use reects standings in social hierarchies journal of language and social psychol ogy 332125143 201433 sanjeev kumar karn ulli waltinger and hinrich schtze end to end trainable attentivedecoder for hierarchical entity classication proceedings of eacl 201734 slava katz estimation of probabilities from sparse data for the language model component of aspeech recognizer ieee transactions on acoustics speech and signal processing 353400401 198735 thomas kober julie weeds jeremy refn and david weir improving sparse word repre sentations with distributional inference for semantic composition in proceedings of emnlp201636 yehuda koren robert bell and chris volinsky matrix factorization techniques for recom mender systems computer 428 200937 omer levy and yoav goldberg neural word embedding as implicit matrix factorization inproceedings of neurips 201438 wei li xuancheng ren damai dai yunfang wu houfeng wang and xu sun sememeprediction learning semantic knowledge from unstructured textual wiki descriptions arxivpreprint arxiv180805437 201839 xingjian li et al lexicon of common words in contemporary chinese 200840 qun liu word similarity computing based on hownet computational linguistics and chineselanguage processing 725976 200241 shu liu jingjing xu xuancheng ren and xu sun evaluating semantic rationality of asentence a sememe word matching neural network based on hownet 201942 andrew l maas raymond e daly peter t pham dan huang andrew y ng and christopherpotts learning word vectors for sentiment analysis in proceedings of acl 201143 tomas mikolov kai chen greg corrado and jeffrey dean efcient estimation of wordrepresentations in vector space in proceedings of iclr 201344 tomas mikolov martin karat lukas burget jan cernocky and sanjeev khudanpur recur rent neural network based language model in proceedings of interspeech 201045 david rh miller tim leek and richard m schwartz a hidden markov model informationretrieval system in proceedings of sigir 199946 jeffmitchell andmirella lapatavector basedmodelsofsemantic compositioninproceedingsof acl 200847 jeff mitchell and mirella lapata language models based on semantic composition in pro ceedings of emnlp 200948 matthew l newman carla j groom lori d handelman and james w pennebaker gen der differences in language use an analysis of 14000 text samples discourse processes453211236 200849 yilin niu ruobing xie zhiyuan liu and maosong sun improved word representation learn ing with sememes in proceedings of acl 201750 francis jeffry pelletier the principle of semantic compositionality topoi 1311124 199451 francis jeffry pelletier semantic compositionality volume 1 201652 james w pennebaker roger j booth and martha e francis linguistic inquiry and word countliwc computer software austin tx liwc net 200753 jeffrey pennington richard socher and christopher manning glove global vectors for wordrepresentation in proceedings of emnlp 201454 jay m ponte and w bruce croft a language modeling approach to information retrieval inproceedings of sigir 199855 fanchao qi junjie huang chenghao yang zhiyuan liu xiao chen qun liu and maosongsun modeling semantic compositionality with sememe knowledge in proceedings of acl201956 fanchao qi yankai lin maosong sun hao zhu ruobing xie and zhiyuan liu cross linguallexical sememe prediction in proceedings of emnlp 2018references16157 alexander m rush sumit chopra and jason weston a neural attention model for abstractivesentence summarization in proceedings of emnlp 201558 badrul sarwar george karypis joseph konstan and john riedl item based collaborativeltering recommendation algorithms in proceedings of www 200159 richard socher john bauer christopher d manning and andrew y ng parsing with com positional vector grammars in proceedings of acl 201360 richard socher brody huval christopher d manning and andrew y ng semantic compo sitionality through recursive matrix vector spaces in proceedings of emnlp conll 201261 richard socher alex perelygin jean y wu jason chuang christopher d manning andrew yng and christopher potts recursive deep models for semantic compositionality over a senti ment treebank in proceedings of emnlp 201362 jingguang sun dongfeng cai dexin lv and yanju dong hownet based chinese questionautomatic classication journal of chinese information processing 2119095 200763 maosong sun and xinxiong chen embedding for words and word senses based on humanannotatedknowledgebaseacasestudyonhownetjournalofchineseinformationprocessing3016 201664 ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neuralnetworks in proceedings of neurips 201465 davidweirjulieweedsjeremyrefnandthomaskoberaligningpackeddependencytreesa theory of composition for distributional semantics computational linguistics 424727761 december 201666 ruobing xie xingchi yuan zhiyuan liu and maosong sun lexical sememe prediction viaword embeddings and matrix factorization in proceedings of ijcai 201767 binyong yin quantitative research on chinese morphemes studies of the chinese language5338347 198468 xiangkai zeng cheng yang cunchao tu zhiyuan liu and maosong sun chinese liwclexicon expansion via hierarchical classication of word embeddings with sememe attentionin proceedings of aaai 201869 meng zhang haoruo peng yang liu huan bo luan and maosong sun bilingual lexiconinduction from non parallel data with minimal supervision in proceedings of aaai 201770 yuntao zhang ling gong and yongcheng wang chinese word sense disambiguation usinghownet in proceedings of icnc71 yu zhao zhiyuan liu and maosong sun phrase type sensitive tensor indexing model forsemantic composition in proceedings of aaai 201572 xiaodan zhu parinaz sobhani and hongyu guo dag structured long short term memoryfor semantic compositionality in proceedings of naacl hlt 2016open access this chapter is licensed under the terms of the creative commons attribution 40international license httpcreativecommonsorglicensesby40 which permits use sharingadaptation distribution and reproduction in any medium or format as long as you give appropriatecredit to the original authors and the source provide a link to the creative commons license andindicate if changes were madethe images or other third party material in this chapter are included in the chapters creativecommons license unless indicated otherwise in a credit line to the material if material is notincluded in the chapters creative commons license and your intended use is not permitted bystatutory regulation or exceeds the permitted use you will need to obtain permission directly fromthe copyright holderchapter 7world knowledge representationabstract world knowledge representation aims to represent entities and relationsin the knowledge graph in low dimensional semantic space which have been widelyused in large knowledge driven tasks in this chapter we rst introduce the conceptof the knowledge graph next we introduce the motivations and give an overviewof the existing approaches for knowledge graph representation further we discussseveral advanced approaches that aim to deal with the current challenges of knowl edge graph representation we also review the real world applications of knowledgegraph representation such as language modeling question answering informationretrieval and recommender systems71introductionknowledge graph kg which is also named as knowledge base kb is a signif icant multi relational dataset for modeling concrete entities and abstract concepts inthe real world it provides useful structured information and plays a crucial role inlots of real world applications such as web search and question answering it is notexaggerated to say that knowledge graphs teach us how to model the entities as wellas the relationships among them in this complicated real worldto encode knowledge into a real world application knowledge graph represen tation which represents entities and relations in knowledge graphs with distributedrepresentations has been proposed and applied to various real world articial intel ligence elds including question answering information retrieval and dialogue sys tem that is knowledge graph representation learning plays a vital role as a bridgebetween knowledge graphs and knowledge driven tasksin this section we will introduce the concept of knowledge graph several typicalknowledge graphs knowledge graph representation learning and several typicalknowledge driven tasks the authors 2020z liu et al representation learning for natural language processinghttpsdoiorg101007978 981 15 5573 2 71631647world knowledge representation711world knowledge graphsin ancient times knowledge was stored and inherited through books and letterswritten on parchment or bamboo slip with the internet thriving in the twenty rstcentury millions of thousands of messages have ooded into the world wide weband knowledge was transferred to the semi structured textual information on theweb however due to the information explosion it is not easy to extract knowledgewe want from the signicant noisy plain text on the internet to obtain knowledgeeffectively people notice that the world is not only made of strings but also made ofentities and relations knowledge graph which arranges structured multi relationaldata of concrete entities and abstract concepts in the real world is blooming in recentyears and attracts wide attention in both academia and industrykgs are usually constructed from existing semantic web datasets in resourcedescription framework rdf with the help of manual annotation while it canalso be automatically enriched by extracting knowledge from large plain texts onthe internet a typical kg usually contains two elements including entities ieconcrete entities and abstract concepts in the real world and relations betweenentities it usually represents knowledge with large quantities of triple facts in thetriple form of head entity relation tail entityabridged as hr t for examplewilliam shakespeare is a famous english poet and playwright who is widelyregarded as the greatest writer in the english language and romeo and julietis one of his masterpieces in knowledge graph we will represent this knowledge aswilliam shakespeare works written romeo and juliet notethat in the real world the same head entity and relation may have multiple tailentities eg william shakespeare also wrote hamlet and a midsummernights dream and reversely the same situation will happen when tail entityand relation are xed even it is possible when both the head entity and tail entity aremultiple eg in relations like actor in movie however in kg all knowl edge can be represented in triple facts regardless of the types of entities and relationsthrough these triples we can generate a huge directed graph whose nodes corre spond to entities and edges correspond to relations to model the real world with thewell structured united knowledge representation kgs are widely used in a varietyof applications to enhance their system performancethere are several kgs widely utilized nowadays in applications of informationretrieval and question answering in this subsection we will introduce some famouskgs such as freebase dbpedia yago and wordnet in fact there are also lotsof comparatively smaller kgs in specic elds of knowledge functioned in verticalsearch7111freebasefreebase is one of the most popular knowledge graphs in the world it is a largecommunity curated database consisting of well known people places and things71 introduction165fig 71 an example of search results in freebasewhich is composed of existing databases and its community members freebasewas rst developed by metaweb an american software company and ran sincemarch 2007 in july 2010 metaweb was acquired by google and freebase wascombined to power up googles knowledge graph in december 2014 the freebaseteam ofcially announced that the website as well as the api of freebase wouldbe shut down by june 30 2015 while the data in freebase would be transferredto wikidata which is another collaboratively edited knowledge base operated bywikimedia foundation up to march 24 2016 freebase arranged 58726427 topicsand 3197653841 factsfreebase contains well structured data representing relationships between entitiesaswellastheattributesofentitiesintheformoftriplefactsfig71datainfreebasewas mainly harvested from various sources including wikipedia fashion modeldirectory nndb musicbrainz and so on moreover the community members alsocontributed a lot to freebase freebase is an open and shared database that aims toconstruct a global database which encodes the worlds knowledge it announced anopen api rdf endpoint and a database dump for its users for both commercial andnoncommercial use as described by tim oreilly freebase is the bridge betweenthe bottom up vision of web 20 collective intelligence and the more structured worldof the semantic web7112dbpediadbpedia is a crowd sourced community effort aiming to extract structured contentfrom wikipedia and make this information accessible on the web it was started byresearchers at free university of berlin leipzig university and openlink software1667world knowledge representationinitially released to the public in january 2007 dbpedia allows users to ask semanticqueries associated with wikipedia resources even including links to other relateddatasets which makes it easier for us to fully utilize the massive amount of informa tion in wikipedia in a novel and effective way dbpedia is also an essential part ofthe linked data effort described by tim berners leethe english version of dbpedia describes 458 million entities out of which422 million are classied in a consistent ontology including 1445000 persons735000 places 411000 creative works 251000 species 241000 organizations and6000 diseases there are also localized versions of dbpedia in 125 languages all ofwhich contain 383 million entities besides dbpedia also contains a great number ofinternal and external links including 809 million links to wikipedia categories 412million links to yago categories 252 million links to images and 298 million linksto external web pages moreover dbpedia maintains a hierarchical cross domainontology covering overall 685 classes which has been manually created based onthe commonly used infoboxes in wikipediadbpedia has several advantages over other kgs first dbpedia has a close con nection to wikipedia and can automatically evolve as wikipedia changes it makesthe update process of dbpedia more efcient second dbpedia is multilingual thatis convenient for users over the world with their native languages7113yagoyago which is short for yet another great ontology is a high quality kg devel opedbymaxplanckinstituteforcomputerscienceinsaarbruckeninitiallyreleasedin 2008 knowledge in yago is automatically extracted from wikipedia wordnetand geonames whose accuracy has been manually evaluated and proves a con rmed accuracy of 95 yago is special not only because of the condence valueevery fact possesses depending on the manual evaluation but also because that yagois anchored in space and time which can provide a spatial dimension or temporaldimension to part of its entitiescurrently yago has more than 10 million entities including persons organi zations and locations with over 120 million facts about these entities yago alsocombines knowledge extracted from wikipedias of 10 different languages and clas sies them into approximately 350000 classes according to the wikipedia categorysystem and the taxonomy of wordnet yago has also joined the linked data projectand been linked to the dbpedia ontology and the sumo ontology fig7272knowledge graph representationknowledge graphs provide us with a novel aspect to describe the world with entitiesand triple facts which attract growing attention from researchers large kgs suchas freebase dbpedia and yago have been constructed and widely used in anenormous amount of applications such as question answering and web search72 knowledge graph representation167fig 72 an example of search results in yagohowever with kg size increasing we are facing two main challenges data spar sity and computational inefciency data sparsity is a general problem in lots ofelds like social network analysis or interest mining it is because that there are toomany nodes eg users products or entities in a large graph while too few edgeseg relationships between these nodes since the number of relations of a node islimited in the real world computational efciency is another challenge we need toovercome with the increasing size of knowledge graphsto tackle these problems representation learning is introduced to knowledge rep resentation representation learning in kgs aims to project both entities and relationsinto a low dimensional continuous vector space to get their distributed representa tions whose performance has been conrmed in word representation and social rep resentation compared with the traditional one hot representation distributed repre sentation has much fewer dimensions and thus lowers the computational complexitywhat is more distributed representation can explicitly show the similarity betweenentities through some distance calculated by the low dimensional embeddings whileall embeddings in one hot representation are orthogonal making it difcult to tellthe potential relations between entitieswith the advantages above knowledge graph representation learning is bloomingin knowledge applications signicantly improving the ability of kgs on the taskof knowledge completion knowledge fusion and reasoning it is considered as thebridge between knowledge construction knowledge graphs and knowledge drivenapplications up till now a high number of methods have been proposed using adistributed representation for modeling knowledge graphs with the learned knowl edge representations widely utilized in various knowledge driven tasks like questionanswering information retrieval and dialogue system1687world knowledge representationin summary knowledge graph representation learning krl aims to constructdistributed knowledge representations for entities and relations projecting knowl edge into low dimensional semantic vector spaces recent years have witnessed sig nicant advances in knowledge graph representation learning with a large amount ofkrl methods proposed to construct knowledge representations among which thetranslation based methods achieve state of the art performance in many kg taskswith a right balance in both effectiveness and efciencyin this section we will rst describe the notations that we will use in krl thenwe will introduce transe which is the fundamental version of translation basedmethods next we will explore the various extension methods of transe in detailat last we will take a brief look over other representation learning methods utilizedin modeling knowledge graphs721notationsfirst we introduce the general notations used in the rest of this section we useg  e r t  to denote the whole kg in which e  e1 e2     ee stands forthe entity set r  r1r2    rr stands for the relation set and t stands for thetriple set e and r are the corresponding entity and relation numbers in theiroverall sets as stated above we represent knowledge in the form of triple facthr t where h e means the head entity t e means the tail entity and r rmeans the relation between h and t722transetranse 7 is a translation based model for learning low dimensional embeddings ofentities and relations it projects entities as well as relations into the same semanticembeddingspaceandthenconsidersrelationsastranslationsintheembeddingspacefirst we will start with the motivations of this method and then discuss the detailsin how knowledge representations are trained under transe finally we will explorethe advantages and disadvantages of transe for a deeper understanding7221motivationthere are three main motivations behind the translation based knowledge graphrepresentation learning method the primary motivation is that it is natural to con sider relationships between entities as translating operations through distributedrepresentations entities are projected to a low dimensional vector space intuitivelywe agree that a reasonable projection should map entities with similar semanticmeanings to the same eld while entities with different meanings should belong to72 knowledge graph representation169distinct clusters in the vector space for example william shakespeare andjane austen may be in the same cluster of writers romeo and juliet andpride and prejudice may be in another cluster of books in this case theyshare the same relation works written and the translations between writers andbooks in the vector space are similarthe secondary motivation of transe derives from the breakthrough in word repre sentation by word2vec 49 word2vec proposes two simple models skip gram andcbow to learn word embeddings from large scale corpora signicantly improv ing the performance in word similarity and analogy the word embeddings learnedby word2vec have some interesting phenomena if two word pairs share the samesemantic or syntactic relationships their subtraction embeddings in each word pairwill be similar for instance we havewking wman wqueen wwoman71which indicates that the latent semantic relation between king and man which issimilar to the relation between queen and woman is successfully embedded inthe word representation this approximate relation could be found not only with thesemantic relations but also with the syntactic relations we havewbigger wbig wsmaller wsmall72the phenomenon found in word representation strongly implies that there mayexist an explicit method to represent relationships between entities as translatingoperations in vector spacethe last motivation comes from the consideration of computational complexityon the one hand a substantial increase in model complexity will result in highcomputational costs and obscure model interpretability moreover a complex modelmay lead to overtting on the other hand experimental results on model complexitydemonstrate that the simpler models perform almost as good as more expressive mod els in most kg applications in the condition that there are sizeable multi relationaldataset and a relatively large amount of relations as kg size increases computa tional complexity becomes the primary challenge in the knowledge graph represen tation the intuitive assumption of translation leads to a better trade off betweenaccuracy and efciency7222methodologyas illustrated in fig73 transe projects entities and relations into the same low dimensional space all embeddings take values in rd where d is a hyperparameterindicating the dimension of embeddings with the translation assumption for eachtriple hr tin t  we want the summation embedding h  r to be the nearestneighbor of tail embedding t the score function of transe is then dened as follows1707world knowledge representationfig 73 the architecture oftranse model 47htre hr t  h  r t73more specically to learn such embeddings of entities and relations transeformalizes a margin based loss function with negative sampling as objective fortraining the pair wise function is dened as followsl hrtthrtt max  e hr t e hr t 074in which e hr t is the score of energy function for a positive triple ie triple int  and e hr t is that of a negative triple the energy function e can be eithermeasured by l1 or l2 distance   0 is a hyperparameter of margin and a bigger means a wider gap between positive and the corresponding negative scores t isthe negative triple set with respect to t since there are no explicit negative triples in knowledge graphs we dene t asfollowst  hr th e hr tr r hr tt ehr tt75which means the negative triple set t is composed of the positive triple hr twith head entity relation or tail entity randomly replaced by any other entities orrelations in kg note that the new triple generated after replacement will not beconsidered as a negative sample if it has already been in t transe is optimized using mini batch stochastic gradient descent sgd withentities and relations randomly initialized knowledge completion which is a linkprediction task aiming to predict the third element in a triple could be either entityor relation with the given rest two elements is designed to evaluate the learnedknowledge representations72 knowledge graph representation1717223disadvantages and challengestranse is effective and efcient and has shown its power on link prediction howeverit still has several disadvantages and challenges to be further exploredfirst in knowledge completion we may have multiple correct answers with thegiven two elements in a triple for instance with the given head entity williamshakespeare and the relation works written we will get a list of master piecesincludingromeo and juliethamletand a midsummer nightsdream these books share the same information in the writer while differing in manyother elds such as theme background and famous roles in the book howeverwith the translation assumption in transe every entity has only one embeddingin all triples which signicantly limits the ability of transe in knowledge graphrepresentations in 7 the authors categorize all relations into four classes 1 to 11 to many many to 1 many to many according to the cardinalities of their headand tail arguments a relation is considered as 1 to 1 if most heads appear withone tail 1 to many if a head can appear with many tails many to 1 if a tail canappear with many heads and many to many if multiple heads appear with multipletails statistics demonstrate that the 1 to many many to 1 many to many relationsoccupy a large proportion transe does well in 1 to 1 but it has issues when dealingwith 1 to many many to 1 many to many relations similarly transe may alsostruggle with reexive relationssecond the translating operation is intuitive and effective only considering thesimple one step translation which may limit the ability to model kgs taking enti ties as nodes and relations as edges we can construct a huge knowledge graphwith the triple facts however transe focuses on minimizing the energy functione hr t  h  r t which only utilize the one step relation information inknowledge graphs regardless of the latent relationships located in long distancepaths for example if we know the triple fact that the forbidden citylocate in beijingand beijing capital of china we can inferthat the forbidden city locates in china transe can be further enhancedwith the favor of multistep informationthird the representation and the dissimilarity function in transe are oversimpli ed for the consideration of efciency therefore transe may not be capable enoughof modeling those complicated entities and relations in knowledge graphs there stillexist challenges on how to balance the effectiveness and efciency avoiding bothovertting and underttingbesides the disadvantages and challenges stated above multisource informationsuch as textual information and hierarchical typelabel information is of great sig nicance which will be further discussed in the following1727world knowledge representation723extensions of transethere are lots of extension methods following transe to address the challengesabove specically transh transr transd and transparse are proposed to solvethe challenges in modeling 1 to many many to 1 and many to many relationsptranse is proposed to encode long distance information located in multistep pathsand ctransr transa transg and kg2e further extend the oversimplied modelof transe we will discuss these extension methods in detail7231transhwith distributed representation entities are projected to the semantic vector spaceand similar entities tend to be in the same cluster however it seems that williamshakespeare should be in the neighborhood of isaac newton when talkingabout nationality while it should be next to mark twain when talking about occu pation to accomplish this we want entities to show different preferences in differentsituations that is to have multiple representations in different triplesto address the issue when modeling 1 to many many to 1 many to many andreexive relations transh 77 enables an entity to have multiple representationswhen involved in different relations as illustrated in fig74 transh proposes arelation specic hyperplane wr for each relation and judge dissimilarities on thehyperplane instead of the original vector space of entities given a triple hr ttransh rst projects h and t to the corresponding hyperplane wr to get the projectionhand t and the translation vector r is used to connect hand ton the hyperplanethe score function is dened as followse hr t  h r t76in which we haveh h wr hwrt t wr twr77where wr is a vector and wr2 is restricted to 1 as for training transh alsominimizes the margin based loss function with negative sampling which is similarto transe and use mini batch sgd to learn representations7232transrctransrtransh enables entities to have multiple representations in different relations withthe favor of hyperplanes while entities and relations are still restricted in the samesemantic vector space which may limit the ability for modeling entities and relationstransr 39 assumes that entities and relations should be arranged in distinct spacesthat is entity space for all entities and relation space for each relation72 knowledge graph representation173hrtrrhtfig 74 the architecture of transh model 47as illustrated in fig75 for a triple hr t h t rk and r rd transr rstprojects h and t from entity space to the corresponding relation space of r that isto say every entity has a relation specic representation for each relation and thetranslating operation is processed in the specic relation space the energy functionof transr is dened as followse hr t  hr  r tr78where hr and tr stand for the relation specic representation for h and tr in thecorresponding relation space of r the projection from entity space to relation spaceishr  hmrtr  tmr79where mr rkd is a projection matrix mapping entities from the entity space tothe relation space of r transr also constrains the norms of the embeddings and hash2 1 t2 1 r2 1 hr2 1 tr2 1 as for training transr sharesthe same margin based score function as transefurthermore the author found that some relations in knowledge graphs couldbe divided into a few sub relations that give more precise information the dif ferences between those sub relations can be learned from corresponding entitypairs for instance the relation location contains has head tail patterns likecity street country city and even country university showingdifferent attributes in cognition with the sub relations being considered entitiesmay be projected to more precise positions in the semantic vector spacecluster based transr ctransr which is an enhanced version of transr with thesub relations into consideration is then proposed more specically for each relationr all entity pairs h t are rst clustered into several groups the clustering of entitypairs depends on the subtraction result of t h in which h and t are pretrained bytranse next we learn a distinct sub relation vector rc for each cluster according tothe corresponding entity pairs and the original energy function is modied as1747world knowledge representationthtrhrrmrmrentity spacerelation space of rfig 75 the architecture of transr model 47e hr t  hr  rc tr rc r710where rc rwants the sub relation vector rc not to be too distinct from the uniedrelation vector r7233transdtransh and transr focus on the multiple representations of entities in differentrelations improving the performance on knowledge completion and triple classi cation however both models only project entities according to the relations intriples ignoring the diversity of entities moreover the projection operation withmatrix vector multiplication leads to a higher computational complexity comparedto transe which is time consuming when applied on large scale graphs to addressthis problem transd 32 proposes a novel projection method with a dynamic map ping matrix depending on both entity and relation which takes the diversity of entitiesas well as relations into considerationtransd denes two vectors for each entity and relation ie the original vector thatis also used in transe transh and transr for distributed representation of entitiesandrelationsandtheprojectionvectorthatisusedinconstructingprojectionmatricesfor mapping entities from entity space to relation space as illustrated in fig76transd uses h t r to represent the original vectors while hp tp and rp are used torepresent the projection vectors there are two projection matrices mrh mrt rmnused to project from entity space to relation space and the projection matrices aredynamically constructed as followsmrh  rphp  imnmrt  rptp  imn71172 knowledge graph representation175t2h1h1rmrhimrtientity spacerelation space of rt3t1h2h3t1rh2rt2rt3rh3rfig 76 the architecture of transd model 47which means the projection vectors of entity and relation are combined to determinethe dynamic projection matrix the score function is then dened ase hr t  mrhh  r mrtt712the projection matrices are initialized with identity matrices and there are alsosome normalization constraints as in transrtransd proposes a dynamic method to construct projection matrices with theconsideration of diversity in both entities and relations achieving better performancecompared to existing methods in link prediction and triple classication moreoverit lowers both computational and spatial complexity compared to transr7234transparsethe extension methods of transe stated above focus on the multiple representa tions for entities in different relations and entity pairs however there are still twochallenges ignored 1 the heterogeneity relations in knowledge graphs differ ingranularity some complex relations may link to many entity pairs while some rela tively simple relations not 2 the unbalance some relations may have more linksto head entities and fewer links to tail entities and vice versa the performance willbe further improved if we consider these rather than merely treat all relations equallyexisting methods like transr build projection matrices for each relation whilethese projection matrices have the same number of parameters regardless of the vari ety in the complexity of relations transparse 33 is then proposed to address theissues the underlying assumption of transparse is that complex relations shouldhave more parameters to learn while simple relations have fewer where the complex ity of a relation is judged from the number of triples or entities linked by the relation1767world knowledge representationto accomplish this two models ie transparseshare and transparseseparateare proposed for avoiding overtting and underttinginspired by transr transparseshare builds a projection matrix mrr for eachrelationrthisprojectionmatrixissparseandthesparsedegreer mainlydependsonthe number of entity pairs linked tor suppose nr is the number of linked entity pairsn r represents the maximum number of nr and min denotes the minimum sparsedegree of projection matrix mr that 0 min 1 the sparse degree of relation r isdened as followsr  1 1 minnrn r 713both head and tail entities share the same sparse projection matrix mrr intranslation the score function ise hr t  mrrh  r mrrt714differing from transparseshare transparseseparate builds two differentsparse matrices mrhrh and mrtrt for head and tail entities the sparse degreerh or rt then depends on the number of head or tail entities linked by relationr we have nrh or nrt to represent the number of head or tail entities as well asn rh or n rt to represent the maximum number of nrh or nrt and min will alsobe set as the minimum sparse degree of projection matrices that 0 min 1 wehaverh  1 1 minnrhn rhrt  1 1 minnrtn rt715the score function of transparseseparate ise hr t  mrhrhh  r mrtrtt716through the sparse projection matrix transparse solves the heterogeneity andthe unbalance simultaneously7235ptransethe extension models of transe stated above are mainly focused on the challenge ofmultiple representations of entities in different scenarios however those extensionmodelsonlyconsiderthesimpleone steppathsierelationintranslatingoperationignoring the rich global information located in the whole knowledge graphs consid ering the multistep relational path is a potential method to utilize the global informa tion for instance if we notice the multistep relational path that the forbiddencity locate in beijingbeijing capital of china we caninference with condence that the triple the forbidden city locate inchinamay exist the relational path provides us with a powerful way to con 72 knowledge graph representation177struct better knowledge graph representations and even get a better understanding ofknowledge reasoningthere are two main challenges when encoding the information in multistep rela tional paths first how to select reliable and meaningful relational paths amongenormous path candidates in kgs since there are lots of relation sequence patternswhich do not indicate reasonable relationships let us just consider the relationalpath the forbidden city locate in beijingbeijing held2008 summer olympics it is hard to describe the relationship between theforbidden city and 2008 summer olympics second how to modelthose meaningful relational paths once we get them since it is difcult to solvethis composition semantic problem in relational pathsptranse 38 is then proposed to model the multistep relational paths to selectmeaningful relational paths the authors propose a path constraint resource alloca tion pcra algorithm to judge the relation path reliability suppose there is infor mation or resource in head entity h which will ow to tail entity t through somecertain relational paths the basic assumption of pcra is that the reliability of pathdepends on the resource amount that nally ows from head to tail formally weset  r1    rl for a certain path between h and t the resource travels from hto t and the path could be represented as s0hr1s1r2  rlslt for an entitym si the resource amount of m is dened as followsrm nsi1m1sin  rn717where si1 m indicates all direct predecessors of entity m along with relation ri insi1 and sin  indicates all direct successors of n si1 with relation r finallythe resource amount of tail rt is used to measure the reliability of in the giventriple h  tonce we have learned the reliability and select those meaningful relational pathcandidates the next challenge is how to model the meaning of those multistep pathsptranse proposes three types of composition operation namely addition multipli cation and recurrent neural networks to get the representation l of  r1    rlthrough those relations the score function of the path triple h  tis dened asfollowse h  t  l t hl r e r718where r indicates the golden relation between h and t since ptranse wants to meetthe assumption in transe that r t h simultaneously ptranse directly utilizes rin training the optimization objective of ptranse isl hrtsl hr t  1zphtrh tl r7191787world knowledge representationt2h1at1h2t3r1r2t2h1bt1h2t3r1r2fig 77 the architecture of transa model 47where l hr t is the margin based score function with e hr t and l r isthe margin based score function with e r the reliability rh t of in h  tis well considered in the overall loss functionbesides ptranse similar ideas such as 21 22 also consider the multistep rela tional paths on different tasks such as knowledge completion and question answer ing successfully these works demonstrate that there is plentiful information locatedin multi step relational paths which could signicantly improve the performanceof knowledge graph representation and further explorations on more sophisticatedmodels for relational paths are still promising7236transatransa 78 is proposed to solve the following problems in transe and otherextensions 1 transe and its extensions only consider the euclidean distance intheir energy functions which seems to be less exible 2 existing methods regardeach dimension in the semantic vector space identically whatever the triple is whichmay bring in errors when calculating dissimilarities to solve these problems asillustrated in fig77 transa replaces the inexible euclidean distance with adaptivemahalanobis distance which is more adaptive and exible the energy function oftransa is as followse hr t  h  r twrh  r t720where wr is a relation specic nonnegative symmetric matrix corresponding to theadaptive matric note that the h  r t stands for a nonnegative vector that eachdimension is the absolute value of the translating operation we haveh  r t h1  r1 t1 h2  r2 t2    hn  rn tn72172 knowledge graph representation1797237kg2eexisting translation based models usually consider entities and relations as vectorsembeddedinlow dimensionalsemanticspaceshoweverasexplainedaboveentitiesand relations in kgs are various with different granularities therefore the marginin the margin based score function that is used to distinguish positive triples fromnegative triples should be more exible due to the diversity and the uncertainties ofentities and relations should be taken into considerationto solve this kg2e 30 is proposed introducing the multidimensional gaus sian distributions to kg representations as illustrated in fig78 kg2e representseach entity and relation with a gaussian distribution specically the mean vectordenotes the entityrelations central position and the covariance matrix denotes itsuncertainties to learn the gaussian distributions for entities and relations kg2ealso follows the score function proposed in transe for a triple hr t the gaussiandistributions of entity and relation are dened as followsh n h ht n t tr n r r722note that the covariances are diagonal for the consideration of efciency kg2ehypothesizes that the head and tail entity are independent with specic relations thenthe translation h t could be dened ash t  e n h t h  t723to measure the dissimilarity between e and r kg2e proposes two methods con sidering both asymmetric similarity and symmetric similaritythe asymmetric similarity is based on the kl divergence between e and r whichis a straightforward method to measure the similarity between two probability dis tributions the energy function is as followse hr t  dklerxrke n x r r log n x e en x r rdx 12tr1r r  r e1r r e log detedetr ke724where tr indicates the trace of  and 1 indicates the inversethe symmetric similarity is based on the expected likelihood or probability prod uct kernel ke2g takes the inner product between pe and pr as the measurement ofsimilarity the logarithm of energy function is1807world knowledge representationfig 78 the architecture ofkg2e model 47bill clintonhillary clintonspouseusanationalityarkansasborn one hr t xrke n x e en x r rdx log n 0 e r e  r 12e re  r1e r  log dete  r  ke log2725the optimization objective of kg2e is also margin based similar to transe bothasymmetric and symmetric similarities are constrained by some regularization toavoid overttingl e rl2 1cmini l cmaxicmin  0726figure78 shows a brief example of representations in kg2e7238transgwe have discussed the problem of transe in the session of transrctransr that somerelations in knowledge graphs such as location contains or has part mayhave multiple sub meanings these relations are more likely to be some combina tions that could be divided into several more precise relations to address this issuectransr is proposed with a preprocess of clustering for each relation r dependingon the entity pairs h t transg 79 also focuses on this issue more elegantly byintroducing a generative model as illustrated in fig79 it assumes that differentsemantic component embeddings should follow a gaussian mixture model thegenerative process is as follows1 for each entity e e transg sets a standard normal distribution e n 0 i2 for a triple hr t transg uses chinese restaurant process to automaticallydetect semantic components ie sub meanings in a relation rn crp72 knowledge graph representation181t2hbt1t3rrt4t5t2hat1t3rt4t5fig 79 the architecture of transg model 473 drawtheheadembeddingtoformastandardnormaldistribution h n h  2h i4 draw the tail embedding to form a standard normal distribution t n t  2t i5 draw the relation embedding for this semantic component rn  t h n t h  2h   2t i is the mean embedding and  is the variance finally the score function ise hr t nrn1rnn t h  2h   2t i727in which nr is the number of semantic components of r and rn is the weight of ithcomponent generated by the chinese restaurant processfigure79 shows the advantages of the generative gaussian mixture model7239manifoldekg2e and transg introduce gaussian distributions to knowledge graph represen tation learning improving the exibility and diversity with the various forms ofentity and relation representation however transe and its most extensions view thegolden triples as almost points in the low dimensional vector space following theassumption of translation this point assumption may lead to two problems beingan ill posed algebraic system and being over strict with the geometric formmanifolde 80 is proposed to address this issue considering the possible positionof the golden candidate in vector space as a manifold instead of one point the overallscore function of manifolde is dened as followse hr t  m hr t d2r 27281827world knowledge representationin which d2r is a relation specic manifold parameter indicating the bias two kindsofmanifoldsarethenproposedinmanifoldemanifoldesphereisastraightforwardmanifold that supposes t should be located in the sphere which has h  r to be thecenter and dr to be the radius we havem hr t  h  r t22729the second manifold utilized is the hyperplane for it is much easier for twohyperplanes to intersect the function of manifoldehyperplane ism hr t  h  rht  rt730in which rh and rt represent the two relation embeddings this indicates that fora triple hr t the tail entity t should locate in the hyperplane whose directionis h  rh with the bias to be d2r  furthermore manifoldehyperplane considersabsolute values in m hr t as h  rht  rt to double the solution number ofpossible tails for both manifolds the author applies kernel forms on reproducingkernel hilbert space724other modelstranslation based methods such as transe are simple but effective whose powerhas been consistently veried on various tasks like knowledge graph completion andtriple classication achieving state of the art performance however there are alsosome other representation learning methods performing well on knowledge graphrepresentation in this part we will take a brief look at these methods as inspiration7241structured embeddingsstructured embeddings se 8 is a classical representation learning method forkgs in se each entity is projected to a d dimensional vector space se designs tworelation specic matrices mr1 mr2 rdd for each relation r projecting both headand tail entities with these relation specic matrices when calculating the similaritiesthe score function of se is dened as followse hr t  mr1h mr2t1731in which both h and t are transformed into a relation specic vector space withthose projection matrices the assumption of se is that the projected head and tailembeddings should be as similar as possible according to the loss function differentfrom the translation based methods se models entities as embeddings and relations72 knowledge graph representation183as projection matrices in training se considers all triples in the training set andminimizes the overall loss function7242semantic matching energysemantic matching energy sme 5 6 proposes a more complicated representationlearning method differing from se sme considers both entities and relations aslow dimensional vectors for a triple hr t h and r are combined using a projectionfunction g to get a new embedding lhr and the same with t and r to get ltr next apoint wise multiplication function is used on the two combined embeddings lhr andltr to get the score of this triple sme proposes two different projection functions inthe second step among which the linear form ise hr t  m1h  m2r  b1m3t  m4r  b2732and the bilinear form ise hr t  m1h m2r  b1m3t m4r  b2733where is the element wise hadamard product m1 m2 m3 m4 are weightmatrices in the projection function and b1 and b2 are the bias bordes et al 6 isbased on sme and improves the bilinear form with three way tensors instead ofmatrices7243latent factor modellatent factor model lfm is proposed for modeling large multi relational datasetslfm is based on a bilinear structure which models entities as embeddings andrelations as matrices it could share sparse latent factors among different relationssignicantly reducing the model and computational complexity the score functionof lfm is dened as followse hr t  hmrt734in which mr is the representation of the relation r moreover 92 proposes dist mult model which restricts mr to be a diagonal matrix this enhanced model notonly reduces the parameter number of lfm and thus lowers the models computa tional complexity but also achieves better performance1847world knowledge representation7244rescalrescal is a knowledge graph representation learning method based on matrixfactorization 54 55 in rescal to represent all triple facts in knowledge graphsthe authors employ a three way tensor x rddk in which d is the dimension ofentities and k is that of relations in the three way tensor x  two modes stand forthe head and tail entities while the third mode represents the relations the entries ofx are based on the existence of the corresponding triple facts that is x i jm  1 ifthe triple ith entity mth relation jth entityholds in the training set and otherwisex i jm  0 if the triple is nonexistingto capture the inherent structure of all triples a tensor factorization model namedrescal is then proposed suppose x  x1     xk for each slice xn we havethe following rank r factorizationxn arna735where a rdr stands for the r dimensional entity representations and rn rrrrepresents the interactions of the r latent components for n th relation the assump tion in this factorization is similar to lfm while rescal also optimizes the nonex isting triples where x i jm  0 instead of only considering the positive instancesfollowing this tensor factorization assumption the loss function of rescal isdened as followsl  12nxn arna2f 12a2f nrn2f736in which the second term is a regularization term and  is a hyperparameter7245holerescal works well with multi relational data but suffers from high computationalcomplexity to leverage both effectiveness and efciency holographic embeddingshole is proposed as an enhanced version of rescal 53hole employs an operation named circular correlation to generate compositionalrepresentations which is similar to those holographic models of associative memorythe circular correlation operation  rd  rd rd between two entities h and tis as followsh ttk d1i0hitkimod d737figure710a also demonstrates a simple instance of this operation the probabilityof a triple hr tis then dened as72 knowledge graph representation185headtaileh0reh1eh2et0et1et2prhtaholeheadtaileh0reh1eh2et0et1et2prhtbrescalfig 710 the architecture of rescal and hole modelsprh t  1  sigmoidrh t738considering circular correlation brings in lots of advantages 1 unlike otheroperations like multiplication or convolution circular correlation is noncommutativeie h t  t h which is capable of modeling asymmetric relations in knowledgegraphs 2 circular correlation has lower computational complexity compared totensor product in rescal whats more the circular correlation could further speedup with the help of fast fourier transform fft which is formalized as followsh t  f 1fh fb7391867world knowledge representationf and f1 represent the fft and its inverse while f denotes the complexconjugate in cd and stands for the element wise hadamard product due tofft the computational complexity of circular correlation is od log d which ismuch lower than that of tensor product7246complex embedding complexcomplex 70 employs an eigenvalue decomposition model which makes use ofcomplex valued embeddings the composition of complex embeddings can handle alarge variety of binary relations among the symmetric and antisymmetric relationsformally the log odd of the probability that the fact hr tis true isfrh t  sigmoidxhrt740where frh t is expected to be 1 when hr t holds otherwise 1 here xhrt iscalculated as followsxhrt  rer h t rer reh ret rer imh imtimr reh imtimr imh ret741where x y z i xi yizi denotes the trilinear dot product rex and imx indi cate the real part and the imaginary part of the number x respectively in fact com plex can be viewed as an extension of rescal which assigns complex embeddingof the entities and relationsbesides 29 has proved that hole is mathematically equivalent to complexrecently7247convolutional 2d embeddings conveconve 16 uses 2d convolution over embeddings and multiple layers of nonlinearfeatures to model knowledge graphs it is the rst nonlinear model that signicantlyoutperforms previous linear modelsspecically conve uses convolutional and fully connected layers to model theinteractions between input entities and relationships after that the obtained featuresare attened transformed through a fully connected layer and the inner product istaken with all object entity vectors to generate a score for each triplefor each triple hr t conve denes its score function asfrh t  f vec f h r wt74272 knowledge graph representation187where denotes the convolution operator and vec means compressing a matrixinto a vector r rk is a relation parameter depending on r h and r denote a 2dreshaping of h and r respectively if h r rk then h r rkakb where k  kakbconve can be seen as an improvement on hole compared with hole it learnsmultiple layers of nonlinear features and thus theoretically more expressive thanhole7248rotation embeddings rotaterotate 67 denes each relation as a rotation from the head entity to the tail entity inthe complex vector space thus it is able to model and infer various relation patternsincluding symmetryantisymmetry inversion and composition formally the scorefunction of the fact hr tof rotate is dened asfrh t  h r t743where denotes the element wise hadamard product h r t ck and ri  1rotate is simple but achieves quite good performance compared with previouswork it is the rst model that is capable of modeling and inferring all the threerelation patterns above7249neural tensor networksocher et al 65 propose neural tensor network ntn as well as single layermodel slm while ntn is an enhanced version of slm inspired by the previousattempts in krl slm represents both entities and relations as low dimensionalvectors and also designs relation specic projection matrices to map entities fromentity space to relation space similar to se the score function of slm is as followse hr t  rtanhmr1h  mr2t744where h t rd represent head and tail embeddings r rk represents relationembedding and mr1 mr2 rdk stand for the relation specic matricesalthough slm has introduced relation embeddings as well as a nonlinear layerinto the score function the model representation capability is still restricted neuraltensor network is then proposed with tensors being introduced into the slm frame work besides the original linear neural network layer that projects entities to therelation space ntn also adds another tensor based neural layer which combineshead and tail embeddings with a relation specic tensor as illustrated in fig711the score function of ntn is then dened as followse hr t  rtanhhmrt  mr1h  mr2t  br7451887world knowledge representationthword spaceentity spacerscoreneural tensornetworkfig 711 the architecture of ntn model 47where mr rddk is a 3 way relation specic tensor br is the bias and mr1mr2 rdk is the relation specic matrices similar to slm note that slm is thesimplied version of ntn if the tensor and bias are set to zerobesides the improvements in score function ntn also attempts to utilize thelatent textual information located in entity names and successfully achieves signif icant improvements differing from previous rl models that provide each entitywith a vector ntn represents each entity as the average of its entity names wordembeddings for example the entity bengal tiger will be represented as theaverage word embeddings of bengal and tiger it is apparent that the entityname will provide valuable information for understanding an entity since bengaltiger may come from bengal and be related to other tigers moreover the numberof words is far less than that of entities therefore using the average word embed dings of entity names will also lower the computational complexity and alleviate theissue of data sparsityntn utilizes tensor based neural networks to model triple facts and achievesexcellent successes however the overcomplicated method will lead to higher com putational complexity compared to other methods and the vast number of parameterswill limit the performance on rather sparse and large scale kgs72410neural association model namnam 43 adopts multilayer nonlinear activations in the deep neural network tomodel the conditional probabilities between head and tail entities nam studiestwo model structures deep neural network dnn and relation modulated neuralnetwork rmnnnam dnn feeds the head and tail entities embeddings into an mlp with l fullyconnected layers which is formalized as follows72 knowledge graph representation189zl  sigmoidmlzl1  bl l  1     l746where z0  h r ml and bl is the weight matrix and bias vector for the l thfully connected layer respectively and nally the score function of nam dnn isdened asfrh t  sigmoidtzl747different from nam dnn nam rmnn feds the relation embedding r intoeach layer of the deep neural network as followszl  sigmoidmlzl1  blr l  1     l748where z0  h r ml and bl indicate the weight matrices the score functionof nam rmnn is dened asfrh t  sigmoidtzl  bl1r74973multisource knowledge graph representationwe are living in a complicated pluralistic real world in which we can get informationthrough all senses and learn knowledge not only from structured knowledge graphsbut also from plain texts categories images and videos this cross modal infor mation is considered as multisource information besides the structured knowledgegraph which is well utilized in previous krl methods we will introduce the otherkinds of krl methods utilizing multisource information1 plain text is one of the most common information we deliver receive andanalyze every day there are vast amounts of plain texts we possess remaining to bedetected in which the signicant knowledge that structured knowledge graphs maynot include locates entity description is a special kind of textual information thatdescribes the corresponding entity within a few sentences or a short paragraph usu ally entity descriptions are maintained by some knowledge graphs ie freebaseor could be automatically extracted from huge databases like wikipedia2 entity type is another important structured information for building knowledgerepresentations to learn new objects within our prior knowledge systems humanbeings tend to systemize those objects into existing categories an entity type is usu ally represented with hierarchical structures which consist of different granularitiesof entity subtypes it is natural that entities in the real world usually have multipleentity types most of the existing famous knowledge graphs own their customizedhierarchical structures of entity types3 images provide intuitive visual information to describe what the entity lookslike which is conrmed to be the most signicant information we receive and processevery day the latent information located in images helps a lot especially whendealing with concrete entities for instance we may nd out the potential relationship1907world knowledge representationbetween cherry and plum there are both plants belonging to rosaceae fromtheir appearances images could be downloaded from websites and there are alsosubstantial image datasets like imagenetmultisourceinformationlearningprovidesanovelmethodtolearnknowledgerep resentations not only from the internal information of structured knowledge graphsbut also from the external information of plain texts hierarchical types and imagesmoreover the exploration in multisource information learning helps to further under stand human cognition with all senses in the real world the cross modal represen tations learned based on knowledge graphs will also provide possible relationshipsbetween different kinds of information731knowledge graph representation with textstextual information is one of the most common and widely used information thesedays there are large plain texts generated every day on the web and easy to beextracted words are compressed symbols of our thoughts and can provide the con nections between entities which are of great signicance in krl7311knowledge graph and text joint embeddingwang et al 76 attempt to utilize textual information by jointly embedding entitiesrelations and words into the same low dimensional continuous vector space theirjoint model contains three parts namely the knowledge model the text model andthe alignment model more specically the knowledge model is learned based on thetriple facts in kgs by translation based models while the text model is learned basedon the concurrences of words in the large corpus by skip gram as for the alignmentmodel two methods are proposed utilizing wikipedia anchors and entity names themain idea of alignment by wikipedia anchors is replacing the word word pair w vwith the word entity pair w ev according to the anchors in wiki pages while themain idea of alignment by entity names is replacing the entities in original triplehr twith the corresponding entity names whr t hr wt and whr wtmodeling entities and words into the same vector space are capable of encodingboth information in knowledge graphs and that in plain texts while the performanceof this joint model depends on the completeness of wikipedia anchors and may sufferfrom the weak interactions merely based on entity names to address this issue101 proposes a new joint embedding based on 76 and improves the alignmentmodel with entity descriptions into consideration assuming that entities should besimilar to all words in their descriptions these joint models learn knowledge and textjoint embeddings improving evaluation performance in both word and knowledgerepresentations73 multisource knowledge graph representation191cnncbowdescription of headw1w2wnw1w2wndescription of tailheadrelationtailcnncbowfig 712 the architecture of dkrl model7312description embodied knowledge graph representationanother way of utilizing textual information is directly constructing knowledge rep resentations from entity descriptions instead of merely considering the alignmentsxie et al 82 proposes description embodied knowledge graph representationlearning dkrl that provides two kinds of knowledge representations the rst isthe structure based representation hs and ts which can directly represent entitieswidely used in previous methods and the second is the description based represen tation hd and td which derives from entity descriptions the energy function derivesfrom translation based frameworke hr t  hs  r ts hs  r td hd  r ts hd  r td750the description based representation is constructed via cbow or cnn encodersthat encode rich textual information from plain texts into knowledge representationsthe architecture of dkrl is shown in fig712compared to conventional translation based methods the two types of entityrepresentations in dkrl are constructed with both structural information and textualinformation and thus could get better performance in knowledge graph completionand type classication besides dkrl could represent an entity even if it is not inthe training set as long as there are a few sentences to describe the entity as theirmillions of new entities come up every day dkrl is capable of handling zero shotlearning1927world knowledge representation732knowledge graph representation with typesentity types which serve as a kind of category information of entities and are usu ally arranged with hierarchical structures could provide structured information tounderstand entities in krl better7321type constraint knowledge graph representationkrompa et al 36 take type information as type constraints and improves exist ing methods like rescal and transe via type constraints it is intuitive that in aparticular relation the head or tail entities should belong to some specic types forexample the head entities of the relation write books should be a human ormore precisely an author and the tail entities should be a bookspecically in rescal the original factorization xr arrais modied toxr aheadrrratailr751in which headr tailr are the set of entities tting the type constraints of head or tailand xr is a sparse adjacency matrix of shape headr  tailr in the enhanced ver sion only the entities that t type constraints will be considered during factorizationin transe type constraints are utilized in negative sampling the margin basedscore functions of translation based methods need negative instances which aregenerated through randomly replacing head or tail entities with another entity intriples with type constraints the negative samples are chosen byh eheadr et etailr e752where eheadr is the subset of entities following type constraints for head in relationr and etailr is that for tail7322type embodied knowledge graph representationconsidering type information as constraints is simple but effective while the per formance is still limited instead of merely viewing type information as type con straints xie et al 83 propose type embodied knowledge graph representationlearning tkrl utilizing hierarchical type structures to instruct the constructionof projection matrices inspired by transr that every entity should have multiplerepresentations in different scenarios the energy function of tkrl is dened asfollowse hr t  mrhh  r mrtt75373 multisource knowledge graph representation193hhtthchmhchhchtctrhewhertctrmctmmchm 1mctm 1mct1mct1iimch1iimct1fig 713 the architecture of tkrl modelin which mrh and mrt are two projection matrices for h and t that depend on theircorresponding hierarchical types in this triple two hierarchical type encoders areproposed to learn the projection matrices regarding all subtypes in the hierarchyas projection matrices in which recursive hierarchy encoder is based on matrixmultiplication while weighted hierarchy encoder is based on matrix summationmrh ec mi1mci  mc1mc2    mcm754mw h ec mi1imci  1mc1      mmcm755where mci stands for the projection matrix of the ith subtype of the hierarchicaltype c i is the corresponding weight of the subtype figure713 demonstrates asimple illustration of tkrl taking rhe for instance given an entity williamshakespeare it is rst projected to a rather general sub type space like humanand then sequentially projected to a more precise subtype like author or englishauthor moreover tkrl also proposes an enhanced soft type constraint to alle viate the problems caused by type information incompleteness1947world knowledge representationarmetsuit of armourhas partfig 714 examples of entity images 81733knowledge graph representation with imagesimages could provide intuitive visual information of their corresponding entitiesoutlook which may give signicant hints suggesting some latent attributes of entitiesfrom certain aspects for instance fig714 demonstrates some examples of entityimages of their corresponding entities suit of armour and armet the leftside shows the triple facts that suit of armour has a part armet andsurprisingly we can infer this knowledge directly from the images7331image embodied knowledge graph representationxie et al 81 propose image embodied knowledge graph representation learningikrl to take visual information into consideration when constructing knowledgerepresentations inspired by the multiple entity representations in 82 ikrl alsoproposes the image based representation hi and ti besides the original structure based representation and jointly learn both two types of entity representations simul taneously within the translation based frameworke hr t  hs  r ts hs  r ti hi  r ts hi  r ti756more specically ikrl rst constructs the image representations for all entityimages with neural networks and then project these image representations fromimage space to entity space via a projection matrix since most entities may havemultiple images with different qualities ikrl selects the more informative anddiscriminative images via an attention based method the evaluation results ofikrl not only conrm the signicance of visual information in understanding73 multisource knowledge graph representation195  dresserdrawerpianofortekeyboard  cat felidaetigertoothed whaledolphinwpart ofwhypernymfig 715 an example of semantic regularities in word space 81entities but also show the possibility of a joint heterogeneous semantic spacemoreover the authors also nd some interesting semantic regularities such aswman wking wwoman wqueen found in word space which areshown in fig715734knowledge graph representation with logic rulestypicalknowledgegraphsstoreknowledgeintheformoftriplefactswithonerelationlinkingtwoentitiesmostexistingkrlmethodsonlyconsidertheinformationwithintriple facts separately ignoring the possible interactions and correlations between dif ferent triples logic rules which are certain kinds of summaries deriving from humanbeings prior knowledge could help us with knowledge inference and reasoning forinstance if we know the triple fact that beijing is capital of chinawe can easily infer with high condence that beijing located in chinasince we know the logic rule that the relation is capital of located insome works are focusing on introducing logic rules to knowledge acquisition andinference among which markov logic networks are intuitively utilized to addressthis challenge 3 58 75 the path based transe 38 stated above also implicitlyconsiders the latent logic rules between different relations via relation paths7341kalekale is a translation based krl method that jointly learns knowledge representa tions with logic rules 24 the joint learning consists of two parts namely the triplemodeling and the rule modeling for triple modeling kale follows the translationassumption with minor alteration in scoring function as follows1967world knowledge representatione hr t  1 13dh  r t757in which d stands for the dimension of knowledge embeddings e hr t takes valuein 0 1 for the convenience of joint learningfor the newly added rule modeling kale uses the t norm fuzzy logics proposedin 25 that represent the truth value of a complex formula with the truth values of itsconstituents specially kale focuses on two typical types of logic rules the rst ish t  hr1 thr2 teg given beijing is capital of chinawecaninferthatbeijinglocated inchinakalerepresentsthescoringfunction of this logic rule f1 via specic t norm based logical connectives as followse  f1  e hr1 te hr2 t e hr1 t  1758thesecondish e t  hr1 eer2 thr3 teggiventsinghualocated in beijing and beijing located in china we can inferthat tsinghua located in china and kale denes the second scoringfunction ase  f2  e hr1 ee er2 te hr3 t e hr1 ee er2 t  1759the joint training contains all positive formulae including triple facts as well aslogic rules note that for the consideration of logic rule qualities kale ranks allpossible logic rules by their truth values with pretrained transe and manually lterssome rules ranked at the top74applicationsrecent years have witnessed the great thrive in knowledge driven articial intelli gence such as qa systems and chatbot ai agents are expected to accurately anddeeply understand user demands and then appropriately and exibly give responsesand solutions such kind of work cannot be done without certain forms of knowledgeto introduce knowledge to ai agents researchers rst extract knowledge fromheterogeneous information like plain texts images and structured knowledge basesthese various kinds of heterogeneous information are then fused and stored withcertain structures like knowledge graphs next the knowledge is projected to a low dimensional semantic space following some krl methods and nally these learnedknowledge representations are utilized in various knowledge applications like infor mation retrieval and dialogue system figure716 demonstrates a brief pipeline ofknowledge driven applications from scratchfrom the illustration we can observe that knowledge graph representation learn ing is the critical component in the whole knowledge driven applications pipelineit bridges the gap between knowledge graphs that store knowledge and knowledge74 applications197hamlet is a tragedywritten by willianmshakespeare at anuncertain date between1599 and 1602heterogeneousinformationknowledge graphknowledgeconstructionknowledge representationkrlmethods embeddingmodels knowledgeapplications informationretrieval questionanswering dialoguesystemfig 716 an illustration of knowledge driven applicationsapplications that use knowledge knowledge representations with distributed meth ods compared to those with symbolic methods are able to solve the data sparsity andmodeling the similarities between entities and relations moreover embedding basedmethods are convenient to be used with deep learning methods and are naturally tfor the combination with heterogeneous informationin this section we will introduce possible applications of knowledge represen tations mainly from two aspects first we will introduce the usage of knowledgerepresentations for knowledge driven applications and then we will show the powerof knowledge representations for knowledge extraction and construction741knowledge graph completionknowledge graph completion aims to build structured knowledge bases by extract ing knowledge from heterogeneous sources such as plain texts existing knowledgebases and images knowledge construction consists of several subtasks like relationextraction and information extraction making the fundamental step in the wholeknowledge driven frameworkrecently automatic knowledge construction has attracted considerable attentionsince it is incredibly time consuming and labor intensive to deal with enormousexisting and new information in the following section we will introduce someexplorations on neural relation extraction and concentrate on the combination ofknowledge representations7411knowledge representations for relation extractionrelation extraction focuses on predicting the correct relation between two entitiesgiven a short plain text containing the two entities generally all relations to predictare predened which is different to open information extraction entities are usuallymarked with named entity recognition systems or extracted according to anchor textsor automatically generated via distance supervision 501987world knowledge representationkgfacttextwordpositionencoderplaceofbirthwasborninmark twainmark twainhrtfloridafloridafig 717 the architecture of joint representation learning framework for knowledge acquisitionconventional methods for relation extraction and classication are mainly basedon statistical machine learning which strongly depends on the qualities of extractedfeatures zeng et al 96 rst introduce cnn to relation classication and achievegreat improvements lin et al 40 further improves neural relation extraction modelswith attention based models over instanceshan et al 27 28 propose a novel joint representation learning framework forknowledge acquisition the key idea is that the joint model learns knowledge and textrepresentations within a unied semantic space via kg text alignments figure717shows the brief framework of the kg text joint model for the text part the sen tence with two entities mark twain and florida is regarded as the input fora cnn encoder and the output of cnn is considered to be the latent relationplace of birth of this sentence while for the kg part entity and relationrepresentations are learned via translation based methods the learned representa tions of kg and text parts are aligned during training this work is the rst attemptto encode knowledge representations from existing kgs to knowledge construc tion tasks and achieves improvements in both knowledge completion and relationextraction74 applications199fig 718 the architecture of knet model742knowledge guided entity typingentity typing is the task of detecting semantic types for a named entity or entity men tion in plain text for example given a sentence jordan played 15 seasonsin the nbaentitytypingaimstoinferthat jordaninthissentenceisa personan athlete and even a basketball player entity typing is important fornamed entity disambiguation since it can narrow down the range of candidates for anentity mention 10 moreover entity typing also benets massive natural languageprocessing nlp tasks such as relation extraction 46 question answering 90and knowledge base population 9conventional named entity recognition models 69 73 typically classify entitymentions into a small set of coarse labels eg person organizationlocation and others since these entity types are too coarse grained for manynlp tasks a number of works 15 41 94 95 have been proposed to introduce amuch larger set of ne grained types which are typically subtypes of those coarse grained types previous ne grained entity typing methods usually derive featuresusing nlp tools such as pos tagging and parsing and inevitably suffer from errorpropagation dong et al 18 make the rst attempt to explore deep learning in entitytyping the method only employs word vectors as features discarding complicatedfeature engineering shimaoka et al 63 further introduce the attention scheme intoneural models for ne grained entity typingneural models have achieved state of the art performance for ne grained entitytyping however these methods face the following nontrivial challenges1 entity context separation existing methods typically encode contextwords without utilizing crucial correlations between entity and context how ever it is intuitive that the importance of words in the context for entity typ 2007world knowledge representationing is signicantly inuenced by which entity mentions we concern about forexample in a sentence in 1975 gates and paul allen co foundedmicrosoft which became the worlds largest pc softwarecompany the word company is much more important for determining the type ofmicrosoft than for the type of gates2 entity knowledge separation existing methods only consider text informa tion of entity mentions for entity typing in fact knowledge graphs kgs providerich and effective additional information for determining entity types for examplein the above sentence in 1975 gates  microsoft  companyeven if we have no type information of microsoft in kg entities similar tomicrosoft such as ibm will also provide supplementary informationin order to address the issues of entity context separation and entity knowledgeseparation we propose knowledge guided attention knet neural entity typingas illustrated in fig718 knet mainly consists of two parts firstly knet buildsa neural network including a long short term memory lstm and a fully con nected layer to generate context and named entity representations secondly knetintroduces knowledge attention to emphasize those critical words and improve thequality of context representations here we introduce the knowledge attention indetailknowledge graphs provide rich information about entities in the form of tripleshr t where h and t are entities and r is the relation between them many krlworks have been devoted to encoding entities and relations into real valued semanticvector space based on triple information in kgs krl provides us with an efcientway to exploit kg information for entity typingknet employs the most widely used krl method transe to obtain entity embed ding e for each entity e during the training scenario it is known that the entity men tion m indicates the corresponding e in kgs with embedding e and hence knetcan directly compute knowledge attention as followskai fewkahihi760where wka is a bilinear parameter matrix and akaiis the attention weight for theith wordknowledge attention in testing the challenge is that in the testing scenariowe do not know the corresponding entity in the kg of a certain entity mention asolution is to perform entity linking but it will introduce linking errors besidesin many cases kgs may not contain the corresponding entities for many entitymentionsto address this challenge we build an additional text based representation forentities in kgs during training concretely for an entity e and its context sentences we encode its left and right context into cl and cr using an one directional lstmand further learn the text based representation e as follows74 applications201e  tanhwmclcr761where w is the parameter matrix and m is the mention representation note thatlstm used here is different from those in context representation in order to preventinterference in order to bridge text based and kg based representations in thetraining scenario we simultaneously learn e by putting an additional component inthe objective functionokg  ee e2762in this way in the testing scenario we can directly use eq761 to obtain the corre sponding entity representation and compute knowledge attention using eq760743knowledge guided information retrievalthe emergence of large scale knowledge graphs has motivated the development ofentity oriented search which utilizes knowledge graphs to improve search enginesrecent progresses in entity oriented search include better text representations withentity annotations 61 85 richer ranking features 14 entity based connectionsbetween query and documents 45 84 and soft match query and documents throughknowledgegraphrelationsorembeddings19 88theseapproachesbringinentitiesand semantics from knowledge graphs and have greatly improved the effectivenessof feature based search systemsanother frontier of information retrieval is the development of neural rankingmodels neural ir deep learning techniques have been used to learn distributedrepresentations of queries and documents that capture their relevance relationsrepresentation based 62 or to model the query document relevancy directly fromtheir word level interactions interaction based 13 23 87 neural ir approachesespecially the interaction based ones have greatly improved the ranking accuracywhen large scale training data are available 13entity oriented search and neural ir push the boundary of search engines fromtwo different aspects entity oriented search incorporates human knowledge fromentities and knowledge graph semantics it has shown promising results on feature based ranking systems on the other hand neural ir leverages distributed repre sentations and neural networks to learn more sophisticated ranking models formlarge scale training data entity duet neural ranking model edrm as shown infig719 incorporates entities in interaction based neural ranking models edrmrst learns the distributed representations of entities using their semantics fromknowledge graphs descriptions and types then it follows a recent state of the artentity oriented search framework the word entity duet 86 and matches documentsto queries with both bag of words and bag of entities instead of manual features2027world knowledge representationobamafamilytreeobamadescriptiontypefamily treedescriptiontypeattentioncnncnnquerydocumentunigramsbigramstrigramsenriched entityembeddinginteraction matrixsoft match featuren gramembeddingenriched entityembeddingkernelpoolingfinalrankingscorevwqveqvwdvedmwwmewmwemeem mfig 719 the architecture of edrm modeledrm uses interaction based neural models 13 to match the query and documentswith word entity duet representations as a result edrm combines entity orientedsearch and the interaction based neural ir it brings the knowledge graph semanticsto neural ir and enhances entity oriented search with neural networks7431interaction based ranking modelsgiven a query q and a document d interaction based models rst build the word level translation matrix between q and d the translation matrix describes word pairs similarities using word correlations which are captured by word embeddingsimilarities in interaction based modelstypically interaction based ranking models rst map each word w in q and d toan l dimensional embedding vwvw  embww763it then constructs the interaction matrix m based on query and document embed dings each element mi j in the matrix compares the ith word in q and the jth wordin d eg using the cosine similarity of word embeddingsmi j  cosvwqi  vwdj 764with the translation matrix describing the term level matches between query anddocuments the next step is to calculate the nal ranking score from the matrix manyapproaches have been developed in interaction based neural ranking models but ingeneral that would include a feature extractor on m and then one or several rankinglayers to combine the features to the ranking score74 applications2037432semantic entity representationedrm incorporates the semantic information about an entity from the knowledgegraphs into its representation the representation includes three embeddings entityembedding description embedding and type embedding all in l dimension and arecombined to generate the semantic representation of the entityentity embedding uses an l dimensional embedding layer embe to get the entityembedding e for eve  embee765description embedding encodes an entity description which contains m wordsand explains the entity edrm rst employs the word embedding layer embv toembed the description word v to v then it combines all embeddings in the text toan embedding matrix v next it leverages convolutional lters to slide over the textand compose the l length n gram as g jeg je  reluwcnn  v j jhw bcnn766where wcnn and bcnn are two parameters of the convolutional lterthen we use max pooling after the convolution layer to generate the descriptionembedding vdese vdese maxg1e  g je  gme 767type embedding encodes the categories of entities each entity e has n kinds oftypes fe   f1  f j  fn edrm rst gets the f j embedding v f j through thetype embedding layer embtypevembf j embtypee768then edrm utilizes an attention mechanism to combine entity types to the typeembedding vtypeevtypeenj jv f j769where  j is the attention score calculated as j expy jnl expyl770y j iwbowvti v f j7712047world knowledge representationwhere y j is the dot product of the query or document representation and type embed ding f j we leverage bag of words for query or document encoding wbow is aparameter matrixcombination the three embeddings are combined by a linear layer to generatethe semantic representation of the entityvseme vembe wevdese  vtypee be772in which we is an l  2l matrix and be is an l dimensional vector7433neural entity duet frameworkword entity duet 86 is a recently developed framework in entity oriented search itutilizes the duet representation of bag of words and bag of entities to match questionq and document d with handcrafted features this work introduces it to neural irthey rst construct bag of entities qe and de with entity annotation as well asbag of words qw and dw for q and d the duet utilizes a four way interaction querywords to document words qw dw query words to documents entities qw de queryentities to document words qe dw and query entities to document entities qe deinstead of features edrm uses a translation layer that calculates the similaritybetween a pair of query document terms viwq or vieq and v jwd or v jed it constructs theinteraction matrix m  mww mwe mew mee and mww mwe mew mee denoteinteractions of qw dw qw de qe dw qe de respectively and elements in them arethe cosine similarities of corresponding termsmi jww  cosviwq v jwd mi jee  cosvieq v jedmi jew  cosvieq v jwd mi jwe  cosviwq v jed773the nal ranking feature m is a concatenation of four cross matches mm  mww mwe mew mee774where the  can be any function used in interaction based neural ranking modelsthe entity duet presents an effective way to crossly match query and documentin entity and word spaces in edrm it introduces the knowledge graph semanticsrepresentations into neural ir modelsthe duet translation matrices provided by edrm can be plugged into any standardinteraction basedneuralrankingmodelssuchask nrm87andconv knrm13with sufcient training data the whole model is optimized end to end with back propagation during the process the integration of the knowledge graph semanticsentity embedding description embeddings type embeddings and matching withentities is learned jointly with the ranking neural network74 applications205744knowledge guided language modelsknowledge is an important external information for language modeling it is becausethe statistical co occurrences cannot instruct the generation of all kinds of knowledgeespecially for those named entities with low frequencies researchers try to incorpo rate external knowledge into language models for better performance on generationand representation7441nklmlanguage models aim to learn the probability distribution over sequences of wordswhich is a classical and essential nlp task widely studied recently sequence tosequence neural models seq2seq are blooming and widely utilized in sequentialgenerative tasks like machine translation 68 and image caption generation 72however most seq2seq models have signicant limitations when modeling and usingbackground knowledgeto address this problem ahn et al 1 propose a neural knowledge languagemodel nklm that considers knowledge provided by knowledge graphs whengenerating natural language sequences with rnn language models the key idea isthat nklm has two ways to generate a word the rst is the same way as conventionalseq2seq models that generate a vocabulary word according to the probabilities ofsoftmax and the second is to generate a knowledge word according to the externalknowledge graphsspecically the nklm model takes lstm as the framework of generatingvocabulary word for external knowledge graph information nklm denotes thetopic knowledge as k  a1    ak  in which ai represents the entities ienamed as topic in 1 that appear in the same triple of a certain entity at each stept nklm takes both vocabulary word wvt1 and knowledge word wot1 as well asthe fact at1 predicted at step t 1 as the inputs of lstm next the hidden state oflstm ht is combined with the knowledge context e to get the fact key kt via an mlpmodule the knowledge context ek derives from the mean embeddings of all relatedfacts of fact k the fact key kt is then used to extract the most appropriate fact at fromthe corresponding topic knowledge and nally the selected fact at is combined withhidden state ht to predict 1 both vocabulary word wvt and knowledge word wot and 2 which word to generate at this step the architecture of nklm is shown infig720the nklm model explores a novel neural model that combines the symbolicknowledge information in external knowledge graphs with seq2seq language modelshowever the topic of knowledge is given when generating natural languages whichmakes nklm less practical and scalable for more general free talks neverthelesswe still believe that it is promising to encode knowledge into language models withsuch methods2067world knowledge representationfig 720 the architectureof nklm modeltopic knowledgecopyfact searchxta1a2a3a4annafo1o2o3o4onhtektatwtvwtoht 1at 1wvt 1wot 1lstmzt7442erniepretrained language models like bert 17 have a strong ability to representlanguage information from text with rich language representation pretrainedmodels obtain state of the art results on various nlp applications however theexisting pretrained language models rarely consider incorporating external knowl edge to provide related background information for better language understandingfor example given a sentence bob dylan wrote blowin in the windand chronicles volume onewithoutknowingblowin in thewind and chronicles volume one are song and book respectively itis difcult to recognize the two occupations of bob dylan ie songwriterand writerto enhance language representation models with external knowledge zhang etal 100 propose an enhanced language representation model with informative enti ties ernie knowledge graphs kgs are important external knowledge resourcesand they think informative entities in kgs can be the bridge to enhance languagerepresentation with knowledge ernie considers overcoming two main challengesfor incorporating external knowledge structured knowledge encoding and hetero geneous information fusionfor extracting and encoding knowledge information ernie rstly recognizesnamed entity mentions in text and then aligns these mentions to their correspondingentities in kgs instead of directly using the graph based facts in kgs ernieencodes the graph structure of kgs with knowledge embedding algorithms liketranse 7 and then takes the informative entity embeddings as input based on thealignments between text and kgs ernie integrates entity representations in theknowledge module into the underlying layers of the semantic module74 applications207bobdylanwrote1962multi head attentionmulti head attention information fusiontoken inputentity inputtoken outputentity outputbob dylan wrote blowin in the wind in 1962blowmulti headattentionfeedforwardnxmulti headattentioninformationfusiontoken inputmulti headattentionentity inputmxtoken outputentity outputblowin in the windbob dylanaggregatortransformeraggregatora model achitectureb aggregatork encodert encoderw1i 1w2i 1w3i 1w4i 1wni 1e1i 1e2i 1w1i 1w2i 1w3i 1w4i 1wni 1e1i 1e2i 1e1i 1e2i 1w1iw2iw3iw4iwnie1ie2ie1ie2ifig 721 the architecture of ernie modelsimilar to bert ernie adopts the masked language model and the next sen tence prediction as the pretraining objectives besides for the better fusion of textualand knowledge features ernie uses a new pretraining objective denoising entityauto encoder by randomly masking some of the named entity alignments in theinput text and training to select appropriate entities from kgs to complete the align ments unlike the existing pre trained language representation models only utilizinglocal context to predict tokens these objectives require ernie to aggregate bothcontext and knowledge facts for predicting both tokens and entities and lead to aknowledgeable language representation modelfigure721 is the overall architecture the left part shows that ernie consistsof two encoders t encoder and k encoder where t encoder is stacked by severalclassical transformer layers and k encoder is stacked by the new aggregator layersdesigned for knowledge integration the right part is the detail of the aggregator layerin the aggregator layer the input token embeddings and entity embeddings from thepreceding aggregator are fed into two multi head self attention respectively thenthe aggregator adopts an information fusion layer for the mutual integration of thetoken and entity sequence and computes the output embedding for each token andentityernie explores how to incorporate knowledge information into language repre sentation models the experimental results demonstrate that ernie has more pow erful abilities of both denoising distantly supervised data and ne tuning on limiteddata than bert7443kalmpre trained language models can do many tasks without supervised training data likereading comprehension summarization and translation 60 however traditionallanguage models are unable to efciently model entity names observed in text to2087world knowledge representationsolve this problem liu et al 42 propose a new language model architecture calledknowledge augmented language model kalm to use the entity types of wordsfor better language modelingkalm is a language model with the option to generate words from a set of entitiesfrom a knowledge database an individual word can either come from a generalword dictionary as in the traditional language model or be generated as a name of anentity from a knowledge database the training objectives just supervise the outputand ignore the decision of the word type entities in the knowledge database arepartitioned by type and they use the database to build the types of words accordingto the context observed so far the model decides whether the word is a general termor a named entity in a given type thus kalm learns to predict whether the contextobserved is indicative of a named entity and what tokens are likely to be entities ofa given typewith the language modeling kalm learns a named entity recognizer without anyexplicit supervision by using only plain text and the potential types of words andit achieves a comparable performance with the state of the art supervised methods745other knowledge guided applicationsknowledge enables ai agents to understand infer and address user demands whichisessentialinmostknowledge drivenapplicationslikeinformationretrievalquestionanswering and dialogue system the behavior of ai agents will be more reasonableand accurate with the favor of knowledge representations in the following subsec tions we will introduce the great improvements made by knowledge representationin question answering7451knowledge guided question answeringquestion answering aims to give correct answers according to users questionswhich needs the capabilities of both natural language understanding of questionsand inference on answer selection therefore combining knowledge with questionanswering is a straightforward application for knowledge representations most con ventional question answering systems directly utilize knowledge graphs as certaindatabases ignoring the latent relationships between entities and relations recentlywith the thriving in deep learning explorations have focused on neural models forunderstanding questions and even generating answersconsideringtheexibilityanddiversityofgeneratedanswersinnaturallanguagesyin et al 93 propose a neural generative question answering model genqawhich explores on generating answers to simple factoid questions in natural lan guages figure722 demonstrates the workow of genqa first a bidirectionalrnn is regarded as the interpreter to transform question q from natural languageto compressed representation hq next enquirer takes hq as the key to rank rel 74 applications209fig 722 the architectureof genqa modelqhow tall is yao mingahe is 229m  and visible from spaceanswerergeneratorenquirerinterpreterlong term memoryknowledge baseattmodelshort term mernoryevant triples facts of q in knowledge graphs and retrieves possible entities in rqfinally answerer combines hq and rq to generate answers in the form of naturallanguages similar to 1 at each step answerer rst decides whether to generatecommon words or knowledge words according to a logistic regression model forcommon words answerer acts in the same way as rnn decoders with hq selectedby attention based methods as for knowledge words answerer directly generatesentities with higher ranksthere are gradually more efforts focusing on encoding knowledge representationsinto knowledge driven tasks like information retrieval and dialogue systems how ever how to exibly and effectively combine knowledge with ai agents remains tobe explored in the future7452knowledge guided recommendation systemdue to the rapid growth of web information recommendation systems have beenplaying an essential role in the web application the recommendation system aimsto predict the rating or preference that users may give to items and since kgscan provide rich information including both structured and unstructured data rec ommendation systems have utilized more and more knowledge from kgs to enrichtheir contextscheekula et al 11 explore to utilize the hierarchical knowledge from the dbpe dia category structure in the recommendation system and employs the spreadingactivation algorithm to identify entities of interest to the user besides passant 56measures the semantic relatedness of the artist entity in a kg to build music recom mendation systems however most of these systems mainly investigate the problemby leveraging the structure of kgs recently with the development of representation2107world knowledge representationlearning 98 proposes to jointly learn the latent representations in a collaborativeltering recommendation system as well as entities representations in kgsexcept the tasks stated above there are gradually more efforts focusing on encod ing knowledge graph representations into other tasks such as dialogue system 37103 entity disambiguation 20 31 knowledge graph alignment 12 102 depen dency parsing 35 etc moreover the idea of krl has also motivated the researchon visual relation extraction 2 99 and social relation extraction 7175summaryin this chapter we rst introduce the concept of the knowledge graph knowledgegraph contains both entities and the relationships among them in the form of triplefacts providing an effective way of human beings learning and understanding thereal world next we introduce the motivations of knowledge graph representationwhich is considered as a useful and convenient method for a large amount of data andis widely explored and utilized in multiple knowledge based tasks and signicantlyimproves the performance and we describe existing approaches for knowledgegraph representation further we discuss several advanced approaches that aim todeal with the current challenges of knowledge graph representation we also reviewthe real world applications of knowledge graph representation such as languagemodeling question answering information retrieval and recommendation systemsfor further understanding of knowledge graph representation you can nd morerelated papers in this paper list httpsgithubcomthunlpkrlpapers there are alsosome recommended surveys and books including bengio et al representation learning a review and new perspectives 4 liu et al knowledge representation learning a review 47 nickel et al a review of relational machine learning for knowledge graphs 52 wang et al knowledge graph embedding a survey of approaches and applications74 ji et al a survey on knowledge graphs representation acquisition and applications34in the future for better knowledge graph representation there are some directionsrequiring further efforts1 utilizing more knowledge current krl approaches focus on represent ing triple based knowledge from world knowledge graphs such as freebase wiki data etc in fact there are various kinds of knowledge in the real world such asfactual knowledge event knowledge commonsense knowledge etc whats morethe knowledge is stored with different formats such as attributions quantier textand so on the researchers have formed a consensus that utilizing more knowledgeis a potential way toward more interpretable and intelligent nlp some existingworks 44 82 have made some preliminary attempts of utilizing more knowledge75 summary211in krl beyond these works is it possible to represent different knowledge in aunied semantic space which can be easily applied in downstream nlp tasks2 performing deep fusion of knowledge and language there is no doubtthat the joint learning of knowledge and language information can further benetdownstream nlp tasks existing works 76 89 97 have preliminarily veried theeffectiveness of joint learning recently erine 100 and knowbert 57 furtherprovide us a novel perspective to fuse knowledge and language in pretraining soareset al 64 learn the relational similarity in text with the guidance of kgs which isalso a pioneer of knowledge fusion besides designing novel pretraining objectiveswe could also design novel model architectures for downstream tasks which aremore suitable to utilize krl such as memory based models 48 91 and graphnetwork based models 66 nevertheless it still remains an unsolved problem foreffectively performing the deep fusion of knowledge and language3orientingheterogeneousmodalitieswiththefastdevelopmentoftheworldwide web the data size of audios images and videos on the web have becomelarger and larger which are also important resources for krl besides texts somepioneer works 51 81 explore to learn knowledge representations on a multi modalknowledge graph but are still preliminary attempts intuitively audio and visualknowledge can provide complementary information which benets related nlptasks to the best of our knowledge there still lacks research on applying multi modalkrl in downstream tasks how to efciently and effectively integrate multi modalknowledge is becoming a critical and challenging problem for krl4 exploring knowledge reasoning most of the existing krl methods rep resent knowledge information in low dimensional semantic space which is feasiblefor the computation of complex knowledge graphs in neural based nlp modelsalthough beneting from the usability of low dimensional embeddings krl cannotperform explainable reasoning such as symbolic rules which is of great importancefor downstream nlp tasks recently there has been increasing interest in the com bination of embedding methods and symbolic reasoning methods 26 59 aiming attaking both advantages of them beyond these works there remain lots of unsolvedproblems for developing better knowledge reasoning ability for krlreferences1 sungjin ahn heeyoul choi tanel prnamaa and yoshua bengio a neural knowledge lan guage model arxiv preprint arxiv160800318 20162 stephan baier yunpu ma and volker tresp improving visual relationship detection usingsemantic modeling of scene descriptions in proceedings of iswc 20173 islam beltagy and raymond j mooney efcient markov logic inference for natural languagesemantics in proceedings of aaai workshop 20144 yoshua bengio aaron courville and pascal vincent representation learning a review andnew perspectives tpami 35817981828 20135 antoine bordes xavier glorot jason weston and yoshua bengio joint learning of wordsand meaning representations for open text semantic parsing in proceedings of aistats20122127world knowledge representation6 antoine bordes xavier glorot jason weston and yoshua bengio a semantic matchingenergy function for learning with multi relational data machine learning 94223325920147 antoine bordes nicolas usunier alberto garcia duran jason weston and oksanayakhnenko translating embeddings for modeling multi relational data in proceedings ofneurips 20138 antoine bordes jason weston ronan collobert and yoshua bengio learning structuredembeddings of knowledge bases in proceedings of aaai 20119 andrew carlson justin betteridge richard c wang estevam r hruschka jr and tom mmitchell coupled semi supervised learning for information extraction in proceedings ofwsdm 201010 mohamed chabchoub michel gagnon and amal zouaq collective disambiguation andsemantic annotation for entity linking and typing in proceedings of swec 201611 siva kumar cheekula pavan kapanipathi derek doran prateek jain and amit p shethentity recommendations using hierarchical knowledge bases 201512 muhao chen yingtao tian mohan yang and zaniolo carlo multilingual knowledge graphembeddings for cross lingual knowledge alignment in proceedings of ijcai 201713 zhuyun dai chenyan xiong jamie callan and zhiyuan liu convolutional neural networksfor soft matching n grams in ad hoc search in proceedings of wsdm 201814 jeffrey dalton laura dietz and james allan entity query feature expansion using knowledgebase links in proceedings of sigir 201415 luciano del corro abdalghani abujabal rainer gemulla and gerhard weikum finetcontext aware ne grained named entity typing in proceedings of emnlp 201516 tim dettmers pasquale minervini pontus stenetorp and sebastian riedel convolutional2d knowledge graph embeddings in proceedings of aaai 201817 jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training ofdeep bidirectional transformers for language understanding in proceedings of naacl 201918 li dong furu wei hong sun ming zhou and ke xu a hybrid neural model for typeclassication of entity mentions in proceedings of ijcai 201519 faezeh ensan and ebrahim bagheri document retrieval model through semantic linking inproceedings of wsdm 201720 wei fang jianwen zhang dilin wang zheng chen and ming li entity disambiguation byknowledge and text jointly embedding in proceedings of conll 201621 alberto garca durn antoine bordes and nicolas usunier composing relationships withtranslations in proceedings of emnlp 201522 kelvin gu john miller and percy liang traversing knowledge graphs in vector space inproceedings of emnlp 201523 jiafeng guo yixing fan qingyao ai and w bruce croft semantic matching by non linearword transportation for information retrieval in proceedings of cikm 201624 shu guo quan wang lihong wang bin wang and li guo jointly embedding knowledgegraphs and logical rules in proceedings of emnlp 201625 petr hjek metamathematics of fuzzy logic volume 4 springer science  business media199826 will hamilton payal bajaj marinka zitnik dan jurafsky and jure leskovec embeddinglogical queries on knowledge graphs in proceedings of nips 201827 xu han zhiyuan liu and maosong sun joint representation learning of text and knowledgefor knowledge graph completion arxiv preprint arxiv161104125 201628 xu han zhiyuan liu and maosong sun neural knowledge acquisition via mutual attentionbetween knowledge graph and text in proceedings of aaai pages 48324839 201829 katsuhiko hayashi and masashi shimbo on the equivalence of holographic and complexembeddings for link prediction in proceedings of acl 201730 shizhu he kang liu guoliang ji and jun zhao learning to represent knowledge graphswith gaussian embedding in proceedings of cikm 2015references21331 hongzhao huang larry heck and heng ji leveraging deep neural networks and knowledgegraphs for entity disambiguation arxiv preprint arxiv150407678 201532 guoliang ji shizhu he liheng xu kang liu and jun zhao knowledge graph embeddingvia dynamic mapping matrix in proceedings of acl 201533 guoliang ji kang liu shizhu he and jun zhao knowledge graph completion with adaptivesparse transfer matrix in proceedings of aaai 201634 shaoxiong ji shirui pan erik cambria pekka marttinen and philip s yu a survey on knowl edge graphs representation acquisition and applications arxiv preprint arxiv200200388202035 a yeong kim hyun je song seong bae park and sang jo lee a re ranking model fordependency parsing with knowledge graph embeddings in proceedings of ialp 201536 denis krompa stephan baier and volker tresp type constrained representation learningin knowledge graphs in proceedings of iswc 201537 phong le marc dymetman and jean michel renders lstm based mixture of experts forknowledge aware dialogues in proceedings of acl workshop 201638 yankai lin zhiyuan liu huanbo luan maosong sun siwei rao and song liu modelingrelation paths for representation learning of knowledge bases in proceedings of emnlp201539 yankai lin zhiyuan liu maosong sun yang liu and xuan zhu learning entity and relationembeddings for knowledge graph completion in proceedings of aaai 201540 yankai lin shiqi shen zhiyuan liu huanbo luan and maosong sun neural relationextraction with selective attention over instances in proceedings of acl 201641 xiao ling and daniel s weld fine grained entity recognition in proceedings of aaai 201242 angli liu jingfei du and veselin stoyanov knowledge augmented language model and itsapplication to unsupervised named entity recognition in proceedings of naacl 201943 quan liu hui jiang andrew evdokimov zhen hua ling xiaodan zhu si wei andyu hu probabilistic reasoning via deep learning neural association models arxiv preprintarxiv160307704 201644 quan liu hui jiang zhen hua ling xiaodan zhu si wei and yu hu commonsenseknowledge enhanced embeddings for solving pronoun disambiguation problems in winogradschema challenge arxiv preprint arxiv161104146 201645 xitong liu and hui fang latent entity space a novel retrieval approach for entity bearingqueries information retrieval journal 186473503 201546 yang liu kang liu liheng xu jun zhao et al exploring ne grained entity type constraintsfor distantly supervised relation extraction in proceedings of coling 201447 zhiyuan liu maosong sun yankai lin and ruobing xie knowledge representation learn ing a review jcrd 532247261 201648 todor mihaylov and anette frank knowledgeable reader enhancing cloze style readingcomprehension with external commonsense knowledge in proceedings of acl 201849 tomas mikolov kai chen greg corrado and jeffrey dean efcient estimation of wordrepresentations in vector space in proceedings of iclr 201350 mike mintz steven bills rion snow and dan jurafsky distant supervision for relationextraction without labeled data in proceedings of acl ijcnlp 200951 hatem mousselly sergieh teresa botschen iryna gurevych and stefan roth a multimodaltranslation based approach for knowledge graph representation learning in proceedings ofjclcs 201852 maximilian nickel kevin murphy volker tresp and evgeniy gabrilovich a review ofrelational machine learning for knowledge graphs 201553 maximilian nickel lorenzo rosasco and tomaso poggio holographic embeddings ofknowledge graphs in proceedings of aaai 201654 maximilian nickel volker tresp and hans peter kriegel a three way model for collectivelearning on multi relational data in proceedings of icml 201155 maximilian nickel volker tresp and hans peter kriegel factorizing yago scalable machinelearning for linked data in proceedings of www 20122147world knowledge representation56 alexandre passant dbrecmusic recommendations using dbpedia in proceedings of iswc201057 matthew e peters mark neumann robert l logan roy schwartz vidur joshi sameersingh and noah a smith knowledge enhanced contextual word representations in pro ceedings of emnlp ijcnlp 201958 jay pujara hui miao lise getoor and william w cohen knowledge graph identicationin proceedings of iswc 201359 meng qu and jian tang probabilistic logic neural networks for reasoning in proceedings ofnips 201960 alec radford jeffrey wu rewon child david luan dario amodei and ilya sutskeverlanguage models are unsupervised multitask learners openai blog 18 201961 hadas raviv oren kurland and david carmel document retrieval using entity based lan guage models in proceedings of sigir 201662 yelong shen xiaodong he jianfeng gao li deng and grgoire mesnil a latent semanticmodelwithconvolutional poolingstructureforinformationretrievalin proceedingsofcikm201463 sonse shimaoka pontus stenetorp kentaro inui and sebastian riedel an attentive neuralarchitecture for ne grained entity type classication in proceedings of akbc workshop201664 livio baldini soares nicholas fitzgerald jeffrey ling and tom kwiatkowski matching theblanks distributional similarity for relation learning in proceedings of acl pages 28952905 201965 richard socher danqi chen christopher d manning and andrew ng reasoning with neuraltensor networks for knowledge base completion in proceedings of neurips 201366 haitian sun bhuwan dhingra manzil zaheer kathryn mazaitis ruslan salakhutdinov andwilliam cohen open domain question answering using early fusion of knowledge bases andtext in proceedings of emnlp 201867 zhiqing sun zhi hong deng jian yun nie and jian tang rotate knowledge graph embed ding by relational rotation in complex space in proceedings of iclr 201968 ilya sutskever oriol vinyals and quoc v le sequence to sequence learning with neuralnetworks in proceedings of neurips 201469 erik f tjong kim sang and fien de meulder introduction to the conll 2003 shared tasklanguage independent named entity recognition in proceedings of hlt naacl 200370 tho trouillon johannes welbl sebastian riedel ric gaussier and guillaume bouchardcomplex embeddings for simple link prediction in proceedings of icml 201671 cunchao tu zhengyan zhang zhiyuan liu and maosong sun transnet translation basednetwork representation learning for social relation extraction in proceedings of ijcai 201772 oriol vinyals alexander toshev samy bengio and dumitru erhan show and tell a neuralimage caption generator in proceedings of cvpr 201573 nina wacholder yael ravin and misook choi disambiguation of proper names in text inproceedings of anlp 199774 quan wang zhendong mao bin wang and li guo knowledge graph embedding a surveyof approaches and applications tkde 291227242743 201775 quan wang bin wang and li guo knowledge base completion using embeddings and rulesin proceedings of ijcai 201576 zhen wang jianwen zhang jianlin feng and zheng chen knowledge graph and text jointlyembedding in proceedings of emnlp pages 15911601 201477 zhen wang jianwen zhang jianlin feng and zheng chen knowledge graph embedding bytranslating on hyperplanes in proceedings of aaai 201478 han xiao minlie huang yu hao and xiaoyan zhu transa an adaptive approach forknowledge graph embedding arxiv preprint arxiv150905490 201579 han xiao minlie huang yu hao and xiaoyan zhu transg a generative mixture model forknowledge graph embedding arxiv preprint arxiv150905488 2015references21580 han xiao minlie huang and xiaoyan zhu from one point to a manifold knowledge graphembedding for precise link prediction in proceedings of ijcai 201681 ruobing xie zhiyuan liu tat seng chua huanbo luan and maosong sun image embodiedknowledge representation learning in proceedings of ijcai 201682 ruobing xie zhiyuan liu jia jia huanbo luan and maosong sun representation learningof knowledge graphs with entity descriptions in proceedings of aaai 201683 ruobing xie zhiyuan liu and maosong sun representation learning of knowledge graphswith hierarchical types in proceedings of ijcai 201684 chenyan xiong and jamie callan esdrank connecting query and documents through exter nal semi structured data in proceedings of cikm 201585 chenyan xiong jamie callan and tie yan liu bag of entities representation for rankingin proceedings of ictir 201686 chenyan xiong jamie callan and tie yan liu word entity duet representations for docu ment ranking in proceedings of sigir 201787 chenyan xiong zhuyun dai jamie callan zhiyuan liu and russell power end to endneural ad hoc ranking with kernel pooling in proceedings of sigir 201788 chenyan xiong russell power and jamie callan explicit semantic ranking for academicsearch via knowledge graph embedding in proceedings of www 201789 jiacheng xu xipeng qiu kan chen and xuanjing huang knowledge graph representationwith jointly structural and textual encoding in proceedings of ijcai 201790 mohamed yahya klaus berberich shady elbassuoni and gerhard weikum robust questionanswering over the web of linked data in proceedings of cikm 201391 bishan yang and tom mitchell leveraging knowledge bases in lstms for improvingmachine reading in proceedings of acl 201792 bishan yang wen tau yih xiaodong he jianfeng gao and li deng embedding entitiesand relations for learning and inference in knowledge bases in proceedings of iclr 201593 jun yin xin jiang zhengdong lu lifeng shang hang li and xiaoming li neural gener ative question answering in proceedings of ijcai 201694 dani yogatama daniel gillick and nevena lazic embedding methods for ne grained entitytype classication in proceedings of acl 201595 mohamed amir yosef sandro bauer johannes hoffart marc spaniol and gerhard weikumhyena hierarchical type classication for entity names in proceedings of coling 201296 daojian zeng kang liu siwei lai guangyou zhou and jun zhao relation classicationvia convolutional deep neural network in proceedings of coling 201497 dongxu zhang bin yuan dong wang and rong liu joint semantic relevance learning withtext data and graph knowledge in proceedings of acl ijcnlp 201598 fuzheng zhang nicholas jing yuan defu lian xing xie and wei ying ma collaborativeknowledge base embedding for recommender systems in proceedings of sigkdd 201699 hanwangzhangzawlinkyawshih fuchangandtat sengchuavisualtranslationembed ding network for visual relation detection in proceedings of cvpr 2017100 zhengyan zhang xu han zhiyuan liu xin jiang maosong sun and qun liu ernieenhanced language representation with informative entities in proceedings of acl 2019101 huaping zhong jianwen zhang zhen wang hai wan and zheng chen aligning knowledgeand text embeddings by entity descriptions in proceedings of emnlp 2015102 hao zhu ruobing xie zhiyuan liu and maosong sun iterative entity alignment via jointknowledge embeddings in proceedings of ijcai 2017103 wenya zhu kaixiang mo yu zhang zhangbin zhu xuezheng peng and qiang yangflexible end to end dialogue system for knowledge grounded conversation arxiv preprintarxiv170904264 20172167world knowledge representationopen access this chapter is licensed under the terms of the creative commons attribution 40international license httpcreativecommonsorglicensesby40 which permits use sharingadaptation distribution and reproduction in any medium or format as long as you give appropriatecredit to the original authors and the source provide a link to the creative commons license andindicate if changes were madethe images or other third party material in this chapter are included in the chapters creativecommons license unless indicated otherwise in a credit line to the material if material is notincluded in the chapters creative commons license and your intended use is not permitted bystatutory regulation or exceeds the permitted use you will need to obtain permission directly fromthe copyright holderchapter 8network representationabstract network representation learning aims to embed the vertexes in a networkinto low dimensional dense representations in which similar vertices in the net work should have close representations usually measured by cosine similarity oreuclidean distance of their representations the representations can be used as thefeature of vertices and applied to many network study tasks in this chapter we willintroduce network representation learning algorithms in the past decade then wewill talk about their extensions when applied to various real world networks finallywe will introduce some common evaluation tasks of network representation learningand relevant datasets81introductionas a natural way to represent objects and their relationships the network is ubiqui tous in our daily lives the rapid development of social networks like facebook andtwitter encourage researchers to design effective and efcient algorithms on networkstructure a key problem of network study is how to represent the network informa tion properly traditional representations of networks are usually high dimensionaland sparse which becomes a weakness when people apply statistical learning tonetworks with the development of machine learning feature learning of vertices ina network is becoming an emerging task therefore network representation learn ing algorithms turn network information into low dimensional dense real valuedvectors which can be used as input for existing machine learning algorithms forexample the representations of vertices can be fed to a classier like support vectormachine svm for the vertex classication task also the representations can beused for visualization by taking the representations as points in euclidean space inthis section we will formalize the network representation learning problemthe original version of this chapter was revised the rst paragraph in section 83 was updatedthe correction to this chapter can be found at httpsdoiorg101007978 981 15 5573 2 12 the authors 2020 corrected publication 2023z liu et al representation learning for natural language processinghttpsdoiorg101007978 981 15 5573 2 82172188network representationfig 81 a visualization of vertex embeddings learned by deepwalk model 93denote a network as g  v e where v is the vertex set and e is the edge setan edge e  vi v j e where vi v j v is a directed edge from vertex vi to v jthe outdegree of vertex vi is dened as degovi  v jvi v j e similarlythe indegree of vertex vi is degivi  v jv j vi e for undirected networkwe have degvi  degovi  degivi taking social network as an example avertex represents a user and an edge represents the friendship between two usersthe indegree and outdegree represent the number of followers and followees of auser respectivelyadjacency matrix a rv v  is a matrix where ai j  1 if vi v j e andai j  0 otherwise we can easily generalize adjacency matrix to weighted networkby setting ai j to the weight of edge vi v j the adjacency matrix is a simpleand straightforward representation of the network each row of adjacency matrixa denotes the relationship between a vertex and other vertices and can be seen asthe representation of the corresponding vertexthoughconvenientandstraightforwardtherepresentationoftheadjacencymatrixsuffers from the scalability problem adjacency matrix a takes v   v  space tostore and it is usually unacceptable when v  grows large also the adjacency matrixis very sparse which means most of its entries are zeros the data sparsity makesdiscrete algorithms applicable but it is still hard to develop efcient algorithms forstatistic learning 93therefore people come up with the idea to learn low dimensional dense rep resentations for vertices in a network formally the goal of network representationlearning is to learn a real valued vector v rd for vertex v v where dimension d ismuch smaller than the number of vertices v  the idea is that similar vertices shouldhave close representations as shown in fig81 network representation learning canbe unsupervised or semi supervised the representations are automatically learnedwithout feature engineering and can be further used for specic tasks like classi cations once they are learned these representations are low dimensional whichenables efcient algorithms to be designed over the representations without consid ering the network structure itself we will discuss more details about the evaluationof network representations later in this chapter82 network representation21982network representationin this section we will introduce several kinds of network representation learningalgorithms in detail821spectral clustering based methodsspectral clustering based methods are a group of algorithms that compute rst keigenvectors or singular vectors of an afnity matrix such as adjacency or lapla cian matrix of the network these methods depend heavily on the construction of theafnity matrix the evaluation result of different afnity matrices varies a lot gener ally speaking spectral clustering based methods have a high complexity because thecomputations of eigenvectors and singular vectors have a nonlinear time complexityon the other hand spectral clustering based methods need to save an afnitymatrix in the memory during the computation thus the space complexity cannot beignored either these disadvantages limit the large scale and online generalization ofthese methods now we will present several algorithms based on spectral clusteringlocally linear embedding lle 98 assumes that the representations of verticesare sampled from a manifold more specically lle supposes that the representa tions of a vertex and its neighbors lie in a locally linear patch of the manifold thatis to say a vertexs representation can be approximated by a linear combination ofthe representation of its neighbors lle uses the linear combination of neighbors toreconstruct the center vertex formally the reconstruction error of all vertices can beexpressed asl w v v i1vi v j1wi jv j281where v rv d is the vertex embedding matrix and wi j is the contribution coef cient of vertex v j to vi lle enforces wi j  0 if vi and v j are not connectedie vi v j e further the summation of a row of matrix w is set to 1 iev j1 wi j  1equation81 is solved by alternatively optimizing weight matrix w and represen tation v the optimization over w can be solved as a least squares problem theoptimization over representation v leads to the following optimization probleml w v v i1vi v j1wi jv j2822208network representationstv i1vi  083and v 1v i1vi vi  id84where id denotes d  d identity matrix the conditions eqs83 and 84 ensurethe uniqueness of the solution the rst condition enforces the center of all vertexembeddings to zero point and the second condition guarantees different coordinateshave the same scale ie equal contribution to the reconstruction errorthe optimization problem can be formulated as the computation of eigenvectorsof matrix iv  wiv  w which is an easily solvable eigenvalue problemmore details can be found in the note 22laplacian eigenmap 8 algorithm simply follows the idea that the representationsof two connected vertices should be close specically the closeness is measuredby the square of euclidean distance we use d to denote diagonal degree matrixwhere d is a v   v  diagonal matrix and the ith diagonal entry dii is the degreeof vertex vi the laplacian matrix l of a graph is dened as the difference of diagonalmatrix d and adjacency matrix a ie l  d alaplacian eigenmap algorithm wants to minimize the following cost functionl v i jviv jevi v j285st vdv  id86the cost function is the summation of square loss of all connected vertex pairsand the condition prevents the trivial all zero solution caused by arbitrary scaleequation85 can be reformulated in matrix form asv argminvdvidtrvlv87algebraic knowledge tells us that the optimal solution vof eq87 is the cor responding eigenvectors of d smallest nonzero eigenvalues of laplacian matrix lnotethat thelaplacianeigenmapalgorithmcanbeeasilygeneralizedtotheweightedgraphboth lle and laplacian eigenmap have a symmetric cost function which indi cates that both algorithms cannot be applied to the directed graph directed graphembedding dge 17 was proposed to generalize laplacian eigenmapfor both directed and undirected graph we can dene a transition probabilitymatrix p rv v  where pi j denotes the probability that vertex vi walks to v j82 network representation221table 81 applicability of lle laplacian eigenmap and dge algorithms on undirectedweighted and directed graphalgorithmcapabilityundirectedweighteddirectedllelaplacian eigenmapdgethe transition matrix denes a markov random walk through the graph we denotethe stationary value of vertex vi as i where i i  1 the stationary distributionof random walk is commonly used in many ranking algorithms such as pagerankdge designs a new cost function which emphasizes the important vertices whichhave a higher stationary valuel v v i1iv j1pi jvi v j288by denoting m  diag1 2     v  the cost function eq88 can be refor mulated asl v  2trvbv89st vmv  id810whereb  m mp pm2811the condition eq810 is added to remove an arbitrary scaling factor similar tolaplacian eigenmap the optimization problem can also be solved as a generalizedeigenvector problemfor comparisons between the above three network embedding learning algo rithms we conclude the following table to illustrate their applicability table81unlike previous works which minimize the distance between vertex representa tions tang and liu 112 introduces modularity 85 into the cost function insteadmodularity is a measurement which characterizes how far the graph is away froma uniform random graph given graph g  v e we assume that vertices v aredivided into k nonoverlapping communities by uniform random graph we meanvertices connect to each other based on a uniform distribution given their degreesthen the expected edges between vi and v j is degvi degvj2e then the modularity of agraph q is dened as2228network representationq 12ei jai j degvi degv j2evi v j812where vi v j  1 if vi and v j belong to the same community and vi v j  0otherwise a larger modularity indicates that the subgraphs inside communities aredenser which follows the intuition that a community is a dense well connectedcluster then the problem is to nd a partition that maximizes the modularity qhowever a hard clustering on modularity maximization is proved to be np hardtherefore they relax the problem to a soft case let d zv  denotes the degree ofall vertices and 1 0 1v k denotes the community indicator matrix where1i j 1if vertex i belongs to community j0otherwise813then we dene modularity matrix b asb  a ddt2e814and modularity q can be reformulated asq 12etr1b1815by relaxing 1 to a continuous matrix it has been proved that the optimal solution1 is the top k eigenvectors of modularity matrix b 84as an alternatively cost function tang and liu also proposed another algorithm113 by optimizing over normalized cut of the graph similarly the algorithm turnsto the computation of top k eigenvectors of normalized graph laplacian ll  d12 l d12  i d12 ad12 816then the community indicator matrix 1 is taken as a k dimensional vertex repre sentationtoconcludespectralclusteringmethodsfornetworkrepresentationlearningthesemethods often dene a cost function that is linear or quadratic to the vertex embed ding then they reformulate the cost function as a matrix form and gure out thatthe optimal solutions are eigenvectors of a particular matrix according to algebraknowledge the major drawback of spectral clustering methods is the complexitythe computation of eigenvectors for large scale matrices is both time consuming andspace consuming82 network representation223822deepwalkas shown in previous subsections accurate computation of the optimal solution suchas eigenvector computation is not very efcient for large scale problems meantimeneural network approaches have proved their effectiveness in many areas such asnatural language and image processing though the gradient descent method cannotalways guarantee an optimal solution of the neural network models the implemen tation and learning of neural networks are relatively fast and they usually have goodperformances on the other hand neural network models can let people get rid offeature engineering and are mostly data driven thus the exploration of the neuralnetwork approach on representation learning is becoming an emerging taskdeepwalk 93 proposes a novel approach that introduces deep learning tech niques into network representation learning for the rst time the benets of model ing truncated random walks instead of the adjacency matrix are twofold rst randomwalks need only local information and thus enable discrete and online algorithms onit while modeling of adjacency matrix may need to store everything in memory andthus be space consuming second modeling random walks can alleviate the varianceand uncertainty of modeling original binary adjacency matrix we will look insightinto deepwalk in the next subsectionunsupervised representation learning algorithms have been widely studied andapplied in the natural language processing area the authors show that the vertex fre quency in short random walks also follows the power law as words in documents doshowing the connection between vertex to the word and random walks to sentencesthe authors adapted a well known word representation learning algorithm word2vec80 into vertex representation learning now we will introduce deepwalk algo rithms in detailgiven graph g  v e we denote a random walk started at vertex vi as viwe use kvi to represent the kth vertex in the random walk vi the next vertex k1viis generated by uniformly random selection from neighbors of vertex kvi randomwalk sequences have been used for many network analysis tasks such as similaritymeasurement and community detection 2 32deepwalk follows the idea of language modeling to model short random walksequences that is to estimate the likelihood of observing vertex vi given all previousvertices in the random walkpviv1 v2     vi1817to the extent of vertex representation learning we turn to predict vertex vi giventhe representations of all previous verticespviv1 v2     vi1818a relaxation of this formula in language modeling turns to use vertex vi to predictits neighboring vertices viw     vi1 vi1     viw where w is the window size2248network representationthis part of model is named as skip gram model in word embedding learning theneighboring vertices are also called context vertices of the center vertex as anothersimplication deepwalk ignores the order and offset of the vertices and thus predictviw and vi1 in the same way the optimization function of a single vertex of arandom walk can be formulated asminv log pviw     vi1 vi1     viwvi819based on independent assumption the loss function can be rewritten asminvwkwk0log pvikvi820the overall loss function can be obtained by adding up over every vertex in everyrandom walknow we talk about how to predict a single vertex v j given center vertex vi indeepwalk each vertex vi has two representations with the same dimension ver tex representation vi rd and context representation ci rd the probability ofprediction pv jvi is dened by a softmax function over all verticespv jvi expvicj v k1 expvick 821here we come to the parameter learning phase of deepwalk we rst present thepseudocode of the deepwalk framework in algorithm 81algorithm 81 deepwalk algorithmgiven graph g  v e window size w embedding size d walks per vertex n and walk length lfor i  1 2     n dofor vi v dovi randomwalkg vilskip gramv vi  wend forend forwhere randomwalkg vil generates a random walk rooted at vi with length l andskip gramv vi  w function is dened in algorithm 82 where l is the learningrate of stochastic gradient descentnote that the parameter updating rule v  v l jv in skip gram has a com plexity of ov  because in the computation of the gradient of pvkv j as shownin eq821 the denominator has v  terms to compute this complexity is unac ceptable for large scale networks82 network representation225algorithm 82 skip gramr wvi  wfor v j vi dofor vk vi  j w  j  w doif vk  v j thenjv  log pvkv jv  v l jvend ifend forend fortable 82 analogy of deepwalk and word2vecmethodobjectinputoutputword2vecwordsentenceword embeddingdeepwalkvertexrandom walkvertex embeddingto address this problem people proposed hierarchical softmax as a variant oforiginal softmax function the core idea is to map the vertices to a balanced binarytree where each vertex corresponds to a leaf of the tree then the prediction of avertex turns to the prediction of the path from the root to the corresponding leafassume that the path from root to vertex vk is denoted by a sequence of tree nodesb1 b2     blog v and then we havelog pvkv j log v i1log pbiv j822a logistic function can easily implement a binary decision on a tree node hencethe time complexity reduces to olog v  from ov  we can accelerate thealgorithm by using huffman coding to map frequent vertices to the tree nodes thatare close to the root we can also use negative sampling which is used in word2vecto replace hierarchical softmax for speeding upso far we have nished the introduction of the deepwalk algorithm deep walk introduces efcient deep learning techniques into network embedding learningtable82 gives an analogy between deepwalk and word2vec deepwalk outper forms traditional network representation learning methods on network classicationtasks and is also efcient for large scale networks besides the generation of randomwalks can be generalized to nonrandom walk such as the information propagationstreams in the next subsection we will give a detailed proof to demonstrate thecorrelation between deepwalk and matrix factorization2268network representation8221matrix factorization comprehension of deepwalkperozzi et al introduced the skip gram model into the study of social network forthe rst time and designed an algorithm named deepwalk 93 for learning vertexrepresentation on a graph in this subsection we prove that the deepwalk algorithmwith skip gram and softmax model is actually factoring a matrix m where eachentry mi j is the logarithm of the average probability that vertex vi randomly walksto vertex v j in x steps we will explain it latersince the skip gram model does not consider the offset of context vertex andpredict context vertices independently we can regard the random walks as a set ofvertex context pairs the useful information on random walks is the co occurrenceof vertex pairs inside a window given network g  v e we suppose that vertex context set d is generated from random walks where each piece of d is a vertex context pair v c let v be the set of nodes and vc be the set of context nodes inmost cases v  vcconsider a vertex context pair v cnvc denotes the number of times v c appears in d nv  cvc nvc andnc  vv nvc denotes the number of times v and c appears in d note thatd  vvcvc nvca context vertex c vc is represented by a d dimension vector c rd and cis a vc  d matrix where row j is vector cj our goal is to gure out a matrixm  vcperozzi et al implemented the deepwalk algorithm with the skip gram and hier archical softmax model note that hierarchical softmax is a variant of softmax forspeeding the training time in this subsection we give proofs for both negative sam pling and softmax with the skip gram modelnegative sampling approximately maximizes the probability of softmax functionby randomly choosing k negative samples from the context set levy and goldbergshowed that skip gram with the negative sampling model sgns is implicitly fac torizing a word context matrix 69 by assuming that dimensionality d is sufcientlylarge in other words we can assign each product v  c a value independent of theothersin sgns model we havepv c d  sigmoidv  c 11  evc 823suppose we choose k negative samples for each vertex context pair v c accord ing to the distribution pdcn ncnd  then the objective function for sgns can bewritten as82 network representation227o vvcvcnvclog sigmoidv  c  kecn pdlog sigmoidv  cvvcvcnvc log sigmoidv  c  kvvnvcn vcncnd log sigmoidv  cvvcvcnvc log sigmoidv  c  knvncd log sigmoidv  c824denote x  v  c by solving ox  0 we havev  c  x  log nvcdnvnclog k825thus we have mi j  lognvi c j dnvidnc jdlog k mi j can be interpreted as point wisemutual informationpmi of vertex context pair vi c j shifted by log ksince both negative sampling and hierarchical softmax are variants of softmaxwe pay more attention to the softmax model and give a further discussion on it wealso assume that the values of v  c are independentin softmax modelpv c d evccvc evc 826and the objective function iso vvcvcnvc logevccvc evc 827after extracting all terms associated to v  c as ov c we haveov c  nvc logevccvccc evc  evc cvcccnvc logevccvccc evc  evc 828note that o 1vcvvcvc ov c denote x  v  c by solving ox  0for all such x we havev  c  x  log nvcnv bv829where bv can be any real constant since it will be canceled when we computepv c d thus we have mi j  lognvi c j nvi   bvi we will discuss what mi jrepresents in next section2288network representationit is clear that the method of sampling vertex context pairs ie random walksgeneration will affect matrix m in this section we will discuss nvdncd and nvcnvbased on an ideal sampling method for deepwalk algorithmassume the graph is connected and undirected and the window size is w thesampling algorithm is illustrated in algorithm 83 we can easily generalize thissampling method to the directed graph by only adding rwi rw j into dalgorithm 83 ideal vertex context pair sampling algorithmgenerate an innite long random walk denote i as the vertex on position i of  where i  0 1 2   for i  0 1 2    dofor j i  1 i  w doadd i j into dadd j i into dend forend foreach appearance of vertex i will be recorded 2w times in d for undirected graphand w times for directed graph thus we can gure out thatnvid is the frequency ofvi that appears in the random walk which is exactly the pagerank value of vi alsonote thatnvi v j nvi 2w is the expectation times that v j is observed in leftright w neighborsof videnote the transition matrix in pagerank algorithm be p more formally letdegvi be the degree of vertex i pi j 1degvi if i j e and pi j  0 otherwisewe use ei to denote a v  dimension row vector where all entries are zero exceptthe ith entry is 1suppose that we start a random walk from vertex i and use ei to denote theinitial state then eip is the distribution over all the vertices where jth entry is theprobability that vertex vi walks to vertex v j hence jth entry of eipw is the probabilitythat vertex vi walks to vertex v j at exactly w steps thus eip  p2      pw jis the expectation times that v j appears in right w neighbors of vihencenviv jnvi2w  2eip  p2      pw jnviv jnvi eip  p2      pw jw830this equality also holds for a directed graphby setting bvi  log 2w for all i mi j  lognvi v j nvi 2w is logarithm of the expectationtimes that v j appears in leftright w neighbors of viby setting bvi  0 for all i mi j  lognvi v j nvi log eiaa2aw jwis logarithmof the average probability that vertex vi randomly walks to vertex v j in w steps82 network representation2298222discussionso far we have seen many different network representation learning algorithms andwe can gure out some patterns that how network representation methods sharethen we will move forward and see how these patterns match some recent networkembedding algorithmsmost network representation algorithms try to reconstruct a data matrix generatedfrom the graph with vertex embeddings the simplest matrix would be the adjacencymatrix however recovering the adjacency matrix may not be the best choice firstreal world networks are mostly very sparse which means oe  ov  there fore the adjacency matrix will be very sparse as well though the sparseness enablesan efcient algorithm it can harm the performance of vertex representation learningbecause of the deciency of useful information second the adjacency matrix maybe noisy and sensitive a single missing link can completely change the correlationbetween two verticeshence people seek to nd an alternative matrix to replace the adjacency matrixthough implicitly take deepwalk as an example deepwalk models the followingmatrix based on matrix factorization comprehension of deepwalkm  p  p2      pw831wherepi j 1degviif vi v j e0otherwise832compared with the adjacency matrix a the matrix m modeled by deepwalk ismuch denser furthermore the window size parameter w can adjust the density alarger window size models a denser matrix but will slow down the algorithm hencethe window size w works as a harmonic factor to balance efciency and effectivenesson the other hand the matrix m can alleviate the noises in the adjacency matrixconsider two similar vertices vi and v j even though the edge between them ismissing they can still have many co occurrences by appearing inside a window sizeof the same random walksin a real world application direct computation of m may have a high time com plexity when window size w grows thus it is essential to choose a proper w how ever window size w is a discrete parameter and thus the matrix m may grow fromtoo sparse to too dense by changing w by 1 here we can see another benet ofrandom walks random walks used by deepwalk serve as monte carlo simulationsfor approximating matrix m the more random walks you walk the more likely youcan approximate the matrixafter we choose a matrix to model we need to correlate the matrix entry withvertex representations pairs there are two widely used measurements of verticespairs euclidean distance and inner product assume that we want to model the entrymi j given vertex representations vi and v j we can employ2308network representationmi j  f vi v j2mi j  f vi  v j833where function f can be any reasonable matching functions such as sigmoid functionor linear function for our propose actually the inner product vi  v j is used morewidely and would correspond to equivalent matrix factorization methodsthe next phase is to design a proper loss function between mi j and f vi  v jseveral loss functions such as square loss and hinge loss can be employed you canalso design a generative model and maximize the likelihood of matrix mthe nal step of a network representation learning algorithm would be parameterlearning the most frequently used parameter learning method would be stochasticgradient descent sgd other variants of sgd such as adagrad and adadelta canmake the learning phase converge faster in the next subsection we will see somerecent network representation learning algorithms which follow deepwalk we willnd that their models can match all these phases above and have some innovationson building matrix m modifying function f  and changing loss function823matrix factorization based methodswe will focus on two network representation learning algorithms line and grarep13 111 in this subsection they both follow the framework introduced in the lastsubsection8231linetang et al 111 proposed a network embedding model named as line line algo rithm can handle large scale networks with arbitrary types undirected or weightedto model the interaction between vertices line models rst order proximity whichis represented by observed links and second order proximity which is determined byshared neighbors but not links between verticesbefore we introduce the details of the algorithm we can move one step backand see how the idea works the modeling of rst order proximity ie observedlinks is the modeling of the adjacency matrix as we said in the last subsectionthe adjacency matrix is usually too sparse hence the modeling of second orderproximity ie vertices with shared neighbors can serve as complement informationto enrich the adjacency matrix and make it denser the enumeration of all vertexpairs which have common neighbors is time consuming thus it is necessary todesign a sampling phase to handle large scale networks the sampling phase workslike monte carlo simulation to approximate the ideal matrix82 network representation231now we only have two questions how to dene rst order and second orderproximity and how to dene the loss function in other words it is equal to how todene m and loss functionfirst order proximity between vertex u and v is dened as the weight wuv onedge u v if there is no edge between vertex u and v then the rst order proximitybetween them is 0second order proximity between vertex u and v is dened as the similaritybetween their neighborhood network let pu  wu1     wuv  denote the rst order proximity between vertex u and all other vertices then the second order prox imity between u and v is dened as the similarity of pu and pv if they have no sharedneighbors then the second order proximity is zerothen we can introduce line model more specically the joint probabilitybetween vi and v j isp1vi v j 11  expvi  v j834where vi and v j are d dimensional row vectors which indicate the representationsof vertex vi and v jto supervise the probabilities empirical probability is dened as p1i j  wi jw where w  viv je wi j thus our goal is to nd vertex embeddings to approximatewi jw with11expviv j following the idea in last subsection it is equivalent to sayvi  v j  mi j  log wwi j 1the loss function between joint probability p1 and its empirical probability p1 isl1  dkl p1  p1835where dkl   is kl divergence of two probability distributionson the other hand we dene the probability that vertex v j appears in vis contextp2v jvi expc j  viv k1 expck  vi836similarly the empirical probability is dened as p2v jvi  wi jdiwhere di k wik and the loss function isl2 idi dkl p2 vi  p2 vi837the rst order and second order proximity embeddings are trained separatelyand we concatenate the embeddings together after the training phase as vertex rep resentations2328network representation8232grarepnow we turn to another network representation learning algorithm grarep whichdirectly follows the proof of matrix factorization form of deepwalk recall thatwe prove deepwalk is actually factorizing a matrix m where m  log aa2awwgrarep algorithm can be divided into 3 steps get k step transition probability matrix ak for each k  1 2     k get each k step representation concatenate all k step representationsgrarep uses a simple idea ie svd decomposition on ak in the second step toget embeddings as k gets large the matrix m gets denser and thus outputs a betterrepresentation however this algorithm is not very efcient especially when k getslarge824structural deep network methodsdifferent from previous methods that use a shallow neural network model to char acterize the network representations structural deep network embedding sdne125 employs the deeper neural model to model the nonlinearity between vertexembeddings as shown in fig82 the whole model can be divided into two parts1 the rst part is supervised by laplacian eigenmaps which models the rst orderproximity 2 the second part is unsupervised deep neural autoencoder which char acterizes the second order proximity finally the algorithm takes the intermediatelayer which is used for the supervised part as the network representationfirst we will give a brief introduction to deep neural autoencoder a neuralautoencoder requires that the output vector should be as similar to the input vectorgenerally speaking the output cannot be the same with the input vector becausethe dimension of intermediate layers of the autoencoder is much smaller than thatof the input and output layer that is to say a deep autoencoder rst compressesthe input into a low dimensional intermediate vector and then tries to reconstructthe original input vector from the low dimensional intermediate vector once thedeep autoencoder is trained we can say that the intermediate layer is an excellentlow dimensional representation of the original inputs since we can recover the inputvector from itmore formally we assume the input vector is xi then the hidden representationof each layer is dened asy1i sigmoidw1xi  b1yki sigmoidwkyk1i bk k  2 3    83882 network representation233unsupervised componentlocal structure preserved costunsupervised componentlocal structure preserved costxiyi1yikyi1xixjyj1yjkyj1xjlaplacianeigenmapsparameter sharingparameter sharingsupervised componentglobal structurepreserved costvertex ivertex jfig 82 the architecture of structural deep network embedding modelwhere wk and bk are weighted matrix and bias vector of kth layer we assumethat the hidden representation of the kth layer has the minimum dimension afterobtaining yki we can get the output xi by reversing the calculation process then theoptimization objective of autoencoder is to minimize the difference between inputvector xi and output vector xil w b ni1xi xi2839where n is the number of input instancesback to the network representation problem sdne applies the autoencoder toevery vertex the input vector xi of each vertex vi is dened as follows if vertex viand v j are connected then the jth entry xi j  0 otherwise xi j  0 for unweighedgraph if vertex vi v j e xi j  1 then the intermediate layer ykican be seenas the low dimension representation of vertex vi also note that there are much morezero entries in input vectors than positive entries due to the sparity of real worldnetwork therefore the loss of positive entries should be emphasized therefore thenal optimization objective of second proximity modeling can be written asl2nd v i1xi xi bi28402348network representationwhere denotes element wise multiplication and bi j  1 if xi j  0 while bi j   1 if xi j  0we have introduced the unsupervised part modeled by deep autoencoder now weturn to the supervised part the supervised part simply requires that the representationof connected vertices should be close to each other thus the loss function of thispart isl1st v i j1xi jykiykj2841finally the overall loss function included regularization term isl  l2nd  l1st  lreg842where  and  are harmonic hyperparameter and regularization loss lreg is the sumof the square of all parameters the model can be optimized by back propagationin a standard neural network way after the training process ykiis taken as therepresentation of vertex vi825extensions8251network representation with internal informationasymmetric transitivity preserving network representation existing networkrepresentation learning algorithms mostly focus on an undirected graph most ofthe methods cannot handle the directed graph well because they do not accuratelycharacterize the asymmetric property high order proximity preserved embeddinghope 89 is proposed to preserve high order proximities of large scale graphs andcapture the asymmetric transitivity the algorithm further derives a general formula tion that covers multiple popular high order proximity measurements and providesan approximate algorithm with an upper bound of rmse root mean squared errornetwork embedding assumes that the more and the shorter paths from vi to v jthe more similar should be their representation vectors in particular the algorithmassigns two vectors ie source and target vectors for each vertex we denote adja cency matrix as a and the user representations as u  us ut where us rv dand ut rv d are source and target vertex embeddings respectively we denea high order proximity matrix as s where si j is the proximity between vi and v jthen our goal is to approximate the matrix s with the product of us and ut theoptimization objective can be written asminusut s usut 2f84382 network representation235many high order proximity measurements which characterize the asymmetrictransitivity share a general formulation which can be used for the approximation ofthe proximitiess  m1g ml844where mg and ml are both polynomials of matrices now we will take three com monly used high order proximity measurements to illustrate the formula katz index katz index is a weighted summation over the path set between twovertices the computation of the katz index can be written recurrentlys   as   a845where the decay parameter  represents how fast the weight decreases when thelength of paths grows rooted pagerank for rooted pagerank si j is the probability that a random walkfrom vertex vi will locate at v j in the stable state the formula can be written ass  sp  1 i846where  is the probability that a random walk returns to its start point and p is thetransition matrix common neighbors si j is the number of vertexes which is the target of an edgefrom vi and the source of an edge to v j the matrix s can be expressed ass  a2847for the three high order proximity measurements introduced above we summa rize their equivalent form s  m1g ml in the following table table83a simple idea of approximating s with the product of matrices is svd decom position however the direct computation of svd decomposition of matrix s has acomplexity of ov 3 by writing matrix s as m1g ml we do not need to computematrix s directly instead we can do jdgsvd decomposition on mg and ml inde pendently and then use their results to derive the decomposition of s the complexityreduces to ed2 for each iteration of jdgsvdcommunity preserving network representation while previous methods aimat preserving the microscopic structure of a network such as rst  and second ordertable 83 general formula for high order proximity measurementsmeasurementmgmlkatz indexi  a arooted pageranki p1 icommon neighborsia22368network representationproximities wang et al 127 proposed modularized nonnegative matrix factor ization m nmf which encodes the mesoscopic community structure informationinto the network representations the basic idea is to consider the modularity as partof the optimization function recall that the modularity is formulated in eq815 ands is the community indicator matrix then the loss function of modularity part is tominimize trsbssimilar to previous methods m nmf also factorizes an afnity matrix whichencodes rst order and second order proximities specically m nmf takes adja cency matrix a as the rst order proximity matrix a1 and computes the cosinesimilarity of corresponding rows of adjacency matrix a as the second order prox imity matrix a2 m nmf uses a mixture of a1 and a2 as the similarity matrix toconclude the overall optimization function of m nmf isminmusca1  a2 mu2f  s uc2f trsbs848wheres rv k m urv m crkm mi j ui j si j ci j 0 ij trss v  and     0 are harmonic hyperparameters subscript f denotes frobeniusnorm here similarity matrix a1  a2 is factorized into two nonnegative matricesm and u then community representation matrix c in the second term bridges thematrix factorization part and the modularity parta concurrent algorithm community enhanced nrl cnrl 116 117 is apipeline algorithm that learns node community assignment at rst and then reformsthe deepwalk algorithm to incorporate community information specically in therst phase cnrl made an analogy between community detection and topic model ing then cnrl started by generating random walks and fed these vertex sequencesinto latent dirichlet allocation lda algorithm by taking a vertex as a word and atopic as a community cnrl can get a soft assignment of vertex community mem bership then in the second phase both the embedding of a center node and theembedding of its community are used to predict the neighborhood vertices in therandom walk sequences the illustration gure is shown in fig838252network representation with external informationnetwork representation with text information we will present the networkembedding algorithm tadw which further generalizes the matrix factorizationframework to take advantage of text information text associated deepwalktadw 136 incorporates text features of vertices into network representationlearning under the framework of matrix factorization the matrix factorization viewof deepwalk enables the introduction of text information into matrix factorization fornetwork representation learning figure84 shows the main idea of tadw factorizevertex afnity matrix m rv v  into the product of three matrices w rkv h rk ft and text features t r ftv  then tadw concatenates w and ht as2k dimensional representations of vertices82 network representation237vetexembeddingssliding windowcommunityembeddingscommunityembeddingassignedcommunities112 2 3 3 4 4 556677vetex sequencerandom walkson a networkcommunity 1community 2community 3fig 83 the architecture of community preserving network embedding modelfig 84 the architecture oftext associated deepwalkmodelvvvkmwthtftthen the question is how to build vertex afnity matrix m and how to extracttext feature t from the text information following the proof of matrix factorizationform of deepwalk tadw set vertex afnity matrix m to a tradeoff between speedand accuracy factorize the matrix m  a  a22 where a is the row normalizedadjacency matrix for text feature matrix t tadw rst constructs the tf idf matrixfrom the text and then reduces the dimension of the tf idf matrix to 200 via svddecomposition2388network representationreconstructededge vectoredge autoencoderlabel2label5uluvvtranslationmechanismbinary edgevertoredge withmulti labelsfig 85 the architecture of transnet modelformally the model of tadw minimizes the following optimization functionminwh m wht2f  2w2f  h2f849where  is the regularization factor the optimization of parameters are processedby updating w and h iteratively via conjugate gradient descenttransnet most existing nrl methods neglect the semantic information of edgesand simplify the edge as a binary or continuous value transnet algorithm 119considers the label information on the edges instead of nodes in particular transnetis based on translation mechanism shown in fig85in the settings of transnet each edge has a number of binary labels on it then theloss function of transnet consists of two parts one part is the translation loss whichmeasures the distance between u  e and v where u e v stand for the embeddingsof head vertex edge and tail vertex another part is the reconstruction loss of theautoencoder which encodes the labels of an edge into its embedding e and restorethe labels from the embedding after the learning phase we can compute the edgeembedding by subtracting two vertices and use the decoder part of the autoencoderto predict the labels of an unobserved edge82 network representation239semi supervised network representation in this part we introduce severalsemi supervised network representation learning methods that are applied to het erogeneous networks all methods learn vertex embeddings and their classicationlabels simultaneously1 lshm the rst algorithm lshm latent space heterogeneous model 52follows the manifold assumption which assumes that two connected nodes tend tohave similar node embeddings thus the regularization loss which forces connectednodes to have similar representations can be formulated asi jwi jvi v j2850where wi j is the weight of edge vi v jas a semi supervised representation learning algorithm lhsm also needs topredict the classication labels for unlabeled vertices to train the classiers lhsmcomputes the loss of observed labels asi fvi yi851where fvi is the predicted label for vertex vi yi is the observed label for vi and  is the loss function between predicted label and ground truth label speci cally f is a linear function and   is set to hinge lossfinally the objective function isl v  i fvi yi  i jwi jvi v j2852where  is a harmonic hyperparameter the algorithm is optimized via stochasticgradient descent2 node2vec node2vec 38 modies deepwalk by changing the generationof random walks as shown in previous subsections deepwalk generates rootedrandom walks by choosing the next vertex according to a uniform distribution whichcould be improved by using a well designed random walk generation strategynode2vec rst considers two extreme cases of vertex visiting sequences breadth first search bfs and depth first search dfs by restricting the search to nearbynodes bfs characterizes the nearby neighborhoods of center vertices and obtainsa microscopic view of the neighborhood of every node vertices in the sampledneighborhoods of bfs tend to repeat many times which can reduce the variance incharacterizing the distribution of neighboring vertices of the source node in contrastthesamplednodesindfsreectamacro viewoftheneighborhoodwhichisessentialin inferring communities based on homophilynode2vec designs a neighborhood sampling strategy which can smoothly inter polate between bfs and dfs more specically consider a random walk that justwalks through edge t v and now stays at vertex v the walk evaluates the transition2408network representationprobabilities of edge v x to decide the next step node2vec sets the unnormalizedtransition probability to vx  pqt x  wvx wherepqt x 1p if dtx  01 if dtx  11q if dtx  2853and dtx denotes the shortest path distance between vertices t and x p and q areparameters that guide the random walk and control how fast the walk explores andleaves the neighborhood of starting vertex a low p will increase the probability ofrevisiting a vertex and make the random walk focus on local neighborhoods while alowq will encourage the random walk to explore further vertices after the generationof the random walks the rest of the algorithm is almost the same as that of deepwalk3 mmdw max margin deepwalk mmdw 118 utilizes the max marginstrategy in svm to generalize deepwalk algorithm for semi supervised learningspecically mmdw employs the matrix factorization form of deepwalk provedin tadw 136 and further add the max margin constraint which requires that theembeddings of nodes from different labels should be far from each other the opti mization function can be written asminxyw l minxyw ldw  12w2  cti1ist wli xi wj xi e ji i i j854where w  w1 w2     wmt is the weight matrix of svm  is the slack variablese ji  1 if li  j and e ji  0 otherwise and ldw is the matrix factorization formdeepwalk loss functionldw  m xy22  2x22  y22855which is introduced in previous sectionsfigure86 shows the visualization result of the deepwalk and mmdw algorithmon the wiki dataset 103 we can see that the embeddings of nodes from differentclasses are more separable with the help of semi supervised max margin represen tation learning4pteanotheralgorithmcalledptepredictivetextembedding110focuseson text network such as the bibliography network where a paper is a vertex andthe citation relationship between papers forms the edges pte considers networkstructure together with plain text and observed vertex labels pte proposes a semi supervised framework to learn vertex representation and predict unobserved vertexlabels82 network representation241fig 86 a visualization of t sne 2d representations on wiki dataset left deepwalk rightmmdw 118a text network is divided into three bipartite networks word word word document and word label networks we will introduce the denition of the threenetworks in more detailfor the word word network the weight wi j of the edge between word vi and v j isdened as the number of times that the two words co occur in the same context win dows for word document network the weight wi j between word vi and documentd j is dened as the number of times vi appears in document d j for the word labelnetwork the weight wi j of the edge between word vi and class c j is dened aswi j  dld j ndi where ndi is the term frequency of word vi in document d and ldis the class label of document dthen following previous work line given bipartite network g  va vb ethe conditional probability of generating vi va from v j vb is dened aspviv j expv j  viv k1 expvk  vi856similar to line model the loss function is dened as the kl divergence betweenempirical distribution and conditional distribution the optimization objective canbe further formulated asl  viv jewi j log pviv j8572428network representationthen the nal objective can be obtained by summing all three bipartite networkslpte  lww  lwd  lwl858wherelww  viv jewwwi j log pviv j859lwd  viv jewdwi j log pvid j860lwl  viv jewlwi j log pvil j861then the optimization can be done by stochastic gradient descent8253task specic network representationnetwork representation for community detection as shown in spectral cluster ing methods people make their effort to learn community indicator matrix based onmodularity and normalized graph cut the continuous community indicator matrixcan be seen as a k dimensional vertex representation where k is the number of com munities note that modularity and graph cut is dened for nonoverlapping commu nities by alternating a cost function for overlapping communities the idea can alsowork for overlapping community detection in this subsection we will introduce sev eral community detection algorithms these community detection algorithms startby learning a k dimensional nonnegative vertex community afnity matrix and thenderive a hard community assignment for vertices based on the matrix therefore thekey procedure of these algorithms can be regarded as an unsupervised k dimensionalnonnegative vertex embedding learningbigclam 140 is an overlapping community detection method it assumes thatmatrix f rv k is the user community afnity matrix where fvc is the strengthbetween vertex v and community c matrix f is nonnegative and fvc  0 indicatesno afliation bigclam builds a generative model by modeling the probability thatvertexvi connectsv j givenuser communityafnitymatrixfmorespecicallygivenmatrix f bigclam generates an edge between vertex vi and v j with a probabilitypvi v j  1 expfvi  fv j862where fvi is the corresponding row of matrix f for vertex vi and can be seen as therepresentation of vi note that the probability pvi v j has an increasing relationshipwith fvi  fv j  c fvicfv jc which indicates that the more communities a pair ofnodes shared the more likely they are connected82 network representation243for the case that fvi  fv j  0 bigclam adds a background probability  2ev v 1 to the pair of nodes to avoid a zero probabilitythen bigclam tries to maximize the log likelihood of the graph g  v eof i jviv jelog pvi v j i jviv jelog1 pvi v j863which can be reformulated asof i jviv jelog1 expfvi  fv j i jviv jefvi  fv j864the parameters f are learned by projected gradient descent note that the train ing objective can be regarded as a variant of nonnegative matrix factorization themaximization of log likelihood function is an approximation of adjacency matrix aby ff compared with l2 norm loss function the gradient of eq864 can be com puted more efciently for a sparse matrix a which is the most case in the real worlddatasetthe model can also be generalized to asymmetric case 141 that is to replaceeq862 bypvi v j  1 expfvi  hv j865where h is another matrix that has the same size with the matrix f the generativemodel can also consider attributes of vertices by adding attribute terms to eq862798254network representation for visualizationdifferent from previous algorithms that focus on machine learning tasks the algo rithms introduced in this subsection are designed for visualization as a commonlyused data structure the visualization of networks is an important task the dimen sions of representations of vertices are usually 2 or 3 to draw the graphrepresentation learning for network visualization generally follows the followingaesthetic criteria 30 distribute the vertices evenly in the frame minimize edge crossings make edge lengths uniform reect inherent symmetry conform to the framefollowing these criteria graph visualization algorithms build a force directedgraph drawing framework the basic assumption is that there is a spring betweeneach pair of vertices then the optimization objective is to minimize the energy ofthe graph according to hookes law2448network representatione i j12ki jvi v jli j2866where ki j is spring constant vi is the position of vertex vi and li j is the length ofshortest path between vertex vi and v j the intuition is straightforward close verticesshould have close positions in the drawing several algorithms have been proposedto improve this framework 34 54 60 by changing the setting of spring constant ki jor the energy function the parameters can be easily learned via gradient descent8255embedding enhancement via high order proximityapproximationyang et al 137 summarize several existing nrl methods into a unied two stepframework including proximity matrix construction and dimension reduction theyconclude that an nrl method can be improved by exploring higher order proximitieswhen building the proximity matrix then they propose network embedding updateneu algorithm which implicitly approximates higher order proximities with the oretical approximation bound and can be applied to any nrl methods to enhancetheir performances neu can make a consistent and signicant improvement oversome nrl methods with almost negligible running timethe two step framework is summarized as followsstep 1 proximity matrix construction compute a proximity matrix m rv v  which encodes the information of k order proximity matrix where k 1 2     k for example m  1k a  1k a2     1k ak stands for an average com bination of k order proximity matrix for k  1 2     k the proximity matrix m isusually represented by a polynomial of normalized adjacency matrix a of degree kand we denote the polynomial as f a rv v  here the degree k of polynomialf a corresponds to the maximum order of proximities encoded in the proximitymatrix note that the storage and computation of proximity matrix m doesnt nec essarily take ov 2 time because we only need to save and compute the nonzeroentriesstep 2 dimension reduction find network embedding matrix v rv d andcontext embedding c rv d so that the product vcapproximates proximitymatrix m here different algorithms may employ different distance functions tominimize the distance between m and vc for example we can naturally use thenorm of matrix m vcto measure the distance and minimize itspectral clustering deepwalk and grarep can be formalized into the two stepframework now we focus on the rst step and study how to dene the right proximitymatrix for nrlwe summarize the comparisons among spectral clustering sc deepwalk andgrarep in table84 and conclude the following observations82 network representation245table 84 comparisons among three nrl methodsscdeepwalkgrarepproximity matrixlkk1akkak k  1    kcomputationaccurateapproximateaccuratescalabilityyesyesnoperformancelowmiddlehighobservation 81 modeling higher order and accurate proximity matrix can improvethe quality of network representation in other words nrl can benet from exploringa polynomial proximity matrix f a of a higher degreefrom the development of nrl methods it can be seen that deepwalk outperformsspectral clustering because deepwalk considers higher order proximity matricesand the higher order proximity matrices can provide complementary information forlower order proximity matrices grarep outperforms deepwalk because grarepaccurately calculates the k order proximity matrix rather than approximating it bymonte carlo simulation as deepwalk doesobservation 82 accurate computation of high order proximity matrix is not fea sible for large scale networksthe major drawback of grarep is the computation complexity of calculating theaccurate k order proximity matrix in fact the computation of high order proximitymatrix takes ov 2 time and the time complexity of svd decomposition alsoincreases as k order proximity matrix gets dense when k grows in summary thetime complexity of ov 2 is too expensive to handle large scale networksthe rst observation provides the motivation to explore higher order proximitymatrices in nrl models but the second observation indicates that an accurate infer ence of higher order proximity matrices isnt acceptable therefore how to learnnetwork embeddings from approximate higher order proximity matrices efcientlybecomes important to be more efcient the network representations which encodethe information of lower order proximity matrices can be used as our basis to avoidrepeated computations the problem is formalized belowproblem formalization assume that we have normalized adjacency matrix aas the rst order proximity matrix network embedding v and context embedding cwhere v c rv d suppose that the embeddings v and c are learned by the abovenrl framework which indicates that the product vcapproximates a polynomialproximity matrix f a of degree k the goal is to learn a better representation vand c which approximates a polynomial proximity matrix ga with higher degreethan f a also the algorithm should be efcient in the linear time of v  notethat the lower bound of time complexity is ov d which is the size of embeddingmatrix rthere is a simple efcient and effective iterative updating algorithm to solve theabove problem2468network representationmethod given hyperparameter  0 12 normalized adjacency matrix a weupdate v and c as followsv  v  avc  c  ac867the time complexity of computing av and ac is ov d because matrix ais sparse and has ov  nonzero entries thus the overall time complexity of oneiteration of operation eq867 is ov drecall that product of previous embedding v and c approximates polynomialproximity matrix f a of degree k it can be proved that the algorithm can learnbetter embeddings v and c where the product vcapproximates a polynomialproximity matrix ga of degree k  2 bounded by matrix innite normtheorem denote the network and context embedding by v and c and supposethat the approximation between vcand proximity matrix m  f a is boundedby r  f a vcand f  is a polynomial of degree k then the productof updated embeddings v and c from eq867 approximates a polynomial ga f a  2af a  2 a2 f a of degree k  2 with approximation bound r 1  2  2r 94rproof assume that s  f a vcand thus r  sga vc ga v  avc ca ga vcavcvca 2 avca s  as  sa  2 asas as sa 2sa2 r  2r  2r868where the second last equality replaces ga and f a vcby the denitionsof ga and s and the last equality uses the fact that a maxij ai j  1because the summation of each row of a equals to 1in the experimental settings it is assumed that the weight of lower order proxim ities should be larger than higher order proximities because they are more directlyrelated to the original network therefore given ga  f a  2af a  2 a2f a we have 1 2 2  0 which indicates that  0 12 the proof indicatesthat the updated embedding can implicitly approximate a polynomial ga of 2 moredegrees within 94 times matrix innite norm of previous embeddingsalgorithm the update eq867 can be further generalized in two directions firstwe can update embeddings v and c according to eq869v  v  1a v  2 a a vc  c  1ac  2 aac86982 network representation247the time complexity is still ov d but eq869 can obtain higher proximitymatrix approximation than eq867 in one iteration more complex update formulasthat explore further higher proximities than eq869 can also be applied but eq869is used in current experiments as a cost effective choiceanother direction is that the update equation can be processed for t rounds toobtain higher proximity approximation however the approximation bound wouldgrow exponentially as the number of rounds t grows and thus the update cannot bedoneinnitelynotethattheupdateoperationofv andcarecompletelyindependenttherefore only updating network embedding v is enough for nrl the abovealgorithm neu avoids an accurate computation of high order proximity matrixbut can yield network embeddings that actually approximate high order proximitieshence this algorithm can improve the quality of network embeddings efcientlyintuitively eqs867 and 869 allow the learned embeddings to further propagate totheir neighbors hence the proximities of longer distances between vertices will beembedded826applicationsin this part we will introduce common applications for network representation learn ing and their evaluation metrics8261multi label classicationa multi label classication task is the most widely used network representationlearning evaluation task the representations of vertices are considered as vertexfeatures and applied to classiers to predict vertex labels more formally we assumethat there are k labels in total the vertex label relationship can be expressed as abinary matrix m 0 1v k where mi j  1 indicates that vertex vi has jth labeland mi j  0 otherwise specically for the multiclass classication problem eachvertex has exactly one label which means there is only an 1 in each row of matrixm for the evaluation task we set a training ratio which indicates how much percentof vertices have observed labels then our goal is to predict the labels for the verticesin the test setfor unsupervised network representation learning algorithms the labels of thetraining set are not used for embedding learning the network representations arefed to classiers like svm or logistic regression each label will have its classi er for semi supervised learning methods they take the observed vertex labels intoaccount in the representation learning period these algorithms will have their spe cic classiers for label predictiononce the label prediction is done we can move to compute the evaluation met rics for multiclass classication we assume that the number of correctly predictedvertices is vr then the classication accuracy is dened as the ratio of correctly2488network representationpredicted vertices which can be formulated as vrv  for multi label classica tion the precision recall and f1 are the most popular metrics which are computedas followsprecision  ncorrectly predicted labelsnpredicted labelsrecall  ncorrectly predicted labelsnunobserved labelsf1 score  2precision  recallprecision  recall 8708262link predictionlink prediction is another important evaluation task for network representation learn ing because a good network embedding should have the ability to model the afnitybetween vertices for evaluation we randomly pick up edges as training set and leavethe rest as test set cross validation can also be employed for training and testingto make link prediction given the vertex representations we rst need to evaluatethe strength of a pair of vertices the strength between two vertices is evaluatedby computing the similarity between their representations this similarity is usuallycomputed by cosine similarity inner product or square loss which depends on thealgorithm for example if an algorithm uses vi c j22 in their objective functionthen square loss should be used to measure the similarity between vertex represen tations then after we get the similarity of all unobserved links we can rank themfor link prediction there are two signicant metrics for link prediction area underthe receiver operating characteristic curve auc and precisionauc the auc value is the probability that a randomly chosen missing link hasa higher score than a randomly chosen nonexistent link for implementation werandomly select a missing link and a nonexistent link and compare their similarityscore assume that there are n1 times that missing link having a higher score and n2times they have the same score among n independent comparisons then the aucvalue isauc  n1  05n2n871note that for a random network representation the auc value should be 05precision given the ranking of all the non observed links we predict the linkswith top l highest score as predicted ones assume that there are lr links that aremissing links then the precision is dened as lrl8263community detectionfor the network representation based community detection algorithm we rst needto convert the nonnegative vertex representation into the hard assignment of commu 82 network representation249nities assume that we have network representation matrix v rv k where row iof v is the nonnegative embedding of vertex vi for community detection we regardeach dimension of the embeddings as a community that is to say vi j denotes theafnity between vertex vi and community c j for each column of matrix v we seta threshold  and the vertices with afnity score higher than the threshold will beconsidered as a member of the corresponding community the threshold can be setin various ways for example we can set  so that a vertex belongs to a community cif the node is connected to other members of c with an edge probability higher than1n 1401n 1 exp2872which indicates that  log1 1nfor evaluation metrics we have two choices modularity and matching scoremodularity recall that the modularity of a graph q is dened asq 12ei jai j degvidegv j2evi v j873where vi v j  1 if vi and v j belong to the same community and vi v j  0otherwise a larger modularity indicates a better community detection algorithmmatching score this is a more sophisticated evaluation metric for communitydetection to compare a set of ground truth communities cto a set of detectedcommunities c we rst need to match each detected community to the most similarground truth community on the other side we also nd the most similar detectedcommunity for each ground truth community then the nal performance is evaluatedby the average of both sides12cci cmaxc jc ci  c j 12cc jcmaxci cci  c j874where ci  c j is a similarity measurement of ground truth community ci anddetected community c j such as jaccard similarity the score is between 0 and 1where 1 indicates a perfect matching of ground truth communities8264recommender systemrecommender systems aim at recommending items eg products movies or loca tions for users and cover a wide range of applications in many cases an applicationcomes with an associated social network between users now we will present anexample to show how to use the idea of network representation for building recom mender systems in location based social networks2508network representationa friendship networkb user trajectoryfig 87 an illustrative example for the data in lbsns a link connections represent the friendshipbetween users b a trajectory generated by a user is a sequence of chronologically ordered check inrecords 138the accelerated growth of mobile trajectories in location based services bringsvaluable data resources to understand users moving behaviors apart from recordingthe trajectory data another major characteristic of these location based services isthat they also allow the users to connect whomever they like or are interested in asshown in fig87 a combination of social networking and location based servicesis called as location based social networks lbsn as shown in 21 locationsthat are frequently visited by socially related persons tend to be correlated whichindicates the close association between social connections and trajectory behaviorsof users in lbsns in order to better analyze and mine lbsn data we need to havea comprehensive view to analyze and mine the information from the two aspectsie the social network and mobile trajectory dataspecically jntm 138 is proposed to model both social networks and mobiletrajectories jointly the model consists of two components the construction of socialnetworks and the generation of mobile trajectories first jntm adopts a networkembedding method for the construction of social networks where a networking rep resentation can be derived for a user secondly jntm considers four factors thatinuence the generation process of mobile trajectories namely user visit prefer ence inuence of friends short term sequential contexts and long term sequentialcontexts then jntm uses real valued representations to encode the four factors andset two different user representations to model the rst two factors a visit interestrepresentation and a network representation to characterize the last two contextsjntm employs the rnn and gru models to capture the sequential relatedness inmobile trajectories at different levels ie short term or long term finally the twocomponents are tied by sharing user network representations the overall model isillustrated in fig8882 network representation251layer 1outputlayer 2representationlayer 3deeper neural networkgrurnnfriendshipuser interestlong term contextshort term contextnetwork gtrajectory tfig 88 the architecture of jntm model8265information diffusion predictioninformation diffusion prediction is an important task which studies how informationitems spread among users the prediction of information diffusion also known ascascade prediction has been studied over a wide range of applications such asproduct adoption 67 epidemiology 124 social networks 63 and the spread ofnews and opinions 68as shown in fig89 microscopic diffusion prediction aims at guessing the nextinfected user while macroscopic diffusion prediction estimates the total numbers ofinfected users during the diffusion process also an underlying social graph amongusers will be available when information diffusion occurs on a social network servicethe social graph will be considered as additional structural inputs for diffusionpredictionforest 139 is the rst work to address both microscopic and macroscopicpredictions as shown in fig810 forest proposes a structural context extrac fig 89 illustrative examples for microscopic next infected user prediction left and macroscopiccascade size prediction right 1392528network representationfig 810 an illustrative example of structural context extraction of the orange node by neighborsampling and feature aggregation 139tion algorithm that was originally introduced for accelerating graph convolutionalnetworks 41 to build an rnn based microscopic cascade model for each userv we rst sample z users u1 u2     uz from v and its neighbors n v thenwe update its feature vector by aggregating the neighborhood features the updateduser feature vector encodes structural information by aggregating features from vsrst order neighbors the operation can also be processed recursively to explore alarger neighborhood of user v empirically a two step neighborhood exploration istime efcient and enough to give promising resultsforest further incorporates the ability of macroscopic prediction ie esti mating the eventual size of a cascade into the model by reinforcement learning themethod can be divided into four steps a encode observed k users by a microscopiccascade model b enable the microscopic cascade model to predict the size of a cas cade by cascade simulations c use mean square log transformed error msleas the supervision signal for macroscopic predictions and d employ a reinforce ment learning framework to update parameters through policy gradient algorithmthe overall workow is illustrated in fig81183graph neural networkswe now give a short introduction to graph neural networks for nrl partially basedon our review 161 and tutorial 162 whose publishing agreement allows the authorsto reuse these contents83 graph neural networks253h1h2hkmicroscopic cascade modelu1u2uka feed observed k users intomicroscopic cascade modelstopb cascade simulations by samplingd update parameters bypolicy gradientsc mslerewardfig 811 the workow of adopting microscopic cascade model for macroscopic size predictionby reinforcement learning831motivationsgraph neural networks gnns are deep learning based methods that operate ongraph domain due to its convincing performance and high interpretability gnnhas been a widely applied graph analysis method recently in this subsection we willillustrate the fundamental motivations of graph neural networksin recent years cnns 65 have made breakthroughs in various machine learningareas especially in the area of computer vision and started the revolution of deeplearning 64 cnns are capable of extracting multiscale localized features and thesefeatures are used to generate more expressive representations as we are going deeperinto cnns and graphs we found the keys of cnns local connection shared weightsand the use of multilayer 64 these are also of great importance in solving problemsof graph domain because 1 graphs are the most typical locally connected structure2 shared weights reduce the computational cost compared with traditional spectralgraph theory 23 and 3 multilayer structure is the key to deal with hierarchicalpatterns which captures the features of various sizes however cnns can onlyoperate on regular euclidean data like images 2d grid and text 1d sequencewhile these data structures can be regarded as instances of graphs therefore it isstraightforward to think of nding the generalization of cnns to graphs as shownin fig812 it is hard to dene localized convolutional lters and pooling operatorswhich hinders the transformation of cnn from euclidean domain to non euclideandomainthe other motivation comes from network embedding 12 24 37 42 149 inthe eld of graph analysis traditional machine learning approaches usually rely onhand engineered features and are limited by its inexibility and high cost follow ing the idea of representation learning and the success of word embedding 81deepwalk 93 which is regarded as the rst graph embedding method based on2548network representationfig 812 left image in euclidean space right graph in non euclidean space 155representation learning applies skip gram model 81 on the generated randomwalks similar approaches such as node2vec 38 line 111 and tadw 136also achieved breakthroughs however these methods suffer from two severe draw backs 42 first no parameters are shared between nodes in the encoder which leadsto computational inefciency since it means the number of parameters grows linearlywith the number of nodes second the direct embedding methods lack the ability ofgeneralization which means they cannot deal with dynamic graphs or generalize tonew graphsbased on cnns and network embedding graph neural networks gnns areproposed to collectively aggregate information from graph structure thus they canmodel input andor output consisting of elements and their dependency further thegraph neural networks can simultaneously model the diffusion process on the graphwith the rnn kernelin the rest of this section we will rst introduce several typical variants of graphneural networks such as graph convolutional networks gcns graph attentionnetworks gats and graph recurrent networks grns then we will introduceseveral extensions to the original model and nally we will give some examples ofapplications that utilize graph neural networks832graph convolutional networksgraph convolutional networks gcns aim to generalize convolutions to the graphdomain advances in this direction are often categorized as spectral approaches andspatial nonspectral approaches83 graph neural networks2558321spectral approachesspectral approaches work with a spectral representation of the graphsspectral network bruna et al 11 proposes the spectral network the convolu tion operation is dened in the fourier domain by computing the eigendecompositionof the graph laplacian the operation can be dened as the multiplication of a signalx rn a scalar for each node with a lter g diag parameterized by  rng x  ugu t x875where u is the matrix of eigenvectors of the normalized graph laplacian l  in d12 ad12  uu t d is the degree matrix and a is the adjacency matrix of thegraph with a diagonal matrix of its eigenvalues this operation results in potentially intense computations and non spatially local ized lters henaff et al 47 attempts to make the spectral lters spatially localizedby introducing a parameterization with smooth coefcientschebnet hammond et al 43 suggests that g can be approximated by atruncated expansion in terms of chebyshev polynomials tkx up to kth order thusthe operation isg x kk0ktk lx876with l  2max l in max denotes the largest eigenvalue of l  rk is nowa vector of chebyshev coefcients the chebyshev polynomials are dened astkx  2xtk1x tk2x with t0x  1 and t1x  x it can be observedthat the operation is k localized since it is a kth order polynomial in the laplaciandefferrard et al 28 proposes the chebnet it uses this k localized convolution todene a convolutional neural network which could remove the need to compute theeigenvectors of the laplaciangcn kipf and welling 59 limits the layer wise convolution operation to k  1to alleviate the problem of overtting on local neighborhood structures for graphswith very wide node degree distributions it further approximates max 2 and theequation simplies tog x 0x  1 l in x  0x 1d12 ad12 x877with two free parameters 0 and 1 after constraining the number of parameterswith   0  1 we can obtain the following expressiong x in  d12 ad12x878note that stacking this operator could lead to numerical instabilities andexplodingvanishinggradients59introducestherenormalizationtrick2568network representationin  d12 ad12 d12 a d12  with a  a  in and dii  j ai j finally 59generalizes the denition to a signal x rnc with c input channels and f ltersfor feature maps as followsh  f  d12 a d12 xw879where w rcf is a matrix of lter parameters h rnf is the convolved signalmatrix and f  is the activation functionthe gcn layer can be stacked for multiple times so that we have the equationht  f  d12 a d12 ht1wt1880where the superscripts t and t 1 denote the layers of the matrices the initial matrixh0 could be x after l layers we can use the nal embedding matrix hl and areadout function to get the nal output matrix zz  readouth l881where the readout function can be any machine learning methods such as mlpfinally as a semi supervised algorithm gcn uses the feature matrix at the toplayer z which has the same dimension with the total number of labels to predict thelabels of all observed labels the loss function can be written asl  lylfyl f ln zl f 882where yl is the set of node indices that have observed labels figure813 shows thealgorithm of gcnanswergreenoutput layerinput layerhiddenlayerscfx1z1z2z3z4y1y4x2x3x4fig 813 the architecture of graph convolutional network model83 graph neural networks2578322spatial approachesin all of the spectral approaches mentioned above the learned lters depend onthe laplacian eigenbasis which depends on the graph structure that is a modeltrained on a specic structure could not be directly applied to a graph with a differentstructurespatial approaches dene convolutions directly on the graph operating on spa tially close neighbors the major challenge of spatial approaches is dening the con volution operation with differently sized neighborhoods and maintaining the localinvariance of cnnsneural fps duvenaud et al 31 uses different weight matrices for nodes withdifferent degreesxt  ht1vnvi1ht1ihtv f wtnvxt883where wtnv is the weight matrix for nodes with degree nv at layer t and the maindrawback of the method is that it cannot be applied to large scale graphs with morenode degreesin the following description of other models we use htvto denote the hiddenstate of node v at layer t nv denotes the neighbor set of node v and nv denotes thesize of the setdcnn atwood and towsley 4 proposes the diffusion convolutional neuralnetworks dcnns transition matrices are used to dene the neighborhood fornodes in dcnn for node classication it hash  fwc p x884where is the element wise multiplication and x is an n  f matrix of inputfeatures p is an n  k  n tensor which contains the power series p p2pk of matrix p and p is the degree normalized transition matrix from the graphsadjacency matrix a each entity is transformed to a diffusion convolutional rep resentation which is a k  f matrix dened by k hops of graph diffusion overf features and then it will be dened by a k  f weight matrix and a nonlin ear activation function f  finally h which is n  k  f denotes the diffusionrepresentations of each node in the graphdgcn zhuang and ma 158 proposes the dual graph convolutional networkdgcn to consider the local consistency and global consistency of graphs jointly ituses two convolutional networks to capture the localglobal consistency and adoptsan unsupervised loss to ensemble them the rst convolutional network is the sameas eq880 and the second network replaces the adjacency matrix with positivepoint wise mutual information ppmi matrix2588network representationht  f d12p x p d12p h t1w885where x p is the ppmi matrix and dp is the diagonal degree matrix of x pgraphsage hamilton et al 41 proposes the graphsage a general induc tive framework the framework generates embeddings by sampling and aggregatingfeatures from a nodes local neighborhoodhtnv  aggregatetht1u u nvhtv f wtht1v htnv886however 41 does not utilize the full set of neighbors in eq886 but a xed size set of neighbors by uniformly sampling and 41 suggests three aggregatorfunctions mean aggregator it could be viewed as an approximation of the convolutionaloperation from the transductive gcn framework 59 so that the inductive versionof the gcn variant could be derived byhtv fw  meanht1v ht1uu nv887the mean aggregator is different from other aggregators because it does not per form the concatenation operation which concatenates ht1vand htnv in eq886 itcan be viewed as a form of skip connection 46 and can achieve better perfor mance lstm aggregator hamilton et al 41 also uses an lstm based aggregator whichhas a larger expressive capability however lstms process inputs in a sequentialmanner so that they are not permutation invariant hamilton et al 41 adaptslstms to operate on an unordered set by permutating nodes neighbors pooling aggregator in the pooling aggregator each neighbors hidden state is fedthrough a fully connected layer and then a max pooling operation is applied to theset of the nodes neighborshtnv  max f wpoolht1u b u nv888note that any symmetric functions could be used in place of the max poolingoperation hereother methods there are still many other spatial methods the patchy sanmodel 86 rst extracts exactly k nodes for each node and normalizes them thenthe convolutional operation is applied to the normalized neighborhood lgcn 35leverages cnns as aggregators it performs max pooling on nodes neighborhoodmatrices to get top k feature elements and then applies 1 d cnn to compute hid den representations monti et al 82 proposes a spatial domain model moneton non euclidean domains which could generalize several previous techniques83 graph neural networks259the geodesic cnn gcnn 78 and anisotropic cnn acnn 10 on manifoldsor gcn 59 and dcnn 4 on graphs could be formulated as particular instancesof monet our readers can refer to their papers for more details833graph attention networksthe attention mechanism has been successfully used in many sequence based taskssuch as machine translation 5 36 121 machine reading 19 etc many worksfocus on generalizing the attention mechanism to the graph domaingat velickovic et al 122 proposes a graph attention network gat whichincorporates the attention mechanism into the propagation step specically it usesthe self attention strategy and each nodes hidden state is computed by attendingover its neighborsvelickovic et al 122 denes a single graph attentional layer and constructsarbitrary graph attention networks by stacking this layer the layer computes thecoefcients in the attention mechanism of the node pair i j byi j expleakyreluawht1i wht1jkni expleakyreluawht1i wht1k889where i j is the attention coefcient of node j to i w rff is the weight matrixof a shared linear transformation which applied to every node a r2f is the weightvector it is normalized by a softmax function and the leakyrelu nonlinearity withnegative input slop 02 is appliedthen the nal output features of each node can be obtained by after applying anonlinearity f hti fjnii jwht1j890moreover the layer utilizes the multi head attention similarly to 121 to stabilizethe learning process it applies k independent attention mechanisms to compute thehidden states and then concatenates their featuresor computes the average resultingin the following two output representationshti kk1 fjniki jwkht1j891hti f1kkk1jniki jwkht1j8922608network representationwhere ki j is normalized attention coefcient computed by the kth attention mecha nism is the concatenation operationthe attention architecture in 122 has several properties 1 the computation ofthe node neighbor pairs is parallelizable thus the operation is efcient 2 it candeal with nodes that have different degrees by assigning reasonable weights to theirneighbors 3 it can be applied to the inductive learning problems easilygaan besides gat gated attention network gaan 150 also uses themulti head attention mechanism however it uses a self attention mechanism togather information from different heads to replace the average operation of gat834graph recurrent networksseveral works are attempting to use the gate mechanism like gru 20 or lstm 48in the propagation step to release the limitations induced by the vanilla gnn architec ture and improve the effectiveness of the long term information propagation acrossthe graph we call these methods graph recurrent networks grns and we willintroduce some variants of grns in this subsectionggnnlietal72proposesthegatedgraphneuralnetworkggnnwhichusesthe gate recurrent units gru in the propagation step it follows the computationsteps from recurrent neural networks for a xed number of l steps then it back propagates through time to compute gradientsspecically the basic recurrence of the propagation model isatvav ht11   ht1n bztv sigmoidwzatv  uzht1vrtv sigmoidwratv  urht1v893htv tanhwatv  urtv ht1vhtv1 ztvht1v ztv htv the node v rst aggregates message from its neighbors where av is the sub matrix of the graph adjacency matrix a and denotes the connection of node v withits neighbors then the hidden state of the node is updated by the gru like functionusing the information from its neighbors and the hidden state from the previoustimestep a gathers the neighborhood information of node v z and r are the updateand reset gateslstms are also used similarly as gru through the propagation process based ona tree or a graphtree lstm tai et al 109 proposes two extensions to the basic lstm architec ture the child sum tree lstm and the n ary tree lstm like in standard lstmunits each tree lstm unit indexed by v contains input and output gates iv and ova memory cell cv and hidden state hv the tree lstm unit replaces the single forget83 graph neural networks261gate by a forget gate fvk for each child k allowing node v to select information fromits children accordingly the equations of the child sum tree lstm areht1vknvht1kitv  sigmoidwixtv  uiht1v biftvk  sigmoidw f xtv  u f ht1k b f otv  sigmoidwoxtv  uoht1v bo894utv  tanhwuxtv  uuht1v buctv  itv utv knvftvk ct1khtv  otv tanhctvwhere xtv is the input vector at time t in the standard lstm settingin a specic case if each nodes number of children is at most k and these childrencan be ordered from 1 to k then the n ary tree lstm can be applied for nodev htvk and ctvk denote the hidden state and memory cell of its kth child at time trespectively the transition equations are the followingitv  sigmoidwixtv kl1uilht1vl biftvk  sigmoidw f xtv kl1u fklht1vl b f otv  sigmoidwoxtv kl1uol ht1vl bo895utv  tanhwuxtv kl1uul ht1vl buctv  itv utv kl1ftvl ct1vl htv  otv tanhctvcompared to the child sum tree lstm the n ary tree lstm introduces sep arate parameters for each child k these parameters allow the model to learn morene grained representations conditioning on each nodes childrengraph lstm the two types of tree lstms can be easily adapted to the graphthe graph structured lstm in 148 is an example of the n ary tree lstm applied2628network representationto the graph however it is a simplied version since each node in the graph has atmost 2 incoming edges from its parent and sibling predecessor peng et al 92proposes another variant of the graph lstm based on the relation extraction taskthe main difference between graphs and trees is that edges of graphs have theirlabels and 92 utilizes different weight matrices to represent different labelsitv  sigmoidwixtv knvuimvkht1k biftvk  sigmoidw f xtv  u fmvkht1k b f otv  sigmoidwoxtv knvuomvkht1k bo896utv  tanhwuxtv knvuumvkht1k buctv  itv utv knvftvk ct1khtv  otv tanhctvwhere mv k denotes the edge label between node v and kbesides 74 proposes a graph lstm network to address the semantic objectparsing task it uses the condence driven scheme to adaptively select the startingnode and determine the node updating sequence it follows the same idea of general izing the existing lstms into the graph structured data but has a specic updatingsequence while the methods we mentioned above are agnostic to the order of nodessentence lstm zhang et al 152 proposes the sentence lstm s lstm forimproving text encoding it converts text into a graph and utilizes the graph lstmto learn the representation the s lstm shows strong representation power in manynlp problems835extensionsin this subsection we will talk about some extensions of graph neural networks8351skip connectionmany applications unroll or stack the graph neural network layer aiming to achievebetter results as more layers ie k layers make each node aggregate more informa tion from neighbors k hops away however it has been observed in many experimentsthat deeper models could not improve the performance and deeper models could even83 graph neural networks263perform worse 59 this is mainly because more layers could also propagate thenoisy information from an exponentially increasing number of expanded neighbor hood membersa straightforward method to address the problem the residual network 45 canbe found from the computer vision community nevertheless even with residualconnections gcns with more layers do not perform as well as the 2 layer gcn onmany datasets 59highway network rahimi et al 96 borrows ideas from the highway net work 159 and uses layer wise gates to build a highway gcn the input of eachlayer is multiplied by the gating weights and then summed with the outputt ht  sigmoidwtht  btht1  ht1 t ht  ht 1 t ht897by adding the highway gates the performance peaks at four layers in a specicproblem discussed in 96 the column network cln proposed in 94 also utilizesthe highway network however it has a different function to compute the gatingweightsjump knowledge network xu et al 134 studies properties and resulting limi tations of neighborhood aggregation schemes it proposes the jump knowledge net work which could learn adaptive structure aware representations the jump knowl edge network selects from all of the intermediate representations whichjump tothe last layer for each node at the last layer which enables the model to select effec tive neighborhood information for each node xu et al 134 uses three approachesof concatenation max pooling and lstm attention in the experiments to aggre gate information the jump knowledge network performs well on the experimentsin social bioinformatics and citation networks it can also be combined with modelslike graph convolutional networks graphsage and graph attention networks toimprove their performance8352hierarchical poolingin the area of computer vision a convolutional layer is usually followed by a poolinglayer to get more general features similar to these pooling layers much work focuseson designing hierarchical pooling layers on graphs complicated and large scalegraphs usually carry rich hierarchical structures that are of great importance fornode level and graph level classication tasksto explore such inner features edge conditioned convolution ecc 106designs its pooling module with the recursively downsampling operation the down sampling method is based on splitting the graph into two components by the sign ofthe largest eigenvector of the laplacian2648network representationdiffpool 144 proposes a learnable hierarchical clustering module by trainingan assignment matrix in each layersl  softmaxgnnlpoolal vl898where vl is node features and al is coarsened adjacency matrix of layer l8353neighborhood samplingthe original graph convolutional neural network has several drawbacks specicallygcn requires the full graph laplacian which is computationally consuming for largegraphs furthermore the embedding of a node at layer l is computed recursively bythe embeddings of all its neighbors at layer l 1 therefore the receptive eld of asingle node grows exponentially with respect to the number of layers so computinggradient for a single node costs a lot finally gcn is trained independently for axed graph which lacks the ability for inductive learninggraphsage 41 is a comprehensive improvement of the original gcn tosolve the problems mentioned above graphsage replaced full graph laplacianwith learnable aggregation functions which are crucial to perform message passingand generalize to unseen nodes as shown in eq886 they rst aggregate neighbor hood embeddings concatenate with target nodes embedding then propagate to thenext layer with learned aggregation and propagation functions graphsage couldgenerate embeddings for unseen nodes also graphsage uses neighbor samplingto alleviate the receptive eld expansionpinsage 143 proposes importance based sampling method by simulating ran dom walks starting from target nodes this approach chooses the top t nodes withthe highest normalized visit countsfastgcn 16 further improves the sampling algorithm instead of samplingneighbors for each node fastgcn directly samples the receptive eld for eachlayer fastgcn uses importance sampling which the important factor is calculatedas belowqv 1nvunv1nu899adapt in contrast to xed sampling methods above 51 introduces a parameter ized and trainable sampler to perform layer wise sampling conditioned on the formerlayer furthermore this adaptive sampler could nd optimal sampling importanceand reduce variance simultaneously83 graph neural networks2658354various graph typesin the original gnn 101 the input graph consists of nodes with label informationand undirected edges which is the simplest graph format however there are manyvariants of graphs in the world in the following we will introduce some methodsdesigned to model different kinds of graphsdirected graphs the rst variant of the graph is directed graphs undirectededge which can be treated as two directed edges shows that there is a relation betweentwo nodes however directed edges can bring more information than undirectededges for example in a knowledge graph where the edge starts from the head entityand ends at the tail entity the head entity is the parent class of the tail entity whichsuggests we should treat the information propagation process from parent classesand child classes differently dgp 55 uses two kinds of the weight matrix wpand wc to incorporate more precise structural information the propagation rule isshown as followsht  f d1p ap f d1c acht1wcwp8100where d1p ap d1c ac are the normalized adjacency matrix for parents and childrenrespectivelyheterogeneous graphs the second variant of the graph is a heterogeneousgraph where there are several kinds of nodes the simplest way to process theheterogeneous graph is to convert the type of each node to a one hot feature vectorwhich is concatenated with the original featurewhats more graphinception 151 introduces the concept of metapath into thepropagation on the heterogeneous graph with metapath we can group the neighborsaccording to their node types and distances for each neighbor group graphinceptiontreats it as a subgraph in a homogeneous graph to do propagation and concatenatesthe propagation results from different homogeneous graphs to do a collective noderepresentationrecently128proposestheheterogeneousgraphattentionnetworkhan which utilizes node level and semantic level attention and the model hasthe ability to consider node importance and metapaths simultaneouslygraphs with edge information in another variant of graph each edge hasadditional information like the weight or the type of the edge we list two ways tohandle this kind of graphsfirstly we can convert the graph to a bipartite graph where the original edgesalso become nodes and one original edge is split into two new edges which meansthere are two new edges between the edge node and beginend nodes the encoderof g2s 7 uses the following aggregation function for neighborshtv f1nvunvwrrtv ht1u br8101where wr and br are the propagation parameters for different types of edgesrelations2668network representationsecondly we can adapt different weight matrices for the propagation of differentkinds of edges when the number of relations is huge r gcn 102 introduces twokinds of regularization to reduce the number of parameters for modeling amounts ofrelations basis  and block diagonal decomposition with the basis decompositioneach wr is dened as followswr bb1rbmb8102here each wr is a linear combination of basis transformations mb rdindoutwith coefcients rb in the block diagonal decomposition r gcn denes each wrthrough the direct sum over a set of low dimensional matrices which needs moreparameters than the rst onedynamic graphs another variant of the graph is dynamic graph which hasa static graph structure and dynamic input signals to capture both kinds of infor mation dcrnn 71 and stgcn 147 rst collect spatial information by gnnsthen feed the outputs into a sequence model like sequence to sequence model orcnns differently structural rnn 53 and st gcn 135 collect spatial and tem poral messages at the same time they extend static graph structure with temporalconnections so they can apply traditional gnns on the extended graphs836applicationsgraph neural networks have been explored in a wide range of problem domains acrosssupervised semi supervised unsupervised and reinforcement learning settings inthis section we simply divide the applications into three scenarios 1 structuralscenarios where the data has explicit relational structure such as physical systemsmolecular structures and knowledge graphs 2 nonstructural scenarios where therelational structure is not explicit include image text etc 3 other applicationscenarios such as generative models and combinatorial optimization problems notethat we only list several representative applications instead of providing an exhaustivelist we further give some examples of gnns in the task of fact verication andrelation extraction figure814 illustrates some application scenarios of graph neuralnetworks8361structural scenariosin the following we will introduce gnns applications in structural scenarios wherethe data are naturally performed in the graph structure for example gnns arewidely being used in social network prediction 41 59 trafc prediction 25 96recommender systems 120 143 and graph representation 144 specically we83 graph neural networks267fig 814 application scenarios of graph neural network 155are discussing how to model real world physical systems with object relationshipgraphs how to predict the chemical properties of molecules and biological interactionproperties of proteins and the applications of gnns on knowledge graphsphysics modeling real world physical systems is one of the most fundamentalaspects of understanding human intelligence by representing objects as nodes andrelations as edges we can perform gnn based reasoning about objects relationsand physics in a simplied but effective waybattaglia et al 6 proposes interaction networks to make predictions and infer encesaboutvariousphysicalsystemsobjectsandrelationsarerstfedintothemodelas input then the model considers the interactions and physical dynamics to predictnew states they separately model relation centric and object centric models mak ing it easier to generalize across different systems in commnet 107 interactionsare not modeled explicitly instead an interaction vector is obtained by averaging allother agents hidden vectors vain 49 further introduced attentional methods intothe agent interaction process which preserves both the complexity advantages andcomputational efciency as well2688network representationvisual interaction networks 132 can make predictions from pixels it learns astate code from two consecutive input frames for each object then after addingtheir interaction effect by an interaction net block the state decoder converts statecodes to the next steps statesanchez gonzalez et al 99 proposes a graph network based model which couldeither perform state prediction or inductive inference the inference model takespartially observed information as input and constructs a hidden graph for implicitsystem classicationmolecular fingerprints molecular ngerprints are feature vectors representingmolecules which are important in computer aided drug design traditional molecu lar ngerprint discovering relies on heuristic methods which are hand crafted andgnns can provide more exible approaches for better ngerprintsduvenaud et al 31 propose neural graph ngerprints neural fps that calculatesubstructure feature vectors via gcn and sum to get overall representation theaggregation function is introduced in eq883kearnes et al 56 further explicitly models atom and atom pairs independentlyto emphasize atom interactions it introduces edge representation etuv instead ofaggregation function ie htnv  unv etuv the node update function isht1v reluw1reluw0htu  htnv8103while the edge update function iset1uv reluw4reluw2etuv reluw3htv  htu 8104protein interface prediction fout et al 33 focuses on the task named proteininterface prediction which is a challenging problem with critical applications indrug discovery and design the proposed gcn based method respectively learnsligand and receptor protein residue representation and merges them for pair wiseclassicationgnn can also be used in biomedical engineering with protein protein inter action network 97 leverages graph convolution and relation network for breastcancer subtype classication zitnik et al 160 also suggest a gcn based modelfor polypharmacy side effects prediction their work models the drug and proteininteraction network and separately deals with edges in different typesknowledge graph hamaguchi et al 40 utilizes gnns to solve the out of knowledge base ookb entity problem in knowledge base completion kbcthe ookb entities in 40 are directly connected to the existing entities thus theembeddings of ookb entities can be aggregated from the existing entities themethod achieves satisfying performance both in the standard kbc setting and theookb settingwang et al 130 utilize gcns to solve the cross lingual knowledge graph align ment problem the model embeds entities from different languages into a uniedembedding space and aligns them based on the embedding similarity83 graph neural networks2698362nonstructural scenariosin this section we will talk about applications on nonstructural scenarios such asimage text programming source code 1 72 and multi agent systems 49 58107 we will only give a detailed introduction to the rst two scenarios due to thelength limit roughly there are two ways to apply the graph neural networks onnonstructural scenarios 1 incorporate structural information from other domainsto improve the performance for example using information from knowledge graphsto alleviate the zero shot problems in image tasks 2 infer or assume the relationalstructure in the scenario and then apply the model to solve the problems dened ongraphs such as the method in 152 which models text as graphsimage classication image classication is a fundamental and essential taskin the eld of computer vision which attracts much attention and has many famousdatasets like imagenet 62 recent progress in image classication benets from bigdata and the strong power of gpu computation which allows us to train a classierwithout extracting structural information from images however zero shot and few shot learning become more and more popular in the eld of image classicationbecause most models can achieve similar performance with enough data there areseveral works leveraging graph neural networks to incorporate structural informationin image classicationfirst knowledge graphs can be used as extra information to guide zero shot recog nition classication 55 129 wang et al 129 builds a knowledge graph whereeach node corresponds to an object category and takes the word embeddings of nodesas input for predicting the classier of different categories as the over smoothingeffect happens with the deep depth of convolution architecture the 6 layer gcn usedin 129 will wash out much useful information in the representation to solve thesmoothing problem in the propagation of gcn 55 uses single layer gcn with alarger neighborhood which includes both one hop and multi hop nodes in the graphand it proved effective in building a zero shot classier with existing onesexcept for the knowledge graph the similarity between images in the dataset isalso helpful for few shot learning 100 satorras and estrach 100 propose to builda weighted fully connected image network based on the similarity and do messagepassing in the graph for few shot recognition as most knowledge graphs are largefor reasoning 77 selects some related entities to build a subgraph based on theresult of object detection and apply ggnn to the extracted graph for predictionbesides 66 proposes to construct a new knowledge graph where the entities are allthe categories and they dened three types of label relations super subordinatepositive correlation and negative correlation and propagate the condence of labelsin the graph directlyvisual reasoning computer vision systems usually need to perform reasoningby incorporating both spatial and semantic information so it is natural to generategraphs for reasoning tasksa typical visual reasoning task is visual question answering vqa 114respectively constructs image scene graph and question syntactic graph then itapplies ggnn to train the embeddings for predicting the nal answer despite spatial2708network representationconnections among objects 87 builds the relational graphs conditioned on thequestions with knowledge graphs 83 131 can perform ner relation explorationand a more interpretable reasoning processother applications of visual reasoning include object detection interaction detec tion and region classication in object detection 39 50 gnns are used to calcu late roi features in interaction detection 53 95 gnns are message passing toolsbetween human and objects in region classication 18 gnns perform reasoningon graphs which connects regions and classestext classication text classication is an essential and classical problem innatural language processing the classical gcn models 4 28 41 47 59 82 andgat model 122 are applied to solve the problem but they only use the structuralinformation between the documents and they do not use much text informationpeng et al 91 propose a graph cnn based deep learning model it rst turnstexts to graph of words then the graph convolution operations in 347 are usedon the word graph zhang et al 152 propose the sentence lstm to encode textthe whole sentence is represented in a single state which contains a global sentence level state and several substates for individual words it uses the global sentence levelrepresentation for classication tasksthese methods either view a document or a sentence as a graph of word nodesor rely on the document citation relation to construct the graph yao et al 142regard the documents and words as nodes to construct the corpus graph henceheterogeneous graph and uses the text gcn to learn embeddings of words anddocuments sentiment classication could also be regarded as a text classicationproblem and a tree lstm approach is proposed by 109sequence labeling as each node in gnns has its hidden state we can utilizethe hidden state to address the sequence labeling problem if we consider every wordin the sentence as a node zhang et al 152 utilize the sentence lstm to label thesequence it has conducted experiments on pos tagging and ner tasks and achievespromising performancesemantic role labeling is another task of sequence labeling marcheggiani andtitov 76 propose a syntactic gcn to solve the problem the syntactic gcn whichoperates on the direct graph with labeled edges is a special variant of the gcn 59it uses edge wise gates that enable the model to regulate the contribution of eachdependency edge the syntactic gcns over syntactic dependency trees are used assentence encoders to learn latent feature representations of words in the sentencemarcheggiani and titov 76 also reveal that gcns and lstms are functionallycomplementary in the task8363other scenariosbesides structural and nonstructural scenarios there are some other scenarios wheregraph neural networks play an important role in this subsection we will introducegenerative graph models and combinatorial optimization with gnns83 graph neural networks271generative models generative models for real world graphs have drawn signif icant attention for its essential applications including modeling social interactionsdiscovering new chemical structures and constructing knowledge graphs as deeplearning methods have a powerful ability to learn the implicit distribution of graphsthere is a surge in neural graph generative models recentlynetgan 104 is one of the rst works to build a neural graph generative modelwhich generates graphs via random walks it transformed the problem of graphgeneration to the problem of walk generation which takes the random walks from aspecic graph as input and trains a walk generative model using gan architecturewhile the generated graph preserves essential topological properties of the originalgraph the number of nodes is unable to change in the generating process which isas same as the original graph graphrnn 146 generate the adjacency matrix of agraph by generating the adjacency vector of each node step by step which can outputrequired networks having different numbers of nodesinstead of generating adjacency matrix sequentially molgan 27 predict adiscrete graph structure the adjacency matrix at once and utilizes a permutation invariant discriminator to solve the node variant problem in the adjacency matrixbesides it applies a reward network for rl based optimization towards desiredchemical properties what is more 75 proposes constrained variational autoen coders to ensure the semantic validity of generated graphs moreover gcpn 145incorporates domain specic rules through reinforcement learningli et al 73 propose a model that generates edges and nodes sequentially andutilize a graph neural network to extract the hidden state of the current graph whichis used to decide the action in the next step during the sequential generative processcombinatorial optimization combinatorial optimization problems over graphsare a set of np hard problems that attract much attention from scientists of all eldssome specic problems like traveling salesman problem tsp have got variousheuristic solutions recently using a deep neural network for solving such problemshas been a hotspot and some of the solutions further leverage graph neural networksbecause of their graph structurebello et al 9 rst propose a deep learning approach to tackle tsp their methodconsistsoftwopartsapointernetwork123forparameterizingrewardsandapolicygradient 108 module for training this work has been proved to be comparable withtraditional approaches however pointer networks are designed for sequential datalike texts while order invariant encoders are more appropriate for such workkhalil et al 57 and kool and welling 61 improve the above method by includ ing graph neural networks the former work rst obtains the node embeddings fromstructure2vec 26 then feeds them into a q learning module for making decisionsthe latter one builds an attention based encoder decoder system by replacing thereinforcement learning module with an attention based decoder it is more efcientfor training these work achieved better performance than previous algorithmswhich proved the representation power of graph neural networksnowaketal88focusonquadraticassignmentproblemiemeasuringthesim ilarity of two graphs the gnn based model learns node embeddings for each graphindependently and matches them using an attention mechanism even in situations2728network representationwhere traditional relaxation based methods may perform not well this model stillshows satisfying performance8364example gnns for fact vericationdue to the rapid development of information extraction ie huge volumes of datahave been extracted how to automatically verify the data becomes a vital problemfor various data driven applications eg knowledge graph completion 126 andopen domain question answering 15 hence many recent research efforts havebeen devoted to fact verication fv which aims to verify given claims with theevidence retrieved from plain text more specically given a claim an fv systemis asked to label it as supported refuted or not enough infowhich indicates that the evidence can support refute or is not sufcient for the claiman example of the fv task is shown in table85existing fv methods formulate fv as a natural language inference nli 3task however they utilize simple evidence combination methods such as concate nating the evidence or just dealing with each evidence claim pair these methods areunable to grasp sufcient relational and logical information among the evidence infact many claims require to simultaneously integrate and reason over several piecesof evidence for verication as shown in table85 for this particular example wecannot verify the given claim by checking any evidence in isolation the claim canbe veried only by understanding and reasoning over multiple evidencetable 85 a case of the claim that requires integrating multiple evidence to verify the represen tation for evidence docname linenum means the evidence is extracted from the documentdocname and of which the line number is linenumclaimal jardine is an american rhythm guitaristtruth evidenceal jardine 0 al jardine 1retrieved evidenceal jardine 1 al jardine 0 al jardine 2 al jardine 5 jardine 42evidence1 he is best known as the bands rhythm guitarist and for occasionally singing lead vocalson singles such as help me rhonda 1965 then i kissed her 1965 and come go withme 19782 alan charles jardine born september 3 1942 is an american musician singer andsongwriter who co founded the beach boys3 in 2010 jardine released his debut solo studio album a postcard from california4 in 1988 jardine was inducted into the rock and roll hall of fame as a member of thebeach boys5 ray jardine american rock climber lightweight backpacker inventor author and globaladventurerlabel supported83 graph neural networks273to integrate and reason over information from multiple pieces of evidence 156proposes a graph based evidence aggregating and reasoning gear frameworkspecically 156 rst builds a fully connected evidence graph and encouragesinformation propagation among the evidence then gear aggregates the piecesof evidence and adopts a classier to decide whether the evidence can supportrefute or is not sufcient for the claim intuitively by sufciently exchanging andreasoning over evidence information on the evidence graph the proposed model canmake the best of the information for verifying claims for example by delivering theinformation los angeles county is the most populous county in the usa to therodney king riots occurred in los angeles county through the evidence graphthe synthetic information can support the rodney king riots took place in themost populous county in the usa furthermore we adopt an effective pretrainedlanguage representation model bert 29 to better grasp both evidence and claimsemanticszhou et al 156 employ a three step pipeline with components for documentretrieval sentence selection and claim verication to solve the task in the documentretrieval and sentence selection stages they simply follow the method from 44 sincetheir method has the highest score on evidence recall in the former fever sharedtask and they propose the gear framework in the nal claim verication stagethe full pipeline is illustrated in fig815given a claim and the retrieved evidence gear rst utilizes a sentence encoderto obtain representations for the claim and the evidence then it builds a fully con nected evidence graph and uses an evidence reasoning network ernet to prop agate information among evidence and reason over the graph finally it utilizes anevidence aggregator to infer the nal resultssentence encoder given an input sentence gear employs bert 29 as thesentence encoder by extracting the nal hidden state of the cls token as therepresentation where cls is the special classication token in bertei  bert ei c c  bert c 8105fig 815 the pipeline used in 156 the gear framework is illustrated in the claim vericationsection2748network representationevidence reasoning network to encourage the information propagation amongevidence gear builds a fully connected evidence graph where each node indi cates a piece of evidence it also adds self loop to every node because each nodeneeds the information from itself in the message propagation process we useht  ht1 ht2     htn to represent the hidden states of nodes at layer t the ini tial hidden state of each evidence node h0i is initialized by the evidence presentationh0i  eiinspired by recent work on semi supervised graph learning and relational rea soning 59 90 122 zhou et al 156 propose an evidence reasoning networkernet to propagate information among the evidence nodes it rst uses an mlpto compute the attention coefcients between a node i and its neighbor j  j niyi j  wt11reluwt10ht1i ht1j8106where ni denotes the set of neighbors of node i wt10and wt11are weightmatrices and   denotes concatenation operationthen it normalizes the coefcients using the softmax functioni j  softmax jyi j expyi jkni expyik8107finally the normalized attention coefcients are used to compute a linear com bination of the neighbor features and thus we obtain the features for node i at layerthtijnii jht1j8108by stacking t layers of ernet 156 assumes that each evidence could graspenough information by communicating with other evidenceevidence aggregator zhou et al 156 employ an evidence aggregator to gatherinformation from different evidence nodes and obtain the nal hidden state o theaggregator may utilize different aggregating strategies and 156 suggests threeaggregatorsattention aggregator zhou et al 156 use the representation of the claim c toattend the hidden states of evidence and get the nal aggregated state oy j  w1reluw0c hj  j  softmaxy j expy jnk1 expyko nk1khk 810983 graph neural networks275max aggregator the max aggregator performs the element wise max operationamong hidden stateso  maxh1  h2      hn8110mean aggregator the mean aggregator performs the element wise mean opera tion among hidden stateso  meanh1  h2      hn8111once the nal state o is obtained gear employs a one layer mlp to get the nalprediction ll  softmaxreluwo  b8112where w and b are parameterszhou et al 156 conduct experiments on the large scale benchmark dataset forfact extraction and verication fever 115 experimental results show thatthe proposed framework outperforms recent state of the art baseline systems thefurther case study indicates that the framework could better leverage multi evidenceinformation and reason over the evidence for fv84summaryin this chapter we have introduced network representation learning which turnsthe network structure information into the continuous vector space and make deeplearning techniques possible on network dataunsupervised network representation learning comes rst during the developmentof nrl spectral clustering deepwalk line grarep and other methods utilizethe network structure for vertex embedding learning afterward tadw incorporatestext information into nrl under the framework of matrix factorization the neualgorithm then moves one step forward and proposes a general method to improvethe quality of any learned network embeddings other unsupervised methods alsoconsider preserving specic properties of the network topology eg community andasymmetryrecently semi supervised nrl algorithms have attracted much attention thiskind of methods focus on a specic task such as classication and use the labels ofthe training set to improve the quality of network embeddings node2vec mmdwand many other methods including the family of graph neural networks gnnsare proposed for this end semi supervised algorithms can achieve better results asthey can take advantage of more information from the specic taskfor further understanding of network representation learning you can also ndmore related papers in this paper list httpsgithubcomthunlpgnnpapers thereare also some recommended surveys and books including the following2768network representation cui et al a survey on network embedding 24 goyal and ferrara graph embedding techniques applications and performancea survey 37 zhang et al network representation learning a survey 149 wu et al a comprehensive survey on graph neural networks 133 zhou et al graph neural networks a review of methods and applications 155 zhang et al deep learning on graphs a survey 154in the future for better network representation learning some directions arerequiring further efforts1 more complex and realistic networks an intriguing direction would bethe representation of learning on heterogeneous and dynamic networks where mostreal world network data fall into this category the vertices and edges in a heteroge neous network may belong to different types networks in real life are also highlydynamic eg the friendship between facebook users may establish and disappearthese characteristics require the researchers to design specic algorithms for themnetwork embedding learning on dynamic network structures is therefore an impor tant task there have been some works proposed 14 105 for much more complexand realistic settings2 deeper model architectures conventional deep neural networks can stackhundreds of layers to get better performance because the deeper structure has moreparameters and may improve the expressive power signicantly however nrl andgnnmodelsareusuallyshallowinfactmostofthemhavenomorethanthreelayerstaking gcn as an example as experiments in 70 show stacking multiple gcnlayers will result in over smoothing the representations of all vertices will convergeto the same although some researchers have managed to tackle this problem 70125 to some extents it remains to be a limitation of nrl designing deeper modelarchitectures is an exciting challenge for future research and will be a considerablecontribution to the understanding of nrl3 scalability scalability determines whether an algorithm is able to be appliedto practical use how to apply nrl methods in real world web scale scenarios suchas social networks or recommendation systems has been an essential problem formost network embedding algorithms scaling up nrl methods especially gnnis difcult because many core steps are computationally consuming in a big dataenvironment for example network data are not regular euclidean and each nodehas its own neighborhood structure therefore batch tricks cannot be easily appliedmoreover computing graph laplacian is also unfeasible when there are millions oreven billions of nodes and edges several works has proposed their solutions to thisproblem 143 153 157 and we are paying close attention to the progressreferences277references1 miltiadis allamanis marc brockschmidt and mahmoud khademi learning to representprograms with graphs in proceedings of iclr 20182 reid andersen fan chung and kevin lang local graph partitioning using pagerank vectorsin proceedings of focs 20063 gabor angeli and christopher d manning naturalli natural logic inference for commonsense reasoning in proceedings of emnlp 20144 james atwood and don towsley diffusion convolutional neural networks in proceedingsof neurips 20165 dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation byjointly learning to align and translate in proceedings of iclr 20156 peter battaglia razvan pascanu matthew lai danilo jimenez rezende et al interactionnetworks for learning about objects relations and physics in proceedings of neurips 20167 daniel beck gholamreza haffari and trevor cohn graph to sequence learning using gatedgraph neural networks in proceedings of acl 20188 mikhail belkin and partha niyogi laplacian eigenmaps and spectral techniques for embed ding and clustering in proceedings of neurips volume 14 20019 irwan bello hieu pham quoc v le mohammad norouzi and samy bengio neural com binatorial optimization with reinforcement learning in proceedings of iclr 201710 davide boscaini jonathan masci emanuele rodol and michael bronstein learning shapecorrespondence with anisotropic convolutional neural networks in proceedings of neurips201611 joan bruna wojciech zaremba arthur szlam and yann lecun spectral networks and locallyconnected networks on graphs in proceedings of iclr 201412 hongyun cai vincent w zheng and kevin chen chuan chang a comprehensive survey ofgraph embedding problems techniques and applications ieee transactions on knowledgeand data engineering 30916161637 201813 shaosheng cao wei lu and qiongkai xu grarep learning graph representations withglobal structural information in proceedings of cikm 201514 shiyu chang wei han jiliang tang guo jun qi charu c aggarwal and thomas s huangheterogeneous network embedding via deep architectures in proceedings of sigkdd 201515 danqi chen adam fisch jason weston and antoine bordes reading wikipedia to answeropen domain questions in proceedings of the acl 201716 jie chen tengfei ma and cao xiao fastgcn fast learning with graph convolutional networksvia importance sampling in proceedings of iclr 201817 mo chen qiong yang and xiaoou tang directed graph embedding in proceedings of ijcai200718 xinlei chen lijia li li feifei and abhinav gupta iterative visual reasoning beyond con volutions in proceedings of cvpr 201819 jianpeng cheng li dong and mirella lapata long short term memory networks formachine reading in proceedings of emnlp 201620 kyunghyun cho bart van merrinboer caglar gulcehre dzmitry bahdanau fethi bougaresholger schwenk and yoshua bengio learning phrase representations using rnn encoderdecoder for statistical machine translation in proceedings of emnlp 201421 yoon sik cho greg ver steeg and aram galstyan socially relevant venue clustering fromcheck in data in proceedings of kdd workshop 201322 wojciech chojnacki and michael j brooks a note on the locally linear embedding algorithminternational journal of pattern recognition and articial intelligence 230817391752200923 fan rk chung and fan chung graham spectral graph theory number 92 american math ematical soc 199724 peng cui xiao wang jian pei and wenwu zhu a survey on network embedding ieeetransactions on knowledge and data engineering 20182788network representation25 zhiyong cui kristian henrickson ruimin ke and yinhai wang trafc graph convolutionalrecurrent neural network a deep learning framework for network scale trafc learning andforecasting ieee transactions on intelligent transportation systems 201926 hanjun dai bo dai and le song discriminative embeddings of latent variable models forstructured data in proceedings of icml 201627 nicola de cao and thomas kipf molgan an implicit generative model for small moleculargraphs arxiv preprint arxiv180511973 201828 michael defferrard xavier bresson and pierre vandergheynst convolutional neural net works on graphs with fast localized spectral ltering in proceedings of neurips 201629 jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training ofdeep bidirectional transformers for language understanding in proceedings of naacl 201930 giuseppe di battista peter eades roberto tamassia and ioannis g tollis algorithms fordrawing graphs an annotated bibliography computational geometry 45235282 199431 david k duvenaud dougal maclaurin jorge aguileraiparraguirre rafael gomezbombarellitimothy d hirzel alan aspuruguzik and ryan p adams convolutional networks on graphsfor learning molecular ngerprints in proceedings of neurips 201532 francois fouss alain pirotte jean michel renders and marco saerens random walk com putation of similarities between nodes of a graph with application to collaborative recommen dation ieee transactions on knowledge and data engineering 193355369 200733 alex fout jonathon byrd basir shariat and asa ben hur protein interface prediction usinggraph convolutional networks in proceedings of neurips 201734 thomas mj fruchterman and edward m reingold graph drawing by force directed place ment software practice and experience 211111291164 199135 hongyang gao zhengyang wang and shuiwang ji large scale learnable graph convolu tional networks in proceedings of sigkdd 201836 jonas gehring michael auli david grangier and yann n dauphin a convolutional encodermodel for neural machine translation in proceedings of acl 201737 palash goyal and emilio ferrara graph embedding techniques applications and perfor mance a survey knowledge based systems 1517894 201838 aditya grover and jure leskovec node2vec scalable feature learning for networks in pro ceedings of sigkdd 201639 jiayuan gu han hu liwei wang yichen wei and jifeng dai learning region features forobject detection in proceedings of eccv 201840 takuo hamaguchi hidekazu oiwa masashi shimbo and yuji matsumoto knowledge trans fer for out of knowledge base entities  a graph neural network approach in proceedings ofijcai 201741 will hamilton zhitao ying and jure leskovec inductive representation learning on largegraphs in proceedings of neurips 201742 william l hamilton rex ying and jure leskovec representation learning on graphs meth ods and applications ieee database engineering bulletin 405274 201743 david k hammond pierre vandergheynst and remi gribonval wavelets on graphs viaspectral graph theory applied and computational harmonic analysis 302129150 201144 andreas hanselowski hao zhang zile li daniil sorokin benjamin schiller claudia schulzand iryna gurevych ukp athene multi sentence textual entailment for claim verication inproceedings of emnlp 201845 kaiming he xiangyu zhang shaoqing ren and jian sun deep residual learning for imagerecognition in proceedings of cvpr 201646 kaiming he xiangyu zhang shaoqing ren and jian sun identity mappings in deep residualnetworks in proceedings of eccv 201647 mikael henaff joan bruna and yann lecun deep convolutional networks on graph structured data arxiv preprint arxiv150605163 201548 sepp hochreiter and jrgen schmidhuber long short term memory neural computation9817351780 1997references27949 yedid hoshen vain attentional multi agent predictive modeling in proceedings of neurips201750 han hu jiayuan gu zheng zhang jifeng dai and yichen wei relation networks for objectdetection in proceedings of cvpr 201851 wenbing huang tong zhang yu rong and junzhou huang adaptive sampling towards fastgraph representation learning in proceedings of neurips 201852 yann jacob ludovic denoyer and patrick gallinari learning latent representations of nodesfor classifying in heterogeneous social networks in proceedings of wsdm 201453 ashesh jain amir r zamir silvio savarese and ashutosh saxena structural rnn deeplearning on spatio temporal graphs in proceedings of cvpr 201654 tomihisa kamada and satoru kawai an algorithm for drawing general undirected graphsinformation processing letters 311715 198955 michael kampffmeyer yinbo chen xiaodan liang hao wang yujia zhang and eric pxing rethinking knowledge graph propagation for zero shot learning in proceedings ofcvpr 201956 steven kearnes kevin mccloskey marc berndl vijay pande and patrick riley molecu lar graph convolutions moving beyond ngerprints journal of computer aided moleculardesign 308595608 201657 elias khalil hanjun dai yuyu zhang bistra dilkina and le song learning combinatorialoptimization algorithms over graphs in proceedings of neurips 201758 thomas kipf ethan fetaya kuanchieh wang max welling and richard s zemel neuralrelational inference for interacting systems in proceedings of icml 201859 thomas n kipf and max welling semi supervised classication with graph convolutionalnetworks in proceedings of iclr 201760 stephen g kobourov spring embedders and force directed graph drawing algorithms arxivpreprint arxiv12013011 201261 wwm kool and m welling attention solves your tsp arxiv preprint arxiv180308475201862 alex krizhevsky ilya sutskever and geoffrey e hinton imagenet classication with deepconvolutional neural networks in proceedings of neurips 201263 theodoros lappas evimaria terzi dimitrios gunopulos and heikki mannila finding effec tors in social networks in proceedings of sigkdd 201064 yann lecun yoshua bengio and geoffrey hinton deep learning nature 5217553436201565 yann lecun lon bottou yoshua bengio and patrick haffner gradient based learningapplied to document recognition proceedings of the ieee 199866 chungwei lee wei fang chihkuan yeh and yuchiang frank wang multi label zero shotlearning with structured knowledge graphs in proceedings of cvpr 201867 jure leskovec lada a adamic and bernardo a huberman the dynamics of viral marketingacm transactions on the web tweb 115 200768 jure leskovec lars backstrom and jon kleinberg meme tracking and the dynamics of thenews cycle in proceedings of sigkdd 200969 omer levy and yoav goldberg neural word embedding as implicit matrix factorization inproceedings of neurips 201470 qimai li zhichao han and xiao ming wu deeper insights into graph convolutional net works for semi supervised learning in proceedings of aaai 201871 yaguang li rose yu cyrus shahabi and yan liu diffusion convolutional recurrent neuralnetwork data driven trafc forecasting in proceedings of iclr 201872 yujia li daniel tarlow marc brockschmidt and richard s zemel gated graph sequenceneural networks in proceedings of iclr 201673 yujia li oriol vinyals chris dyer razvan pascanu and peter battaglia learning deepgenerative models of graphs in proceedings of iclr 201874 xiaodan liang xiaohui shen jiashi feng liang lin and shuicheng yan semantic objectparsing with graph lstm in proceedings of eccv 20162808network representation75 tengfei ma jie chen and cao xiao constrained generation of semantically valid graphs viaregularizing variational autoencoders in proceedings of neurips 201876 diego marcheggiani and ivan titov encoding sentences with graph convolutional networksfor semantic role labeling in proceedings of emnlp 201777 kenneth marino ruslan salakhutdinov and abhinav gupta the more you know usingknowledge graphs for image classication in proceedings of cvpr 201778 jonathan masci davide boscaini michael bronstein and pierre vandergheynst geodesicconvolutional neural networks on riemannian manifolds in proceedings of iccv workshops201579 julian j mcauley and jure leskovec learning to discover social circles in ego networks inproceedings of neurips 201280 t mikolov and j dean distributed representations of words and phrases and their composi tionality proceedings of neurips 201381 tomas mikolov kai chen greg corrado and jeffrey dean efcient estimation of wordrepresentations in vector space in proceedings of iclr 201382 federico monti davide boscaini jonathan masci emanuele rodola jan svoboda andmichael m bronstein geometric deep learning on graphs and manifolds using mixture modelcnns in proceedings of cvpr 201783 medhini narasimhan svetlana lazebnik and alexander gerhard schwing out of the boxreasoning with graph convolution nets for factual visual question answering in proceedingsof neurips 201884 mark ej newman finding community structure in networks using the eigenvectors of matri ces physical review e 743036104 200685 mark ej newman modularity and community structure in networks proceedings of thenational academy of sciences 1032385778582 200686 mathias niepert mohamed ahmed and konstantin kutzkov learning convolutional neuralnetworks for graphs in proceedings of icml 201687 will norcliffebrown stathis vafeias and sarah parisot learning conditioned graph structuresfor interpretable visual question answering in proceedings of neurips 201888 alex nowak soledad villar afonso s bandeira and joan bruna revised note on learningquadratic assignment with graph neural networks in proceedings of ieee dsw 2018 201889 mingdong ou peng cui jian pei and wenwu zhu asymmetric transitivity preserving graphembedding in proceedings of sigkdd 201690 rasmus palm ulrich paquet and ole winther recurrent relational networks in proceedingsof neurips 201891 hao peng jianxin li yu he yaopeng liu mengjiao bao lihong wang yangqiu songand qiang yang large scale hierarchical text classication with recursively regularized deepgraph cnn in proceedings of www 201892 nanyun peng hoifung poon chris quirk kristina toutanova and wentau yih cross sentence n ary relation extraction with graph lstms transactions of the association for com putational linguistics 51101115 201793 bryan perozzi rami al rfou and steven skiena deepwalk online learning of social rep resentations in proceedings of sigkdd 201494 trangphamtruyentrandinhphungandsvethavenkateshcolumnnetworksforcollectiveclassication in proceedings of aaai 201795 siyuan qi wenguan wang baoxiong jia jianbing shen and songchun zhu learninghuman object interactions by graph parsing neural networks in proceedings of eccv 201896 afshin rahimi trevor cohn and timothy baldwin semi supervised user geolocation viagraph convolutional networks in proceedings of acl 201897 sungmin rhee seokjun seo and sun kim hybrid approach of relation network and localizedgraph convolutional ltering for breast cancer subtype classication in proceedings of ijcai201898 sam t roweis and lawrence k saul nonlinear dimensionality reduction by locally linearembedding science 290550023232326 2000references28199 alvaro sanchez gonzalez nicolas heess jost tobias springenberg josh merel martin ried miller raia hadsell and peter battaglia graph networks as learnable physics engines forinference and control in proceedings of icml 2018100 victor garcia satorras and joan bruna estrach few shot learning with graph neural networksin proceedings of iclr 2018101 franco scarselli marco gori ah chung tsoi markus hagenbuchner and gabriele monfar dini the graph neural network model ieee tnn 2009 2016180 2009102 michael schlichtkrull thomas n kipf peter bloem rianne van den berg ivan titov andmax welling modeling relational data with graph convolutional networks in proceedings ofeswc 2018103 prithviraj sen galileo namata mustafa bilgic lise getoor brian galligher and tina eliassi rad collective classication in network data ai magazine 2939393 2008104 oleksandr shchur daniel zugner aleksandar bojchevski and stephan gunnemann netgangenerating graphs via random walks in proceedings of icml 2018105 chuan shi binbin hu wayne xin zhao and s yu philip heterogeneous information networkembedding for recommendation ieee transactions on knowledge and data engineering312357370 2018106 martin simonovsky and nikos komodakis dynamic edge conditioned lters in convolutionalneural networks on graphs in proceedings of cvpr 2017107 sainbayar sukhbaatar rob fergus et al learning multiagent communication with backprop agation in proceedings of neurips 2016108 richard s sutton and andrew g barto reinforcement learning an introduction mit press2018109 kai sheng tai richard socher and christopher d manning improved semantic representa tions from tree structured long short term memory networks in proceedings of acl 2015110 jian tang meng qu and qiaozhu mei pte predictive text embedding through large scaleheterogeneous text networks in proceedings of sigkdd 2015111 jian tang meng qu mingzhe wang ming zhang jun yan and qiaozhu mei line large scale information network embedding in proceedings of www 2015112 lei tang and huan liu relational learning via latent social dimensions in proceedings ofsigkdd 2009113 lei tang and huan liu leveraging social media networks for classication data miningand knowledge discovery 233447478 2011114 damien teney lingqiao liu and anton van den hengel graph structured representationsfor visual question answering in proceedings of cvpr 2017115 james thorne andreas vlachos christos christodoulopoulos and arpit mittal fever alarge scale dataset for fact extraction and verication in proceedings of naacl hlt 2018116 cunchao tu hao wang xiangkai zeng zhiyuan liu and maosong sun community enhancednetworkrepresentationlearningfornetworkanalysisarxivpreprintarxiv161106645 2016117 cunchao tu xiangkai zeng hao wang zhengyan zhang zhiyuan liu maosong sunbo zhang and leyu lin a unied framework for community detection and network rep resentation learning ieee transactions on knowledge and data engineering tkde31610511065 2018118 cunchao tu weicheng zhang zhiyuan liu and maosong sun max margin deepwalk dis criminative learning of network representation in proceedings of ijcai 2016119 cunchao tu zhengyan zhang zhiyuan liu and maosong sun transnet translation basednetwork representation learning for social relation extraction in proceedings of ijcai 2017120 rianne van den berg thomas n kipf and max welling graph convolutional matrix com pletion arxiv preprint arxiv170602263 2017121 ashish vaswani noam shazeer niki parmar llion jones jakob uszkoreit aidan n gomezand lukasz kaiser attention is all you need in proceedings of neurips 2017122 petar velickovic guillem cucurull arantxa casanova adriana romero pietro lio andyoshua bengio graph attention networks in proceedings of iclr 20182828network representation123 oriol vinyals meire fortunato and navdeep jaitly pointer networks in proceedings ofneurips 2015124 jacco wallinga and peter teunis different epidemic curves for severe acute respiratory syn drome reveal similar impacts of control measures american journal of epidemiology 2004125 daixin wang peng cui and wenwu zhu structural deep network embedding in proceedingsof sigkdd 2016126 quan wang zhendong mao bin wang and li guo knowledge graph embedding a surveyof approaches and applications tkde 291227242743 2017127 xiao wang peng cui jing wang jian pei wenwu zhu and shiqiang yang communitypreserving network embedding in proceedings of aaai 2017128 xiao wang houye ji chuan shi bai wang yanfang ye peng cui and philip s yu het erogeneous graph attention network in proceedings of www 2019129 xiaolong wang yufei ye and abhinav gupta zero shot recognition via semantic embed dings and knowledge graphs in proceedings of cvpr 2018130 zhichun wang qingsong lv xiaohan lan and yu zhang cross lingual knowledge graphalignment via graph convolutional networks in proceedings of emnlp 2018131 zhouxia wang tianshui chen jimmy s j ren weihao yu hui cheng and liang lin deepreasoning with knowledge graph for social relationship understanding in proceedings ofijcai 2018132 nicholas watters daniel zoran theophane weber peter battaglia razvan pascanu andandrea tacchetti visual interaction networks learning a physics simulator from video inproceedings of neurips 2017133 zonghan wu shirui pan fengwen chen guodong long chengqi zhang and philip s yua comprehensive survey on graph neural networks arxiv preprint arxiv190100596 2019134 keyulu xu chengtao li yonglong tian tomohiro sonobe kenichi kawarabayashi andstefanie jegelka representation learning on graphs with jumping knowledge networks inproceedings of icml 2018135 sijie yan yuanjun xiong and dahua lin spatial temporal graph convolutional networks forskeleton based action recognition in proceedings of aaai 2018136 cheng yang zhiyuan liu deli zhao maosong sun and edward y chang network repre sentation learning with rich text information in proceedings of ijcai 2015137 cheng yang maosong sun zhiyuan liu and cunchao tu fast network embedding enhance ment via high order proximity approximation in proceedings of ijcai 2017138 cheng yang maosong sun wayne xin zhao zhiyuan liu and edward y chang a neuralnetwork approach to jointly modeling social networks and mobile trajectories acm trans actions on information systems tois 35436 2017139 cheng yang jian tang maosong sun ganqu cui and liu zhiyuan multi scale informationdiffusion prediction with reinforced recurrent networks in proceedings of ijcai 2019140 jaewon yang and jure leskovec overlapping community detection at scale a nonnegativematrix factorization approach in proceedings of wsdm 2013141 jaewon yang julian mcauley and jure leskovec detecting cohesive and 2 mode commu nities indirected and undirected networks in proceedings of wsdm 2014142 liang yao chengsheng mao and yuan luo graph convolutional networks for text classi cation in proceedings of aaai 2019143 rex ying ruining he kaifeng chen pong eksombatchai william l hamilton and jureleskovec graph convolutional neural networks for web scale recommender systems in pro ceedings of sigkdd 2018144 zhitao ying jiaxuan you christopher morris xiang ren will hamilton and jure leskovechierarchical graph representation learning with differentiable pooling in proceedings ofneurips 2018145 jiaxuan you bowen liu zhitao ying vijay s pande and jure leskovec graph convolutionalpolicy network for goal directed molecular graph generation in proceedings of neurips2018references283146 jiaxuan you rex ying xiang ren william hamilton and jure leskovec graphrnn gen erating realistic graphs with deep auto regressive models in proceedings of icml 2018147 bing yu haoteng yin and zhanxing zhu spatio temporal graph convolutional networks adeep learning framework for trafc forecasting in proceedings of iclr 2018148 victoria zayats and mari ostendorf conversation modeling on reddit using a graph structuredlstm transactions of the association for computational linguistics 6121132 2018149 daokun zhang jie yin xingquan zhu and chengqi zhang network representation learninga survey ieee transactions on big data 2018150 jiani zhang xingjian shi junyuan xie hao ma irwin king and dit yan yeung gaangated attention networks for learning on large and spatiotemporal graphs in proceedings ofuai 2018151 yizhou zhang yun xiong xiangnan kong shanshan li jinhong mi and yangyong zhudeep collective classication in heterogeneous information networks in proceedings ofwww 2018152 yue zhang qi liu and linfeng song sentence state lstm for text representation in pro ceedings of acl 2018153 zhengyan zhang cheng yang zhiyuan liu maosong sun zhichong fang bo zhang andleyu lin cosine compressive network embedding on large scale information networksarxiv preprint arxiv181208972 2018154 ziwei zhang peng cui and wenwu zhu deep learning on graphs a survey arxiv preprintarxiv181204202 2018155 jie zhou ganqu cui zhengyan zhang cheng yang zhiyuan liu lifeng wang changchengli and maosong sun graph neural networks a review of methods and applications arxivpreprint arxiv181208434 2018156 jiezhouxuhanchengyangzhiyuanliulifengwangchangchengliandmaosongsungear graph based evidence aggregating and reasoning for fact verication in proceedingsof acl 2019 2019157 zhaocheng zhu shizhen xu jian tang and meng qu graphvite a high performance cpu gpu hybrid system for node embedding in proceedings of www 2019158 chenyi zhuang and qiang ma dual graph convolutional networks for graph based semi supervised classication in proceedings of www 2018159 julian g zilly rupesh kumar srivastava jan koutnik and jurgen schmidhuber recurrenthighway networks in proceedings of icml 2016160 marinka zitnik monica agrawal and jure leskovec modeling polypharmacy side effectswith graph convolutional networks intelligent systems in molecular biology 34132588142018161 zhou jie cui ganqu zhang zhengyan yang cheng liu zhiyuan wang lifeng lichangcheng and sun maosong graph neural networks a review of methods and appli cations arxiv preprint arxiv181208434 2018162 liu zhiyuan and zhou jie introduction to graph neural networks morgan  claypool pub lishers 20202848network representationopen access this chapter is licensed under the terms of the creative commons attribution 40international license httpcreativecommonsorglicensesby40 which permits use sharingadaptation distribution and reproduction in any medium or format as long as you give appropriatecredit to the original authors and the source provide a link to the creative commons license andindicate if changes were madethe images or other third party material in this chapter are included in the chapters creativecommons license unless indicated otherwise in a credit line to the material if material is notincluded in the chapters creative commons license and your intended use is not permitted bystatutory regulation or exceeds the permitted use you will need to obtain permission directly fromthe copyright holderchapter 9cross modal representationabstract cross modal representation learning is an essential part of representationlearning which aims to learn latent semantic representations for modalities includingtextsaudioimagesvideosetcinthischapterwerstintroducetypicalcross modalrepresentation models after that we review several real world applications relatedto cross modal representation learning including image captioning visual relationdetection and visual question answering91introductionas introduced in wikipedia a modality is the classication of a single independentchannel of sensory inputoutput between a computer and a human to be moregeneral modalities are different means of information exchange between humanbeings and the real world the classication is usually based on the form in whichinformation is presented to a human typical modalities in the real world includetexts audio images videos etccross modal representation learning is an important part of representation learn ing in fact articial intelligence is inherently a multi modal task 30 human beingsare exposed to multi modal information every day and it is normal for us to integrateinformation from different modalities and make comprehensive judgments further more different modalities are not independent but they have correlations more orless for example the judgment of a syllable is made by not only the sound we hearbut also the movement of the lips and tongue of the speaker we see an experimentin 48 shows that a voiced ba with a visual ga is perceived by most people asa da another example is human beings ability to consider the 2d image and 3dscan of the same object together and reconstruct its structure correlations betweenimage and scan can be found based on the fact that a discontinuity of depth in thescan usually indicates a sharp line in the image 52 inspired by this it is naturalfor us to consider the possibility of combining inputs from multi modalities in ourarticial intelligence systems and generate cross modal representationngiam et al 52 explore the probability of merging multi modalities into onelearning task the authors divide a typical machine learning task into three stages the authors 2020z liu et al representation learning for natural language processinghttpsdoiorg101007978 981 15 5573 2 92852869cross modal representationfeature learning supervised learning and prediction and they further propose fourkinds of learning settings for multi modalities1 single modal learning all stages are all done on just one modality2 multi modal fusion all stages are all done with all modalities available3 cross modal learning in the feature learning stage all modalities are avail able but in supervised learning and prediction only one modality is used4 shared representation learning in the feature learning stage all modalitiesare available in supervised learning only one modality is used and in prediction adifferent modality is usedexperiments show promising results for these multi modal tasks when moremodalities are provided such as multi modal fusion cross modal learning andshared representation learning the performance of the system is generally betterin the following part of this chapter we will rst introduce cross modal represen tation models which are fundamental parts of cross modal representation learningin nlp and then we will introduce several critical applications such as imagecaptioning visual relationship detection and visual question answering92cross modal representationcross modal representation learning aims to build embeddings using informationfrom multiple modalities existing cross modal representation models involving textmodality can be generally divided into two categories 1 30 77 try to fuse infor mation from different modalities into unied embeddings eg visually groundedword representations 2 researchers also try to build embeddings for differentmodalities in a common semantic space which allows the model to compute cross modal similarity such cross modal similarity can be further utilized for downstreamtasks such as zero shot recognition 5 14 18 53 65 and cross media retrieval 2355 in this section we will introduce these two kinds of cross modal representationmodels respectively921visual word2veccomputing word embeddings is a fundamental task in representation learning fornatural language processing typical word embedding models like word2vec 49are trained on a text corpus these models while being extremely successful cannotdiscoverimplicitsemanticrelatednessbetweenwordsthatcouldbeexpressedinothermodalities kottur et al 30 provide an example even though eat and stare atseem are unrelated from text images might show that when people are eatingsomething they would also tend to stare at it this implies that consideringother modalities when constructing word embeddings may help capture more implicitsemantic relatedness92 cross modal representation287twoducksinwaterswimwalksitcnnfig 91 the architecture for word embedding with global visual contextvision being one of the most critical modalities has attracted attention fromresearchers seeking to improve word representation several models that incorporatevisual information and improve word embeddings with vision have been proposedwe introduce two typical word representation models incorporating visual informa tion in the following9211word embedding with global visual contextxu et al 77 propose a model that makes a natural attempt to incorporate visualfeatures it claims that in most word representation models only local context infor mation eg trying to predict a word using neighboring words and phrases is con sidered global text information eg the topic of the passage on the other handis often neglected this model extends a simple local context model by using visualinformation as global features see fig91the input of the model is an image i and a sequence describing it it is basedon a simple local context language model when we consider a certain word wt ina sequence its local feature is the average of embeddings of words in a windowie wtk     wt1 wt1     wtk the visual feature is computed directly fromthe image i using a cnn and then used as the global feature the local feature andthe global feature are then concatenated into a vector f the predicted probabilityof a word wt in this blank is the softmax normalized product of f and the wordembedding wtowt  wtt f91pwtwtk     wt1wt1     wtk i expowti expowi92the model is optimized by maximizing the average of log probability2889cross modal representationl  1ttktklog pwtwtk     wt1 wt1     wtk i93the classication error will be back propagated to local text vector ie wordembeddings visual vector and all model parameters this accomplishes jointlylearning for a set of word embeddings a language model and the model used forvisual encoding9212word embedding with abstract visual scenekottur et al 30 also propose a neural model to capture ne grained semantics fromvisual information instead of focusing on literal pixels the abstract scene behindthe vision is considered the model takes a pair of the visual scene and a relatedword sequence i w as input at each training step a window is used upon theword sequence w forming a subsequence sw all the words in sw will be fed into theinput layer using one hot encoding and therefore the dimension of the input layeris v  which is also the size of the vocabulary the words are then transformed intotheir embeddings and the hidden layer is the average of all these embeddings thesize of the hidden layer is nh which is also the dimension of the word embeddingsthe hidden layer and the output layer are connected by a full connection matrix ofdimension nh nk and a softmax function the output layer can be regarded asa probability distribution over a discrete valued function g of the visual scene idetails will be given in the following paragraph the entire model is optimized byminimizing the objective functionl  log pgwsw94the most important part of the model is the function g it maps the visual scenei into the set 1 2     nk which indicates what kind of abstract scene it is inpractice it is learned ofine using k means clustering and each cluster representsthe semantics of one kind of visual scenes consequently the word sequence w whichis designed to be related to the scene922cross modal representation for zero shot recognitionlarge scale datasets partially support the success of deep learning methods eventhough the scales of datasets continue to grow larger and more categories areinvolved the annotation of datasets is expensive and time consuming for manycategories there are very limited or even no instances which restricts the scalabilityof recognition systems92 cross modal representation289zero shot recognition is proposed to solve the problem as mentioned above whichaims to classify instances of categories that have not been seen during training manyworksproposetoutilizecross modalrepresentationforzero shotimageclassication5 14 18 53 65 specically image representation and category representationare embedded into a common semantic space where similarities between image andcategory representations can serve for further classication for example in such acommon semantic space the embedding of an image of cat is expected to be closerto the embedding of category cat than the embedding of category truck9221deep visual semantic embeddingthe challenge of zero shot learning lies in the absence of instances of unseen cat egories which makes it challenging to obtain well performed classiers of unseencategories frome et al 18 present a model that utilizes both labeled images andinformation from the large scale plain text for zero shot image classication theytry to leverage semantic information from word embeddings and transfer it to imageclassication systemstheir model is motivated by the fact that word embeddings incorporate semanticinformation of concepts or categories which can be potentially utilized as classi ers of corresponding categories similar categories cluster well in semantic spacefor example in word embedding space the nearest neighbors of the term tigershark are similar kinds of sharks such as bull shark blacktip sharksandbar shark and oceanic whitetip shark in addition bound aries between different clusters are clear the aforementioned properties indicatethat word embeddings can be further utilized as classiers for recognition systemsspecically the model rst pretrains word embeddings using the skip gram textmodel on large scale wikipedia articles for visual feature extraction the model pre trains a deep convolutional neural network for 1 000 object categories on imagenetthe pretrained word embeddings and the convolutional neural network are used toinitialize the proposed deep visual semantic embedding model deviseto train the proposed model they replace the softmax layer of the pretrainedconvolutional neural network with a linear projection layer the model is trained topredict the word embeddings of categories for images using a hinge ranking lossl i y jymax0  wymi  w jmi95where wy and w j are the learned word embeddings of the positive label and samplednegative label respectively i denotes the feature of the image obtained from theconvolutional neural network m is the trainable parameters in linear projectionlayer and  is a hyperparameter in hinge ranking loss given an image the objectiverequires the model to produce a higher score for the correct label than randomlychosen labels where the score is dened as the dot product of the projected imagefeature and word embedding of terms2909cross modal representationat test time given a test image the score of each possible category is obtainedusing the same approach during training note that a crucial difference at test time isthat the classiers word embeddings are expanded to all possible categories includ ing unseen categories thus the model is capable of predicting unseen categoriesexperiment results show that devise can make zero shot predictions with moresemantically reasonable errors which means that even if the prediction is not exactlycorrect it is semantically related to the ground truth class however a drawback isthat although the model can utilize semantic information in word embeddings tomake zero shot image classication using word embeddings as classiers restrictsthe exibility of the model which results in inferior performance in the original1 000 categories compared to the original softmax classier9222convex combination of semantic embeddingsinspired by devise 53 proposes a model conse that tries to utilize semanticinformation from word embeddings for zero shot classication a vital differenceto devise is that they obtain the semantic embedding of test image using a convexcombination of word embeddings of seen categories the score of the correspondingcategory determines the weights of the composing word embeddingsspecically they train a deep convolutional neural network on seen categoriesat test time given a test image i possibly from unseen categories they obtainthe top t condent predictions of seen categories where t is a hyperparameterthen the semantic embedding f i of i is determined by the convex combinationof semantic embeddings of the top t condent categories which can be formallydened as followsf i  1ztt1p y0i ti  w y0i t96where y0i t is the tth most condent training label for i w y0i t is the semanticembedding word embedding of y0i t and z is a normalization factor given byz tt1p y0i ti97after obtaining the semantic embedding f i the score of the category m is givenby the cosine similarity of f i and wmthe motivation of conse is that they assume novel categories can be mod eled as the convex combination of seen categories if the model is highly con dent about a prediction ie p y0i 1i 1 the semantic embedding f i willbe close to w y0i 1 if the predictions are ambiguous eg ptigeri 05 plioni  05 the semantic embedding f i will be between wlionand wtiger and they expect the semantic embedding f i  05wlion 92 cross modal representation29105wtiger to be close to the semantic embedding wliger a hybrid crossbetween lions and tigersalthough conse and devise share many similarities there are also some crucialdifferences devise replaces the softmax layer of the pretrained visual model with aprojection layer while conse preserves the softmax layer conse does not need tobe further trained and uses a convex combination of semantic embeddings to performzero shot classication at test time experiment results show that conse outperformsdevise on unseen categories indicating better generalization capability howeverthe performance of conse on seen categories is not as competitive as devise andthe original softmax classier9223cross modal transfersocher et al 65 present a cross modal representation model for zero shot recogni tion in their model all word vectors are initialized with pretrained 50 dimensionalword vectors and are kept xed during training each image is represented by a vectori constructed by a deep convolutional neural network they rst project an imageinto semantic word spaces by minimizingl  yysi ix ywy 2 f 1ii298where ys denotes the set of images classes which can be seen in training datax y denotes the set of images vectors of class y wy denotes the word vector ofclass y and   1 2 denotes parameters of the 2 layer neural network withf   tanh as activation functionthey observe that instances from unseen categories are usually outliers of thecomplete data manifold following this observation they rst classify an instanceinto seen and unseen categories via outlier detection methods then the instance isclassied using corresponding classiersformally they marginalize a binary random variable v s u which denoteswhether an instance belongs to seen categories or unseen categories separately whichmeans probability is given aspyi v supyv ipv i99for seen image classes they simply use softmax classier to determine pys iwhile for unseen classes they assume an isometric gaussian distribution around eachof the novel class word vectors and assign classes based on their likelihood to detectnovelty they calculate a local outlier probability by gaussian error function2929cross modal representation923cross modal representation for cross media retrievallearningcross modalrepresentationfromdifferentmodalitiesinacommonsemanticspace allows one to easily compute cross modal similarities which can facilitatemany important cross modal tasks such as cross media retrieval with the rapidgrowth of multimedia data such as text image video and audio on the internet theneed to retrieve information across different modalities has become stronger cross media retrieval is an important task in the multimedia area which aims to performretrieval across different modalities such as text and image for example a user maysubmit an image of a white horse and retrieve relevant information from differentmodalities such as textual descriptions of horses and vice versaa signicant challenge of cross modal retrieval is the domain discrepanciesbetween different modalities besides for a specic area of interest cross modal datacan be insufcient which limits the performance of existing cross modal retrievalmethods many works have focused on the challenges as mentioned above in cross modal retrieval 23 249231cross modal hybrid transfer networkhuang et al 24 present a framework that tries to relieve the cross modal datasparsity problem by transfer learning they propose to leverage knowledge froma large scale single modal dataset to boost the model training on the small scaledataset the massive auxiliary dataset is denoted as the source domain and thesmall scale dataset of interest is denoted as the target domain in their work theyadopt imagenet 12 a large scale image database as the source domainformally a training set consists of data from source domain src  i ps  y ps pp1and target domain tartr  i js  t js  y js jj1 where i t is the imagetext pair withlabel y similarly a test set can be denoted as tarte  i ms  tms  yms mm1 the goalof their model is to transfer knowledge from src to boost the model performance ontarte for cross media retrievaltheir model consists of a modal sharing transfer subnetwork and a layer sharingcorrelation subnetwork in modal sharing transfer subnetwork they adopt the con volutional layers of alexnet 32 to extract image features for source and targetdomains and use word vectors to obtain text features the image and text featurespass through two fully connected layers where single modal and cross modal knowl edge transfer are performedsingle modal knowledge transfer aims to transfer knowledge from images in thesource domain to images in the target domain the main challenge is the domaindiscrepancy between the two image datasets they propose to solve the domaindiscrepancy problem by minimizing the maximum mean discrepancy mmd ofimage modality between the source and target domains mmd is calculated in alayer wise style in the fully connected layers by minimizing mmd in reproducedkernel hilbert space the image representations from source and target domains are92 cross modal representation293encouraged to have the same distribution so knowledge from images in the sourcedomain is expected to transfer to images in the target domain besides the imageencoder in the source domain is also ne tuned by optimizing softmax loss on labeledimage instancescross modal knowledge transfer aims to transfer knowledge between image andtext in the target domain text and image representations from an annotated pairin the target domain are encouraged to be close to each other by minimizing theireuclidean distance the cross modal transfer loss of image and text representationsis also computed in a layer wise style in the fully connected layers the domaindiscrepancy between image and text modalities is expected to be reduced in high level layersin layer sharing correlation subnetwork representations from modal sharingtransfer subnetwork in the target domain are fed into shared fully connected layersto obtain the nal common representation for both image and text as the parametersare shared between two modalities the last two fully connected layers are expectedto capture the cross modal correlation their model also utilizes label informationin the target domain by minimizing softmax loss on labeled imagetext pairs afterobtaining the nal common representations cross media retrieval can be achievedby simply computing the nearest neighbors in semantic space9232deep cross media knowledge transferas an extension of 23 24 also focuses on dealing with domain discrepancy andinsufcient cross modal data for cross media retrieval in specic areas huang andpeng 23 present a framework that transfers knowledge from a large scale cross mediadatasetsourcedomaintoboostthemodelperformanceonanothersmall scalecross media dataset target domaina crucial difference from 24 is that the dataset in the source domain also consistsof imagetext pairs with label annotations instead of a single modal setting in 24since both domains contain image and text media types domain discrepancy comesfrom the media level discrepancy in the same media type and correlation level dis crepancy in imagetext correlation patterns between different domains they proposeto transfer intra media semantic and inter media correlation knowledge by jointlyreducing domain discrepancies on media level and correlation leveltoextractthedistributedfeaturesfordifferentmediatypestheyadoptvgg1963for image encoder and word cnn 29 for text encoder the two domains have thesame architecture but do not share parameters the extracted imagetext featurespass through two fully connected layers respectively where the media level transferis performed similar to 24 they reduce domain discrepancies within the samemodalities by minimizing maximum mean discrepancy mmd between the sourceand target domains the mmd is computed in a layer wise style to transfer knowl edge within the same modalities they also minimize euclidean distance betweenimagetext representations pairs in both source and target domains to preserve thesemantic information across modalities2949cross modal representationcorrelation level transfer aims to reduce domain discrepancy in imagetext corre lation patterns in different domains in two domains both image and text representa tions share the last two fully connected layers to obtain the common representationfor each domain they optimize layer wise mmd loss between the shared fully con nected layers in different domains for correlation level knowledge transfer whichencourages source and target domains to have the same imagetext correlation pat terns finally both domains are trained with label information of imagetext pairsnote that the source domain and target domain do not necessarily share the samelabel setin addition they propose a progressive transfer mechanism which is a curricu lum learning method aiming to promote the robustness of the model training this isachieved by selecting easy samples for model training in the early period and grad ually increases the difculty during the training the difculty of training samplesis measured according to the bidirectional cross media retrieval consistency93image captioningimage captioning is the task of automatically generating natural language descrip tions for images it is a fundamental task in articial intelligence which connectsnatural language processing and computer vision compared with other computervision tasks such as image classication and object detection image captioningis signicantly harder for two reasons rst not only objects but also relationshipsbetween them have to be detected second besides basic judgments and classicationnatural language sentences have to be generatedtraditional methods for image captioning are usually using retrieval models orgeneration models of which the ability to generalize is comparatively weaker com pared with that of novel deep neural network models in this section we will introduceseveral typical models of both genres in the following931retrieval models for image captioningthe primary pipeline of retrieval models is 1 represent images andor sentencesusing special features 2 for new images andor sentences search for probablecandidates according to the similarity of featureslinking words to images has a rich history and 50 a retrieval model is the rstimage annotation system this paper tries to build a keyword assigning system forimages from labeled data the pipeline is as follows1 image segmentation every image is divided into several parts using thesimplest rectangular division the reason for doing so is that an image is typicallyannotated with multiple labels each of which often corresponds to only a part of itsegmentation would help reduce noises in labeling93 image captioning2952 feature extraction features of every part of the image are extracted3 clustering feature vectors of image segments are divided into several clusterseach cluster accumulates word frequencies and thereby calculates word likelihoodconcretelypwic j pc jwipwik pc jwkpwk  n jin j910where n ji is the number of times word wi appears in cluster j and n j is the number oftimes that all words appear in cluster j the calculation is based on using frequenciesas probabilities4inferenceforanewimagethemodeldividesitintosegmentsextractsfeaturesfor every part and nally aggregates keywords assigned to every part to obtain thenal predictionthe key idea of this model is image segmentation take a landscape picture forinstance there are two parts mountain and sky and both parts will be annotatedwith both labels however if another picture has two parts mountain and riverthe two mountain parts would hopefully be in the same cluster and discover thatthey share the same label mountain in this way labels can be assigned to thecorrect part of the image and noises could be alleviatedanother typical retrieval model is proposed by 17 which can assign a linkingscore between an image and a sentence an intermediate space of meaning calculatesthis score of linking the representation of the meaning space is a triple in the form ofobject action scene  each slot of the triple has a nite discrete candidate set theproblem of mapping images and sentences into the meaning space involves solvinga markov random elddifferent from the previous model this system can do not only image captionbut also do the inverse that is given a sentence the model provides certain probableassociated images at the inference stage the image sentence is rst mapped to theintermediate meaning space then we search in the pool for the sentence image thathas the best matching scoreafter that researchers also proposed a lot of retrieval models which considerdifferent kinds of characteristics of the images such as 21 28 34932generation models for image captioningdifferent from the retrieval based model the basic pipeline of generation models is1 use computer vision techniques to extract image features 2 generate sentencesfrom these features using methods such as language models or sentence templateskulkarni et al 33 propose a system that makes a tight connection between theparticular image and the sentence generating process the model uses visual detectorsto detect specic objects as well as attributes of a single object and relationshipsbetween multiple objects then it constructs a conditional random eld to incorporateunary image potentials and higher order text potentials and thereby predicts labels2969cross modal representationfor the image labels predicted by conditional random elds crf is arranged as atriple eg white cloud in blue skythen sentences are generated according to the labels there are two ways to builda sentence based on the triple skeleton 1 the rst is to use an n gram languagemodel for example when trying to decide whether or not to put a glue word xbetween a pair of meaningful words which means they are inside the triple a and bthe probabilities paxb and pab are compared for the decision p is the standardlength normalized probability of the n gram language model 2 the second is touse a set of descriptive language templates which alleviates the problem of grammarmistakes in the language modelfurther 16 proposes a novel framework to explicitly represent the relationshipbetween image structure and its caption sentences structure the method visualdependency representation detects objects in the image and detects the relationshipbetween these objects based on the proposed visual dependency grammar whichincludes eight typical relations like beside or above then the image can bearranged as a dependency graph where nodes are objects and edges are relations thisimage dependency graph can be aligned with the syntactic dependency representationof the caption sentence the paper further provides four templates to generatingdescriptive sentences from the extracted dependency representationbesides these two typical works there are massive generation models for imagecaptioning such as 15 35 78933neural models for image captioningin 33 it was claimed in 2011 that in image captioning tasks natural languagegeneration still remains an open research problem most previous work is based onretrieval and summarization from 2015 inspired by advances in neural languagemodel and neural machine translation a number of end to end neural image caption ing models based on the encoder decoder system have been proposed these newmodels signicantly improve the ability to generate natural language descriptions9331the basic modeltraditional machine translation models typically stitch many subtasks together suchas individual word translation and reordering to perform sentence and paragraphtranslation recent neural machine translation models such as 8 use a singleencoder decoder model which can be optimized by stochastic gradient descent con venientlythetaskofimagecaptioningisinherentlyanalogoustomachinetranslationbecause it can also be regarded as a translation task where the source languageis an image the encoders and decoders used for machine translations are typicallyrnns which is a natural selection for sequences of words for image captioningcnn is chosen to be the encoder and rnn is still used as the decoder93 image captioning297input imageooopppw1w2endw0w1wncnnlstmsoftmaxcorrect caption sentence sw1 w2  wnfig 92 the architecture of encoder decoder framework for image captioningvinyals et al 70 is the most typical model which uses encoder decoder forimage captioning see fig92 concretely a cnn model is used to encode theimage into a x length vector which is believed to contain the necessary informationfor captioning with this vector an rnn language model is used to generate naturallanguage descriptions and this is the decoder here the decoder is similar to thelstm used for machine translation the rst unit takes the image vector as theinput vector and the rest units take the previous word embedding as input each unitoutputs a vector o and passes a vector to the next unit o is further fed into a softmaxlayer whose output p is the probability of each word within the vocabulary theways to deal with these calculated probabilities are different in training and testingtraining these probabilities p are used to calculate the likelihood of the provideddescription sentences considering the nature of rnns it is easy to model the jointprobability into conditional probabilitieslog psi nt0log pwti w0     wt1911where s  w1 w2  wn is the sentence and its words w0 is a special starttoken and i is the image stochastic gradient descent can thereby be performed tooptimize the modeltesting there are multiple approaches to generate sentences given an imagethe rst one is called sampling for each step the single word with the highestprobability in p is chosen and used as the input of the next unit until the endtoken is generated or a maximal length is reached the second one is called beamsearch for each step now the length of sentences is t k best sentences are kepteach of them generates several new sentences of length t  1 and again only k new2989cross modal representationfig 93 an example of image captioning with attention mechanismsentences are kept beam search provides a better approximation fors arg maxslog psi9129332variants of the basic modelthe research on image captioning tightly follows that on machine translationinspired by 6 which uses attention mechanism in machine translation 76 intro duces visual attention into the encoder decoder image captioning modelthe major bottleneck of 70 is the fact that information from the image is shownto the lstm decoder only at the rst decoding unit which actually requires theencoder to squeeze all useful information into one xed length vector in contrast76 does not require such compression the cnn encoder does not produce onevector for the entire image instead it produces l region vectors ii each of which isthe representation of a part of the image at every step of decoding the inputs includestandard lstm inputs ie output and hidden state of last step ot1 and ht1 andan input vector z from the encoder here z is the weighted sum of image vectorsii z  i iii where i is the weight computed from ii and ht1 throughoutthe training process the model learns to focus on parts of the image for generatingthe next word by producing larger weights  on more relevant parts as shown infig931while the above paper uses soft attention for the image 27 makes explicitalignment between image fragments and sentence fragments before generating adescription for the image in the rst stage the alignment stage sentence and imagefragments are aligned by being mapped to a shared space concretely sentencefragments ie n consecutive words are encoded using a bidirectional lstm intothe embeddings s and image fragments ie part of the image and also the entireimage are encoded using a cnn into the embeddings i the similarity score betweenimage i and sentence s is computed as1the example is obtained from the implementation of yunjey choi httpsgithubcomyunjeyshow attend and tell93 image captioning299simi s tgsmaxigi 0 ii st913where gs is the sentence fragment set of sentence s and gi is the image fragment setof image i the alignment is then optimized by minimizing the ranking loss l forboth sentences and imagesl ismax0 simi s simi i  1 smax0 sims i simi i  1914the assumption for this alignment procedure is similar to 50 see sect931 alldescription sentences are regarded as possibly noisy labels for every image sectionand are based on the massive training data the model would hopefully be trained toalign caption sentences to their corresponding image fragments the second stage issimilar to the basic model in 70 but the alignment results are used to provide moreprecise training dataas mentioned above 76 makes the decoder have the ability to focus attentionon the different parts of the image for different words however there are somenonvisual words in the decoding process for example words such as the and ofare more dependent on semantic information than visual information furthermorewords such as phone followed by cell or meter before near the parkingare usually generated by the language model to avoid the gradient of a nonvisualword decreasing the effectiveness of visual attention in the process of generatingcaptions 43 adopts an adaptive attention model with a visual sentinel at each timestep the model needs to determine that it depends on an image region or a visualsentineladaptive attention model 43 uses attention in the process of generating a wordrather than updating the lstm state it utilizes visual sentinel vector xt and imageregion vectors ii here xt is produced by the inputs and states of lstm at time stept while ii is provided from cnn encoder then the adaptive context vector ct is theweighted sum of l image region vectors ii and visual sentinel xtct liiii  l1xt915where i are the weights computed by ii xt and the lstm hidden state ht wehave l1i1 i  1 finally the probability of a word in vocabulary at time t can becalculated as a residual formpt  softmaxwpct  ht916where wp is a learned weight parametermany existing image captioning models with attention allocate attention overimages regions whose size is often 7  7 or 14  14 decided by the last pooling3009cross modal representationresizelatent channelactivationactivated regionimagehtctifig 94 an example of the activated region of a latent channellayer in cnn encoder anderson et al 2 rst calculate attention at the level ofobjects it rst employs faster r cnn 58 which is trained on imagenet 60 andgenome 31 to predict attribute class such as an open oven green bottle oraldress and so on after that it applies attention over valid bounding boxes to getne grained attention for helping the caption generationbesides 11 rethinks the form of latent states in image captioning which usu ally compresses two dimensional visual feature maps encoded by cnn to a one dimensional vector as the input of the language model they nd that the languagemodel with 2d states can preserve the spatial locality which can link the input visualdomain and output linguistic domain observed by visualizing the transformation ofhidden statesword embeddings and hidden states in 11 are 3d tensors of size c  h  wwhich means c channels each of size h  w the encoded features maps willbe directly inputted to the 2d language model instead of going through an averagepooling layer in the 2d language model the convolution operator takes the place ofmatrix multiplication in the 1d model and mean pooling will be used to generatethe output word probability distribution from 2d hidden states figure94 showsactivated region of a latent channel at the tth step when we set a threshold for theactivated regions it is revealed that the special channels are associated with specicnouns in the decoding process which help get a better understanding of the processof generating captionstraditional methods train the caption model by maximizing the likelihood oftraining examples which forms a gap between the optimization objective and evalu ating metrics to alleviate the problem 59 uses reinforcement learning to directlymaximize the cider metric 69 cider reects the diversity of generated cap tions by giving high weights to the low frequency n grams in the training set whichdemonstrates that people prefer detailed captions rather than universal ones like aboy is playing a game to encourage the distinctiveness of captions 10adopts contrastive learning their model learns to discriminate the caption of a givenimage and the caption of an alike image by maximizing the difference betweenground truth positive pair and mismatch negative pair the experiment shows thatcontrastive learning increases the diversity of captions signicantlyfurthermore automatic evaluation metrics such as bleu 54 meteor 13rouge 38 cider 69 spice 1 and so on may neglect some novel expres sions restrained by the ground truth captions to better evaluate the naturalness anddiversity of captions 9 proposes a framework based on conditional generativeadversarial networks whose generator tries to achieve a higher score in the evalua 93 image captioning301tor while the evaluator tries to distinguish between the generated caption and humandescriptions for a given image as well as between the given image and the mismatchdescription the user study shows that the trained generator can generate natural anddiverse captions than the model trained by maximum likelihood estimate while thetrained evaluator is more consistent with humans evaluationbesides the works we introduced above there are also a mass of variants of thebasic encoder decoder model such as 20 26 40 45 51 71 7394visual relationship detectionvisual relationship detection is the task of detecting objects in an image and under standing the relationship between them while detecting the objects is always basedon semantic segmentation or object detection methods such as r cnn understand ing the relationship is the key challenge of this task while detecting visual relationwith image information is intuitive and effective 25 62 84 leveraging informationfrom language can further boost the model performance 37 41 82941visual relationship detection with language priorslu et al 41 propose a model that uses language priors to enhance the performanceon infrequent relationships for which sufcient training instances are hard to obtainsolely from images the overall architecture is shown in fig95they rst train a cnn to calculate the unnormalized relations probabilityobtained from visual inputs byrcnnlanguage module     frwhat ride horsehorse pull hatperson ride horseperson wear horsevisual module     vrperson wear hathorse wear hatperson ride horse076011081cnnfig 95 the architecture of visual relationship detection with language prior3029cross modal representationpv ri jk o1 o2  pio1zk cnno1 o2  skpjo2917where pio j denotes the probability that bounding box o j is entity i andc n no1 o2 is the joint feature of box o1 with box o2   zk sk is the set ofparametersbesides language prior is considered in this model by calculating the unnormal ized probability that the entity pair i jhas the relation kpf r w  rk wi w j  bk918where wi and w j are the word embeddings of the text of subject and object respec tively rk is the learned relational embedding of the relation kgiven the probabilities of a relation from visual and textual inputs respectivelythe authors combine them into the integrated probability of a relation the nalprediction is the one with maximal integrated probabilityr maxrpv ri jko1 o2pf r w919the rank of the ground truth relationship r with bounding boxes o1 and o2 ismaximized using the following rank loss functionc w o1o2rmax1 pv r o1 o2pf r wmaxo1o2o1o2rr pv r o1 o2pf r w 0920in addition to the loss that optimizes the rank of the ground truth relationshipsthe authors also propose two regularization functions for language priors the nalloss function of this model is dened asl  c w  1lw  2kw921kw is a variance function to make the similar relationships correspondingf  function closerkw  varpf r w pf r w2dr r r r922where dr r is the sum of the cosine distances in word2vec space between thetwo objects and the predicates of the two relationships r and rlw is a function to encourage less frequent relation to have a lower f  scorewhen r occurs more frequently than r we havelw rrmaxpf r w pf r w  1 092394 visual relationship detection303cnnobject detection modulerelation prediction moduleclassemelocationvisualpersonelephantpersonpersonridetallernext towithelephantpersonelephantpantspersonelephantpersonpantsboxbilinearinterpolationconvfeatscalingscalingwsfeatureextractionlayerwosoftmaxboxfig 96 the architecture of vtranse modela a scenetabletableclothgooseappleporcelainr onr onr insider insider   answer onb the corresponding scene graphfig 97 an illustration for scene graph generation942visual translation embedding networkinspired by recent progress in knowledge representation learning 82 proposesvtranse a visual translation embedding network objects and the relationshipbetween objects are modeled as transe 7 like vector translation vtranse rstprojects subject and object into the same space as relation translation vector r rrsubject and object could be denoted as xs xo rm in the feature space wherem r similar to transe relationship vtranse establishes a relationship aswsxs  r woxo924where ws and wo are projection matrices the overall architecture is shown infig96943scene graph generationli et al 37 further formulate visual relation detection as a scene graph generationtask where nodes correspond to objects and directed edges correspond to visualrelations between objects as shown in fig97this formulation allows 37 to leverage different levels of context informationsuch as information from objects phrases ie subject predicate objecttriples3049cross modal representationcbafig 98 dynamical graph construction a the input image b object bottom phrase middleand caption region top proposals c the graph modeling connections between proposals some ofthe phrase boxes are omittedand region captions to boost the performance of visual relation detection speci cally 37 proposes to construct a graph that aligns these three levels of informationand perform feature renement via message passing as shown in fig98 by leverag ing complementary information from different levels the performances of differenttasks are expected to be mutually improveddynamic graph construction given an image they rst generate three kindsof proposals that correspond to three kinds of nodes in the proposed graph structurethe proposals include object proposals phrase proposals and region proposals theobject and region proposals are generated using region proposal network rpn57 trained with ground truth bounding boxes given n object proposals phraseproposals are constructed based on nn 1 object pairs that fully connect theobject proposals with direct edges where each direct edge represents a potentialphrase between an object paireach phrase proposal is connected to the corresponding subject and object withtwo directed edges a phrase proposal and a region proposal are connected if theiroverlapexceedsacertainfractioneg07ofthephraseproposaltherearenodirectconnections between objects and regions since they can be indirectly connected viaphrasesfeature renement after obtaining the graph structure of different levels ofnodes they perform feature renement by iterative message passing the messagepassing procedure is divided into three parallel stages including object renementphrase renement and region renementin object feature renement the object proposal feature is updated with gatedfeatures from adjacent phrases given an object i the aggregated feature from regionsthat are linked to object i via subject predicate edges xpsican be dened as followsxpsi1eipi jespfopxoi  xpj xpj 92594 visual relationship detection305where esp is the set of subject predicate connections and eip denotes the number ofphrases connected with the object i as the subject predicate pairs fopis a learnablegate function that controls the weights of information from different sourcesfopxoi  xpj  kk1sigmoidkop xoi  xpj 926where kopis a gate template used to calculate the importance of the informationfrom a subject predicate edge and k is the number of templates the aggregatedfeature from object predicate edges xpoican be similarly computedafter obtaining information xpsiand xpoifrom adjacent phrases the objectrenement at time step t can be dened as followsxoit1  xoit  f psxpsi  f poxpoi927where f   wrelu w is a learnable parameter and not shared betweenf ps and f pothe renement scheme of phrases and regions is similar to objects the onlydifference is the information sources phrase proposals receive information fromadjacent objects and regions and region proposals receive information from phrasesafter feature renement via iterative message passing the feature of differentlevels of nodes can be used for corresponding tasks region features can be usedas the initial state of a language model to generate region captions phrase featurescan be used to predict visual relation between objects which composes of the scenegraph of the imagein comparison with scene graph generation methods that model the dependenciesbetween relation instances by attention mechanism or message passing 47 decom poses the scene graph task into a mixture of two phases extracting primary relationsfrom input and completing the scene graph with reasoning the authors propose ahybrid scene graph generator hre that combines these two phases in a uniedframework and generates scene graphs from scratchspecically hre rst encodes the object pair into representations and thenemploys a neural relation extractor resolving primary relations from inputs and a dif ferentiable inductive logic programming model that iteratively completes the scenegraph as shown in fig99 hre contains two units a pair selector and a relationpredictor and runs in an iterative wayat each time step the pair selector takes a look at all object pairs pthat have notbeen associated with a relation and chooses the next pair of entities whose relationis to be determined the relation predictor utilizes the information contained in allpairs p whose relations have been determined and the contextual information ofthe pair to make the prediction on the relation the prediction result is then added top and benets future predictionsto encode object pair into representations hre extends the union box encoderproposed by 41 by adding the object features what are the objects and their3069cross modal representationfig 99 framework of hre that detects primary relations from inputs and iteratively completesthe scene graph via inductive logic programmingfig 910 object pair encoder of hrelocations where are the objects into the object pair representation as shown infig910relation predictor the relation predictor is composed of two modules a neuralmodule predicting the relations between entities based on the given context ie avisual image and a differentiable inductive logic module performing reasoning onp both modules predict the relation score between a pair of objects individuallythe relation scores from the two modules are nally integrated by multiplicationpair selector the selector works as the predictors collaborator with the goalto gure out the next relation which should be determined ideally the choice pmade by the selector should satisfy the condition that all relations that will affectthe predictors prediction on pshould be sent to the predictor ahead of p hreimplements the pair selector as a greedy selector which always chooses the entitypair from pto be added to p as the entity pair of which the relation predictor ismost condent in its prediction94 visual relationship detection307it is worth noting that the task of scene graph generation resembles document level relation extraction in many aspects both tasks seek to extract structured graphsconsisting of entities and relations also they need to model the complex dependen cies between entities and relations in a rich context we believe both tasks are worthyto explore for future research95visual question answeringvisual question answering vqa aims to answer natural language questions aboutan image and can be seen as a single turn of dialogue about a picture in this sectionwe will introduce widely used vqa datasets and several typical vqa models951vqa and vqa datasetsvqa was rst proposed in 46 they rst propose a single world approach bymodeling the probability of an answer a given question q and a world w bypaq w zpaz wpzq928where z is a latent variable associated with the question and the world w is a represen tation of the image they further extend the single world approach to a multi worldapproach by marginalizing over different segments s of the given image the prob ability of an answer a given question q and a world w is given bypaq s wzpaw zpwspzq929they also release the rst dataset of vqa named as daquar in their paperbesides daquar researchers also release a lot of vqa datasets with vari ous characteristics the most widely used dataset was released in 4 where theauthors provided cases and experimental evidence to demonstrate that to answerthese questions a human or an algorithm should use features of the image and exter nal knowledge figure911 shows examples of vqa dataset released in 4 it is alsodemonstrated that this problem cannot be solved by converting images to captionsand answering questions according to captions experiment results show that theperformance of vanilla methods is still far from humanin fact there are also other existing datasets for visual qa such as visual7w85 visual madlibs 80 coco qa 56 and fm iqa 193089cross modal representationquestion why are the men jumpinganswer to catch frisbeequestion is the water stillanswer noquestion what is the kid doinganswer skateboardingquestion what is hanging on the wall above the headboardanswer picturesfig 911 examples of vqa dataset952vqa modelsbesides 4 46 further investigate approaches to solve specic types of questions invqa moreover 83 proposes an approach to solve yesno questions note thatthe model is an ensemble model of two similar models q model and tuple modelthe difference between which will be described later the overall approach can bedivided into two steps 1 language parsing and 2 visual verication in the formerstep they extract p r stuples from questions rst by parsing it and assigning anentity to each word then they summarize the parsed sentences through removingstop words auxiliary verbs and all words before a nominal subject or passivenominal subject and further split the summary into prs arguments according to thepart of speech of phrases the difference between q model and tuple model is thatthe q model is the one used in their previous work 4 embedding the question intoa dense 256 dim vector by lstm while tuple model is to convert p r stuplesinto 256 dim embeddings by mlp as for the visual verication step they use thesame feature of images as in 39 which was encoded into the dense 256 dim vector95 visual question answering309by an inner product layer followed by a tanh layer these two vectors are passedthrough an mlp to produce the nal output yes or nomoreover 61 proposes a method to calculate attention  j by the set of imagefeatures i  i1 i2     ik and the question embedding q by j  w1i j  b1w2q  b2930where w1w2b1b2 are trainable parametersattention based techniques are quite efcient for ltering noises that are irrelevantto the question however some questions are only related to some small regionswhich encourages researchers to use stacked attention to further ltering noises werefer readers to fig1b in 79 for an example of stacked attentionyang et al 79 further extend the attention based model used in 61 whichemploys lstms to predict the answer they take the question as input and attendto different regions in the image to obtain additional input the key idea is to grad ually lter out noises and pinpoint the regions that are highly relevant to the answerby reasoning through multiple stacked attention layers progressively the stackedattention could be calculated by stackinghka  tanhwk1i wk2uk1  bka931note that we denote the addition of a matrix and a vector by  the additionbetween a matrix and a vector is performed by adding each column of the matrix bythe vector u is a rened query vector that combines information from the questionand image regions u0 ie u from the rst attention layer with k  0 could beinitialized as the feature vector of the question hka is then used to compute pki whichcorresponds to the attention probability of each image regionpki  softmaxwk3hka  bkp932uk could be iterated byik ipki ii933uk  uk1  ik934that is in every layer the model progressively uses the combined question andimage vector uk1 as the query vector for attending the image region to obtain thenew querythe above models attend only on images but questions should also be attended44 calculates co attention byz  tanhqwi9353109cross modal representationwhat color on the stop light is lit upwhatcolorwhatthe stoplightcolorcolorquestionwhat color on the stop light is lit upco attentionimageanswergreenstopthe stop lightcolor stop light litstoplightlightfig 912 the architecture of hierarchical co attention modelregionsproposalsmulti label cnncap 1 a dog laying on the floor with abird next to it and a cat behind themon the other side of a sliding glass doorcap 2 a brown and black dog layingon a floor next to a birdcap 3 the dog cat and bird are all onthe floor in the roomcaptioninternal representationaveragepoolingtop 5attributessparqldbpediathe dog is a furry carnivorousmember of the canidae familymammal class the cat is a smallusually furry domesticated andcarnivorous mammal birds avesclass are a group of endothermicvertebratescharacterisedbyfeathers a beak with no teethplants also called green plants are multicellular eukaryotes ofthedoc2vecexternal knowledgehowmanytheretherearearetwomammalsend  1minimizing cost functiongenerationh0h1hnhn1hnl 1lstmcap 1cap 2cap 5fig 913 the architecture of vqa incorporating external knowledge baseswhere zi j represents the afnity of the ith word and jth region figure912 showsthe hierarchical co attention modelanother intuitive approach is to use external knowledge from knowledge baseswhich will help us better explain the implicit information hiding behind the imagesuch an approach was proposed in 75 which rst encodes the image into cap tions and vectors representing different attributes of the image to retrieve documentsabout a different part of the images from knowledge bases documents are encodedthrough doc2vec 36 the representation of captions attributes and documents aretransformed and concatenated to form the initial vector of an lstm which is trainedin seq2seq fashion details of the model are shown in fig913neural module network is a framework for constructing deep networks with adynamic computational structure which was rst proposed in 3 in such a frame work every input is associated with a layout that provides a template for assemblingan instance specic network from a collection of shallow network fragments called95 visual question answering311where isthe dogparserlstmcouchlayoutcountwherecolorstandinggodtaccnnfig 914 the architecture of the neural module network modelmodules the proposed method processes the input question through two separateways 1 parsing and laying out several modules and 2 encoding by an lstmthe corresponding picture is processed by the modules laid out according to thequestion the types of which are predened find transform combinedescribe and measure the authors dened find to be a transformation fromimage to attention map transform to be a mapping from one attention toanother combine to be a combination of two attention describe to be adescription relying on image and attention and measure to be a measure onlyrelying on attention the model is shown in fig914a key drawback of 3 is that it relies on the parser to generate modules 22proposes an end to end model to generate a sequence of reverse polish expressionto describe the module network as shown in fig915 and the overall architectureis shown in fig916graph neural networks gnns have also been applied to vqa tasks 68 triesto build graphs about both the scene and the question the authors described a deepneural network to take advantage of such a structured representation as shown infig917 the gnn based vqa model could capture the relationships between wordsand objects96summaryin this chapter we rst introduce the concept of cross modal representation learn ing cross modal learning is essential since many real world tasks require the abilityto understand the information from different modalities such as text and imagenext we introduce the concept of cross modal representation learning which aimsto exploit the links and enable better utilization of information from different modali 3129cross modal representationfig 915 the architecture of reverse polish expression and corresponding module network modelhow many other things are of thesame size as the green matte ballquestion encoderrnnquestion featureslayout predicionreverse polish notationfindrelocate how many other things are of the same size as thegreen matte ballhow many other things areof the same size as thegreen matte ballquestion attentionsanswercountrelocaterelocatee the same size as the green matte ballimage encodercnnimage features4modulenetworknetwork buildercount layout policy rnnfig 916 the architecture of end to end module network modelfig 917 the architecture of gnn based vqa models96 summary313ties and we overview existing cross modal representation learning methods for sev eral representative cross modal tasks including zero shot recognition cross mediaretrieval image captioning and visual question answering these cross modal learn ing methods either try to fuse information from different modalities into uniedembeddings or try to build embeddings for different modalities in a common seman tic space allowing the model to compute cross modal similarity cross modal repre sentation learning is drawing more and more attention and can serve as a promisingconnection between different research areasfor further understanding of cross modal representation learning there are alsosome recommended surveys and books including skocaj et al cross modal learning 64 spence crossmodal correspondences a tutorial review 66 wang et al a comprehensive survey on cross modal retrieval 72in the future for better cross modal representation learning some directions arerequiring further efforts1 fine grained cross modal grounding cross modal grounding is a funda mental ability in solving cross modal tasks which aims to align semantic units indifferent modalities for example visual grounding aims to ground textual symbolseg words or phrases into visual objects or regions many existing works 27 7476 have been devoted to cross modal grounding which mainly focuses on coarse grained semantic unit grounding eg grounding of sentences and images betterne grained cross modal grounding eg grounding of words and objects couldpromote the development of a broad variety of cross modal tasks2 cross modal reasoning in addition to recognizing and grounding semanticunits in different modalities understanding and inferring the relationship betweensemantic units are also crucial to cross modal tasks many existing works 37 4182 have investigated detecting visual relation between objects however most visualrelations in existing visual relation detection datasets do not require complex reason ing some works 81 have made preliminary attempts on cross modal commonsensereasoning inferring the latent semantic relationships in cross modal context is crit ical for cross modal understanding and modeling3 utilizing unsupervised cross modal data most current cross modal learn ing approaches rely on human annotated datasets the scale of such superviseddatasets is usually limited which also limits the capability of data hungry neuralmodels with the rapid development of the world wide web cross modal data onthe web have become larger and larger some existing works 42 67 have lever aged unsupervised cross modal data for representation learning they rst pretrainedcross modal models on large scale image caption pairs and then ne tuned the mod els on those downstream tasks which shows signicant improvement in a broadvariety of cross modal tasks it is thus promising to better leverage the vast amountof unsupervised cross modal data for representation learning3149cross modal representationreferences1 peter anderson basura fernando mark johnson and stephen gould spice semanticpropositional image caption evaluation in proceedings of eccv 20162 peter anderson xiaodong he chris buehler damien teney mark johnson stephen gouldand lei zhang bottom up and top down attention for image captioning and visual questionanswering in proceedings of cvpr 20183 jacob andreas marcus rohrbach trevor darrell and dan klein neural module networksin proceedings of cvpr pages 3948 20164 stanislaw antol aishwarya agrawal jiasen lu margaret mitchell dhruv batra c lawrencezitnick and devi parikh vqa visual question answering in proceedings of iccv 20155 lei jimmy ba kevin swersky sanja fidler and ruslan salakhutdinov predicting deep zero shot convolutional neural networks using textual descriptions in proceedings of iccv 20156 dzmitry bahdanau kyunghyun cho and yoshua bengio neural machine translation byjointly learning to align and translate in proceedings of iclr 20157 antoine bordes nicolas usunier alberto garcia duran jason weston and oksanayakhnenko translating embeddings for modeling multi relational data in proceedings ofneurips 20138 kyunghyun cho bart van merrienboer caglar gulcehre dzmitry bahdanau fethi bougaresholger schwenk and yoshua bengio learning phrase representations using rnn encoderdecoder for statistical machine translation in proceedings of emnlp 20149 bo dai sanja fidler raquel urtasun and dahua lin towards diverse and natural imagedescriptions via a conditional gan in proceedings of iccv 201710 bo dai and dahua lin contrastive learning for image captioning in proceedings of neurips201711 bo dai deming ye and dahua lin rethinking the form of latent states in image captioningin proceedings of eccv 201812 jia deng wei dong richard socher li jia li kai li and li fei fei imagenet a large scalehierarchical image database in proceedings of cvpr 200913 michael j denkowski and alon lavie meteor universal language specic translation eval uation for any target language in proceedings of acl 201414 mohamed elhoseiny babak saleh and ahmed elgammal write a classier zero shot learn ing using purely textual descriptions in proceedings of iccv 201315 desmond elliott and arjen de vries describing images using inferred visual dependencyrepresentations in proceedings of acl 201516 desmondelliottandfrankkellerimagedescriptionusingvisualdependencyrepresentationsin proceedings of emnlp 201317 ali farhadi mohsen hejrati mohammad amin sadeghi peter young cyrus rashtchianjulia hockenmaier and david forsyth every picture tells a story generating sentences fromimages in proceedings of eccv 201018 andrea frome greg s corrado jon shlens samy bengio jeff dean tomas mikolov et aldevise a deep visual semantic embedding model in proceedings of neurips 201319 haoyuan gao junhua mao jie zhou zhiheng huang lei wang and wei xu are youtalking to a machine dataset and methods for multilingual image question in proceedingsof neurips 201520 jiuxiang gu jianfei cai gang wang and tsuhan chen stack captioning coarse to nelearning for image captioning in proceedings of aaai 201821 micah hodosh peter young and julia hockenmaier framing image description as a rankingtask data models and evaluation metrics journal of machine learning research 47853899 201322 ronghang hu jacob andreas marcus rohrbach trevor darrell and kate saenko learningto reason end to end module networks for visual question answering in proceedings oficcv 2017references31523 xin huang and yuxin peng deep cross media knowledge transfer in proceedings of cvpr201824 xin huang yuxin peng and mingkuan yuan cross modal common representation learningby hybrid transfer network in proceedings of ijcai 201725 zhaoyin jia andrew gallagher ashutosh saxena and tsuhan chen 3d based reasoningwith blocks support and stability in proceedings of iccv 201326 justin johnson andrej karpathy and li fei fei densecap fully convolutional localizationnetworks for dense captioning in proceedings of cvpr 201627 andrej karpathy and li fei fei deep visual semantic alignments for generating imagedescriptions in proceedings of cvpr 201528 andrej karpathy armand joulin and fei fei f li deep fragment embeddings for bidirec tional image sentence mapping in proceedings of neurips 201429 yoon kim convolutional neural networks for sentence classication in proceedings ofemnlp 201430 satwik kottur ramakrishna vedantam jos mf moura and devi parikh visual word2vecvis w2v learning visually grounded word embeddings using abstract scenes in proceed ings of cvpr 201631 ranjay krishna yuke zhu oliver groth justin johnson kenji hata joshua kravitzstephanie chen yannis kalantidis li jia li david a shamma michael s bernstein andli fei fei visual genome connecting language and vision using crowdsourced dense imageannotations international journal of computer vision 12313273 201732 alex krizhevsky ilya sutskever and geoffrey e hinton imagenet classication with deepconvolutional neural networks in proceedings of neurips 201233 girish kulkarni visruth premraj sagnik dhar siming li yejin choi alexander c berg andtamara l berg baby talk understanding and generating image descriptions in proceedingsof cvpr 201134 polina kuznetsova vicente ordonez alexander c berg tamara l berg and yejin choicollective generation of natural image descriptions in proceedings of acl 201235 polina kuznetsova vicente ordonez tamara l berg and yejin choi treetalk composi tion and compression of trees for image descriptions transactions of the association forcomputational linguistics 210351362 201436 quoc v le and tomas mikolov distributed representations of sentences and documents inproceedings of icml 201437 yikang li wanli ouyang bolei zhou kun wang and xiaogang wang scene graph gener ation from objects phrases and region captions in proceedings of iccv 201738 chin yew lin rouge a package for automatic evaluation of summaries text summarizationbranches out 200439 xiao lin and devi parikh dont just listen use your imagination leveraging visual commonsense for non visual tasks in proceedings of cvpr 201540 chenxi liu junhua mao fei sha and alan l yuille attention correctness in neural imagecaptioning in proceedings of aaai 201741 cewu lu ranjay krishna michael bernstein and li fei fei visual relationship detectionwith language priors in proceedings of eccv 201642 jiasen lu dhruv batra devi parikh and stefan lee vilbert pretraining task agnostic visi olinguistic representations for vision and language tasks in proceedings of neurips 201943 jiasen lu caiming xiong devi parikh and richard socher knowing when to look adaptiveattention via a visual sentinel for image captioning in proceedings of cvpr 201744 jiasen lu jianwei yang dhruv batra and devi parikh hierarchical question image co attention for visual question answering in proceedings of neurips 201645 jiasen lu jianwei yang dhruv batra and devi parikh neural baby talk in proceedings ofcvpr 201846 mateusz malinowski and mario fritz a multi world approach to question answering aboutreal world scenes based on uncertain input in proceedings of neurips 20143169cross modal representation47 jiayuan mao yuan yao stefan heinrich tobias hinz cornelius weber stefan wermterzhiyuan liu and maosong sun bootstrapping knowledge graphs from images and textfrontiers in neurorobotics 1393 201948 harry mcgurk and john macdonald hearing lips and seeing voices nature 197649 tomas mikolov kai chen greg corrado and jeffrey dean efcient estimation of wordrepresentations in vector space in proceedings of iclr 201350 yasuhide mori hironobu takahashi and ryuichi oka image to word transformation basedon dividing and vector quantizing images with words in proceedings of wmisr 199951 jonghwan mun minsu cho and bohyung han text guided attention model for image cap tioning in proceedings of aaai 201752 jiquan ngiam aditya khosla mingyu kim juhan nam honglak lee and andrew y ngmultimodal deep learning in proceedings of icml 201153 mohammad norouzi tomas mikolov samy bengio yoram singer jonathon shlens andreafrome greg s corrado and jeffrey dean zero shot learning by convex combination ofsemantic embeddings in proceedings of iclr 201454 kishore papineni salim roukos todd ward and wei jing zhu bleu a method for automaticevaluation of machine translation in proceedings of acl 200255 yuxin peng xin huang and yunzhen zhao an overview of cross media retrieval conceptsmethodologies benchmarks and challenges ieee transactions on circuits and systems forvideo technology 201756 mengye ren ryan kiros and richard zemel exploring models and data for image questionanswering in proceedings of neurips 201557 shaoqing ren kaiming he ross girshick and jian sun faster r cnn towards real timeobject detection with region proposal networks in proceedings of neurips 201558 shaoqing ren kaiming he ross b girshick and jian sun faster r cnn towards real timeobject detection with region proposal networks in proceedings of neurips 201559 steven j rennie etienne marcheret youssef mroueh jarret ross and vaibhava goel self critical sequence training for image captioning in proceedings of cvpr 201760 olga russakovsky jia deng hao su jonathan krause sanjeev satheesh sean ma zhihenghuang andrej karpathy aditya khosla michael s bernstein alexander c berg and fei fei li imagenet large scale visual recognition challenge international journal of computervision 1153211252 201561 kevin j shih saurabh singh and derek hoiem where to look focus regions for visualquestion answering in proceedings of cvpr 201662 nathan silberman derek hoiem pushmeet kohli and rob fergus indoor segmentation andsupport inference from rgbd images in proceedings of eccv 201263 karen simonyan and andrew zisserman very deep convolutional networks for large scaleimage recognition arxiv14091556 201464 danijel skocaj ales leonardis and geert jan m kruijff cross modal learning pp 861864 boston ma 201265 richard socher milind ganjoo christopher d manning and andrew ng zero shot learningthrough cross modal transfer in proceedings of neurips 201366 charles spence crossmodal correspondences a tutorial review attention perception psychophysics 734971995 201167 chen sun austin myers carl vondrick kevin murphy and cordelia schmid videobert ajoint model for video and language representation learning in proceedings of iccv 201968 damien teney lingqiao liu and anton van den hengel graph structured representationsfor visual question answering in proceedings of cvpr 201769 ramakrishna vedantam c lawrence zitnick and devi parikh cider consensus basedimage description evaluation in proceedings of cvpr 201570 oriol vinyals alexander toshev samy bengio and dumitru erhan show and tell a neuralimage caption generator in proceedings of cvpr 201571 cheng wang haojin yang christian bartz and christoph meinel image captioning withdeep bidirectional lstms in proceedings of acmmm 2016references31772 kaiye wang qiyue yin wei wang shu wu and liang wang a comprehensive survey oncross modal retrieval arxiv160706215 201673 yufei wang zhe lin xiaohui shen scott cohen and garrison w cottrell skeleton keyimage captioning by skeleton attribute decomposition in proceedings of cvpr 201774 hao wu jiayuan mao yufeng zhang yuning jiang lei li weiwei sun and wei ying maunied visual semantic embeddings bridging vision and language with structured meaningrepresentations in proceedings of cvpr 201975 qi wu peng wang chunhua shen anthony dick and anton van den hengel ask meanything free form visual question answering based on knowledge from external sourcesin proceedings of cvpr 201676 kelvin xu jimmy ba ryan kiros kyunghyun cho aaron courville ruslan salakhudinovrich zemel and yoshua bengio show attend and tell neural image caption generation withvisual attention in proceedings of icml 201577 ran xu jiasen lu caiming xiong zhi yang and jason j corso improving word represen tations via global visual context in proceedings of neurips workshop 201478 yezhouyangchinglikteohaldaumiiiandyiannisaloimonoscorpus guidedsentencegeneration of natural images in proceedings of emnlp 201179 zichao yang xiaodong he jianfeng gao li deng and alex smola stacked attention net works for image question answering in proceedings of cvpr 201680 licheng yu eunbyung park alexander c berg and tamara l berg visual madlibs fill inthe blank description generation and question answering in proceedings of iccv 201581 rowan zellers yonatan bisk ali farhadi and yejin choi from recognition to cognitionvisual commonsense reasoning in proceedings of cvpr 201982 hanwangzhangzawlinkyawshih fuchangandtat sengchuavisualtranslationembed ding network for visual relation detection in proceedings of cvpr 201783 peng zhang yash goyal douglas summers stay dhruv batra and devi parikh yin andyang balancing and answering binary visual questions in proceedings of cvpr 201684 bo zheng yibiao zhao joey yu katsushi ikeuchi and song chun zhu scene understandingby reasoning stability and safety international journal of computer vision 1122221238201585 yuke zhu oliver groth michael bernstein and li fei fei visual7w grounded questionanswering in images in proceedings of cvpr 2016open access this chapter is licensed under the terms of the creative commons attribution 40international license httpcreativecommonsorglicensesby40 which permits use sharingadaptation distribution and reproduction in any medium or format as long as you give appropriatecredit to the original authors and the source provide a link to the creative commons license andindicate if changes were madethe images or other third party material in this chapter are included in the chapters creativecommons license unless indicated otherwise in a credit line to the material if material is notincluded in the chapters creative commons license and your intended use is not permitted bystatutory regulation or exceeds the permitted use you will need to obtain permission directly fromthe copyright holderchapter 10resourcesabstract deep learning has been shown as a powerful method for a variety ofarticial intelligence tasks including some critical tasks in nlp however training adeep neural network is usually a very time intensive process and requires lots of codeto build related models to alleviate these issues some deep learning frameworkshave been developed and released which incorporate some existing and necessaryarithmetic operators for neural network constructions and these frameworks exploithardware features such as multi core cpus and many core gpus to shorten thetraining time each framework has its advantages and disadvantages in this chapterwe aim to exhibit features and running performance of these frameworks so thatusers can select an appropriate framework for their usage101open source frameworks for deep learningin this section we will introduce several typical open source frameworks for deeplearning including caffe theano tensorflow torch pytorch keras and mxnetin fact as the rapid development of the deep learning community these open sourceframeworks are updating every day and therefore the information in this sectionmay not be up to date in fact this section mainly focuses on introducing the specialfeatures of these frameworks and lets the readers have a preliminary understandingof them to know the latest features of these deep learning frameworks please referto their ofcial sites1011caffecaffe1 is a well known framework and is widely used for computer vision tasks itwas created by yangqing jia and developed by berkeley ai research bair caffeuses a layer wise approach to make building models become easy and it is also1httpcaffeberkeleyvisionorg the authors 2020z liu et al representation learning for natural language processinghttpsdoiorg101007978 981 15 5573 2 1031932010resourcesconvenient to ne tune the existing neural networks without writing too much codevia its simple interfaces the underlying designs of caffe are for the fast constructionof convolutional neural networks which make it efcient and effectiveon the other hand as normal pictures often have a xed size the interfaces ofcaffe are xed and hard to be extended it is thus difcult to use caffe for othertasks with a variable input length such as text sound or other time series datarecurrent neural networks are also not well supported by caffe although users caneasily build an existing network architecture with the layer wise framework it is notexible when dealing with big and complex networks if users want to design a newlayer the users need to use cc and cuda for the underlying coding of the newlayer1012theanotheano2 is the typical framework developed to use symbolic tensor graphs for modelspecication any neural networks or other machine learning models can be repre sented as symbolic tensor graphs forward backward and gradient updates can becalculated based on the ow between tensors hence theano provides more exibil ity than caffe using a layer wise approach to build models in caffe to dene a newlayer that is not already in the existing repository of layers is complicated whichneeds to implement its forward backward and gradient update functions before intheano you only need to use basic operators to dene the customized layer followingthe order of operationstheano is a platform and is easy to congure as compared with other frameworksand some high level frameworks are built on top of theano such as keras whichfurther makes theano easier to use theano supports cross platform congurationwell which means it works on not only linux but also windows because of thismany researchers and engineers use theano to build their models and then releasethese projects rich open resources based on theano attract some more usersthough theano uses python syntax to dene symbolic tensor graphs its graphprocessor will compile the graphs into high performance c or cuda code forcomputing owing to this theano can run very fast and make programmers codemode simply only one deciency is that the compilation process is slow and needssome time if a neural network does not need to be trained for several days it isnot a good idea to select theano compiling too often in theano is maddening andannoying as a comparison the later framework like tensorflow uses the compiledpackage for the symbolic tensor operations which seems a little more relaxingtheano has some other serious disadvantages theano cannot support many coregpus very well which makes it hard to train big neural networks besides thecompilation process importing theano is also slow when you run your code intheano you will be stuck for a long time with a precongured device if you want toimprove and contribute to theano itself this will also be maddening and annoying2httpwwwdeeplearningnetsoftwaretheano101 open source frameworks for deep learning321in fact theano is no longer maintained but it is still worth introducing as a landmarkwork in the history of deep learning frameworks which inspires many subsequentframeworks1013tensorflowtensorflow3 is mainly developed and used by google based on the experience ontheano and distbelief 1 tensorflow and theano are in fact quite similar to someextent both of them allow building a symbolic graph of the neural network archi tecture via the python interface different from theano tensorflow allows imple menting new operations or machine learning algorithms using cc and java withbuilding symbolic graphs the auto gradient can be easily used to train complicatedmodels hence tensorflow is more than a deep learning framework its exibil ity enables it to solve various complex computing problems such as reinforcementlearningin tensorflow both code development and deployment is fast and convenienttrained models can be deployed quickly on a variety of devices including serversand mobile devices without the need to implement a separate model setting codeor load pythonluajit interpreter caffe also allows easy deployment of modelshowever caffe has trouble running on devices without a gpu which is a prevalentsituation of smartphones tensorflow supports model decoding using armneoninstructions and does not need too many operations to choose training devicestensorboard of tensorflow provides a platform for visualization of the modelarchitectures which is beautiful and also useful by visualizing the symbolic graph itis not difcult to nd bugs in the source code to debug models on other deep learningframeworks is relatively bothering tensorboard can also log and generate real timevisualization of variables during training which is a pleasant way to monitor thetraining processthough customizing operations in tensorflow is convenient it usually changes alot of function interfaces in every new release which is challenging for developers tokeep their code compatible with different tensorflow versions and mastering ten sorflow is also not easy as tensorflow 20 has been released recently tensorflowmay gradually handle these issues in the predictable future1014torchtorch4 is a computational framework mainly developed and used by facebook andtwitter torch provides an api written in lua to support the implementation of some3httpswwwtensoroworg4httptorchch32210resourcesmachine learning algorithms especially convolutional neural networks a temporalconvolutional layer implemented by torch can have a variable input length whichis extremely useful for nlp tasks and not designed in theano and tensorflowtorch also contains the 3d convolutional layer which can be easily used in videorecognition tasks besides its various exible convolutional layers torch is light andspeedy the above reasons attract lots of researchers in universities and companiesto customize their own deep learning platformshowever the negative aspects of torch are also apparent though torch is pow erful it is not designed to be widely accessible to the python based academic com munity and there are not any other interfaces but lua lua is a multi paradigmscripting language which was developed in brazil in the early 1990s and is not apopular mainstream programming language hence it needs some time to learn luabefore you use torch to construct models different from convolutional neural net works there is no ofcial support for recurrent neural networks there are some openresources about recurrent neural networks implemented by torch but they are not yetintegrated to the main repository and it is difcult to distinguish the effectivenessof these implementationssimilar to caffe torch is not a framework based on symbolic tensor graphs italso uses the layer wise approach this means that your models in torch are a graphof layers and not a graph of mathematical functions the mechanism is convenientto build a network whose layers are stable and hierarchical if you want to design anew connection layer or change an existing neural model you need lots of code toimplement new layers with full forward backward and gradient update functionshowever those frameworks based on symbolic tensor graphs such as theano andtensorflow give more exibility to do this in fact these issues are handled aspytorch has been released which we will introduce then1015pytorchpytorch5 is a python package built over torch developed by facebook and othercompanies however it is not just an interface and pytorch has amounts of improve ments over torch the most important one is that pytorch can use a symbolic graphto dene neural networks and then use automatic differentiation following the graphto automate the computation of backward passes in neural networks meanwhilepytorch maintains some characteristics of the layer wise approach in torch whichmeans coding with pytorch is easy moreover pytorch has minimal framework over head and custom memory allocators for the gpu which means pytorch is faster andmemory efcient than torch5httppytorchorg101 open source frameworks for deep learning323compared with other deep learning frameworks pytorch has two main advan tages first most frameworks like tensorflow are based on static computationalgraphs dene and run while pytorch uses dynamic computational graphs dene by run what it means is that with dynamic computational graphs you can changethe network architecture based on the data owing through the network there is away to do something similar in tensorflow but your static computational graphsmust contain all possible branches in advance which will limit the performancesecond pytorch is built to be deeply integrated into python and has a seamlessconnection with other popular python packages such as numpy spicy and cythonthus it is easy to extend your model when neededafter facebook released it pytorch has drawn considerable attention from thedeep learning community and many past torch users switch to this new package fornow pytorch already has a thriving community which contributes to its increasingpopularity among researchers it is no exaggeration to say that pytorch is one of themost popular frameworks at present1016keraskeras6 is a top design deep learning framework that is based on theano and tensor flow interestingly keras sits atop theano and tensorflow however its interfacesare similar to torch to use keras needs python code and there are lots of detaileddocuments and examples for a quick start there is also a very active communityof developers and they make keras fastly updated hence it is a very fast growingframeworkbecause theano and tensorflow are the backends of keras disadvantages ofkeras are most similar to theano and tensorflow with tensorflow as the backendit will run even slower than the pure tensorflow code because it is a high levelframework to customize a new neural layer is not easy though you can easily useexisting layers under keras the package is too advanced and it hides too manytraining parameters you cannot touch and change all details of your own modelsunless you use theano tensorflow or pytorch1017mxnetmxnet7 is an effective and efcient open source machine learning frameworkmainly pushed by amazon it supports apis with multiple languages includingc python r scala julia perl matlab and javascript some of which can beadopted for amazon web services some interfaces of mxnet are also reserved for6httpskerasio7httpmxnetio32410resourcesfuture mobile devices just like tensorflow mxnet is built on a dynamic dependencyscheduler that automatically parallelizes both symbolic and imperative operations onthe y a graph optimization layer on top of that makes symbolic execution fast andmemory efcient the mxnet library is portable and lightweight and it scales tomultiple gpus and multiple machines the main problem of mxnet is the lack ofdetailed and well organized documentation the user groups are also smaller thanother frameworks especially as compared with tensorflow and pytorch it is morechallenging to grasp mxnet for newbies the mxnet is developing fastly and theseproblems may be solved in the future102open resources for word representation1021word2vecword2vec8 is a widely used toolkit for word representation learning which providesan effective and efcient implementation of the continuous bag of words and skip gram architectures the word representations learned by word2vec can be used inmany natural language processing elds empirically to use pretrained word vectorsas the model inputs can be a good way to enhance model performancesword2vec takes free text corpus as input and constructs the vocabulary list fromthe training data then it uses simple predictive models based on neural networksto learn the language model which encode the co occurrence information betweenwords into the resulting word representationstheresultingrepresentationsshowcaseinterestinglinearsubstructuresofthewordvector space the euclidean distance or cosine similarity between two word vectorsprovides an effective method for measuring the linguistic or semantic similarity ofthe corresponding words sometimes the nearest neighbors according to this metricreveal rare but relevant words that lie outside an average humans vocabularywords frequently appearing together in the text will have representations withclose distance within the embedding space word2vec also provides a tool to nd theclosest words for a user specied word via the learned representations and distancesbetween representation embeddings1022gloveglove9 is a widely used toolkit which supports an unsupervised learning method forword representation learning similar to word2vec glove also trains on text corpus8httpscodegooglecomarchivepword2vec9httpsnlpstanfordeduprojectsglove102 open resources for word representation325and captures the aggregated global word word co occurrence information for wordembeddings however glove uses count based models instead of predictive modelswhich are different from word2vecthe glove model rst builds a global word word co occurrence matrix whichcan show how frequently words co occur with one another in a given text thenword representations are trained on the nonzero entries of the matrix to constructthis matrix requires the entire corpus traversal for the statistics collection for largecorpora this pass can be computationally expensive but it is a one time up frontcost subsequent training iterations are much faster because the number of nonzeromatrix entries is typically much smaller than the total number of words in the corpus103open resources for knowledge graph representation1031openkeopenke10 2 is an open source toolkit for knowledge embedding ke which pro vides a unied framework and various fundamental ke models openke prioritizesoperational efciency to support quick model validation and large scale knowledgerepresentation learning meanwhile openke maintains sufcient modularity andextensibility to incorporate new models easily besides the toolkit the embeddingsof some existing large scale knowledge graphs pretrained by openke are also avail able the toolkit documentation and pretrained embeddings are all released onhttpopenkethunlporgas compared to other implementations openke has ve advantages firstopenke has implemented nine classical knowledge embedding algorithms includ ing rescal transe transh transr transd complex distmult hole andanalogy which are veried effective and stable second openke shows high per formance due to memory optimization multi threading acceleration and gpu learn ing openke supports multiple computing devices and provides interfaces to controlcpugpu modes third system encapsulation makes openke easy to train and testke models users just need to set hyperparameters via interfaces of the platform toconstruct ke models fourth it is easy to construct new ke models all specicmodels are implemented by inheriting the base class by designing their own scoringfunctions and loss functions fifth besides the toolkit openke also provides theembeddings of some existing large scale knowledge graphs pretrained by openkewhich can be directly applied for many applications including information retrievalpersonalized recommendation and question answering10httpsgithubcomthunlpopenke32610resources1032scikit kgescikit kge11 is an open source python library for knowledge representation learn ing the library supports different building blocks to train and develop models forknowledge graph embeddings the primary purpose of scikit kge is to compute theembeddings of knowledge graphs for the method hole meanwhile it also providessome other methods besides hole rescal transe transr and er mlp canalso be trained in scikit kge the library contains some parameter update methodsnot only the basic sgd but also adagrad it also implements different negativesampling strategies to select negative samples104open resources for network representation1041openneopenne12 is an open source standard nenrl network representation learningtraining and testing framework it unies the input and output interfaces of differentne models and provides scalable options for each model moreover typical nemodels under this framework are based on tensorflow which enables these modelsto be trained with gpus the implemented or modied models include deepwalkline node2vec grarep tadw gcn hope gf sdne and le the frameworkalso provides classication and embedding visualization modules for evaluating theresult of nrl1042gemgem graph embedding methods13 is a python package that offers a general frame work for graph embedding methods it implements many state of the art embeddingtechniques including locally linear embedding laplacian eigenmaps graph fac torization high order proximity preserved embedding hope structural deepnetwork embedding sdne and node2vec furthermore the framework imple ments several functions to evaluate the quality of the obtained embeddings includinggraph reconstruction link prediction visualization and node classication for fasterexecution c backend is integrated using boost for supported methods11httpsgithubcommnickscikit kge12httpsgithubcomthunlpopenne13httpsgithubcompalash1992gem104 open resources for network representation3271043graphvitegraphvite14 is a general and high performance graph embedding system for variousapplications including node embedding knowledge graph embedding and graphhigh dimensional data visualizationgraphvite provides a complete pipeline for users to implement and evaluate graphembedding models for reproducibility the system integrates several commonlyused models and benchmarks and you can also develop your own models with theexible interface additionally for semantic tasks graphvite releases a bunch ofpretrained knowledge graph embedding models to enhance language understandingthere are two core advantages of graphvite over other toolkits fast and large scaletraining graphvite accelerates graph embedding with multiple cpus and gpusit takes around one minute to learn node embeddings for graphs with one millionnodes moreover graphvite is designed to be scalable even with limited memorygraphvite can process node embedding task on billion scale graphs1044cogdlcogdl15 is another graph representation learning toolkit that allows researchersand developers to easily train and evaluate baseline or custom models for nodeclassication link prediction and other tasks on graphs it provides implementationsof many popular models including non gnn models and gnn based onescogdl benets from several unique techniques first utilizing sparse matrixoperation cogdl is capable of performing fast network embedding on large scalenetworks second cogdl has the ability to deal with different types of graph struc tures attributed multiplex and heterogeneous networks third cogdl supportsparallel training with different seeds and different models cogdl performs train ing on multiple gpus and reports the result table automatically finally cogdl isextendable new datasets models and tasks can be added without difculty105open resources for relation extraction1051opennreopennre16 3 is an open source framework for neural relation extraction whichaims to easily build relation extraction re models14httpsgraphviteio15httpkegcstsinghuaeducncogdlindexhtml16httpsgithubcomthunlpopennre32810resourcescompared with other implementations opennre has four advantages firstopennre has implemented various state of the art re models including atten tion mechanism adversarial learning and reinforcement learning second open nre enjoys great system encapsulation it divides the pipeline of relation extractioninto four parts namely embedding encoder selector for distant supervision andclassier for each part it has implemented several methods system encapsulationmakes it easy to train and test models by changing hyperparameters or appoint modelarchitectures by using python arguments third opennre is extendable users canconstruct new re models by choosing specic blocks provided in four parts as men tioned above and combining them freely with only a few lines of codes fourth theframework has implemented multi gpu learning which is efcientreferences1 jeffrey dean greg corrado rajat monga kai chen matthieu devin mark mao marcaurelioranzato andrew senior paul tucker ke yang et al large scale distributed deep networks inadvances in neural information processing systems pages 12231231 20122 xu han shulin cao xin lv yankai lin zhiyuan liu maosong sun and juanzi li openkean open toolkit for knowledge embedding in proceedings of emnlp system demonstrationspages 139144 20183 xu han tianyu gao yuan yao deming ye zhiyuan liu and maosong sun opennre anopen and extensible toolkit for neural relation extraction in proceedings of emnlp systemdemonstrations pages 169174 2019open access this chapter is licensed under the terms of the creative commons attribution 40international license httpcreativecommonsorglicensesby40 which permits use sharingadaptation distribution and reproduction in any medium or format as long as you give appropriatecredit to the original authors and the source provide a link to the creative commons license andindicate if changes were madethe images or other third party material in this chapter are included in the chapters creativecommons license unless indicated otherwise in a credit line to the material if material is notincluded in the chapters creative commons license and your intended use is not permitted bystatutory regulation or exceeds the permitted use you will need to obtain permission directly fromthe copyright holderchapter 11outlookabstract the aforementioned representation learning models and methods haveshowntheireffectivenessinvariousnlpscenariosandtaskswiththerapidgrowthofdatascalesandthedevelopmentofcomputationdevicestherearealsonewchallengesand opportunities for next stage researches of deep learning techniques in the lastchapter we will look into the future directions of representation learning techniquesfor nlp to be more specic we will consider the following directions includingusing more unsupervised data utilizing few labeled data employing deeper neuralarchitectures improving model interpretability and fusing the advantages of otherareas111introductionwe have used ten chapters to introduce the advances of representation learningfor nlp covering both multi grained language entries including words phrasessentences and documents and closely related objects including world knowledgesememe knowledge networks and cross modal data those mentioned models andmethods of representation learning for nlp have shown their effectiveness in variousnlp scenarios and tasksas shown by the unsatisfactory performance of most nlp systems in opendomains and recent great advances of pre trained language models representationlearning for nlp is far from perfect with the rapid growth of data scales and thedevelopment of computation devices we are facing new challenges and opportunitiesfor next stage researches of representation learning and deep learning techniquesin this last chapter we will look into the future research and exploration directionsof representation learning techniques for nlp since we have summarized the futurework of each individual part in the summary section of each previous chapter herewe focus on discussing the general and important issues that should be addressed byrepresentation learning for nlp the authors 2020z liu et al representation learning for natural language processinghttpsdoiorg101007978 981 15 5573 2 1132933011outlookfor general representation learning for nlp we conclude the following direc tions including using more unsupervised data utilizing a few labeled data employ ing deeper neural architectures improving model interpretability and fusing theadvances from other areas112using more unsupervised datathe rapid development of internet technology and the popularization of informationdigitization have brought massive text data for nlp researches and applicationsfor example the whole corpus of wikipedia already contains more than 50 millionarticles including 6 million articles in english1 and is growing rapidly every daycontributed by collaborative work all over the world the amount of user generatedcontent onmanysocial platforms suchas twitter weibo andfacebookalsoincreasesquicklybybillionsofusersitisworthconsideringthesemassivetextdataforlearningbetter nlp models however due to the expensive cost of expert annotations it isimpossible to label such massive amounts of data for specic nlp taskshence an essential direction of nlp is how to take better advantages of unla beled data for efcient unsupervised representation learning though without labeledannotations unsupervised data can help initialize the randomized neural networkparameters and thus improve the performances of those downstream nlp tasksthis line of work usually employs a pipeline strategy rst pretrain the modelparameters and then ne tune these parameters in specic downstream nlp tasksrecurrent language model 7 word embeddings 6 and pre trained language mod els plm such as bert 3 all utilize unsupervised plain text to pretrain neuralparameters and then benet downstream supervised tasks via ne tuningcurrent state of the art plm models still can only learn from limited plain textdue to limited learning efciency and computation power moreover there are varioustypes of large scale data online with abundant informative signals and labels such ashtmltags anchor text keywords document meta information andother structuredand semi structured data how to take full advantage of the large scale web text datahas not been extensively studied in the future with better computation devices eggpus and data resources we are expected to develop more advanced methods toutilize more unsupervised data113utilizing fewer labeled dataas nlp technologies become more powerful people can explore more complicatedand ne grained problems taking text classication as an example early work tar geted on at classication with limited categories and now researchers are more1httpenwikipediaorgwiki113 utilizing fewer labeled data331interested in classication with hierarchical structure and a large number of classeshowever when a problem gets more complicated it requires more knowledge fromexperts to annotate training instances for ne grained tasks and increases the cost ofdata labelingtherefore we expect the models or systems can be developed efciently withvery few labeled data when each class has only one or a few labeled instances theproblem becomes a onefew shot learning problem the few shot learning problem isderivedfromcomputervisionandhasalsobeenstudiedinnlprecentlyforexampleresearchers have explored few shot relation extraction 5 where each relation has afew labeled instances and low resource machine translation 11 where the size ofthe parallel corpus is limiteda promising approach to few shot learning is to compare the semantic similaritybetween the test instance and those labeled ones ie the support set and then makethe prediction the idea is similar to k nearest neighbor classication knn 10since the key is to represent the semantic meanings of each instance for measuringtheir semantic similarity it has been veried that language models pretrained onunsupervised data and ne tuned on the target few shot domain are very effectivefor few shot learninganother approach to few shot learning is to transfer the models from some relateddomains into the target domain with the few shot problem 2 this is usually namedas transfer learning or domain adaptation for these methods representation learningcan also help the transfer or adaptation process by learning joint representations ofboth domainsin the future one may go beyond the abovementioned frameworks and designmore appropriate methods according to the characteristics of nlp tasks and prob lems the goal is to develop effective nlp methods with as less annotated data in thetarget domain as possible by better utilizing unsupervised data that are much cheaperto get from the web and existing supervised data from other domains the explo ration of the few shot learning problem in nlp will help us develop data efcientmethods for language learning114employing deeper neural architecturesas the amount of available text data rapidly increases the size of the training corpusfor nlp tasks grows as well with more training data a natural way to boost modelperformances is to employ deeper neural architectures for modeling intuitivelydeeper neural models that have more sophisticated architecture and parameters canbetter t the increasing data another motivation for using deeper architectures formodeling comes from the development of computation devices eg gpus cur rent state of the art methods are usually a compromise between efciency and effec tiveness as the computation devices operate faster the timespace complexities ofcomplicated models become acceptable which motivate researchers to design more33211outlookcomplex but effective models to summarize employing deeper neural architectureswould be one of the denite orientations for representation learning in nlpvery deep neural network architectures have been widely used in computer visionfor example the well known vgg 8 network which was proposed in the famousimagenet contest has 16 layers of convolutional and fully connected layers in nlpthe depths of neural architectures were relatively shallow until the transformer 9structure was proposed specically as compared with word embedding 6 which isbased on shallow models the state of the art pre trained language model bert 3can be regarded as a giant model that stacks 12 self attention layers and each layer has8 attention heads bert has demonstrated its effectiveness in a number of nlp tasksbesides the well designed model architecture and training objectives the success ofbert also benets from tpus which is one of the most powerful devices for parallelcomputations in contrast it may take months or years for a single cpu to nish thetraining process of bert when these computation devices go popular we can expectmore deep neural architectures to be developed for nlp as well115improving model interpretabilitymodel transparency and interpretability are hot topics in articial intelligence andmachine learning human interpretable predictions are very important for decision critical applications related to ethics privacy and safety however neural networkmodels or deep learning techniques are short of model transparency for human inter pretable predictions and thus are often treated as black boxesmost nlp techniques based on neural networks and distributed representation arealso hard to be interpreted except for the attention mechanism where the attentionweights can be interpreted as the importance of corresponding inputs for the sake ofemploying representation learning techniques for decision critical applications thereis a need to improve model interpretability and transparency of current representationlearning and neural network modelsa recent survey 1 classies interpretable machine learning methods into twomaincategoriesinterpretablemodelsandpost hocexplainabilitytechniquesmodelsthat are understandable by themselves are called interpretable models for examplelinear models decision trees and rule based systems are such transparent modelshowever in most cases we have to probe into the model by a second one for expla nations namely post hoc explainability techniques in nlp there have been someresearches to visualize neural models such as neural machine translation 4 forinterpretable explanations however the understanding of most neural based mod els remains unsolved we are looking forward to more studies on improving modelinterpretability to facilitate the extensive use of representation learning methods fornlp116 fusing the advances from other areas333116fusing the advances from other areasduring the development of deep learning techniques mutual learning between dif ferent research areas has never stoppedfor example word2vec aims to learn word embeddings from large scale text cor pus published in 2013 and can be regarded as a milestone of representation learningfor nlp in 2014 the idea of word2vec was adopted for learning node embeddings ina networkgraph by treating random walks over the network as sentences named asdeepwalk the analogical reasoning phenomenon learned by word2vec ie king man  queen woman also inspired the representation learning of world knowledgenamed as transe meanwhile graph convolutional networks were rst proposed forsemi supervised graph learning in 2016 and have been widely applied on many nlptasks such as relation extraction and text classication recently another example isthe transformer model which was proposed for neural machine translation at rstand then transferred to computer vision data mining and many other areasthe fusion also appears between two quite distant disciplines we should recallagain that the idea of distributed representation proposed in the 1980s is inspired bytheneuralcomputationschemeofhumansandotheranimalsittakesabout40yearstosee the development of distributed representation and deep learning come to fruitionin fact many ideas such as convolution in cnn and the attention mechanism areinspired by the computation scheme of human cognitiontherefore an intriguing direction of representation learning for nlp is to fusethe advances from other areas including not only those closely related areas in aisuch as machine learning computer vision and data mining but also those distantareas to some extent such as linguistics brain science psychology and sociologythis line of work requires researchers to have sufcient knowledge of other eldsreferences1 alejandro barredo arrieta natalia daz rodrguez javier del ser adrien bennetot sihamtabik alberto barbado salvador garca sergio gil lpez daniel molina richard ben jamins et al explainable articial intelligence xai concepts taxonomies opportunities andchallenges toward responsible ai information fusion 5882115 20202 wei yu chen yen cheng liu zsolt kira yu chiang frank wang and jia bin huang acloser look at few shot classication in proceedings of iclr 20193 jacob devlin ming wei chang kenton lee and kristina toutanova bert pre training ofdeep bidirectional transformers for language understanding in proceedings of naacl 20194 yanzhuo ding yang liu huanbo luan and maosong sun visualizing and understandingneural machine translation in proceedings of acl 20175 xu han hao zhu pengfei yu ziyun wang yuan yao zhiyuan liu and maosong sunfewrel a large scale supervised few shot relation classication dataset with state of the artevaluation in proceedings of emnlp 20186 t mikolov and j dean distributed representations of words and phrases and their composi tionality proceedings of neurips 20137 tomas mikolov martin karat lukas burget jan cernocky and sanjeev khudanpur recur rent neural network based language model in proceedings of interspeech 201033411outlook8 karen simonyan and andrew zisserman very deep convolutional networks for large scaleimage recognition arxiv preprint arxiv14091556 20149 ashish vaswani noam shazeer niki parmar llion jones jakob uszkoreit aidan n gomezand lukasz kaiser attention is all you need in proceedings of neurips 201710 yan wang wei lun chao kilian q weinberger and laurens van der maaten sim pleshot revisiting nearest neighbor classication for few shot learning arxiv preprintarxiv191104623 201911 barret zoph deniz yuret jonathan may and kevin knight transfer learning for low resourceneural machine translation in proceedings of emnlp 2016open access this chapter is licensed under the terms of the creative commons attribution 40international license httpcreativecommonsorglicensesby40 which permits use sharingadaptation distribution and reproduction in any medium or format as long as you give appropriatecredit to the original authors and the source provide a link to the creative commons license andindicate if changes were madethe images or other third party material in this chapter are included in the chapters creativecommons license unless indicated otherwise in a credit line to the material if material is notincluded in the chapters creative commons license and your intended use is not permitted bystatutory regulation or exceeds the permitted use you will need to obtain permission directly fromthe copyright holdercorrection to representation learningfor natural language processingcorrection toz liu et al representation learning for natural languageprocessing httpsdoiorg101007978 981 15 5573 2in the original version of the book the following belated correction has been incor porated in both preface and chapter 8 in the preface a new reference citation hasbeen added in chapter 8the original version of chapter 8 was revised with a new paragraph replacing therst paragraph in section 83the chapter and the book have been updated with the changesthe updated version of this chapter can be found athttpsdoiorg101007978 981 15 5573 2httpsdoiorg101007978 981 15 5573 2 8 the authors 2023z liu et al representation learning for natural language processinghttpsdoiorg101007978 981 15 5573 2 12c1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATHNAME = '/home/ngoni97/Documents/Python Programming/Natural Language Processing/Representation Learning for Natural Language Processing.pdf'\n",
    "Test = PdfDataCollector(PATHNAME)\n",
    "text, clean_text = Test.returnText()\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae9084-df62-4a85-8a90-08eb289aed62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
