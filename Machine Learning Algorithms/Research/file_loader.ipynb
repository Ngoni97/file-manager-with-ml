{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da27284c-ac06-4735-a491-4fbf9a65fb3b",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b543693-d0a2-44fe-9746-7d5b20bb8739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process in progress\n",
      "\n",
      "reading pdfs and writing text files in progress\n",
      "\n",
      "ocr reading process started!\n",
      "\n",
      "ocr reading process started!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Updated on Tue Jul  8 00:37:18 2025\n",
    "\n",
    "@author: ngoni97\n",
    "\"\"\"\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb 18 00:50:46 2025\n",
    "\n",
    "@author: ngoni97\n",
    "\"\"\"\n",
    "# どうもありがとうございます == Dōmo arigatōgozaimasu\n",
    "# testing multithreading and multiprocessing\n",
    "\n",
    "# reading the PDFs after colleciing them\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections.abc import Iterable\n",
    "from threading import Thread\n",
    "import multiprocessing as mp\n",
    "import concurrent.futures\n",
    "import time\n",
    "import os, pathlib, pymupdf, fitz\n",
    "import re\n",
    "\n",
    "from dataset_collector_saver_class import LoadDataset \n",
    "from updated_pdf_reader_data_collector_class import PdfDataCollector # change back to  original if it doesn't work\n",
    "from stripping_file_types import fileExtensionStripper\n",
    "from folder_iterator_class import FolderIterator\n",
    "from file_extension_tester import listDirFiles\n",
    "\n",
    "\n",
    "class DocumentContentDataset():\n",
    "    def __init__(self, folder,*,\n",
    "                 specific_file=None,\n",
    "                 pages=10, clean_text=True, save_as_text_file=False):\n",
    "        \"\"\"specific_files refers to pdf, docx, etc. \"\"\"\n",
    "        self._folder_path = folder # the parent folder containing the children folders\n",
    "        self._specific_file = specific_file\n",
    "        self._pages = pages\n",
    "        self._clean_text = clean_text\n",
    "        self._save_as_text_file = save_as_text_file\n",
    "        \n",
    "        self.Folder = FolderIterator(self._folder_path, folder_name=False)\n",
    "        self.subFolders = self.Folder.returnCategories()\n",
    "        # a list of folder -> files, that is the individual lists are\n",
    "        # folders which contain files\n",
    "        self.files = [file for file, *_ in\n",
    "                      [listDirFiles(folder, size=False, fullpath=True) for folder in self.subFolders]]\n",
    "        \"\"\"print('files = {}\\n'.format(list(self.Flatten(self.files))))\n",
    "        for idx, file in enumerate(self.Flatten(self.files),1):\n",
    "            print(\"{} : {}\\n\".format(idx, file))\"\"\"\n",
    "        self.returnFiles()\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        print(\"reading pdfs and writing text files in progress\\n\")\n",
    "        #self.returnData()\n",
    "        self.processData()\n",
    "        end_time = time.perf_counter()\n",
    "        print(\"reading and writing finished in {}s\\n\".format(round(end_time - start_time, 3)))\n",
    "\n",
    "    def start_process(self, file):\n",
    "        \"\"\"\n",
    "        Initialises the data collector saver,\n",
    "        which collects pdf files data and then saves it as a text file\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file : pdf file path\n",
    "            can be entered as a directory or file name\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        text file\n",
    "            that has been filtered and is saved locally in the default directory\n",
    "\n",
    "        \"\"\"\n",
    "        self.process = PdfDataCollector(file, self._pages, self._save_as_text_file)\n",
    "        return self.process.returnText()\n",
    "    \n",
    "    def processData(self):\n",
    "        \"\"\" Returns a dictionary of files (keys) and their respective text (values)\"\"\"\n",
    "\n",
    "        # using another method\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            results = [executor.submit(self.start_process, file) for file in list(self.Flatten(self.files))]\n",
    "            \n",
    "            concurrent.futures.wait(results)\n",
    "\n",
    "    def returnFiles(self):\n",
    "        \"\"\" Returns a dictionary where the keys are the folder paths\n",
    "            and the values corresponding to each key are the files (full path) contained\n",
    "            in the respective folders\n",
    "        \"\"\"\n",
    "        self.Data = {}\n",
    "        for folders in self.files:\n",
    "            folder_path_name = os.path.commonprefix(folders)\n",
    "            self.Data[folder_path_name] = folders\n",
    "        \n",
    "        return self.Data\n",
    "\n",
    "    def Flatten(self, items, ignore_types=(str, bytes)):\n",
    "        \"\"\" Flattening a Nested Sequence\n",
    "            converts a nested sequence into a single list whilst preserving the order \n",
    "            \"\"\"\n",
    "        for x in items:\n",
    "            if isinstance(x, Iterable) and not isinstance(x, ignore_types):\n",
    "                yield from self.Flatten(x)\n",
    "            else:\n",
    "                yield x\n",
    "                \n",
    "    def remove_characters_before_tokenization(self, sentence,keep_apostrophes=False):\n",
    "        # string\n",
    "        sentence = (sentence.replace('_', ' ')).replace('-', ' ')\n",
    "        # bytes\n",
    "        #b_sentence = (sentence.replace(b'_', b' ')).replace(b'-', b' ')\n",
    "        \n",
    "        # string\n",
    "        PATTERN = re.compile('[?|$|&|*|%|@|(|)|~]') # add other characters here to remove them\n",
    "        pattern = re.compile('[^a-zA-Z]') # remove numbers\n",
    "        replacement = r' '\n",
    "        # bytes\n",
    "        b_PATTERN = re.compile(b'[?|$|&|*|%|@|(|)|~]') # add other characters here to remove them\n",
    "        b_pattern = re.compile(b'[^a-zA-Z]') # remove numbers\n",
    "        b_replacement = re.compile(b' ')\n",
    "        \n",
    "        if keep_apostrophes:\n",
    "            filtered_sentence = re.sub(pattern, replacement, re.sub(PATTERN, replacement, sentence))\n",
    "        else:\n",
    "            filtered_sentence = re.sub(pattern, replacement, re.sub(PATTERN, replacement, sentence))\n",
    "        return filtered_sentence.lower()\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    path_1 = '/home/ngoni97/Documents/Python Programming'\n",
    "    path_2 = '/home/ngoni97/Documents/PHYSICS'\n",
    "    Path = '/home/ngoni97/Documents/MATHEMATICS'\n",
    "    # start\n",
    "    print('process in progress\\n')\n",
    "    start = time.perf_counter()\n",
    "    test = DocumentContentDataset(path_2, pages=13, save_as_text_file=True)\n",
    "    #test.processData()\n",
    "   \n",
    "    #print(test.returnFiles())\n",
    "    # end\n",
    "    end = time.perf_counter()\n",
    "    print('process finished\\n')\n",
    "    \n",
    "    print('time taken for execution = {}s'.format(round(end-start, 3)))\n",
    "\n",
    "    \n",
    "# link folder_name the file is found\n",
    "# with the file_name\n",
    "# and the file_content\n",
    "\n",
    "# folder_name -> file_name -> file_content\n",
    "\n",
    "# at the end the folder_name == classification -> sum total file_content of each file\n",
    "\n",
    "\n",
    "# error: Get_Scanned_Document is not working properly, it still skips out scanned pdf's\n",
    "# so, for now I am gonna work with the digitally-born pdf's with the machine learning algorithms\n",
    "# the problem is that the algorithm works perfectly if fed files one at a time and not as threads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf3e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
