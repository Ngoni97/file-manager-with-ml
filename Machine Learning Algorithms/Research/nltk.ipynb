{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e3b957-afe1-4cce-929f-1dcb5f07eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f475460-4c43-469d-99c0-69e76060ff87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there Mr. Harold, how are you doing today?', 'The weather is great and warmer after it rained in the morning.', 'Right now am coding using python idle environment']\n",
      "['Hello', 'there', 'Mr.', 'Harold', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'warmer', 'after', 'it', 'rained', 'in', 'the', 'morning', '.', 'Right', 'now', 'am', 'coding', 'using', 'python', 'idle', 'environment']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for i in word_tokenize(my_text):\\n    print(i)'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# natural language toolkit\n",
    "\n",
    "#import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# tokenizing - word tokenizers..... sentence tokenizers\n",
    "# lexicon and corporas\n",
    "# corpora - body of text. ex: medical journals, English language\n",
    "# lexicon - words and their meaning\n",
    "\n",
    "my_text = 'Hello there Mr. Harold, how are you doing today? \\\n",
    "The weather is great and warmer after it rained in the morning. \\\n",
    "Right now am coding using python idle environment'\n",
    "print(sent_tokenize(my_text))\n",
    "print(word_tokenize(my_text))\n",
    "\n",
    "\n",
    "'''for i in word_tokenize(my_text):\n",
    "    print(i)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85ecdf7b-3b91-452a-aa93-53072d3bb013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', 'Mr.', 'Harold', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'warmer', 'after', 'it', 'rained', 'in', 'the', 'morning', '.', 'Right', 'now', 'am', 'coding', 'using', 'python', 'idle', 'environment']\n"
     ]
    }
   ],
   "source": [
    "sent_word_tokenizer = word_tokenize(' '.join([sent for sent in sent_tokenize(my_text)]))\n",
    "print(sent_word_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b557463-2108-49c5-9f81-2f84bada71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw('2005-GWBush.txt')\n",
    "sample_text = state_union.raw('2006-GWBush.txt')\n",
    "#print(train_text)\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "#print(tokenized) # this turns whole paragraphs into a list of the words\n",
    "\n",
    "#def process_content():\n",
    "#    try:\n",
    "#        for i in tokenized:\n",
    "#            words = nltk.word_tokenize(i)\n",
    "#            tagged = nltk.pos_tag(words)\n",
    "#            print(tagged)\n",
    "#    except Exception as e:\n",
    "#        print(str(e))\n",
    "#process_content()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78080074-c631-4663-819a-6a3fc22af1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_set = ', '.join([item.strip('.pdf') for item in os.listdir('/home/ngoni97/Documents/Documents/PHYSICS/ADVANCED')])\n",
    "test_set = ', '.join([item for item in os.listdir('/home/ngoni97/Downloads/Downloads/Unsorted')])\n",
    "# strip the file types so that the tokenizing algorithm works efficiently\n",
    "\n",
    "Custom_sent_tokenizer = PunktSentenceTokenizer(train_set)\n",
    "\n",
    "tokenized = Custom_sent_tokenizer.tokenize(test_set)\n",
    "\n",
    "#def Run_Process():\n",
    "#    try:\n",
    "#        for i in tokenized:\n",
    "#            words = nltk.word_tokenize(i)\n",
    "#            tagged = nltk.pos_tag(words)\n",
    "#            print(tagged)\n",
    "#    except Exception as e:\n",
    "#        print(str(e))\n",
    "#\n",
    "#Run_Process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19121458-d866-4c1f-848a-60e241a010b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
