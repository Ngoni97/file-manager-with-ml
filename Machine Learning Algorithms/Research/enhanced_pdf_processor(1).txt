#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Enhanced PDF OCR Processor with Image Processing and Multithreading
Created on Thu Jul 10 00:49:47 2025

@author: ngoni97
"""

import os
import re
import pymupdf
import pathlib
from threading import Thread, Lock
import concurrent.futures
import time
from collections.abc import Iterable
import cv2
from pdf2image import convert_from_path
from PIL import Image, ImageEnhance, ImageFilter, ImageOps
import numpy as np
from typing import List, Dict, Tuple, Optional
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PdfDataCollector:
    def __init__(self, file_path, pages=None, save_as_text_file=False, output_dir=None):
        self.file_path = file_path
        self.pages = pages or 10
        self.save_as_text_file = save_as_text_file
        self.output_dir = output_dir or '/home/ngoni97/Documents/Documents/File Manager with ML/train_test_text_files'
        
        self.file_name = os.path.basename(self.file_path)
        self.book_dict = {}
        self.lock = Lock()  # Thread safety for book_dict
        
        # Validate file exists and is accessible
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")
        if not os.access(file_path, os.R_OK):
            raise PermissionError(f"Cannot read file: {file_path}")
    
    def remove_characters_before_tokenization(self, sentence, keep_apostrophes=False):
        """Clean text by removing unwanted characters"""
        if not sentence:
            return ""
        
        sentence = sentence.replace('_', ' ').replace('-', ' ')
        
        PATTERN = re.compile('[?|$|&|*|%|@|(|)|~]')
        pattern = re.compile('[^a-zA-Z]') if not keep_apostrophes else re.compile('[^a-zA-Z\']')
        replacement = r' '
        
        filtered_sentence = re.sub(pattern, replacement, re.sub(PATTERN, replacement, sentence))
        return ' '.join(filtered_sentence.lower().split())  # Remove extra spaces
    
    def advanced_image_preprocessing(self, image):
        """Advanced image preprocessing for better OCR results"""
        try:
            # Convert to grayscale if needed
            if image.mode != 'L':
                image = image.convert('L')
            
            # Convert to numpy array
            img_array = np.array(image)
            
            # 1. Noise reduction using median filter
            img_array = cv2.medianBlur(img_array, 3)
            
            # 2. Gaussian blur for smoothing
            img_array = cv2.GaussianBlur(img_array, (1, 1), 0)
            
            # 3. Adaptive thresholding for better text separation
            img_array = cv2.adaptiveThreshold(
                img_array, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                cv2.THRESH_BINARY, 11, 2
            )
            
            # 4. Morphological operations to clean up
            kernel = np.ones((1, 1), np.uint8)
            img_array = cv2.morphologyEx(img_array, cv2.MORPH_CLOSE, kernel)
            img_array = cv2.morphologyEx(img_array, cv2.MORPH_OPEN, kernel)
            
            # Convert back to PIL Image
            processed_image = Image.fromarray(img_array)
            
            # 5. PIL-specific enhancements
            # Enhance contrast
            enhancer = ImageEnhance.Contrast(processed_image)
            processed_image = enhancer.enhance(1.2)
            
            # Enhance sharpness
            enhancer = ImageEnhance.Sharpness(processed_image)
            processed_image = enhancer.enhance(1.5)
            
            return processed_image
            
        except Exception as e:
            logger.error(f"Error in image preprocessing: {e}")
            return image
    
    def convert_pdf_to_images(self, save_images=True, dpi=300):
        """Convert PDF pages to images with enhanced preprocessing"""
        try:
            # Setup directory structure
            base_name = os.path.splitext(os.path.basename(self.file_path))[0]
            images_dir = os.path.join(self.output_dir, 'images', base_name)
            
            if save_images:
                os.makedirs(images_dir, exist_ok=True)
                
                # Create subdirectories
                original_dir = os.path.join(images_dir, 'original')
                enhanced_dir = os.path.join(images_dir, 'enhanced')
                os.makedirs(original_dir, exist_ok=True)
                os.makedirs(enhanced_dir, exist_ok=True)
            
            # Convert PDF to images
            logger.info(f"Converting PDF to images: {self.file_path}")
            images = convert_from_path(
                self.file_path, 
                last_page=self.pages, 
                dpi=dpi,
                fmt='PNG'
            )
            
            enhanced_images = []
            image_paths = []
            
            for i, img in enumerate(images, 1):
                # Save original image
                if save_images:
                    original_path = os.path.join(original_dir, f"page_{i:04d}.png")
                    img.save(original_path, 'PNG')
                
                # Enhance image
                enhanced_img = self.advanced_image_preprocessing(img)
                enhanced_images.append(enhanced_img)
                
                # Save enhanced image
                if save_images:
                    enhanced_path = os.path.join(enhanced_dir, f"page_{i:04d}.png")
                    enhanced_img.save(enhanced_path, 'PNG')
                    image_paths.append(enhanced_path)
            
            logger.info(f"Converted {len(enhanced_images)} pages to images")
            return enhanced_images, image_paths
            
        except Exception as e:
            logger.error(f"Error converting PDF to images: {e}")
            return [], []
    
    def read_ocr_from_pdf_page(self, page_num):
        """Extract text from PDF page using PyMuPDF OCR directly"""
        try:
            doc = None
            text = ""
            
            # Open document
            doc = pymupdf.open(self.file_path)
            if doc is None or page_num >= len(doc):
                return ""
            
            page = doc[page_num]
            if page is None:
                return ""
            
            # Use PyMuPDF's OCR functionality directly on the page
            try:
                text_page = page.get_textpage_ocr(dpi=300, full=True)
                if text_page:
                    text = text_page.extractTEXT()
                    # Clean up textpage
                    del text_page
                else:
                    text = ""
            except Exception as e:
                logger.warning(f"OCR extraction failed for page {page_num}: {e}")
                text = ""
            
            # Clean up document
            if doc:
                doc.close()
            
            # Thread-safe dictionary update
            with self.lock:
                self.book_dict[page_num] = text
            
            return text
            
        except Exception as e:
            logger.error(f"Error in OCR for page {page_num}: {e}")
            with self.lock:
                self.book_dict[page_num] = ""
            return ""
    
    def is_ocr_needed(self, max_test_pages=3):
        """Determine if PDF needs OCR processing"""
        try:
            with pymupdf.open(self.file_path) as doc:
                test_pages = min(max_test_pages, len(doc))
                
                for page_num in range(test_pages):
                    page = doc[page_num]
                    text = page.get_text().strip()
                    
                    # If we find substantial text, it's likely a digital PDF
                    if text and len(text) > 50:
                        return False
                
                return True  # Likely needs OCR
                
        except Exception as e:
            logger.error(f"Error checking OCR requirement: {e}")
            return True  # Default to OCR if uncertain
    
    def process_digital_pdf(self):
        """Process digital PDF (text-based)"""
        try:
            text = ""
            with pymupdf.open(self.file_path) as doc:
                pages_to_process = min(self.pages, len(doc))
                
                for page_num in range(pages_to_process):
                    page = doc[page_num]
                    page_text = page.get_text()
                    text += page_text + "\n"
            
            return text
            
        except Exception as e:
            logger.error(f"Error processing digital PDF: {e}")
            return ""
    
    def process_ocr_pdf_multithreaded(self, enhanced_images=None):
        """Process OCR PDF using multithreading for individual pages"""
        try:
            # Get total pages to process
            with pymupdf.open(self.file_path) as doc:
                total_pages = min(self.pages, len(doc))
            
            # Use ThreadPoolExecutor for better control
            max_workers = min(4, total_pages)  # Limit concurrent threads
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # Submit OCR tasks for each page directly from PDF
                future_to_page = {
                    executor.submit(self.read_ocr_from_pdf_page, page_num): page_num
                    for page_num in range(total_pages)
                }
                
                # Wait for all tasks to complete
                for future in concurrent.futures.as_completed(future_to_page):
                    page_num = future_to_page[future]
                    try:
                        future.result(timeout=60)  # 60 second timeout per page
                    except concurrent.futures.TimeoutError:
                        logger.warning(f"OCR timeout for page {page_num}")
                        with self.lock:
                            self.book_dict[page_num] = ""
                    except Exception as e:
                        logger.error(f"OCR error for page {page_num}: {e}")
                        with self.lock:
                            self.book_dict[page_num] = ""
            
            # Combine text from all pages in order
            combined_text = ""
            for page_num in sorted(self.book_dict.keys()):
                page_text = self.book_dict[page_num]
                if page_text:
                    combined_text += page_text + "\n\n"
            
            return combined_text
            
        except Exception as e:
            logger.error(f"Error in multithreaded OCR processing: {e}")
            return ""
    
    def get_document(self):
        """Main method to extract text from PDF"""
        try:
            # Check if OCR is needed
            if self.is_ocr_needed():
                logger.info(f"Processing OCR PDF: {self.file_name}")
                
                # For OCR PDFs, we can optionally still convert to images for enhancement
                # or work directly with the PDF pages
                enhanced_images, image_paths = self.convert_pdf_to_images()
                
                # Process with multithreaded OCR using PyMuPDF directly
                text = self.process_ocr_pdf_multithreaded()
                
            else:
                logger.info(f"Processing digital PDF: {self.file_name}")
                text = self.process_digital_pdf()
            
            # Save as text file if requested
            if self.save_as_text_file and text:
                self.save_text_file(text)
            
            return text
            
        except Exception as e:
            logger.error(f"Error processing document: {e}")
            return ""
    
    def save_text_file(self, text):
        """Save extracted text to file"""
        try:
            base_name = os.path.splitext(os.path.basename(self.file_path))[0]
            text_dir = os.path.join(self.output_dir, 'text_files')
            os.makedirs(text_dir, exist_ok=True)
            
            text_file_path = os.path.join(text_dir, f"{base_name}.txt")
            
            with open(text_file_path, 'w', encoding='utf-8') as f:
                f.write(text)
            
            logger.info(f"Saved text file: {text_file_path}")
            
        except Exception as e:
            logger.error(f"Error saving text file: {e}")
    
    def returnText(self):
        """Return both raw and cleaned text"""
        raw_text = self.get_document()
        clean_text = self.remove_characters_before_tokenization(raw_text)
        return raw_text, clean_text
    
    def get_bookmarks(self):
        """Extract bookmarks from PDF"""
        bookmarks = {}
        try:
            with pymupdf.open(self.file_path) as doc:
                toc = doc.get_toc()
                for level, title, page in toc:
                    bookmarks[page] = title
        except Exception as e:
            logger.error(f"Error extracting bookmarks: {e}")
        
        return bookmarks


class DocumentContentDataset:
    def __init__(self, folder, *, specific_file=None, pages=10,
                 clean_text=True, save_as_text_file=False, max_workers=None):
        """
        Process PDFs from a folder structure using concurrent processing
        
        Args:
            folder: Path to folder containing PDF files
            specific_file: Process specific file only
            pages: Number of pages to process per PDF
            clean_text: Whether to clean extracted text
            save_as_text_file: Whether to save as text files
            max_workers: Maximum number of concurrent workers for processing multiple books
        """
        self._folder_path = folder
        self._specific_file = specific_file
        self._pages = pages
        self._clean_text = clean_text
        self._save_as_text_file = save_as_text_file
        # Conservative concurrency for processing multiple books
        self._max_workers = min(max_workers or 2, 3)  # Maximum 3 books concurrently
        
        self.results = {}
        self.errors = []
        
        # Get all PDF files
        self.pdf_files = self._get_pdf_files()
        
        # Process files
        start_time = time.perf_counter()
        self.process_data_concurrent()
        end_time = time.perf_counter()
        
        logger.info(f"Total processing time: {end_time - start_time:.2f} seconds")
    
    def _get_pdf_files(self):
        """Get all PDF files from the folder structure"""
        pdf_files = []
        
        if self._specific_file:
            if os.path.isfile(self._specific_file) and self._specific_file.lower().endswith('.pdf'):
                pdf_files.append(self._specific_file)
        elif os.path.isfile(self._folder_path):
            if self._folder_path.lower().endswith('.pdf'):
                pdf_files.append(self._folder_path)
        else:
            for root, dirs, files in os.walk(self._folder_path):
                for file in files:
                    if file.lower().endswith('.pdf'):
                        pdf_files.append(os.path.join(root, file))
        
        return pdf_files
    
    def process_single_file(self, file_path):
        """Process a single PDF file - thread-safe"""
        try:
            # File validation
            if not os.path.exists(file_path):
                return self._create_error_result(file_path, 'File not found')
            
            # Check file size to prevent memory issues
            file_size = os.path.getsize(file_path)
            if file_size > 100 * 1024 * 1024:  # 100MB limit
                return self._create_error_result(file_path, 'File too large')
            
            # Process the PDF
            processor = PdfDataCollector(
                file_path, 
                pages=self._pages, 
                save_as_text_file=self._save_as_text_file
            )
            
            raw_text, clean_text = processor.returnText()
            
            # Clean up processor
            del processor
            
            result = {
                'file_path': file_path,
                'raw_text': raw_text,
                'clean_text': clean_text if self._clean_text else None,
                'success': True,
                'error': None,
                'file_size': file_size
            }
            
            logger.info(f"Successfully processed: {os.path.basename(file_path)}")
            return result
            
        except Exception as e:
            logger.error(f"Error processing {file_path}: {e}")
            return self._create_error_result(file_path, str(e))
    
    def _create_error_result(self, file_path, error_msg):
        """Create a standardized error result"""
        return {
            'file_path': file_path,
            'raw_text': None,
            'clean_text': None,
            'success': False,
            'error': error_msg,
            'file_size': None
        }
    
    def process_data_concurrent(self):
        """Process all PDFs using concurrent processing for multiple books"""
        if not self.pdf_files:
            logger.warning("No PDF files found to process")
            return
        
        # Process files sequentially if only 1 worker or few files
        if self._max_workers == 1 or len(self.pdf_files) <= 2:
            for file_path in self.pdf_files:
                result = self.process_single_file(file_path)
                if result['success']:
                    self.results[file_path] = result
                else:
                    self.errors.append(result)
                # Small delay between files to prevent resource conflicts
                time.sleep(0.5)
        else:
            # Use concurrent processing for multiple books
            with concurrent.futures.ThreadPoolExecutor(max_workers=self._max_workers) as executor:
                # Submit all tasks
                future_to_file = {
                    executor.submit(self.process_single_file, file_path): file_path 
                    for file_path in self.pdf_files
                }
                
                # Process completed tasks
                for future in concurrent.futures.as_completed(future_to_file):
                    file_path = future_to_file[future]
                    try:
                        result = future.result(timeout=300)  # 5 minute timeout per book
                        
                        if result['success']:
                            self.results[file_path] = result
                        else:
                            self.errors.append(result)
                            
                    except concurrent.futures.TimeoutError:
                        logger.error(f"Timeout processing: {file_path}")
                        self.errors.append(self._create_error_result(file_path, 'Timeout'))
                        
                    except Exception as e:
                        logger.error(f"Unexpected error processing {file_path}: {e}")
                        self.errors.append(self._create_error_result(file_path, str(e)))
    
    def get_results(self):
        """Get processing results"""
        return self.results
    
    def get_errors(self):
        """Get processing errors"""
        return self.errors
    
    def get_summary(self):
        """Get processing summary"""
        total_files = len(self.pdf_files)
        successful = len(self.results)
        failed = len(self.errors)
        
        return {
            'total_files': total_files,
            'successful': successful,
            'failed': failed,
            'success_rate': (successful / total_files * 100) if total_files > 0 else 0
        }


if __name__ == "__main__":
    # Test with a single file first
    test_file = '/home/ngoni97/Documents/PHYSICS/ADVANCED/physics-for-scientists-and-engineers-with-modern-physics-serwayjewett.pdf'
    
    try:
        if os.path.exists(test_file):
            print("Testing single file processing...")
            start = time.perf_counter()
            
            processor = PdfDataCollector(
                test_file, 
                pages=5,  # Process 5 pages
                save_as_text_file=True
            )
            
            raw_text, clean_text = processor.returnText()
            end = time.perf_counter()
            
            print(f"Single file processing time: {end - start:.2f} seconds")
            print(f"Extracted text length: {len(raw_text)} characters")
            
            # Clean up
            del processor
        
        # Test with folder processing
        test_folder = '/home/ngoni97/Documents/PHYSICS'
        
        if os.path.exists(test_folder):
            print("\nTesting folder processing...")
            start = time.perf_counter()
            
            dataset = DocumentContentDataset(
                test_folder,
                pages=3,  # Process 3 pages per PDF
                save_as_text_file=True,
                max_workers=2  # Process 2 books concurrently
            )
            
            end = time.perf_counter()
            
            summary = dataset.get_summary()
            print(f"Folder processing time: {end - start:.2f} seconds")
            print(f"Processing summary: {summary}")
            
            # Clean up
            del dataset
        
    except Exception as e:
        logger.error(f"Error in main execution: {e}")
        
    print("\nProcessing complete!")
