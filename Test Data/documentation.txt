# -*- coding: utf-8 -*-
"""
Created on Tue Oct  1 22:19:35 2024

@author: Ngoni
"""

i. MAIN IDEA: mimic how the brain creates associations of new memories; and how you can recall specific information and
           sometimes get lost through all the activated associated memories 
ii. the application should be able to look up stuff from a word description when grouping files
iii. what I really want is a split second lookup for a file just from a few descriptive words
iv. also be able to group stuff from the general description
v. must be able to link with the File Explorer (Windows for now):
    1. edit files in folders by grouping them in terms of their shared properties
    2. open folders to edit and link with other folders
    3. move certain files from one folder to the next
    4. first sort by the general types e.g., video, audio, pdf, etc.
    5. then do a further selection based on what certain files match the general description
    
# try to link it up with the File Explorer and manipulate the files from there
# should be able to:
    1. create new folders
    2. edit already existing ones
    3. move files from one folder to the next after approving
    4. display the results
    
    
    
### create all the files as modules in the end and then using the import method rather than adding the all the lines
of code in a single file
# for training and testing, the app should at least create non existing folders in the current directory
# and moving the files if they match the description and if both files have a somewhat ambiguous matching description
# then copy to box folders and the human intervention should be there to finalize which folder best suits the
# description

# develop a search engine to compile the pdf documents I have and give a summary for each
# this is helpful for literature review and writing my scientific papers when i'm offline and 
# want to get the general idea and citation of the papers 
# I download online and will probably have no time to read them later on due 
# to the fact of procrastinating and having downloaded a lot of them of which sorting through them is a waste of time

# it's no biggie but to me it will help me sort things out without
# getting sucked in the vortex of a myriad of papers to read and start
# writing my own papers or dissertation and thesis in the first place



# add the capability of choosing the machine learning algorithms depending on the task at hand
    whether it be images
    or documents
    or videos (this one will be a premium feature since it requires large processing power and hence online)
	

 ==============================================================================
 	
 {How the application is supposed to work}
     
 <\File>
 	\New Process
 		- initialise the process after selecting the folder and the sorting description and the algorithms
 	\Open Folder
 		- open the folder manually to sort
 	\Recent Process
 		- display the recent sorted folder statistics
 	\Save
 		- here the human decides based on the accuracy of the algorithms and saves the work
 		- thereby finalising the moving of items to their desired locations
 		- this means there must be statistical and graphical and visual display of the inferred and proposed
 	
 <\Algorithms>
 	[in general this is a classification application, so only put forth the best classification algorithms]
 	- here you choose based on whether you wanna train the algorithms (this is for programmers) or do the actual
 	- sorting but still choose whether they're images only or videos or documents or it's a mixture
 		= supervised
 		= unsupervised
 		= reinforcement (this is for premium versions)
 	
 <\View>
 	\Alternating Row Colours
 		- change the colour of rows for clearer, distinguished view
 	\Statistics
 		- display the statistics of each folder in order to make a decision
 	
 <\Settings>
 	\Preferences
 		- general application settings
 
 <\Help>
 	\KeyBoard Shortcuts
 		- shortcuts
 	\About
 		- the license, version and other stuff
 
 
 ==============================================================================

# add a tab widget for the algorithm

 ==============================================================================
 # on the data collection
 # must be able to choose the destination folder's and/or subfolder's names to be used as target labels
 # in the machine learning models
 # start training the models using unsupervised learning so that the app can 
 # extract the features and labels that can then be used in the supervised learning 
 ==============================================================================

 ==============================================================================
 Machine Learning
 </Unsupervised Learning>
     $just give it folders and it should try to recognize folder names 
      and the contents of the folder and try to find a pattern for the features,
      that is, the file names and sizes, later on for PDFs try to open the file to 
      further recognize the pattern of the content of the file and match it to the respective folder
 
 </Supervised Learning>
     $give it respective folders with names that it should move the files after completing the match
      and the folder with the unsorted files
      
 ==============================================================================
       
 ==============================================================================
 !!!!!!!
 NEED TO FIND A WAY TO ENCODE THE DATA: FOLDER NAMES AND FILE NAMES
 
 So, first i need to understand unsupervised learning
 !found there was actually a way to encode categorical data
 
 # Document Clustering
     - text classification in order to create vectors of words or nouns commonly found in selected folders
     - to help in encoding the data for machine learning algorithms stage
 
 # method 1    
 use the TfdfVectorizer from sklearn.feature_extraction.text
 must add a lot of descriptory information in order to train my model based on
 the different words and topics for each folder which will be my category
 e.g., for mathematics must add the expected topics and 
 gather that information from articles and contents pages that describe mathematics topics
 without the formulas
 
 model-tuning for the NaiveBayes algorithm
  
  # method 2   
 using nltk tokenizers
     need to find a way of comparing a single sentence against a model trained on multiple
     lines
     
 ==============================================================================
    
 ==============================================================================
 need to use the pickle module to load my ml-algorithms into a pickle file to save
 myself from retraining different models for the voting system to select which is
 better
 ==============================================================================

 ==============================================================================
 first the document classifier, classifies between main folders
 then later classifies into subfolders of each main folder
 ntil every last file is delegated into its respective folder
 ==============================================================================

 ==============================================================================
 # turns out that classifying files using their names is kinda tricky 
 # because there isn't enough data to play around with
 # so i'm gonna implement the other plan of reading the pdfs and collecting the contents
 # and use those as a better bet against insufficient training data
 ==============================================================================

 ==============================================================================
 # i was able to find the code for reading pdf files to text files
 # now comes the fact about the number of pages to read for sufficient reconnaissance
 # about the doucument
 # need to consider the size and total number of pages
 # take into consideration either the contents pages or the index pages
 ==============================================================================

 ==============================================================================
 # there are two types of documents:
     1. typed and,
     2. scanned
     
     - typed ones are simple and straight-forward in converting them directly to text files
     - as for scanned ones, there is need to provoke another method to first read 
      the document using OCR tools. and then converting the result to text files
 ==============================================================================
 
 for supervised training, i need to find a list of topics in mathematics, physics, etc.,
 and create a list of stop_words that are everything outside the topics list
 
 ==============================================================================
  need to create a class that collects only the contents of the 'Contents Pages'
  
  class:
      = must be able to read the file size
          if it is less than some low_threshold MBs
              then it must not be a big book but rather a research paper or saved webpage pdf file
      = must be able to read if the file is a scanned OCR or typed pdf
      
      = must then proceed to read just a few pages of the file if it is a paper
          or read only the contents of the 'Contents Pages'
          
     it turns out that the module pymupdf has the option of get_toc()
         get table of contents,
             ill try to see how effective it is for gathering the information i need for
             my algorithms
     and also PyPDF2 class object has get_contents(), access the page contents
     
 pymupdf get_toc() is working as designed
     if the book doesn't have bookmarks then it returns an empty dictionary
         else it returns a dictionary where the keys() are page numbers and the values()
         are the topics
 ==============================================================================
        
 ==============================================================================
 link dataset_collector_saver_class with pdf_reader_data_collector_class
      feed the collected documents list from dataset collector into the pdf reader      
if pdf file is digitally-born        
 then
     if table of contents exist
         then read up to the first chapter
             and/or the indexes pages: subject/topic index not name/authors index
 else
     read a minimum_threshold of pages (a percentage of the total number of pages) at least 20 pages
else
    if pdf file is scanned
    then
        first convert images to text
        then
            do the similar operations as for the digitally-born pdf
            
     
 ==============================================================================

# need to learn multithreading and parallel computing to speed up my algorithms

# create a function that collects folder names 
    if there are multiple subfolders before getting the files
    so that i can get the categories of the folders